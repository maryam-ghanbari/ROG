{"id": "WebQTest-0", "question": "what does jamaican people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["people.ethnicity.languages_spoken"], ["location.country.languages"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does jamaican people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken <PATH>"], "scores": [-0.008169879205524921, -0.023380707949399948, -0.05129488557577133, -0.07644195109605789, -0.1557205617427826, -0.1921449899673462, -0.19584493339061737, -0.19895637035369873, -0.19960951805114746, -0.20178720355033875], "norm_scores": [0.11265811324119568, 0.11095745116472244, 0.10790299624204636, 0.1052233874797821, 0.09720353037118912, 0.09372664988040924, 0.09338051080703735, 0.09309041500091553, 0.09302962571382523, 0.09282726794481277]}}
{"id": "WebQTest-1", "question": "what did james k polk do before he was president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "government.governmental_jurisdiction.government_positions"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["common.topic.subjects", "people.person.nationality", "government.governmental_jurisdiction.government_positions"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "base.onephylogeny.type_of_thing.things_of_this_type"]], "ground_paths": [["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did james k polk do before he was president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.nationality <SEP> government.government_office_or_title.jurisdiction </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> common.topic.subjects <SEP> people.person.nationality <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>"], "scores": [-0.01987524889409542, -0.022307390347123146, -0.027912622317671776, -0.02822958305478096, -0.03187326714396477, -0.03502440080046654, -0.035217203199863434, -0.0361034981906414, -0.03914252296090126, -0.04175600782036781], "norm_scores": [0.10119166970252991, 0.1009458526968956, 0.10038161277770996, 0.1003497987985611, 0.09998482465744019, 0.09967025369405746, 0.099651038646698, 0.09956275671720505, 0.09926063567399979, 0.09900156408548355]}}
{"id": "WebQTest-3", "question": "who plays ken barlow in coronation street", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ken barlow in coronation street [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.017500298097729683, -0.026603903621435165, -0.031674016267061234, -0.033370889723300934, -0.03470223397016525, -0.0503060519695282, -0.05192939564585686, -0.0520988330245018, -0.057781193405389786, -0.060411252081394196], "norm_scores": [0.1024332344532013, 0.10150495171546936, 0.1009916141629219, 0.100820392370224, 0.10068624466657639, 0.09912735223770142, 0.0989665761590004, 0.09894979745149612, 0.09838912636041641, 0.09813070297241211]}}
{"id": "WebQTest-6", "question": "where is jamarcus russell from", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.nationality"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.location"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["people.place_lived.person", "people.place_lived.location"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.player"], ["location.location.people_born_here", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.place"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jamarcus russell from [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.location </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.place </PATH>"], "scores": [-0.007794998120516539, -0.020432954654097557, -0.09204300493001938, -0.09443444758653641, -0.0959671288728714, -0.09864775836467743, -0.10261998325586319, -0.10856527835130692, -0.11040981113910675, -0.11598853021860123], "norm_scores": [0.10792132467031479, 0.10656600445508957, 0.09920162707567215, 0.09896469116210938, 0.09881311655044556, 0.09854859113693237, 0.09815791249275208, 0.09757605940103531, 0.09739624708890915, 0.09685441106557846]}}
{"id": "WebQTest-7", "question": "where was george washington carver from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["symbols.namesake.named_after"], ["education.education.student", "education.education.place_of_birth"], ["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.ethnicity.people_with_this_ethnicity"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was george washington carver from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> education.education.student <SEP> education.education.place_of_birth </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>"], "scores": [-0.005067965015769005, -0.03712397813796997, -0.039870649576187134, -0.11065185815095901, -0.11218208074569702, -0.1167646124958992, -0.1259797215461731, -0.1270751804113388, -0.12904046475887299, -0.13256995379924774], "norm_scores": [0.10914961248636246, 0.10570620000362396, 0.10541626065969467, 0.09821271151304245, 0.09806253761053085, 0.09761419892311096, 0.09671880304813385, 0.09661290794610977, 0.09642321616411209, 0.09608349949121475]}}
{"id": "WebQTest-8", "question": "what else did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"], ["influence.influence_node.influenced_by"], ["base.inventor.inventions"], ["law.us_patent.inventor"], ["common.topic.notable_types"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> base.inventor.inventions </PATH>", "<PATH> law.us_patent.inventor </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.006875547580420971, -0.02558816410601139, -0.06982287019491196, -0.07501500099897385, -0.08917664736509323, -0.11499829590320587, -0.12400409579277039, -0.1336594969034195, -0.13637253642082214, -0.14001081883907318], "norm_scores": [0.10872667282819748, 0.10671103000640869, 0.10209357738494873, 0.10156486928462982, 0.10013667494058609, 0.0975840762257576, 0.09670919924974442, 0.09577993303537369, 0.09552042931318283, 0.09517353773117065]}}
{"id": "WebQTest-9", "question": "who was richard nixon married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.person.profession", "common.topic.notable_types"], ["people.person.spouse_s", "people.person.spouse_s"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_ownership.owner"], ["people.person.children", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was richard nixon married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> base.americancomedy.impersonated_celebrity.impersonated_by <SEP> base.americancomedy.impersonated_celebrity.impersonated_by </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.022514022886753082, -0.03356068208813667, -0.03689292445778847, -0.045365020632743835, -0.04666167125105858, -0.046692024916410446, -0.04791625961661339, -0.048303570598363876, -0.04977821186184883, -0.05173518508672714], "norm_scores": [0.10205994546413422, 0.10093872994184494, 0.10060293227434158, 0.0997542068362236, 0.09962495416402817, 0.09962192922830582, 0.09950004518032074, 0.09946151077747345, 0.09931495040655136, 0.09912078082561493]}}
{"id": "WebQTest-12", "question": "who is governor of ohio 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["location.location.containedby", "people.person.place_of_birth"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["location.location.containedby", "people.deceased_person.place_of_death"]], "ground_paths": [["government.political_district.representatives", "government.politician.government_positions_held"], ["base.locations.states_and_provences.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is governor of ohio 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.appointed_by </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.016790717840194702, -0.021623725071549416, -0.03273899480700493, -0.033302124589681625, -0.03431641682982445, -0.03589850664138794, -0.036643002182245255, -0.04450937360525131, -0.04659367725253105, -0.05395590886473656], "norm_scores": [0.10189690440893173, 0.10140562802553177, 0.10028471052646637, 0.10022825002670288, 0.10012664645910263, 0.09996835887432098, 0.0998939648270607, 0.09911124408245087, 0.09890487790107727, 0.098179392516613]}}
{"id": "WebQTest-13", "question": "who was vice president after kennedy died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["fictional_universe.character_occupation.characters_with_this_occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["event.speech_or_presentation.speaker_s", "event.speech_or_presentation.speaker_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president after kennedy died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> event.speech_or_presentation.speaker_s <SEP> event.speech_or_presentation.speaker_s </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.012042798101902008, -0.013509583659470081, -0.0631779357790947, -0.06637801975011826, -0.072041817009449, -0.07511274516582489, -0.0839906632900238, -0.08491145074367523, -0.08715580403804779, -0.0872647762298584], "norm_scores": [0.10535275936126709, 0.10519833862781525, 0.10010094940662384, 0.09978112578392029, 0.09921758621931076, 0.09891336411237717, 0.09803910553455353, 0.09794887900352478, 0.09772928804159164, 0.09771864116191864]}}
{"id": "WebQTest-14", "question": "where is the fukushima daiichi nuclear plant located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.street_address", "location.mailing_address.citytown"], ["location.location.contains"], ["location.location.street_address", "location.mailing_address.state_province_region"], ["location.location.containedby", "location.location.contains"], ["location.location.street_address", "location.mailing_address.country"], ["location.location.contains", "location.location.containedby"], ["location.location.street_address"], ["location.location.containedby", "location.country.administrative_divisions"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear plant located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> location.location.containedby <SEP> location.location.contains </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.country </PATH>", "<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> location.location.street_address </PATH>", "<PATH> location.location.containedby <SEP> location.country.administrative_divisions </PATH>"], "scores": [-0.009367109276354313, -0.02642880752682686, -0.039489906281232834, -0.05097367987036705, -0.05955858156085014, -0.06756101548671722, -0.07592076063156128, -0.08134939521551132, -0.09164147078990936, -0.09260556846857071], "norm_scores": [0.10510294139385223, 0.10332491993904114, 0.10198415815830231, 0.10081969946622849, 0.09995786100625992, 0.09916115552186966, 0.09833565354347229, 0.0978032723069191, 0.09680183231830597, 0.09670854359865189]}}
{"id": "WebQTest-16", "question": "what countries are part of the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.country.first_level_divisions"], ["location.administrative_division.first_level_division_of"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.022173680365085602, -0.02461867220699787, -0.03108382783830166, -0.03397354483604431, -0.04273870959877968, -0.047471851110458374, -0.05821940302848816, -0.10205555707216263, -0.11655283719301224, -0.11699627339839935], "norm_scores": [0.10374612361192703, 0.10349278151988983, 0.1028258353471756, 0.1025291234254837, 0.10163437575101852, 0.10115446150302887, 0.10007312148809433, 0.09578105807304382, 0.0944025069475174, 0.09436065703630447]}}
{"id": "WebQTest-19", "question": "what is my timezone in louisiana", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.states_provinces_within", "location.location.time_zones"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["location.administrative_division.country", "location.location.time_zones"], ["base.biblioness.bibs_location.state", "location.location.time_zones"], ["location.location.time_zones", "location.location.time_zones"], ["location.administrative_division.first_level_division_of", "location.location.time_zones"], ["location.administrative_division.country"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in louisiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.states_provinces_within <SEP> location.location.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> location.administrative_division.country <SEP> location.location.time_zones </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.administrative_division.first_level_division_of <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-1.280491596844513e-05, -0.1586393564939499, -0.16598087549209595, -0.1672242134809494, -0.170613095164299, -0.17222510278224945, -0.17815135419368744, -0.17822390794754028, -0.18295548856258392, -0.18826380372047424], "norm_scores": [0.11673811078071594, 0.09961436688899994, 0.09888572990894318, 0.09876285493373871, 0.09842872619628906, 0.09827019274234772, 0.09768953919410706, 0.09768245369195938, 0.09722135215997696, 0.09670663625001907]}}
{"id": "WebQTest-20", "question": "what did st augustine do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["base.activism.activist.area_of_activism"], ["book.written_work.author"], ["book.author.works_written"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st augustine do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.0018916799454018474, -0.05469179153442383, -0.0671035572886467, -0.12577781081199646, -0.1271836906671524, -0.13365678489208221, -0.13426421582698822, -0.1359654814004898, -0.14222748577594757, -0.1584652066230774], "norm_scores": [0.11108173429965973, 0.10536875575780869, 0.10406901687383652, 0.09813853353261948, 0.0980006605386734, 0.09736835211515427, 0.09730921685695648, 0.09714381396770477, 0.09653739631175995, 0.09498250484466553]}}
{"id": "WebQTest-21", "question": "who is keyshia cole dad", "prediction": [["people.person.parents"], ["people.person.children"], ["fictional_universe.fictional_character.parents"], ["people.person.gender"], ["people.person.parent"], ["fictional_universe.fictional_character.children"], ["influence.influence_node.influenced"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is keyshia cole dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0063391258008778095, -0.02584242634475231, -0.15403738617897034, -0.1677245944738388, -0.17123335599899292, -0.1760212481021881, -0.1828572005033493, -0.18659751117229462, -0.18693892657756805, -0.18829868733882904], "norm_scores": [0.1145751029253006, 0.11236216127872467, 0.09884296357631683, 0.0974992886185646, 0.097157783806324, 0.09669371694326401, 0.09603498131036758, 0.09567644447088242, 0.0956437885761261, 0.09551382064819336]}}
{"id": "WebQTest-22", "question": "what kind government does egypt have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["people.person.nationality", "government.form_of_government.countries"], ["government.form_of_government.countries", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind government does egypt have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>"], "scores": [-0.005579920019954443, -0.023498672991991043, -0.12503653764724731, -0.12775596976280212, -0.13949643075466156, -0.14093926548957825, -0.1464398205280304, -0.1517265886068344, -0.15447185933589935, -0.15823042392730713], "norm_scores": [0.11166403442621231, 0.10968097299337387, 0.09909094125032425, 0.09882184118032455, 0.09766841679811478, 0.09752759337425232, 0.0969926118850708, 0.09648118168115616, 0.09621667861938477, 0.0958557203412056]}}
{"id": "WebQTest-23", "question": "what town was martin luther king assassinated in", "prediction": [["people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.country"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.events"], ["base.biblioness.bibs_location.country", "location.administrative_division.second_level_division_of"], ["base.biblioness.bibs_location.country", "location.location.containedby"], ["base.biblioness.bibs_location.country", "base.biblioness.bibs_location.country"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town was martin luther king assassinated in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> location.administrative_division.second_level_division_of </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.001649135141633451, -0.06920795142650604, -0.07014656811952591, -0.07187309861183167, -0.07551813870668411, -0.07628604024648666, -0.08300896733999252, -0.09013346582651138, -0.09132979065179825, -0.09489723294973373], "norm_scores": [0.10729731619358063, 0.10028786957263947, 0.10019379109144211, 0.10002094507217407, 0.09965703636407852, 0.0995805412530899, 0.09891331195831299, 0.09821110218763351, 0.098093681037426, 0.09774436056613922]}}
{"id": "WebQTest-24", "question": "where did edgar allan poe died", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.place_of_death.location"], ["people.deceased_person.place_of_burial"], ["location.location.people_born_here"], ["people.person.place_of_death"], ["people.person.nationality", "location.administrative_division.second_level_division_of"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did edgar allan poe died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.second_level_division_of </PATH>"], "scores": [-0.00021213827130850405, -0.09122028946876526, -0.11157359927892685, -0.112574003636837, -0.11966408789157867, -0.1199302226305008, -0.12585976719856262, -0.12787100672721863, -0.1324799507856369, -0.1410149782896042], "norm_scores": [0.1113242357969284, 0.10164017230272293, 0.09959237277507782, 0.09949279576539993, 0.09878986328840256, 0.09876357764005661, 0.09817969053983688, 0.09798242151737213, 0.09753187000751495, 0.09670297056436539]}}
{"id": "WebQTest-26", "question": "what to do today in atlanta with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["metropolitan_transit.transit_system.area_served"], ["olympics.olympic_participating_country.olympics_participated_in"], ["location.location.contains"], ["organization.organization.geographic_scope"], ["olympics.olympic_games.host_city"], ["film.film_festival.location"], ["film.film.featured_film_locations"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do today in atlanta with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> olympics.olympic_games.host_city </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> film.film.featured_film_locations </PATH>"], "scores": [-0.015394735150039196, -0.01637488417327404, -0.024271387606859207, -0.11648169159889221, -0.12320838868618011, -0.1266113966703415, -0.12863251566886902, -0.14103595912456512, -0.1436733454465866, -0.14500835537910461], "norm_scores": [0.10846587270498276, 0.10835961252450943, 0.10750731825828552, 0.09803735464811325, 0.09738010168075562, 0.097049281001091, 0.0968533307313919, 0.09565943479537964, 0.09540747851133347, 0.09528019279241562]}}
{"id": "WebQTest-28", "question": "what electorate does anna bligh represent", "prediction": [["people.person.nationality", "base.biblioness.bibs_location.country"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["people.person.nationality", "location.administrative_division.country"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["people.person.place_of_birth", "location.location.containedby"], ["government.election.winner", "government.election.district"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_parent"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["government.politician.government_positions_held", "government.political_district.representatives"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what electorate does anna bligh represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> government.election.winner <SEP> government.election.district </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.014746669679880142, -0.019655808806419373, -0.025813870131969452, -0.04231037572026253, -0.04267183691263199, -0.04916413873434067, -0.050230227410793304, -0.057115793228149414, -0.058714061975479126, -0.05992763862013817], "norm_scores": [0.10275378078222275, 0.1022505834698677, 0.10162284225225449, 0.09996017813682556, 0.09992405772209167, 0.09927742183208466, 0.09917164593935013, 0.0984911322593689, 0.09833383560180664, 0.09821458160877228]}}
{"id": "WebQTest-31", "question": "where did andy murray started playing tennis", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_facility.teams"], ["people.person.place_of_birth"], ["sports.sports_team_location.teams"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_facility.athlete", "sports.sports_facility.location"], ["sports.sports_facility.athlete", "sports.sports_facility.teams"], ["sports.sports_team_roster.player", "sports.sports_team.location"], ["sports.sports_facility.athlete", "sports.sports_facility.athlete"], ["location.location.people_born_here"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did andy murray started playing tennis [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_facility.athlete <SEP> sports.sports_facility.location </PATH>", "<PATH> sports.sports_facility.athlete <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.location </PATH>", "<PATH> sports.sports_facility.athlete <SEP> sports.sports_facility.athlete </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.022978514432907104, -0.034043215215206146, -0.0405861958861351, -0.04365737736225128, -0.05354337766766548, -0.05855473875999451, -0.06607640534639359, -0.06908226013183594, -0.07073783129453659, -0.07183482497930527], "norm_scores": [0.10304535925388336, 0.10191147774457932, 0.10124684870243073, 0.10093638300895691, 0.09994344413280487, 0.09944384545087814, 0.09869866073131561, 0.09840243309736252, 0.09823966026306152, 0.0981319472193718]}}
{"id": "WebQTest-32", "question": "what is the australian dollar called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_used", "common.topic.notable_types", "common.topic.notable_types"], ["location.country.currency_used", "location.location.containedby", "location.location.containedby"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["location.country.currency_used", "location.country.currency_used"], ["location.country.currency_used", "location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.country.currency_used", "location.location.time_zones"], ["location.country.currency_used", "location.country.official_language", "language.human_language.countries_spoken_in"], ["finance.currency.countries_used", "common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the australian dollar called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.country.currency_used <SEP> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <SEP> location.country.languages_spoken <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_used <SEP> location.location.time_zones </PATH>", "<PATH> location.country.currency_used <SEP> location.country.official_language <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_used <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.007541390135884285, -0.029200280085206032, -0.06065083295106888, -0.06629811972379684, -0.07490868121385574, -0.07528293877840042, -0.07701238244771957, -0.07821813225746155, -0.08042094856500626, -0.08366204053163528], "norm_scores": [0.10570593178272247, 0.10344107449054718, 0.10023841261863708, 0.09967394918203354, 0.09881937503814697, 0.09878240525722504, 0.09861171245574951, 0.09849287569522858, 0.09827616065740585, 0.09795815497636795]}}
{"id": "WebQTest-33", "question": "what timezone is sweden", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["locations.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> locations.location.time_zones </PATH>"], "scores": [-0.0005855548661202192, -0.06167342886328697, -0.15356869995594025, -0.15604089200496674, -0.19691677391529083, -0.2008199244737625, -0.20096415281295776, -0.2012089192867279, -0.20129497349262238, -0.20985890924930573], "norm_scores": [0.11680669337511063, 0.10988479107618332, 0.10023698955774307, 0.09998948872089386, 0.09598472714424133, 0.09561081975698471, 0.0955970287322998, 0.09557363390922546, 0.09556541591882706, 0.09475049376487732]}}
{"id": "WebQTest-34", "question": "who did cam newton sign with", "prediction": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.team"], ["american_football.football_player.passing", "american_football.player_passing_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cam newton sign with [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_passing_statistics.player <SEP> american_football.player_passing_statistics.team </PATH>", "<PATH> american_football.football_player.passing <SEP> american_football.player_passing_statistics.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.023364396765828133, -0.0248672217130661, -0.02525469660758972, -0.029043594375252724, -0.03211076557636261, -0.03868706896901131, -0.047557126730680466, -0.05371697619557381, -0.05378757789731026, -0.055164966732263565], "norm_scores": [0.10150256752967834, 0.10135014355182648, 0.10131087899208069, 0.10092774778604507, 0.10061865299940109, 0.0999591276049614, 0.09907640516757965, 0.09846799075603485, 0.09846103191375732, 0.09832550585269928]}}
{"id": "WebQTest-35", "question": "what county is frederick md in", "prediction": [["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.location.contains"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.county.county_seat"], ["location.administrative_division.second_level_division_of"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is frederick md in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.county.county_seat </PATH>", "<PATH> location.administrative_division.second_level_division_of </PATH>"], "scores": [-0.019364237785339355, -0.021134192124009132, -0.030071644112467766, -0.03281180188059807, -0.08939841389656067, -0.1305023580789566, -0.13376471400260925, -0.13718459010124207, -0.14139731228351593, -0.14577187597751617], "norm_scores": [0.10696867108345032, 0.10677950829267502, 0.10582943260669708, 0.10553983598947525, 0.09973351657390594, 0.09571719169616699, 0.09540542960166931, 0.09507972002029419, 0.09468001127243042, 0.09426673501729965]}}
{"id": "WebQTest-36", "question": "what highschool did harper lee go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["common.topic.notable_types"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.education", "education.educative_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did harper lee go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educative_institution.students_graduates </PATH>"], "scores": [-0.01286640390753746, -0.01953756995499134, -0.02688284032046795, -0.030052877962589264, -0.05499081686139107, -0.07257752865552902, -0.10126708447933197, -0.11185163259506226, -0.11269646883010864, -0.1297287791967392], "norm_scores": [0.10549604147672653, 0.10479459911584854, 0.10402768105268478, 0.10369843244552612, 0.10114438831806183, 0.09938114136457443, 0.0965704470872879, 0.0955536812543869, 0.09547299146652222, 0.09386064112186432]}}
{"id": "WebQTest-37", "question": "what timezone is utah in", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"], ["base.locations.time_zones.locations_in_this_time_zone"], ["base.locations.countries.states_provinces_within", "location.location.time_zones"], ["base.biblioness.bibs_location.country", "location.location.time_zones"], ["base.locations.countries.states_provinces_within"], ["base.biblioness.bibs_location.state", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is utah in [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> base.locations.time_zones.locations_in_this_time_zone </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> base.locations.countries.states_provinces_within <SEP> location.location.time_zones </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> location.location.time_zones </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> location.location.time_zones </PATH>"], "scores": [-1.332894044026034e-05, -0.14443440735340118, -0.18849095702171326, -0.19616836309432983, -0.19643722474575043, -0.19831325113773346, -0.20119455456733704, -0.2075529247522354, -0.21121425926685333, -0.21766041219234467], "norm_scores": [0.11902238428592682, 0.10301663726568222, 0.0985766053199768, 0.09782269597053528, 0.09779638797044754, 0.09761309623718262, 0.09733224660158157, 0.09671533107757568, 0.09636187553405762, 0.09574270993471146]}}
{"id": "WebQTest-38", "question": "who did george w. bush run against for the second term", "prediction": [["time.event.locations", "people.person.nationality"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.election.campaigns", "government.election_campaign.candidate"], ["government.election_campaign.election", "government.politician.election_campaigns"], ["government.election_campaign.election", "government.election_campaign.candidate"], ["government.election.campaigns", "government.politician.election_campaigns"], ["government.election_campaign.candidate", "government.election_campaign.candidate"], ["base.obamabase.cabinet_position.potential_cabinet_members", "base.obamabase.cabinet_position.potential_cabinet_members"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["film.film.subjects", "film.film.country", "people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did george w. bush run against for the second term [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations <SEP> people.person.nationality </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> government.election.campaigns <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election_campaign.election <SEP> government.politician.election_campaigns </PATH>", "<PATH> government.election_campaign.election <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election.campaigns <SEP> government.politician.election_campaigns </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.candidate </PATH>", "<PATH> base.obamabase.cabinet_position.potential_cabinet_members <SEP> base.obamabase.cabinet_position.potential_cabinet_members </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> film.film.subjects <SEP> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.012817103415727615, -0.022767918184399605, -0.023305943235754967, -0.0375312864780426, -0.04654974862933159, -0.05429907143115997, -0.0634525939822197, -0.06568383425474167, -0.07570890337228775, -0.08289773762226105], "norm_scores": [0.10360606014728546, 0.10258020460605621, 0.10252504050731659, 0.10107690840959549, 0.10016944259405136, 0.09939619898796082, 0.09849052131175995, 0.09827101230621338, 0.0972907617688179, 0.09659386426210403]}}
{"id": "WebQTest-39", "question": "who is niall ferguson 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "common.topic.notable_types"], ["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["people.person.children", "people.person.children"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is niall ferguson 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>"], "scores": [-0.027353812009096146, -0.029127158224582672, -0.03173426166176796, -0.03908441215753555, -0.04234134778380394, -0.044800229370594025, -0.04497362673282623, -0.04540834203362465, -0.0465119406580925, -0.04662499204277992], "norm_scores": [0.10124937444925308, 0.10106997936964035, 0.100806824862957, 0.1000686064362526, 0.09974321722984314, 0.09949825704097748, 0.099481001496315, 0.09943776577711105, 0.09932808578014374, 0.09931685775518417]}}
{"id": "WebQTest-41", "question": "where is the galapagos islands located on a world map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.island.island_group"], ["base.locations.continents.countries_within"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.locations.countries.continent"], ["book.written_work.subjects"], ["book.book_subject.works"], ["location.country.administrative_divisions"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the galapagos islands located on a world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.0002290683623868972, -0.08745621889829636, -0.10676514357328415, -0.1205834150314331, -0.12732364237308502, -0.13638196885585785, -0.1408897191286087, -0.14231328666210175, -0.14678214490413666, -0.14679476618766785], "norm_scores": [0.11211945116519928, 0.10275398939847946, 0.10078895837068558, 0.09940580278635025, 0.09873804450035095, 0.09784767776727676, 0.09740760177373886, 0.09726903587579727, 0.09683532267808914, 0.09683410078287125]}}
{"id": "WebQTest-42", "question": "what are the songs that justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"], ["music.composition.composer"], ["music.lyricist.lyrics_written"], ["music.album.artist"], ["music.artist.album"], ["music.artist.track"], ["music.composition.lyricist"], ["music.recording.producer"], ["music.recording.featured_artists"]], "ground_paths": [["music.artist.album"], ["music.recording.featured_artists"], ["music.album.artist"], ["music.composer.compositions"], ["music.lyricist.lyrics_written"], ["music.recording.artist"], ["music.composition.lyricist"], ["music.composition.composer"], ["music.featured_artist.albums"], ["music.featured_artist.recordings"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the songs that justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.recording.producer </PATH>", "<PATH> music.recording.featured_artists </PATH>"], "scores": [-0.018415754660964012, -0.021426916122436523, -0.03311138227581978, -0.05248091369867325, -0.06799330562353134, -0.07015281170606613, -0.07574549317359924, -0.07855932414531708, -0.08534850925207138, -0.08632388710975647], "norm_scores": [0.10410550236701965, 0.10379249602556229, 0.1025867909193039, 0.10061885416507721, 0.09907004982233047, 0.09885633736848831, 0.09830502420663834, 0.09802879393100739, 0.09736551344394684, 0.09727059304714203]}}
{"id": "WebQTest-43", "question": "what did george clemenceau do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["fictional_universe.fictional_character.occupation"], ["government.political_appointer.appointees", "government.government_office_or_title.office_holders"], ["people.profession.people_with_this_profession"], ["government.political_appointer.appointees", "government.government_position_held.office_position_or_title"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george clemenceau do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.0034729870967566967, -0.04812921956181526, -0.05231035500764847, -0.054573800414800644, -0.05813263729214668, -0.06467776745557785, -0.0838714987039566, -0.08560891449451447, -0.0856529027223587, -0.08773984760046005], "norm_scores": [0.10603924840688705, 0.10140810906887054, 0.10098499804735184, 0.1007566750049591, 0.10039874166250229, 0.09974376112222672, 0.09784756600856781, 0.09767770767211914, 0.09767340868711472, 0.09746978431940079]}}
{"id": "WebQTest-44", "question": "what is the state flower of arizona", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.governing_officials", "government.governmental_jurisdiction.governing_officials", "location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.jurisdiction_of_office", "location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.governing_officials", "government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.jurisdiction_of_office", "government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of arizona [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_jurisdiction.governing_officials <SEP> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.jurisdiction_of_office <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>"], "scores": [-0.012965121306478977, -0.016391653567552567, -0.02250770293176174, -0.02449500560760498, -0.04915734380483627, -0.05016415938735008, -0.05018094554543495, -0.05117139592766762, -0.05299845337867737, -0.05428408086299896], "norm_scores": [0.10256598889827728, 0.10221514850854874, 0.10159189999103546, 0.10139020532369614, 0.09892027080059052, 0.09882072359323502, 0.0988190695643425, 0.09872124344110489, 0.09854103624820709, 0.09841442853212357]}}
{"id": "WebQTest-45", "question": "where was rihanna born and raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"], ["people.person.place_of_birth"], ["music.album.artist"], ["symbols.namesake.named_after"], ["location.location.people_born_here", "location.location.containedby"], ["geography.river.basin_countries"], ["influence.influence_node.influenced_by"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna born and raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.007202547043561935, -0.02687043510377407, -0.05534883961081505, -0.09122724831104279, -0.11357253044843674, -0.16130736470222473, -0.16272954642772675, -0.16505300998687744, -0.1698591113090515, -0.17103950679302216], "norm_scores": [0.11089048534631729, 0.10873080790042877, 0.1056780144572258, 0.1019536629319191, 0.09970074146986008, 0.09505335241556168, 0.09491825848817825, 0.0946979746222496, 0.09424394369125366, 0.09413276612758636]}}
{"id": "WebQTest-46", "question": "what year did president william henry harrison take office", "prediction": [["government.election_campaign.candidate", "government.election_campaign.election"], ["government.election_campaign.candidate", "time.event.instance_of_recurring_event"], ["people.person.nationality", "location.location.events"], ["government.us_president.vice_president"], ["base.culturalevent.event.entity_involved"], ["government.election_campaign.candidate", "time.event.locations"], ["government.us_vice_president.to_president"], ["government.politician.election_campaigns", "government.election_campaign.election"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did president william henry harrison take office [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.candidate <SEP> government.election_campaign.election </PATH>", "<PATH> government.election_campaign.candidate <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> people.person.nationality <SEP> location.location.events </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.election_campaign.candidate <SEP> time.event.locations </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.politician.election_campaigns <SEP> government.election_campaign.election </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.019192030653357506, -0.03599511831998825, -0.037126392126083374, -0.03873961791396141, -0.0400058813393116, -0.04224627465009689, -0.04476306959986687, -0.04887763410806656, -0.05119745060801506, -0.05218730866909027], "norm_scores": [0.10220387578010559, 0.10050088167190552, 0.10038725286722183, 0.10022543370723724, 0.10009860247373581, 0.0998746007680893, 0.09962354600429535, 0.09921448677778244, 0.09898459911346436, 0.09888666123151779]}}
{"id": "WebQTest-47", "question": "where george lopez was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality"], ["people.person.nationality", "location.administrative_division.second_level_division_of"], ["people.person.place_of_birth", "location.location.containedby"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["place.place_lived.person", "people.place_lived.location"], ["place.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where george lopez was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.second_level_division_of </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> place.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> place.person.place_of_birth </PATH>"], "scores": [-0.008128685876727104, -0.020507454872131348, -0.08228422701358795, -0.09304884076118469, -0.13746514916419983, -0.1382080763578415, -0.14145848155021667, -0.1476854830980301, -0.15123207867145538, -0.15589144825935364], "norm_scores": [0.11030545085668564, 0.10894841700792313, 0.10242161899805069, 0.10132500529289246, 0.09692300856113434, 0.09685102850198746, 0.09653674066066742, 0.09593746811151505, 0.09559781849384308, 0.09515343606472015]}}
{"id": "WebQTest-48", "question": "where are samsung based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded"], ["location.location.containedby"], ["organization.organization.place_founded", "location.country.administrative_divisions"], ["organization.organization_founder.organizations_founded", "people.person.place_of_birth"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are samsung based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.place_founded <SEP> location.country.administrative_divisions </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> people.person.place_of_birth </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.00884380005300045, -0.03222053870558739, -0.0392766147851944, -0.05347646027803421, -0.055944960564374924, -0.07405728101730347, -0.09020014107227325, -0.09106813371181488, -0.09257764369249344, -0.09896676987409592], "norm_scores": [0.10559046268463135, 0.10315072536468506, 0.10242544859647751, 0.10098130255937576, 0.10073233395814896, 0.09892426431179047, 0.0973401665687561, 0.09725570678710938, 0.09710901230573654, 0.09649054706096649]}}
{"id": "WebQTest-49", "question": "what did the islamic people believe in", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"], ["religion.deity.deity_of"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["base.argumentmaps.intellectual_dispute.positions"], ["religion.religion.deities"], ["symbols.namesake.named_after"], ["belief.belief.belief_of"], ["people.person.religion"], ["common.topic.notable_types"]], "ground_paths": [["religion.religion.beliefs"], ["religion.belief.belief_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the islamic people believe in [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions </PATH>", "<PATH> religion.religion.deities </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> belief.belief.belief_of </PATH>", "<PATH> people.person.religion </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.008384204469621181, -0.017488958314061165, -0.10743692517280579, -0.11294833570718765, -0.11613301187753677, -0.12075625360012054, -0.12121589481830597, -0.12987005710601807, -0.13146713376045227, -0.1441161036491394], "norm_scores": [0.10958801954984665, 0.10859478265047073, 0.09925331920385361, 0.09870779514312744, 0.09839394688606262, 0.09794009476900101, 0.09789510071277618, 0.09705153852701187, 0.09689667075872421, 0.09567875415086746]}}
{"id": "WebQTest-51", "question": "who will play mr gray in the film", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films_dubbed", "film.actor.dubbing_performances"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will play mr gray in the film [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films_dubbed <SEP> film.actor.dubbing_performances </PATH>"], "scores": [-0.022104263305664062, -0.022205226123332977, -0.02951396442949772, -0.032206110656261444, -0.03934120014309883, -0.047365352511405945, -0.051896799355745316, -0.05593840777873993, -0.0561646968126297, -0.07976822555065155], "norm_scores": [0.10216289013624191, 0.10215257853269577, 0.1014086902141571, 0.10113605111837387, 0.10041700303554535, 0.0996144637465477, 0.09916409105062485, 0.09876412153244019, 0.09874176979064941, 0.09643841534852982]}}
{"id": "WebQTest-52", "question": "what did george orwell died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["influence.influence_node.influenced_by", "people.deceased_person.cause_of_death"], ["deceased_person.cause_of_death"], ["influence.influence_node.influenced", "people.deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["people.people.deceased_person.cause_of_death"], ["deceased.deceased_person.cause_of_death"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george orwell died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> deceased.deceased_person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>"], "scores": [-0.001587298815138638, -0.043562714010477066, -0.11770790815353394, -0.12571698427200317, -0.1340051293373108, -0.14262087643146515, -0.14533396065235138, -0.15129747986793518, -0.1563594788312912, -0.15861855447292328], "norm_scores": [0.11216655373573303, 0.10755576193332672, 0.0998694971203804, 0.09907282143831253, 0.09825508296489716, 0.09741218388080597, 0.09714826196432114, 0.09657063335180283, 0.09608302265405655, 0.09586621820926666]}}
{"id": "WebQTest-54", "question": "what country did adolf hitler control", "prediction": [["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_commander.military_commands", "military.military_command.military_combatant"], ["people.person.nationality"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["government.political_appointer.appointees", "government.governmental_jurisdiction.governing_officials"], ["government.politician.government_positions_held", "government.governmental_jurisdiction.governing_officials"], ["organization.organization_founder.organizations_founded", "organization.organization.place_founded"]], "ground_paths": [["organization.organization_founder.organizations_founded", "organization.organization.place_founded"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_commander.military_commands", "military.military_command.military_combatant"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["people.deceased_person.place_of_death", "location.country.capital"], ["military.military_command.military_commander", "military.military_combatant.military_commanders"], ["government.political_appointer.appointees", "government.governmental_jurisdiction.governing_officials"], ["organization.organization.founders", "organization.organization.place_founded"], ["base.crime.crime.victim_s", "time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did adolf hitler control [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_command.military_combatant </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> organization.organization.place_founded </PATH>"], "scores": [-0.026728427037596703, -0.027595441788434982, -0.028071463108062744, -0.030199674889445305, -0.041442204266786575, -0.046490881592035294, -0.048598773777484894, -0.051239073276519775, -0.05424204096198082, -0.05452511087059975], "norm_scores": [0.10142242163419724, 0.10133452713489532, 0.10128629952669144, 0.10107097029685974, 0.09994103759527206, 0.09943774342536926, 0.09922835230827332, 0.09896671026945114, 0.09866996109485626, 0.09864203631877899]}}
{"id": "WebQTest-55", "question": "who is michael buble", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced_by", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"], ["common.topic.image"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "people.person.profession"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael buble [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0003190364805050194, -0.08164290338754654, -0.13599219918251038, -0.15818607807159424, -0.17926786839962006, -0.18625973165035248, -0.18709662556648254, -0.18850082159042358, -0.19035960733890533, -0.19314469397068024], "norm_scores": [0.11594066768884659, 0.10688513517379761, 0.10123104602098465, 0.09900908172130585, 0.09694364666938782, 0.0962681919336319, 0.09618765115737915, 0.09605269134044647, 0.09587430208921432, 0.09560766071081161]}}
{"id": "WebQTest-56", "question": "what county is kansas city kansas", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["base.biblioness.bibs_location.state"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.biblioness.bibs_location.city"]], "ground_paths": [["location.hud_county_place.county"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is kansas city kansas [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.biblioness.bibs_location.city </PATH>"], "scores": [-0.01794765703380108, -0.022570807486772537, -0.037641704082489014, -0.04250846058130264, -0.05865335091948509, -0.06462442129850388, -0.06736978143453598, -0.07409117370843887, -0.0751902386546135, -0.08886366337537766], "norm_scores": [0.10374284535646439, 0.10326432436704636, 0.10171970725059509, 0.10122586786746979, 0.09960471093654633, 0.09901173412799835, 0.09874028712511063, 0.09807884693145752, 0.0979711040854454, 0.09664061665534973]}}
{"id": "WebQTest-58", "question": "who did jackie robinson first play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.lifetime_batting_statistics", "baseball.lifetime_batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jackie robinson first play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.baseball_player.lifetime_batting_statistics <SEP> baseball.lifetime_batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>"], "scores": [-0.013898034580051899, -0.017122847959399223, -0.03328016772866249, -0.03453518822789192, -0.03631982579827309, -0.05413421615958214, -0.0648781806230545, -0.0672675147652626, -0.07010553777217865, -0.0722409039735794], "norm_scores": [0.10327861458063126, 0.10294609516859055, 0.10129612684249878, 0.10116908699274063, 0.10098869353532791, 0.09920556843280792, 0.09814541041851044, 0.09791118651628494, 0.0976337119936943, 0.0974254459142685]}}
{"id": "WebQTest-59", "question": "when did annie open", "prediction": [["theater.theater_character.plays_appears_in"], ["theater.play.characters"], ["film.film.directed_by"], ["film.film_story_contributor.film_story_credits"], ["film.film.country"], ["film.film_character.portrayed_in_films", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.film_regional_release_date.film_regional_debut_venue"], ["film.film.written_by"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did annie open [/INST]", "raw_output": {"paths": ["<PATH> theater.theater_character.plays_appears_in </PATH>", "<PATH> theater.play.characters </PATH>", "<PATH> film.film.directed_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.country </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film_regional_release_date.film_regional_debut_venue </PATH>", "<PATH> film.film.written_by </PATH>"], "scores": [-0.030872493982315063, -0.04285520315170288, -0.04370072856545448, -0.05052454397082329, -0.05620571970939636, -0.061740193516016006, -0.062062207609415054, -0.06733974814414978, -0.07335016876459122, -0.07584122568368912], "norm_scores": [0.10258101671934128, 0.10135915875434875, 0.10127349197864532, 0.10058477520942688, 0.10001494735479355, 0.09946295619010925, 0.09943092614412308, 0.09890756011009216, 0.09831486642360687, 0.09807026386260986]}}
{"id": "WebQTest-60", "question": "where did eleanor roosevelt die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.person.place_of_birth"], ["people.person.place_of_death"], ["symbols.name_source.namesakes"], ["people.place_of_death.people_deceased_here"], ["people.place_lived.person", "people.place_lived.location"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["symbols.namesake.named_after"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did eleanor roosevelt die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_death </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0014928205637261271, -0.067364402115345, -0.0795617327094078, -0.08341675251722336, -0.08474823087453842, -0.08681412786245346, -0.09008046239614487, -0.09500216692686081, -0.10048691183328629, -0.10275822132825851], "norm_scores": [0.10803545266389847, 0.10114830732345581, 0.0999220684170723, 0.09953761100769043, 0.09940516203641891, 0.09920001775026321, 0.09887652099132538, 0.0983910784125328, 0.09785290062427521, 0.09763089567422867]}}
{"id": "WebQTest-61", "question": "what are the religions practiced in indonesia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.kingdom.rulers", "people.person.religion"], ["book.book_subject.works", "base.argumentmaps.thing_of_disputed_value.disparagement"], ["royalty.monarch.kingdom", "people.person.religion"], ["religion.religion.countries"], ["people.person.place_of_birth", "people.person.religion"], ["people.person.quotations", "people.person.religion"], ["book.book_subject.works", "base.argumentmaps.expression_of_disvalue.thing_of_disputed_value"], ["people.family.country", "people.person.religion"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["book.book_subject.works", "book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the religions practiced in indonesia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>", "<PATH> book.book_subject.works <SEP> base.argumentmaps.thing_of_disputed_value.disparagement </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> religion.religion.countries </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>", "<PATH> book.book_subject.works <SEP> base.argumentmaps.expression_of_disvalue.thing_of_disputed_value </PATH>", "<PATH> people.family.country <SEP> people.person.religion </PATH>"], "scores": [-0.006213267333805561, -0.01704215817153454, -0.07293296605348587, -0.08920793235301971, -0.0918809026479721, -0.09596972912549973, -0.10038527846336365, -0.10095995664596558, -0.10544700175523758, -0.11345880478620529], "norm_scores": [0.1075192242860794, 0.10636119544506073, 0.1005796492099762, 0.09895596653223038, 0.09869182854890823, 0.0982891097664833, 0.0978560671210289, 0.09779985249042511, 0.09736199676990509, 0.09658507257699966]}}
{"id": "WebQTest-62", "question": "what was jesse james killed with", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.deceased_person.weapons", "people.deceased_person.cause_of_death"], ["base.culturalevent.event.entity_involved", "people.deceased_person.cause_of_death"], ["people.deceased_person.weapon", "people.deceased_person.cause_of_death"], ["people.person.deceased_by", "people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.cause_of_death"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse james killed with [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.weapons <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.weapon <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_by <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0014902467373758554, -0.06024783477187157, -0.07107088714838028, -0.07571446895599365, -0.08317824453115463, -0.08524700999259949, -0.08630890399217606, -0.08842213451862335, -0.09282863885164261, -0.09498880803585052], "norm_scores": [0.10747760534286499, 0.10134442150592804, 0.10025348514318466, 0.09978902339935303, 0.09904699772596359, 0.0988423079252243, 0.09873740375041962, 0.09852895885705948, 0.09809575974941254, 0.09788407385349274]}}
{"id": "WebQTest-63", "question": "who was vp for lincoln", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.political_appointer.appointees", "government.politician.government_positions_held"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["government.political_party_tenure.politician", "government.political_party_tenure.politician"], ["government.election_campaign.candidate", "government.election_campaign.candidate"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation", "people.person.profession"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["government.political_party_tenure.politician", "government.politician.party"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for lincoln [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.politician.government_positions_held </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.politician </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.candidate </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.politician.party </PATH>"], "scores": [-0.00777672603726387, -0.01863253489136696, -0.08926187455654144, -0.09639517217874527, -0.09731721132993698, -0.10108517110347748, -0.10241363197565079, -0.10418536514043808, -0.1053222194314003, -0.10944066196680069], "norm_scores": [0.10776329785585403, 0.10659977048635483, 0.09933042526245117, 0.09862440079450607, 0.0985335111618042, 0.0981629341840744, 0.09803261607885361, 0.09785907715559006, 0.09774789214134216, 0.09734614938497543]}}
{"id": "WebQTest-64", "question": "what other books did charles dickens write", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"], ["fictional_universe.fictional_character.character_created_by"], ["film.film_story_contributor.film_story_credits"], ["book.book_edition.author_editor"], ["tv.tv_program_creator.programs_created"], ["book.author.works_written"], ["fictional_universe.fictional_character_creator.fictional_characters_created", "book.book_edition.author_editor"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes"]], "ground_paths": [["tv.tv_program_creator.programs_created"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did charles dickens write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created <SEP> book.book_edition.author_editor </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>"], "scores": [-0.005766755435615778, -0.02911459468305111, -0.048134613782167435, -0.05315582826733589, -0.06523653864860535, -0.07043053209781647, -0.07962020486593246, -0.07997851073741913, -0.08050704747438431, -0.08341982215642929], "norm_scores": [0.10549241304397583, 0.10305792093276978, 0.10111627727746964, 0.10060983151197433, 0.09940170496702194, 0.09888675808906555, 0.09798217564821243, 0.09794707596302032, 0.0978953167796135, 0.09761058539152145]}}
{"id": "WebQTest-65", "question": "who was the first dictator of the soviet union", "prediction": [["people.person.nationality"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["organization.organization_founder.organizations_founded"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["organization.organization.founders"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first dictator of the soviet union [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.015821855515241623, -0.01959148608148098, -0.032722942531108856, -0.035103894770145416, -0.04300409182906151, -0.043165843933820724, -0.05479234829545021, -0.0573178231716156, -0.05734237655997276, -0.06784489005804062], "norm_scores": [0.10270778089761734, 0.10232134163379669, 0.10098649561405182, 0.10074634104967117, 0.0999535545706749, 0.09993738681077957, 0.09878219664096832, 0.09853304177522659, 0.09853062033653259, 0.09750121086835861]}}
{"id": "WebQTest-66", "question": "what is the currency of puerto rico called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"], ["location.country.currency_used", "location.country.currency_used"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["location.country.currency_used", "finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["location.location.containedby", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of puerto rico called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.official_language <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <SEP> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> location.location.containedby <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.000575490586925298, -0.06194540858268738, -0.13328996300697327, -0.1377945989370346, -0.1430303007364273, -0.145757257938385, -0.14677806198596954, -0.14728878438472748, -0.1542855203151703, -0.15515711903572083], "norm_scores": [0.1128428727388382, 0.10612593591213226, 0.09881820529699326, 0.09837406873703003, 0.09786035865545273, 0.09759385883808136, 0.09749428927898407, 0.09744450449943542, 0.09676508605480194, 0.09668079018592834]}}
{"id": "WebQTest-67", "question": "what kind of cancer did carl wilson have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.people", "people.deceased_person.cause_of_death"], ["medicine.disease.parent_disease"], ["people.cause_of_death.parent_cause_of_death"], ["medicine.cancer_patient.cancer_type"], ["film.film_subject.films", "film.film_subject.films"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did carl wilson have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.people <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.parent_disease </PATH>", "<PATH> people.cause_of_death.parent_cause_of_death </PATH>", "<PATH> medicine.cancer_patient.cancer_type </PATH>", "<PATH> film.film_subject.films <SEP> film.film_subject.films </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.012508414685726166, -0.0158526711165905, -0.03528236225247383, -0.052656687796115875, -0.09681141376495361, -0.10185287892818451, -0.10663769394159317, -0.11563308537006378, -0.11714604496955872, -0.11917448788881302], "norm_scores": [0.1066083237528801, 0.10625239461660385, 0.10420787334442139, 0.10241296142339706, 0.09798932820558548, 0.09749656170606613, 0.09703116863965988, 0.09616225212812424, 0.09601687639951706, 0.0958223044872284]}}
{"id": "WebQTest-68", "question": "who plays the voice of kitt in knight rider", "prediction": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.languages", "people.person.languages"], ["tv.tv_program_creator.programs_created"], ["film.film.country", "people.person.nationality"], ["tv.tv_program_writer_relationship.tv_program", "tv.tv_writer.tv_programs"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of kitt in knight rider [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.languages <SEP> people.person.languages </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program_writer_relationship.tv_program <SEP> tv.tv_writer.tv_programs </PATH>", "<PATH> tv.tv_guest_role.character <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.tv_guest_role.character <SEP> tv.tv_guest_role.actor </PATH>"], "scores": [-0.01601129025220871, -0.02109595201909542, -0.021515395492315292, -0.05575917288661003, -0.056359488517045975, -0.05861561372876167, -0.06432069092988968, -0.06615598499774933, -0.07044557482004166, -0.0712292268872261], "norm_scores": [0.10345063358545303, 0.10292595624923706, 0.10288278758525848, 0.09941934049129486, 0.0993596762418747, 0.09913575649261475, 0.0985717922449112, 0.09839104115962982, 0.0979698896408081, 0.09789314866065979]}}
{"id": "WebQTest-69", "question": "what county is brentwood tennessee in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"], ["location.citytown.postal_codes"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.second_level_division_of"], ["location.place_with_neighborhoods.neighborhoods"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is brentwood tennessee in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.second_level_division_of </PATH>", "<PATH> location.place_with_neighborhoods.neighborhoods </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.01447814516723156, -0.023293010890483856, -0.035404011607170105, -0.03561313450336456, -0.07904624193906784, -0.0977945402264595, -0.1111869215965271, -0.11192374676465988, -0.11387846618890762, -0.12032919377088547], "norm_scores": [0.10607776790857315, 0.10514681041240692, 0.10388106852769852, 0.10385934263467789, 0.09944497793912888, 0.09759791195392609, 0.09629956632852554, 0.09622862935066223, 0.09604071080684662, 0.09542317688465118]}}
{"id": "WebQTest-71", "question": "who did michael j fox marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.spouse_s", "people.marriage.spouse"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.parents", "people.person.parents"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did michael j fox marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.025658028200268745, -0.027392184361815453, -0.04059183970093727, -0.04342031106352806, -0.04468194395303726, -0.04687394201755524, -0.046958647668361664, -0.048094652593135834, -0.04817649722099304, -0.05078061297535896], "norm_scores": [0.10167081654071808, 0.10149466246366501, 0.1001637727022171, 0.09988085925579071, 0.09975492209196091, 0.09953650087118149, 0.09952806681394577, 0.09941507130861282, 0.09940693527460098, 0.09914840012788773]}}
{"id": "WebQTest-72", "question": "what battles did stonewall jackson fight in", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts"], ["military.military_conflict.military_personnel_involved"], ["military.military_command.military_commander", "military.military_conflict.commanders"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["book.written_work.subjects"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_command.military_commander", "military.military_command.military_conflict"], ["people.person.nationality", "base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_command.military_conflict"]], "ground_paths": [["military.military_conflict.military_personnel_involved"], ["people.person.nationality", "book.written_work.subjects"], ["base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["common.topic.image"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what battles did stonewall jackson fight in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_command.military_commander <SEP> military.military_conflict.commanders </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_conflict.commanders </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> military.military_command.military_commander <SEP> military.military_command.military_conflict </PATH>", "<PATH> people.person.nationality <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_command.military_conflict </PATH>"], "scores": [-0.011240587569773197, -0.029422249644994736, -0.03032800741493702, -0.03207007795572281, -0.05094710737466812, -0.055694740265607834, -0.06057768315076828, -0.06290166079998016, -0.0639379695057869, -0.06407107412815094], "norm_scores": [0.1035328358411789, 0.10166744887828827, 0.10157539695501328, 0.10139860957860947, 0.09950245171785355, 0.09903117269277573, 0.0985487848520279, 0.09832002222537994, 0.09821818023920059, 0.09820511192083359]}}
{"id": "WebQTest-73", "question": "what language do the maasai tribe speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["people.ethnicity.languages_spoken", "base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken", "language.human_language.region", "language.human_language.region"], ["people.ethnicity.languages_spoken", "common.topic.notable_types", "common.topic.notable_types"], ["people.ethnicity.languages_spoken", "language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken", "language.human_language.language_family", "language.human_language.language_family"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai tribe speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.region <SEP> language.human_language.region </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.language_family <SEP> language.human_language.language_family </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0006446689949370921, -0.08105408400297165, -0.08518730849027634, -0.09017421305179596, -0.09098804742097855, -0.09200511872768402, -0.09849105030298233, -0.09849444031715393, -0.1007765457034111, -0.10135488212108612], "norm_scores": [0.10863868147134781, 0.10024509578943253, 0.09983161836862564, 0.09933499991893768, 0.09925419092178345, 0.0991532951593399, 0.09851226955652237, 0.0985119417309761, 0.09828738123178482, 0.09823055565357208]}}
{"id": "WebQTest-74", "question": "who did benjamin franklin get married to", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did benjamin franklin get married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.028307897970080376, -0.030682029202580452, -0.031373053789138794, -0.0370187908411026, -0.04215143620967865, -0.04281948506832123, -0.043130822479724884, -0.04874422028660774, -0.04896042123436928, -0.051428720355033875], "norm_scores": [0.10121968388557434, 0.10097966343164444, 0.10090991109609604, 0.100341796875, 0.09982810169458389, 0.09976144134998322, 0.09973037987947464, 0.09917211532592773, 0.09915068000555038, 0.0989062488079071]}}
{"id": "WebQTest-75", "question": "what disease did patrick swayze died from", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition", "medicine.disease.notable_people_with_this_condition"], ["common.topic.image", "medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition"], ["common.topic.notable_types", "common.topic.notable_types"], ["medicine.symptom.symptom_of", "medicine.symptom.symptom_of"], ["medicine.symptom.symptom_of_this_condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did patrick swayze died from [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> common.topic.image <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> medicine.symptom.symptom_of <SEP> medicine.symptom.symptom_of </PATH>", "<PATH> medicine.symptom.symptom_of_this_condition </PATH>"], "scores": [-0.005158514715731144, -0.027571532875299454, -0.03892435133457184, -0.08018352091312408, -0.10796111077070236, -0.11491431295871735, -0.11664942651987076, -0.1192658543586731, -0.12833045423030853, -0.1318107396364212], "norm_scores": [0.1084301546216011, 0.10602694749832153, 0.1048300489783287, 0.10059285908937454, 0.09783708304166794, 0.09715916216373444, 0.09699072688817978, 0.09673729538917542, 0.09586436301469803, 0.09553130716085434]}}
{"id": "WebQTest-76", "question": "what works of art did leonardo da vinci produce", "prediction": [["visual_art.visual_artist.art_forms"], ["film.film.subjects"], ["visual_art.art_form.artists"], ["visual_art.visual_art_form.artists"], ["film.film_story_contributor.film_story_credits"], ["visual_art.art_form.artist"], ["book.author.book_editions_published"], ["influence.influence_node.influenced_by"], ["visual_art.art_period_movement.artist"], ["book.written_work.author"]], "ground_paths": [["visual_art.artwork.artist"], ["visual_art.visual_artist.artworks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what works of art did leonardo da vinci produce [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> visual_art.art_form.artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> visual_art.art_form.artist </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.art_period_movement.artist </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.0008097450481727719, -0.07548794150352478, -0.08453133702278137, -0.09542970359325409, -0.1036541536450386, -0.10677923262119293, -0.11146516352891922, -0.11205506324768066, -0.11308059096336365, -0.1163329929113388], "norm_scores": [0.1094830259680748, 0.10160485655069351, 0.10069014877080917, 0.09959875047206879, 0.0987829640507698, 0.09847473353147507, 0.09801436960697174, 0.0979565754532814, 0.09785616397857666, 0.09753841906785965]}}
{"id": "WebQTest-77", "question": "what capital of austria", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.first_level_division_of"], ["location.country.first_level_divisions"], ["location.administrative_division.country"], ["location.country.second_level_divisions"], ["base.aareas.schema.administrative_area.capital"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.country.administrative_divisions"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital of austria [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.second_level_divisions </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.006857809145003557, -0.03888293355703354, -0.06424841284751892, -0.06845061480998993, -0.06882718205451965, -0.08126849681138992, -0.08325029164552689, -0.08583410084247589, -0.1001252830028534, -0.10278117656707764], "norm_scores": [0.10648296773433685, 0.10312686115503311, 0.10054390132427216, 0.10012228041887283, 0.100084587931633, 0.09884712100028992, 0.09865140914916992, 0.09839684516191483, 0.09700064361095428, 0.09674336016178131]}}
{"id": "WebQTest-78", "question": "what type of cancer did eva peron have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.people"], ["people.cause_of_death.parent_cause_of_death"], ["medicine.disease.parent_disease"], ["medicine.cancer_patient.cancer_type"], ["film.film_subject.films", "film.film_subject.films"], ["medicine.cancer_patient.cancer_parent"], ["medicine.cancer_patient.cancer_patients"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.cause_of_death.parent_cause_of_death </PATH>", "<PATH> medicine.disease.parent_disease </PATH>", "<PATH> medicine.cancer_patient.cancer_type </PATH>", "<PATH> film.film_subject.films <SEP> film.film_subject.films </PATH>", "<PATH> medicine.cancer_patient.cancer_parent </PATH>", "<PATH> medicine.cancer_patient.cancer_patients </PATH>"], "scores": [-0.008005231618881226, -0.02739250473678112, -0.03588512912392616, -0.038707390427589417, -0.08921010047197342, -0.09750410169363022, -0.11852551251649857, -0.12336021661758423, -0.12538285553455353, -0.1253841519355774], "norm_scores": [0.10724575072526932, 0.1051865667104721, 0.10429704934358597, 0.10400310158729553, 0.09888108819723129, 0.0980643630027771, 0.09602443128824234, 0.09556130319833755, 0.09536821395158768, 0.09536808729171753]}}
{"id": "WebQTest-79", "question": "what country did buddha come from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"], ["people.person.ethnicity"], ["people.deceased_person.place_of_death"], ["people.ethnicity.people_with_this_ethnicity"], ["symbols.name_source.namesakes"], ["people.ethnicity.people"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.006754787638783455, -0.03342792019248009, -0.0483250617980957, -0.06325415521860123, -0.09090172499418259, -0.09399458020925522, -0.09867245703935623, -0.10312753915786743, -0.11024493724107742, -0.12155371904373169], "norm_scores": [0.10721196234226227, 0.10439008474349976, 0.10284649580717087, 0.10132250189781189, 0.09855954349040985, 0.09825518727302551, 0.09779663383960724, 0.09736191481351852, 0.09667140990495682, 0.09558433294296265]}}
{"id": "WebQTest-80", "question": "what county is greeley colorado in", "prediction": [["location.us_county.county_seat"], ["location.us_county.hud_county_place"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.location.contains"], ["location.citytown.postal_codes"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.place_with_neighborhoods.neighborhoods"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is greeley colorado in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.place_with_neighborhoods.neighborhoods </PATH>"], "scores": [-0.017946340143680573, -0.024640310555696487, -0.026631973683834076, -0.032574914395809174, -0.0834348052740097, -0.10534444451332092, -0.1081712618470192, -0.12049897015094757, -0.12482108920812607, -0.13776341080665588], "norm_scores": [0.10610014945268631, 0.10539229214191437, 0.10518258810043335, 0.10455935448408127, 0.0993744432926178, 0.09722086042165756, 0.09694642573595047, 0.09575863927602768, 0.09534565359354019, 0.0941196084022522]}}
{"id": "WebQTest-82", "question": "what did peter tchaikovsky do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["influence.influence_node.influenced_by", "people.person.profession"], ["influence.influence_node.influenced_by"], ["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["influence.influence_node.influenced"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did peter tchaikovsky do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.0008787699625827372, -0.06996620446443558, -0.07266582548618317, -0.08257719874382019, -0.11740750819444656, -0.12970252335071564, -0.14006508886814117, -0.1411610096693039, -0.14335988461971283, -0.14486002922058105], "norm_scores": [0.11077945679426193, 0.10338439047336578, 0.10310565680265427, 0.1020887941122055, 0.09859421849250793, 0.09738942235708237, 0.09638543426990509, 0.09627986699342728, 0.0960683822631836, 0.09592437744140625]}}
{"id": "WebQTest-83", "question": "which countries speak german officially", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.local_name.locale"], ["location.country.form_of_government"], ["government.form_of_government.countries"], ["location.country.currency_used"], ["location.country.administrative_divisions"], ["languages.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak german officially [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> languages.human_language.countries_spoken_in </PATH>"], "scores": [-0.0180027037858963, -0.021004412323236465, -0.023965738713741302, -0.12237965315580368, -0.13903166353702545, -0.14214906096458435, -0.16396401822566986, -0.169695645570755, -0.1703329086303711, -0.17578475177288055], "norm_scores": [0.10992169380187988, 0.10959223657846451, 0.1092681810259819, 0.0990268811583519, 0.0973915308713913, 0.09708839654922485, 0.09499335289001465, 0.094450443983078, 0.09439028054475784, 0.09387706965208054]}}
{"id": "WebQTest-84", "question": "what type of music did john lennon sing", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["music.producer.releases_produced"], ["music.artist.album", "music.album.genre"], ["music.composer.compositions"], ["music.artist.label"], ["music.musical_group.member", "music.group_membership.group"], ["music.musical_group.member"], ["music.artist.album", "music.artist.genre"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did john lennon sing [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> music.artist.album <SEP> music.album.genre </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.artist.label </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.musical_group.member </PATH>", "<PATH> music.artist.album <SEP> music.artist.genre </PATH>"], "scores": [-0.0012159555917605758, -0.05478234216570854, -0.11371883004903793, -0.11572790145874023, -0.11606082320213318, -0.11838231235742569, -0.12052195519208908, -0.12316728383302689, -0.12673147022724152, -0.1274353563785553], "norm_scores": [0.11049270629882812, 0.10472973436117172, 0.09873570501804352, 0.09853754192590714, 0.09850474447011948, 0.09827633202075958, 0.09806627035140991, 0.09780719876289368, 0.09745921194553375, 0.09739064425230026]}}
{"id": "WebQTest-85", "question": "who are the colorado representatives", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.political_district.elections", "common.image.appears_in_topic_gallery"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.election.district", "government.election.winner"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the colorado representatives [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.elections <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.person.place_of_birth </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>"], "scores": [-0.02498283050954342, -0.027012702077627182, -0.028490565717220306, -0.03963663801550865, -0.041269879788160324, -0.04242488369345665, -0.047708697617053986, -0.05034671351313591, -0.050996068865060806, -0.052744701504707336], "norm_scores": [0.10156506299972534, 0.10135910660028458, 0.10120942443609238, 0.10008759796619415, 0.09992426633834839, 0.09980891644954681, 0.09928294271230698, 0.09902137517929077, 0.09895709156990051, 0.09878420829772949]}}
{"id": "WebQTest-86", "question": "which country does greenland belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["location.country.administrative_divisions"], ["location.location.contains"], ["common.topic.notable_types"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["government.governmental_jurisdiction.governing_officials", "government.political_district.representatives"], ["location.country.languages_spoken"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.country.administrative_divisions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country does greenland belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.political_district.representatives </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.01739290915429592, -0.033905357122421265, -0.038315098732709885, -0.045862868428230286, -0.054979704320430756, -0.06541852653026581, -0.06777594238519669, -0.07099942862987518, -0.07466913014650345, -0.07677411288022995], "norm_scores": [0.10377296805381775, 0.10207349061965942, 0.10162436217069626, 0.10086021572351456, 0.09994486719369888, 0.0989069864153862, 0.09867409616708755, 0.09835653007030487, 0.09799625724554062, 0.0977901890873909]}}
{"id": "WebQTest-87", "question": "what the zip code for seattle washington", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"], ["citytown.citytown.postal_codes"], ["location.mailing_address.citytown"], ["location.citytown.postal_ codes"], ["location.country.administrative_divisions"]], "ground_paths": [["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the zip code for seattle washington [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> citytown.citytown.postal_codes </PATH>", "<PATH> location.mailing_address.citytown </PATH>", "</PATH> location.citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_ codes </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.0001600459509063512, -0.09008345007896423, -0.1542404741048813, -0.16956427693367004, -0.17594130337238312, -0.18173342943191528, -0.18230187892913818, -0.18306565284729004, -0.18793915212154388, -0.19672395288944244], "norm_scores": [0.11621411144733429, 0.1062198355793953, 0.09961909055709839, 0.09810418635606766, 0.09748056530952454, 0.09691758453845978, 0.09686250239610672, 0.09678854793310165, 0.09631800651550293, 0.09547557681798935]}}
{"id": "WebQTest-88", "question": "who is willow smith mom name", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"], ["music.artist.genre", "music.artist.genre"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["base.popstra.celebrity.lived_with", "base.popstra.lived_with.lived_with"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is willow smith mom name [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.popstra.celebrity.breakup <SEP> base.popstra.celebrity.breakup </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.celebrity.canoodled </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> base.popstra.celebrity.lived_with <SEP> base.popstra.lived_with.lived_with </PATH>"], "scores": [-0.007940533570945263, -0.02247610129415989, -0.12802202999591827, -0.13276715576648712, -0.13460521399974823, -0.14252343773841858, -0.14324073493480682, -0.1447838544845581, -0.14508987963199615, -0.14627321064472198], "norm_scores": [0.11113103479146957, 0.10952736437320709, 0.09855635464191437, 0.0980898067355156, 0.09790966659784317, 0.0971374660730362, 0.09706781059503555, 0.09691813588142395, 0.0968884825706482, 0.0967739075422287]}}
{"id": "WebQTest-89", "question": "what do you call the chinese writing system", "prediction": [["language.human_language.writing_system"], ["language.language_writing_system.writing_systems"], ["location.country.writing_system"], ["language.language_writing_system.writing_system"], ["location.country.languages_spoken"], ["language.writing_system.writing_system"], ["language.language_writing_system.languages"], ["language.human_language.writing_systems"], ["language.language_family.languages"], ["common.topic.notable_types"]], "ground_paths": [["language.language_writing_system.languages"], ["language.human_language.writing_system"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call the chinese writing system [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.writing_system </PATH>", "<PATH> language.language_writing_system.writing_systems </PATH>", "<PATH> location.country.writing_system </PATH>", "<PATH> language.language_writing_system.writing_system </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.writing_system.writing_system </PATH>", "<PATH> language.language_writing_system.languages </PATH>", "<PATH> language.human_language.writing_systems </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.00804873462766409, -0.04445246234536171, -0.05319881811738014, -0.05330183357000351, -0.05540113523602486, -0.06850764155387878, -0.07116289436817169, -0.07725023478269577, -0.08981055021286011, -0.09531644731760025], "norm_scores": [0.10547585785388947, 0.10170519351959229, 0.10081952065229416, 0.10080914199352264, 0.10059772431850433, 0.09928784519433975, 0.09902456402778625, 0.09842359274625778, 0.097195103764534, 0.09666141867637634]}}
{"id": "WebQTest-90", "question": "who played on the jeffersons", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"]], "ground_paths": [["award.award_honor.honored_for", "award.award_winner.awards_won"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played on the jeffersons [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_winning_work.awards_won <SEP> award.award_winner.awards_won </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_honor.award_winner </PATH>", "<PATH> award.award_winning_work.awards_won <SEP> award.award_honor.award_winner </PATH>"], "scores": [-0.023959042504429817, -0.025053609162569046, -0.02849121205508709, -0.0306917205452919, -0.03199462592601776, -0.03774208202958107, -0.03888136148452759, -0.04537692293524742, -0.04840224236249924, -0.048581939190626144], "norm_scores": [0.10119909048080444, 0.10108838230371475, 0.10074148327112198, 0.10052003711462021, 0.10038915276527405, 0.09981383383274078, 0.09970018267631531, 0.09905466437339783, 0.09875544160604477, 0.0987377017736435]}}
{"id": "WebQTest-91", "question": "what is the name of the san francisco newspaper", "prediction": [["book.newspaper.circulation_areas"], ["periodicals.newspaper_circulation_area.newspapers"], ["location.location.containedby"], ["book.newspaper.headquarters"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["book.periodical.language"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["base.biblioness.bibs_location.newspaper"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["location.location.containedby", "book.newspaper.circulation_areas"]], "ground_paths": [["periodicals.newspaper_circulation_area.newspapers"], ["book.newspaper.circulation_areas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the san francisco newspaper [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> book.newspaper.headquarters </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> book.periodical.language </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> periodicals.newspaper_circulation_area.newspapers </PATH>", "<PATH> base.biblioness.bibs_location.newspaper </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> location.location.containedby <SEP> book.newspaper.circulation_areas </PATH>"], "scores": [-0.0031240591779351234, -0.029983533546328545, -0.09309157729148865, -0.09722691774368286, -0.10675905644893646, -0.10710085183382034, -0.1092020645737648, -0.10952376574277878, -0.11041580885648727, -0.11280928552150726], "norm_scores": [0.10877547413110733, 0.10589271038770676, 0.09941653162240982, 0.09900625795125961, 0.09806699305772781, 0.09803348034620285, 0.09782771021127701, 0.0977962538599968, 0.0977090448141098, 0.09747546166181564]}}
{"id": "WebQTest-93", "question": "what continent does armenia belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["base.schemastaging.disputed_location_claimant.disputed_territories", "location.location.partially_contains"], ["base.schemastaging.disputed_location_claimant.disputed_territories", "location.location.partially_containedby"], ["geography.river.basin_countries"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["geography.river.basin_countries", "location.location.partially_contains"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does armenia belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories <SEP> location.location.partially_contains </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories <SEP> location.location.partially_containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> geography.river.basin_countries <SEP> location.location.partially_containedby </PATH>", "<PATH> geography.river.basin_countries <SEP> location.location.partially_contains </PATH>"], "scores": [-0.014971357770264149, -0.024590997025370598, -0.028520358726382256, -0.056929346174001694, -0.0678606852889061, -0.0862736850976944, -0.09138740599155426, -0.09427043050527573, -0.09740394353866577, -0.1004481092095375], "norm_scores": [0.10521125793457031, 0.10420401394367218, 0.1037953570485115, 0.10088814049959183, 0.09979129582643509, 0.09797065705060959, 0.09747093915939331, 0.09719032794237137, 0.09688626229763031, 0.0965917706489563]}}
{"id": "WebQTest-94", "question": "what did randy savage died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["people.person.deceased_cause_of_death"], ["people.marriage.spouse", "people.marriage.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did randy savage died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.person.deceased_cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>"], "scores": [-0.0008969305781647563, -0.054266609251499176, -0.12368550896644592, -0.12482289969921112, -0.13155359029769897, -0.13391408324241638, -0.136332169175148, -0.15041393041610718, -0.15277117490768433, -0.15617036819458008], "norm_scores": [0.11212483793497086, 0.10629764944314957, 0.09916888177394867, 0.09905615448951721, 0.09839167445898056, 0.09815969318151474, 0.09792262315750122, 0.09655336290597916, 0.09632603824138641, 0.09599915891885757]}}
{"id": "WebQTest-95", "question": "what was the title of the book charles darwin wrote", "prediction": [["book.written_work.author"], ["common.topic.notable_types"], ["book.book_edition.author_editor"], [], ["book.written_work.subjects"], ["book.author.works_written"], ["base.argumentmaps.innovator.original_ideas"], ["film.film_story_contributor.film_story_credits"], ["influence.influence_node.influenced_by", "book.written_work.author"], ["influence.influence_node.influenced", "book.written_work.author"]], "ground_paths": [["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"], [], ["base.argumentmaps.innovator.original_ideas"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the title of the book charles darwin wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced <SEP> book.written_work.author </PATH>"], "scores": [-0.0014605801552534103, -0.07517612725496292, -0.07551496475934982, -0.07767704874277115, -0.08041919767856598, -0.09101473540067673, -0.10691565275192261, -0.10728172957897186, -0.11021121591329575, -0.12100525945425034], "norm_scores": [0.10862044990062714, 0.10090143233537674, 0.10086725652217865, 0.1006494089961052, 0.10037379711866379, 0.09931588917970657, 0.09774916619062424, 0.09771338850259781, 0.09742756187915802, 0.09638157486915588]}}
{"id": "WebQTest-96", "question": "where did richard nixon die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people"], ["deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.place_of_interment.interred_here"], ["location.deceased_person.place_of_death"], ["people.place_of_death.people", "people.place_of_death.location"], ["people.place_of_death.people", "location.location.containedby"], ["people.person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard nixon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> people.place_of_death.people <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_death </PATH>"], "scores": [-0.0005530203343369067, -0.07795251905918121, -0.09875558316707611, -0.10648908466100693, -0.10910564661026001, -0.1112225204706192, -0.1121978759765625, -0.11631360650062561, -0.11822175234556198, -0.1282220482826233], "norm_scores": [0.11015619337558746, 0.10195175558328629, 0.09985275566577911, 0.09908352792263031, 0.09882460534572601, 0.0986156314611435, 0.09851949661970139, 0.09811484813690186, 0.09792780876159668, 0.0969533771276474]}}
{"id": "WebQTest-97", "question": "what songs does smokey robinson sing", "prediction": [["music.recording.artist"], ["music.artist.track"], ["music.composition.composer"], ["music.composer.compositions"], ["music.album.artist"], ["music.artist.album"], ["music.composition.lyricist"], ["music.producer.tracks_produced"], ["music.recording.producer"], ["music.lyricist.lyrics_written"]], "ground_paths": [["music.artist.album"], ["music.recording.featured_artists"], ["music.album.artist"], ["music.lyricist.lyrics_written"], ["music.recording.artist"], ["music.composition.lyricist"], ["music.recording.producer"], ["music.producer.releases_produced"], ["music.featured_artist.recordings"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs does smokey robinson sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.producer.tracks_produced </PATH>", "<PATH> music.recording.producer </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>"], "scores": [-0.01433422788977623, -0.03711500018835068, -0.04733332619071007, -0.048791222274303436, -0.0518043078482151, -0.05241979658603668, -0.06834855675697327, -0.07018136978149414, -0.0722079649567604, -0.07541592419147491], "norm_scores": [0.10400828719139099, 0.1016656830906868, 0.10063212364912033, 0.10048551857471466, 0.10018319636583328, 0.10012155771255493, 0.09853937476873398, 0.09835893660783768, 0.09815981239080429, 0.09784542769193649]}}
{"id": "WebQTest-98", "question": "who was vice president when jfk was president", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.gender", "people.person.gender"], ["people.person.profession", "common.topic.notable_types"], ["religion.religion.practices", "people.person.religion"], ["film.film.subjects", "film.film_subject.films"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president when jfk was president [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> military.military_person.participated_in_conflicts <SEP> military.military_person.participated_in_conflicts </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>", "<PATH> religion.religion.practices <SEP> people.person.religion </PATH>", "<PATH> film.film.subjects <SEP> film.film_subject.films </PATH>"], "scores": [-0.010202493518590927, -0.014706031419336796, -0.07103729993104935, -0.0713186040520668, -0.08045683801174164, -0.08737912774085999, -0.08860013633966446, -0.09678955376148224, -0.09955848008394241, -0.10967445373535156], "norm_scores": [0.1064218059182167, 0.10594361275434494, 0.10014065355062485, 0.10011248290538788, 0.09920179843902588, 0.09851747006177902, 0.09839725494384766, 0.0975947380065918, 0.09732486307621002, 0.09634529799222946]}}
{"id": "WebQTest-99", "question": "what is serbian language called", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.language_family"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.language_family.languages"], ["language.human_language.region"], ["base.rosetta.languoid.languoid_class"], ["language.language_writing_system.languages"], ["language.human_language.writing_system"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is serbian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.language_writing_system.languages </PATH>", "<PATH> language.human_language.writing_system </PATH>"], "scores": [-0.01769992709159851, -0.025972090661525726, -0.035587746649980545, -0.038476668298244476, -0.07017503678798676, -0.07480957359075546, -0.0860755443572998, -0.0881241112947464, -0.11087603867053986, -0.12543414533138275], "norm_scores": [0.10502385348081589, 0.10415865480899811, 0.10316190868616104, 0.10286431014537811, 0.09965481609106064, 0.09919402748346329, 0.0980827808380127, 0.09788206219673157, 0.09568020701408386, 0.0942973643541336]}}
{"id": "WebQTest-100", "question": "what language is spoken in haiti today", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["book.written_work.subjects"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in haiti today [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.011784941889345646, -0.01684231497347355, -0.051358748227357864, -0.06763402372598648, -0.16676022112369537, -0.17822568118572235, -0.18077747523784637, -0.18561305105686188, -0.18760430812835693, -0.19618172943592072], "norm_scores": [0.11160368472337723, 0.11104068905115128, 0.10727335512638092, 0.10554157942533493, 0.09558144956827164, 0.09449183195829391, 0.09425100684165955, 0.09379635006189346, 0.0936097651720047, 0.0928102657198906]}}
{"id": "WebQTest-102", "question": "who played barbara gordon batgirl", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played barbara gordon batgirl [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.011475411243736744, -0.032346464693546295, -0.03930867835879326, -0.04399827867746353, -0.04727176949381828, -0.04913075640797615, -0.04914340004324913, -0.050272468477487564, -0.05178751051425934, -0.060496602207422256], "norm_scores": [0.10324811935424805, 0.10111555457115173, 0.10041400790214539, 0.09994420409202576, 0.09961757808923721, 0.0994325652718544, 0.09943130612373352, 0.09931910783052444, 0.09916875511407852, 0.09830881655216217]}}
{"id": "WebQTest-103", "question": "what basketball teams has shaq played for", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.sports_team.sport"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "ground_paths": [["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball teams has shaq played for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_team.sport </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.021366197615861893, -0.02157003991305828, -0.024833083152770996, -0.026021651923656464, -0.039479173719882965, -0.04381859302520752, -0.053060133010149, -0.05797790363430977, -0.05919322371482849, -0.07385076582431793], "norm_scores": [0.10208101570606232, 0.1020602136850357, 0.10172772407531738, 0.10160688310861588, 0.10024867206811905, 0.09981458634138107, 0.09889639914035797, 0.09841124713420868, 0.09829171746969223, 0.09686151146888733]}}
{"id": "WebQTest-104", "question": "where is jay leno from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["people.person.place_of_birth", "location.location.containedby"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["location.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jay leno from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> location.person.place_of_birth </PATH>"], "scores": [-0.004594186320900917, -0.03396337106823921, -0.059948187321424484, -0.10095232725143433, -0.12554199993610382, -0.12922121584415436, -0.13031946122646332, -0.13559852540493011, -0.1369594931602478, -0.13768556714057922], "norm_scores": [0.1098330169916153, 0.10665421187877655, 0.10391852259635925, 0.09974361956119537, 0.09732086211442947, 0.09696345031261444, 0.09685702621936798, 0.09634705632925034, 0.09621602296829224, 0.09614618122577667]}}
{"id": "WebQTest-105", "question": "what language do people from thailand speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"], ["people.ethnicity.languages_spoken"], ["royalty.kingdom.rulers", "language.human_language.countries_spoken_in"], ["royalty.kingdom.rulers", "location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from thailand speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> royalty.kingdom.rulers <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> royalty.kingdom.rulers <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.01563311740756035, -0.019285377115011215, -0.029232170432806015, -0.06328167021274567, -0.13637825846672058, -0.13799482583999634, -0.15473796427249908, -0.16010728478431702, -0.16278141736984253, -0.16612814366817474], "norm_scores": [0.10909330099821091, 0.10869558900594711, 0.10761977732181549, 0.10401705652475357, 0.0966850146651268, 0.09652883559465408, 0.09492610394954681, 0.09441777318716049, 0.0941656231880188, 0.0938510000705719]}}
{"id": "WebQTest-106", "question": "who did armie hammer play in the social network", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.performance.character"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.performance.character"], ["film.film.country", "people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did armie hammer play in the social network [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.performance.character </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.018954066559672356, -0.02700614184141159, -0.027962686493992805, -0.03343576937913895, -0.04168016090989113, -0.050002098083496094, -0.05250999331474304, -0.05362477898597717, -0.06573818624019623, -0.07814270257949829], "norm_scores": [0.10261314362287521, 0.10179021209478378, 0.10169289261102676, 0.10113783925771713, 0.10030744224786758, 0.099476158618927, 0.09922699630260468, 0.09911644458770752, 0.09792304784059525, 0.09671586751937866]}}
{"id": "WebQTest-107", "question": "what was thomas jefferson role in the declaration of independence", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.governmental_body", "government.government_office_or_title.office_holders"], ["book.author.book_editions_published"], ["book.written_work.subjects"], ["book.written_work.author"], ["government.government_position_held.governmental_body", "government.government_position_held.office_position_or_title"], ["book.author.book_editions_published", "people.person.profession"], ["government.governmental_body.members", "government.government_office_or_title.office_holders"], ["base.culturalevent.event.entity_involved", "people.person.profession"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was thomas jefferson role in the declaration of independence [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> book.author.book_editions_published <SEP> people.person.profession </PATH>", "<PATH> government.governmental_body.members <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.person.profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.02980918250977993, -0.031293366104364395, -0.0329793244600296, -0.03361532464623451, -0.03987406939268112, -0.04588044062256813, -0.048692040145397186, -0.051541518419981, -0.051795732229948044, -0.051839087158441544], "norm_scores": [0.1011955589056015, 0.10104547441005707, 0.10087525844573975, 0.10081112384796143, 0.10018213838338852, 0.09958221018314362, 0.09930262714624405, 0.09902007132768631, 0.0989948958158493, 0.09899059683084488]}}
{"id": "WebQTest-108", "question": "what was the book written by charles darwin", "prediction": [["book.written_work.subjects"], ["book.written_work.author"], ["book.book_subject.works"], ["book.book_edition.author_editor"], ["common.topic.notable_types"], [], ["book.author.works_written"], ["media_common.literary_genre.books_in_this_genre"], ["base.argumentmaps.innovator.original_ideas"], ["book.written_work.book_editions_published"]], "ground_paths": [["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"], [], ["base.argumentmaps.innovator.original_ideas"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the book written by charles darwin [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> media_common.literary_genre.books_in_this_genre </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> book.written_work.book_editions_published </PATH>"], "scores": [-0.013750042766332626, -0.017475241795182228, -0.05699220299720764, -0.06209701672196388, -0.06753181666135788, -0.0821404978632927, -0.10422340035438538, -0.1131768450140953, -0.11996499449014664, -0.12083913385868073], "norm_scores": [0.1063283309340477, 0.10593298077583313, 0.10182846337556839, 0.10130997002124786, 0.10076086968183517, 0.09929957985877991, 0.09713079780340195, 0.09626501053571701, 0.09561377018690109, 0.09553022682666779]}}
{"id": "WebQTest-109", "question": "who did tim tebow play college football for", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality", "location.location.containedby"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["common.topic.notable_types"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tim tebow play college football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.016086263582110405, -0.022332752123475075, -0.028061166405677795, -0.03650110960006714, -0.03818920627236366, -0.041137922555208206, -0.04461372271180153, -0.06223972141742706, -0.06722259521484375, -0.08000969141721725], "norm_scores": [0.10277431458234787, 0.1021343320608139, 0.10155094414949417, 0.10069746524095535, 0.10052761435508728, 0.10023162513971329, 0.09988384693861008, 0.0981387197971344, 0.0976509302854538, 0.09641019999980927]}}
{"id": "WebQTest-110", "question": "who does peyton manning play football for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["people.person.employment_history", "business.employer.employees"], ["sports.professional_sports_team.owner_s", "sports.sports_team.previously_known_as"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peyton manning play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.00967712327837944, -0.021171322092413902, -0.031035391613841057, -0.035967759788036346, -0.04890889674425125, -0.05262073501944542, -0.06174071133136749, -0.07619545608758926, -0.07902958244085312, -0.08565153926610947], "norm_scores": [0.10410445928573608, 0.1029147133231163, 0.10190454870462418, 0.101403146982193, 0.1000993400812149, 0.09972846508026123, 0.09882308542728424, 0.09740488976240158, 0.097129225730896, 0.09648817032575607]}}
{"id": "WebQTest-111", "question": "where is the carpathian mountain range located", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.contains"], ["geography.river.basin_countries"], ["geography.mountain_range.mountains"], ["location.location.primarily_containedby"], ["geography.mountain_pass.range"], ["base.locations.continents.countries_within"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the carpathian mountain range located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> geography.mountain_pass.range </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.016155105084180832, -0.016889257356524467, -0.03748007491230965, -0.06648945808410645, -0.08598934859037399, -0.1021655797958374, -0.10903317481279373, -0.12000279128551483, -0.13654236495494843, -0.13835851848125458], "norm_scores": [0.10679853707551956, 0.10672016441822052, 0.10454517602920532, 0.10155594348907471, 0.09959480166435242, 0.09799669682979584, 0.09732599556446075, 0.09626420587301254, 0.0946851298213005, 0.09451331943273544]}}
{"id": "WebQTest-112", "question": "what was robert burns", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"], ["influence.influence_node.influenced", "people.person.profession"], ["influence.influence_node.influenced"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["influence.influence_node.influenced_by"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["influence.influence_node.influenced_by", "common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> common.topic.notable_types </PATH>"], "scores": [-0.0037882537581026554, -0.04373690485954285, -0.06078343465924263, -0.07105091214179993, -0.08723709732294083, -0.08900667726993561, -0.09427729994058609, -0.096156544983387, -0.11794644594192505, -0.12782077491283417], "norm_scores": [0.10776607692241669, 0.10354582965373993, 0.10179569572210312, 0.10075584799051285, 0.09913812577724457, 0.09896284341812134, 0.09844262152910233, 0.09825780242681503, 0.09613993018865585, 0.09519527852535248]}}
{"id": "WebQTest-114", "question": "who did the voice of darth vader in episode 3", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice of darth vader in episode 3 [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.025564322248101234, -0.027204221114516258, -0.031525321304798126, -0.033859509974718094, -0.03701625019311905, -0.037970688194036484, -0.03975659981369972, -0.04562382400035858, -0.046071138232946396, -0.047581546008586884], "norm_scores": [0.10116937011480331, 0.10100359469652176, 0.10056809335947037, 0.10033362358808517, 0.10001739114522934, 0.09992197901010513, 0.09974368661642075, 0.09916018694639206, 0.09911583364009857, 0.09896624088287354]}}
{"id": "WebQTest-115", "question": "what super bowl did peyton manning win", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_championship.events"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_championship_event.champion"], ["sports.sports_team_roster.player", "sports.sports_championship_event.champion"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what super bowl did peyton manning win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.005586696323007345, -0.03775835409760475, -0.04411962255835533, -0.055224254727363586, -0.05939439311623573, -0.06041036918759346, -0.06144372373819351, -0.06560096144676208, -0.07388850301504135, -0.07475285232067108], "norm_scores": [0.10492126643657684, 0.10159949958324432, 0.10095524042844772, 0.09984038025140762, 0.09942489117383957, 0.09932393580675125, 0.09922134876251221, 0.09880971908569336, 0.09799420833587646, 0.09790954738855362]}}
{"id": "WebQTest-116", "question": "which country was justin bieber born in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["music.artist.origin"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["music.recording.artist", "music.artist.origin"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["music.recording_contribution.contributor", "music.artist.origin"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was justin bieber born in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> music.recording.artist <SEP> music.artist.origin </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> music.recording_contribution.contributor <SEP> music.artist.origin </PATH>"], "scores": [-0.0072303516790270805, -0.0265927966684103, -0.05423557758331299, -0.05629134178161621, -0.10560301691293716, -0.1210704818367958, -0.1295875459909439, -0.13036271929740906, -0.130989670753479, -0.13430465757846832], "norm_scores": [0.10847186297178268, 0.1063917800784111, 0.10349109768867493, 0.10327856242656708, 0.09830924868583679, 0.09680035710334778, 0.09597939997911453, 0.09590502828359604, 0.09584491699934006, 0.09552772343158722]}}
{"id": "WebQTest-118", "question": "who fought in the gulf war 1991", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["time.event.locations"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["military.casualties.military_conflict", "military.casualties.combatant"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["military.military_command.military_conflict", "military.military_command.military_combatant"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["military.military_conflict.casualties", "military.military_combatant.casualties"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["film.film_genre.films_in_this_genre", "film.film.country"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["time.event.locations"], ["film.film_subject.films", "tv.tv_program.country_of_origin"], ["film.film.subjects", "film.film.country"], ["film.film.genre", "film.film.country"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the gulf war 1991 [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>", "<PATH> time.event.includes_event <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> time.event.included_in_event <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.casualties.military_conflict <SEP> military.military_combatant.casualties </PATH>", "<PATH> military.casualties.military_conflict <SEP> military.casualties.combatant </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_combatant.casualties </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_command.military_combatant </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.020200444385409355, -0.029141224920749664, -0.03268294036388397, -0.033668942749500275, -0.03400048241019249, -0.036822814494371414, -0.04335050284862518, -0.04845263436436653, -0.04879743605852127, -0.04960474371910095], "norm_scores": [0.10175821930170059, 0.10085248202085495, 0.10049591213464737, 0.10039687901735306, 0.1003635972738266, 0.10008072853088379, 0.09942957013845444, 0.09892355650663376, 0.09888945519924164, 0.09880965203046799]}}
{"id": "WebQTest-119", "question": "who plays london tipton in suite life on deck", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.genre", "music.artist.genre"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays london tipton in suite life on deck [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.015272214077413082, -0.029613617807626724, -0.03257118538022041, -0.03611289709806442, -0.04307166486978531, -0.04493004083633423, -0.046752527356147766, -0.05112815275788307, -0.05145115777850151, -0.05527149885892868], "norm_scores": [0.10255996137857437, 0.10109960287809372, 0.10080103576183319, 0.10044465959072113, 0.09974811226129532, 0.09956292062997818, 0.09938163310289383, 0.09894772619009018, 0.09891577064990997, 0.0985385924577713]}}
{"id": "WebQTest-121", "question": "what team did david beckham play for in 2011", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["book.written_work.subjects", "book.written_work.subjects"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for in 2011 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.022395582869648933, -0.024841129779815674, -0.028724931180477142, -0.028999799862504005, -0.03079143352806568, -0.035129811614751816, -0.035731613636016846, -0.036196716129779816, -0.040539223700761795, -0.04303421080112457], "norm_scores": [0.10102754831314087, 0.10078077763319016, 0.10039012879133224, 0.10036253184080124, 0.10018287599086761, 0.09974919259548187, 0.09968917816877365, 0.09964282810688019, 0.09921106696128845, 0.09896384179592133]}}
{"id": "WebQTest-122", "question": "where did francisco coronado come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.ethnicity.people_with_this_ethnicity"], ["people.person.nationality", "location.location.containedby"], ["people.ethnicity.people"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did francisco coronado come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.007554495241492987, -0.0224453192204237, -0.06633483618497849, -0.08066859841346741, -0.1156403049826622, -0.11949252337217331, -0.12722444534301758, -0.1274774819612503, -0.1321529895067215, -0.13279198110103607], "norm_scores": [0.1088302880525589, 0.10722171515226364, 0.10261759161949158, 0.10115718096494675, 0.09768068790435791, 0.09730512648820877, 0.0965556725859642, 0.09653123468160629, 0.0960809513926506, 0.09601958096027374]}}
{"id": "WebQTest-123", "question": "what all does google now do", "prediction": [["organization.organization.sectors"], ["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.sectors"], ["business.brand.products"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["business.business_operation.industry"], ["organization.organization_scope.organizations_with_this_scope"], ["computer.software_developer.software"], ["law.invention.inventor"]], "ground_paths": [["computer.software.developer"], ["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["business.consumer_company.products", "business.consumer_product.company"], ["business.consumer_company.products"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"], ["computer.software_developer.software"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["computer.computer_manufacturer_brand.computer_models"], ["internet.website.owner"], ["projects.project_participation.participant", "projects.project_participation.project"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all does google now do [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.sectors </PATH>", "<PATH> business.brand.products </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> business.business_operation.industry </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> computer.software_developer.software </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.042816560715436935, -0.04594121128320694, -0.04690312221646309, -0.05337795615196228, -0.055692773312330246, -0.058159828186035156, -0.06006702035665512, -0.06539422273635864, -0.0674901157617569, -0.07884716242551804], "norm_scores": [0.10147041827440262, 0.10115385800600052, 0.10105659067630768, 0.10040438920259476, 0.10017223656177521, 0.09992540627717972, 0.09973501414060593, 0.09920511394739151, 0.09899741411209106, 0.09787946194410324]}}
{"id": "WebQTest-124", "question": "what language does egyptian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.main_country"], ["people.ethnicity.languages_spoken", "common.topic.notable_types", "common.topic.notable_types"], ["location.country.official_language"], ["people.ethnicity.people", "people.person.nationality", "location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does egyptian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.03664487972855568, -0.03676772117614746, -0.03796815872192383, -0.04050248861312866, -0.040603410452604294, -0.045030627399683, -0.04549550637602806, -0.047608938068151474, -0.04762320592999458, -0.048872459679841995], "norm_scores": [0.1006075069308281, 0.1005951464176178, 0.10047446191310883, 0.10022015124559402, 0.1002100333571434, 0.09976736456155777, 0.09972099959850311, 0.09951046109199524, 0.09950904548168182, 0.09938480705022812]}}
{"id": "WebQTest-125", "question": "what are abraham sons names", "prediction": [["people.person.children"], ["people.person.parents"], ["fictional_universe.fictional_character.parents"], ["people.person.sibling_s"], ["fictional_universe.fictional_character.children"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["fictional_universe.sibling_relationship.sibling", "fictional_universe.sibling_relationship.sibling"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are abraham sons names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.person.sibling_s </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>", "<PATH> people.sibling_relationship.sibling <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.person.sibling_s <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> fictional_universe.sibling_relationship.sibling <SEP> fictional_universe.sibling_relationship.sibling </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.005857801530510187, -0.030298765748739243, -0.09314686805009842, -0.09762214869260788, -0.10625357925891876, -0.10953609645366669, -0.11848640441894531, -0.12915180623531342, -0.13542643189430237, -0.13862212002277374], "norm_scores": [0.10938269644975662, 0.10674168169498444, 0.10023963451385498, 0.09979203343391418, 0.09893439710140228, 0.09861017763614655, 0.09773152321577072, 0.096694715321064, 0.09608988463878632, 0.0957833006978035]}}
{"id": "WebQTest-126", "question": "who wrote the jana gana mana", "prediction": [["music.composition.composer"], ["music.composition.lyricist"], ["music.composer.compositions"], ["music.lyricist.lyrics_written"], ["people.person.national_anthem"], ["music.recording.artist"], ["common.topic.notable_types"], ["music.composition.language"], ["people.person.gender"], ["people.person.compositions"]], "ground_paths": [["music.composition.composer"], ["music.lyricist.lyrics_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the jana gana mana [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>", "<PATH> people.person.national_anthem </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.composition.language </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> people.person.compositions </PATH>"], "scores": [-0.008282833732664585, -0.028677698224782944, -0.043203216046094894, -0.07965493947267532, -0.09909737855195999, -0.10231314599514008, -0.11036496609449387, -0.11774051934480667, -0.12531398236751556, -0.13060687482357025], "norm_scores": [0.10783128440380096, 0.10565435886383057, 0.10413076728582382, 0.10040336102247238, 0.09847013652324677, 0.0981539860367775, 0.09736683964729309, 0.0966513454914093, 0.09592212736606598, 0.09541576355695724]}}
{"id": "WebQTest-127", "question": "what type of cancer did gilda radner die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.parent_cause_of_death"], ["people.cause_of_death.people", "people.deceased_person.cause_of_death"], ["people.cause_of_death.people", "medicine.disease.notable_people_with_this_condition"], ["film.film.subjects", "film.film_subject.films"], ["people.person.cause_of_death"], ["film.film.subjects", "film.film.subjects"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did gilda radner die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.parent_cause_of_death </PATH>", "<PATH> people.cause_of_death.people <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> film.film.subjects <SEP> film.film_subject.films </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> film.film.subjects <SEP> film.film.subjects </PATH>"], "scores": [-0.008225803263485432, -0.026472067460417747, -0.028329282999038696, -0.05563928931951523, -0.10460082441568375, -0.1092013269662857, -0.1128450259566307, -0.11318937689065933, -0.11594286561012268, -0.11791978776454926], "norm_scores": [0.10726388543844223, 0.10532446950674057, 0.10512903332710266, 0.10229681432247162, 0.09740884602069855, 0.09696174412965775, 0.09660908579826355, 0.09657581895589828, 0.09631027281284332, 0.09612005949020386]}}
{"id": "WebQTest-128", "question": "who plays juni cortez", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays juni cortez [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.020575398579239845, -0.029411546885967255, -0.032621320337057114, -0.035896096378564835, -0.03940091282129288, -0.04298623278737068, -0.04589152708649635, -0.04653448611497879, -0.058188434690237045, -0.09008809179067612], "norm_scores": [0.10236960649490356, 0.10146903991699219, 0.10114387422800064, 0.10081319510936737, 0.1004604771733284, 0.10010093450546265, 0.09981053322553635, 0.09974638372659683, 0.09859068691730499, 0.0954953134059906]}}
{"id": "WebQTest-129", "question": "what was lucille ball", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["influence.influence_node.influenced_by"], ["common.topic.image"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.character"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lucille ball [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.character </PATH>"], "scores": [-0.0015120835741981864, -0.06353229284286499, -0.07349564880132675, -0.09318219870328903, -0.12415927648544312, -0.13051249086856842, -0.14089447259902954, -0.14385931193828583, -0.151073157787323, -0.1515912115573883], "norm_scores": [0.11104564368724823, 0.10436779260635376, 0.10333309322595596, 0.10131871700286865, 0.09822826832532883, 0.09760619699954987, 0.09659808874130249, 0.09631211310625076, 0.09561982750892639, 0.09557031095027924]}}
{"id": "WebQTest-131", "question": "who inspired obama", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["people.person.profession"], ["people.person.gender", "people.person.gender"], ["people.person.parents"], ["people.person.nationality"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["base.americancomedy.impersonated_celebrity.impersonated_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who inspired obama [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> base.americancomedy.impersonated_celebrity.impersonated_by </PATH>"], "scores": [-0.010130642913281918, -0.015915177762508392, -0.11115320026874542, -0.11568105965852737, -0.11707530915737152, -0.13961543142795563, -0.1416996866464615, -0.14536915719509125, -0.14754673838615417, -0.14779232442378998], "norm_scores": [0.11027394980192184, 0.10963790118694305, 0.09967801719903946, 0.09922770410776138, 0.09908945858478546, 0.0968809574842453, 0.09667923301458359, 0.09632512927055359, 0.0961155965924263, 0.09609199315309525]}}
{"id": "WebQTest-132", "question": "who is michael j fox wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.spouse_s", "people.marriage.spouse"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.parents", "people.person.parents"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.022270124405622482, -0.03634679690003395, -0.03875257819890976, -0.04007165506482124, -0.04504229128360748, -0.04529626667499542, -0.04634793847799301, -0.04710471257567406, -0.04820598289370537, -0.051984816789627075], "norm_scores": [0.10200382769107819, 0.10057801008224487, 0.10033633559942245, 0.10020406544208527, 0.0997072234749794, 0.09968190640211105, 0.09957712888717651, 0.09950180351734161, 0.09939228743314743, 0.09901741147041321]}}
{"id": "WebQTest-133", "question": "where did dolly parton grow up", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin", "location.location.people_born_here"], ["influence.influence_node.influenced_by", "location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced", "location.location.people_born_here"], ["music.artist.origin", "location.location.containedby"], ["music.recording.artist"], ["music.recording.artist", "location.location.people_born_here"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did dolly parton grow up [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin <SEP> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced <SEP> location.location.people_born_here </PATH>", "<PATH> music.artist.origin <SEP> location.location.containedby </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.recording.artist <SEP> location.location.people_born_here </PATH>"], "scores": [-0.008674586191773415, -0.03153187409043312, -0.032896168529987335, -0.09897090494632721, -0.11229129135608673, -0.11497043073177338, -0.12352306395769119, -0.12536533176898956, -0.13279764354228973, -0.13386590778827667], "norm_scores": [0.10852082073688507, 0.10606846213340759, 0.1059238463640213, 0.09915117174386978, 0.09783919900655746, 0.09757742285728455, 0.09674644470214844, 0.09656837582588196, 0.0958532989025116, 0.09575096517801285]}}
{"id": "WebQTest-134", "question": "what type of books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"], ["book.author.book_editions_published"], ["book.book_subject.works"], ["book.book.genre"], ["book.writer.works_written"], ["film.film.genre"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.book.genre </PATH>", "<PATH> book.writer.works_written </PATH>", "<PATH> film.film.genre </PATH>"], "scores": [-0.010636189952492714, -0.021950148046016693, -0.041253432631492615, -0.05732779949903488, -0.07612483203411102, -0.09699040651321411, -0.1191682294011116, -0.12718603014945984, -0.1279999166727066, -0.1287086457014084], "norm_scores": [0.10715947300195694, 0.10595390945672989, 0.10392826050519943, 0.10227103531360626, 0.10036659985780716, 0.09829408675432205, 0.096138134598732, 0.09537040442228317, 0.09529281407594681, 0.09522530436515808]}}
{"id": "WebQTest-138", "question": "what are the four main languages spoken in spain", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["base.rosetta.languoid.languoid_class"], ["location.country.currency_formerly_used"], ["location.country.main_languages"], ["location.country.internet_tld"], ["location.country.currency_used"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four main languages spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.main_languages </PATH>", "<PATH> location.country.internet_tld </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.01753167062997818, -0.018418429419398308, -0.024735204875469208, -0.06109973415732384, -0.12604418396949768, -0.14326174557209015, -0.1553739607334137, -0.15905113518238068, -0.15914595127105713, -0.17226684093475342], "norm_scores": [0.10878793150186539, 0.1086915135383606, 0.10800709575414658, 0.10415001958608627, 0.0976010113954544, 0.09593494981527328, 0.09477997571229935, 0.09443210065364838, 0.09442313760519028, 0.09319231659173965]}}
{"id": "WebQTest-139", "question": "where is the nra headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.place_founded"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded"], ["location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.location"], ["organization.organization.headquarters", "location.mailing_address.street_address"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the nra headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.location </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.street_address </PATH>"], "scores": [-0.003643060801550746, -0.037996284663677216, -0.050831425935029984, -0.06733086705207825, -0.0698002427816391, -0.0711112841963768, -0.10654239356517792, -0.10681800544261932, -0.10941534489393234, -0.10960584878921509], "norm_scores": [0.10715297609567642, 0.10353443771600723, 0.10221405327320099, 0.10054142028093338, 0.10029344260692596, 0.10016204416751862, 0.0966753214597702, 0.0966486856341362, 0.09639797359704971, 0.09637962281703949]}}
{"id": "WebQTest-141", "question": "who is gimli 's father in the hobbit", "prediction": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.parents"], ["people.person.gender"], ["fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["people.person.parents"], ["people.person.children"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["fictional_universe.fictional_character.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is gimli 's father in the hobbit [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.02952183410525322, -0.03037889674305916, -0.03940495476126671, -0.039785172790288925, -0.04450997710227966, -0.045228734612464905, -0.04522981867194176, -0.045905403792858124, -0.0472341813147068, -0.049454305320978165], "norm_scores": [0.10121958702802658, 0.10113286972045898, 0.10022414475679398, 0.1001860499382019, 0.09971380233764648, 0.09964216500520706, 0.09964205324649811, 0.0995747521519661, 0.09944253414869308, 0.09922200441360474]}}
{"id": "WebQTest-142", "question": "who developed the tcp ip reference model", "prediction": [["law.invention.inventor"], ["base.biblioness.bibs_location.country", "people.person.nationality"], ["common.topic.notable_types"], ["internet.website.owner", "organization.organization_founder.organizations_founded"], ["symbols.namesake.named_after"], ["internet.website.owner"], ["common.topic.notable_types", "common.topic.notable_types"], ["computer.software.developer"], ["internet.website_category.sites", "people.person.profession"], ["influence.influence_node.influenced"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who developed the tcp ip reference model [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> internet.website.owner <SEP> organization.organization_founder.organizations_founded </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> internet.website.owner </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> computer.software.developer </PATH>", "<PATH> internet.website_category.sites <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.010522686876356602, -0.04852762818336487, -0.06098724901676178, -0.06800925731658936, -0.07323656976222992, -0.07591185718774796, -0.08071455359458923, -0.08532357960939407, -0.08650512993335724, -0.08700384199619293], "norm_scores": [0.10585492104291916, 0.1019074022769928, 0.10064554959535599, 0.09994129091501236, 0.09942023456096649, 0.09915460646152496, 0.0986795425415039, 0.09822577238082886, 0.09810978174209595, 0.09806086868047714]}}
{"id": "WebQTest-145", "question": "what college did albert einstein go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.employment_history", "business.employer.employees"], ["common.topic.notable_types"], ["business.employment_tenure.person", "business.employer.employees"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "organization.organization.founders"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.employment_history", "business.employer.employees"], ["business.employment_tenure.person", "business.employer.employees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did albert einstein go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.020781386643648148, -0.022344518452882767, -0.023273663595318794, -0.036347415298223495, -0.04765284061431885, -0.053422026336193085, -0.05482000112533569, -0.06018097698688507, -0.07523281127214432, -0.08511179685592651], "norm_scores": [0.10272766649723053, 0.10256721079349518, 0.1024719625711441, 0.10114099085330963, 0.10000398010015488, 0.0994287058711052, 0.09928980469703674, 0.09875893592834473, 0.09728356450796127, 0.09632723033428192]}}
{"id": "WebQTest-146", "question": "what team does jordan own", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team_owner.teams_owned"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.professional_sports_team.owner_s"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team.location"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jordan own [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team_owner.teams_owned </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.professional_sports_team.owner_s </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.003519457997754216, -0.044067174196243286, -0.04939848929643631, -0.053285423666238785, -0.07621869444847107, -0.08517776429653168, -0.09056178480386734, -0.09536787122488022, -0.09625266492366791, -0.09683841466903687], "norm_scores": [0.10672857612371445, 0.10248754173517227, 0.10194259136915207, 0.10154712200164795, 0.09924481809139252, 0.09835963696241379, 0.09783150255680084, 0.0973624438047409, 0.09727633744478226, 0.09721937775611877]}}
{"id": "WebQTest-147", "question": "who is the state governor of tennessee", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.administrative_division.country", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_children", "symbols.name_source.namesakes"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "ground_paths": [["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["base.locations.countries.states_provinces_within", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of tennessee [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> symbols.name_source.namesakes </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.appointed_by </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.020732877776026726, -0.025308215990662575, -0.027915647253394127, -0.028366947546601295, -0.0341237336397171, -0.037054069340229034, -0.043885111808776855, -0.050172094255685806, -0.05299428105354309, -0.05471185967326164], "norm_scores": [0.10168666392564774, 0.10122247785329819, 0.10095888376235962, 0.10091333836317062, 0.10033407062292099, 0.1000404879450798, 0.0993594378232956, 0.0987367257475853, 0.09845846891403198, 0.09828949719667435]}}
{"id": "WebQTest-149", "question": "what do portuguese people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.language_family"], ["language.human_language.dialects"], ["people.ethnicity.languages_spoken"], ["location.country.form_of_government"], ["common.topic.notable_types"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do portuguese people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.dialects </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.012957221828401089, -0.022523000836372375, -0.03343234211206436, -0.07233183830976486, -0.13400819897651672, -0.1612742841243744, -0.17600226402282715, -0.17627781629562378, -0.17641474306583405, -0.19830144941806793], "norm_scores": [0.1106242910027504, 0.10957112908363342, 0.10838227719068527, 0.10424721240997314, 0.09801188111305237, 0.09537558257579803, 0.09398119151592255, 0.09395530074834824, 0.09394244104623795, 0.09190867841243744]}}
{"id": "WebQTest-150", "question": "what language does cuba speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["book.written_work.subjects"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does cuba speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken <PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.007859772071242332, -0.021835505962371826, -0.07572244852781296, -0.0757981538772583, -0.17053520679473877, -0.20005814731121063, -0.20071236789226532, -0.2025202065706253, -0.20377323031425476, -0.21515709161758423], "norm_scores": [0.11347624659538269, 0.11190136522054672, 0.10603092610836029, 0.106022909283638, 0.09643971920013428, 0.09363415092229843, 0.09357290714979172, 0.0934039056301117, 0.09328693151473999, 0.09223099052906036]}}
{"id": "WebQTest-153", "question": "what are the sights to see in madrid", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"], ["film.film.featured_film_locations"], ["geography.lake.basin_countries"], ["transportation.bridge.locale"], ["film.film_festival.location"]], "ground_paths": [["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["common.topic.image"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sights to see in madrid [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> transportation.bridge.locale </PATH>", "<PATH> film.film_festival.location </PATH>"], "scores": [-0.017066715285182, -0.01910591870546341, -0.020215356722474098, -0.07962467521429062, -0.11903058737516403, -0.12428148835897446, -0.12531325221061707, -0.13529111444950104, -0.1376025676727295, -0.1423989087343216], "norm_scores": [0.10764118283987045, 0.10742190480232239, 0.10730278491973877, 0.10111366957426071, 0.09720667451620102, 0.09669759124517441, 0.09659788012504578, 0.09563883394002914, 0.09541801363229752, 0.09496144950389862]}}
{"id": "WebQTest-154", "question": "what instruments did louis armstrong play", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.musical_group.member", "music.group_membership.role"], ["music.instrument.family"], ["music.artist.instruments_played"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["common.topic.image", "music.group_member.instruments_played"], ["music.track_contribution.role", "music.track_contribution.role"], ["music.artist.track_contributions"], ["music.music_group.member", "music.group_membership.role"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.artist.instruments_played </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> common.topic.image <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.track_contribution.role <SEP> music.track_contribution.role </PATH>", "<PATH> music.artist.track_contributions </PATH>", "<PATH> music.music_group.member <SEP> music.group_membership.role </PATH>"], "scores": [-0.0002650408132467419, -0.08417270332574844, -0.10385709255933762, -0.12057466059923172, -0.12275011837482452, -0.1231929138302803, -0.12621504068374634, -0.12772610783576965, -0.13194094598293304, -0.13583330810070038], "norm_scores": [0.11125162988901138, 0.10229767113924026, 0.10030369460582733, 0.0986407995223999, 0.0984264388680458, 0.0983828753232956, 0.09808599203824997, 0.09793789684772491, 0.09752596914768219, 0.09714709967374802]}}
{"id": "WebQTest-155", "question": "what year was george w bush elected", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["government.election_campaign.candidate"], ["time.event.locations", "location.location.events"], ["time.event.locations"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.election.winner"], ["time.event.includes_event"], ["time.event.locations", "time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was george w bush elected [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> government.election_campaign.candidate </PATH>", "<PATH> time.event.locations <SEP> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> government.election.winner </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.01497067604213953, -0.03390992805361748, -0.04172087833285332, -0.046581629663705826, -0.050574373453855515, -0.05210603401064873, -0.060746125876903534, -0.07148976624011993, -0.07166113704442978, -0.0748940259218216], "norm_scores": [0.10374172031879425, 0.10179542005062103, 0.10100339353084564, 0.10051364451646805, 0.10011311620473862, 0.09995989501476288, 0.09909994900226593, 0.09804095327854156, 0.0980241522192955, 0.09770776331424713]}}
{"id": "WebQTest-156", "question": "what time zone am i in california", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.states_provinces_within", "location.location.time_zones"], ["base.locations.states_and_provences.country", "location.location.time_zones"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["base.locations.countries.states_provinces_within"], ["base.biblioness.bibs_location.state", "location.location.time_zones"], ["location.location.time_zones", "location.location.time_zones"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone am i in california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.states_provinces_within <SEP> location.location.time_zones </PATH>", "<PATH> base.locations.states_and_provences.country <SEP> location.location.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.7171863873954862e-05, -0.13619698584079742, -0.15215463936328888, -0.15346747636795044, -0.156991109251976, -0.15844781696796417, -0.16340892016887665, -0.17289432883262634, -0.18144591152668, -0.18802355229854584], "norm_scores": [0.115597665309906, 0.10088042914867401, 0.09928338974714279, 0.09915313124656677, 0.09880436211824417, 0.09866053611040115, 0.09817229956388474, 0.0972454845905304, 0.09641742706298828, 0.09578531235456467]}}
{"id": "WebQTest-157", "question": "where is mission san buenaventura located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.containedby", "location.location.containedby"], ["location.location.nearby_airports"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby", "location.administrative_division.second_level_division_of"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mission san buenaventura located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby <SEP> location.administrative_division.second_level_division_of </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.0008681218605488539, -0.062498729676008224, -0.09102048724889755, -0.1295745074748993, -0.13807249069213867, -0.15114150941371918, -0.15817850828170776, -0.15830287337303162, -0.17538198828697205, -0.18086853623390198], "norm_scores": [0.11300168931484222, 0.10624759644269943, 0.10326002538204193, 0.09935471415519714, 0.09851397573947906, 0.09723487496376038, 0.09655303508043289, 0.09654101729393005, 0.09490619599819183, 0.09438690543174744]}}
{"id": "WebQTest-159", "question": "what time in hilo hawaii", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["time.location.time_zones"], ["location.location.containedby"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.contains"], ["base.biblioness.bibs_location.country", "location.location.time_zones"], ["time.event.locations"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in hilo hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> time.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> time.event.locations </PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-7.563122198916972e-05, -0.09453705698251724, -0.15309998393058777, -0.15703514218330383, -0.1783958226442337, -0.17873340845108032, -0.17988084256649017, -0.1863458752632141, -0.1872488409280777, -0.1911994069814682], "norm_scores": [0.1160551980137825, 0.10559430718421936, 0.09958799183368683, 0.09919686615467072, 0.09710042923688889, 0.09706765413284302, 0.09695633500814438, 0.09633152931928635, 0.09624458849430084, 0.09586511552333832]}}
{"id": "WebQTest-160", "question": "where did rudolf virchow conduct his research", "prediction": [["people.deceased_person.place_of_death", "location.location.containedby"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.employment_history", "business.employer.employees"], ["people.deceased_person.place_of_death"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_burial", "location.location.containedby"], ["influence.influence_node.influenced_by", "organization.organization.founders"]], "ground_paths": [["people.person.employment_history", "business.employment_tenure.company"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["business.employment_tenure.person", "business.employer.employees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rudolf virchow conduct his research [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_burial <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> organization.organization.founders </PATH>"], "scores": [-0.012303841300308704, -0.024859067052602768, -0.03362181782722473, -0.03575776517391205, -0.04023396223783493, -0.045086950063705444, -0.07167430222034454, -0.07458881288766861, -0.0813598558306694, -0.08897721767425537], "norm_scores": [0.10389724373817444, 0.10260094702243805, 0.10170581191778183, 0.10148880630731583, 0.10103553533554077, 0.10054639726877213, 0.09790836274623871, 0.09762341529130936, 0.09696464240550995, 0.09622883796691895]}}
{"id": "WebQTest-161", "question": "what university did gordon brown attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did gordon brown attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.01427322719246149, -0.019455209374427795, -0.02284756302833557, -0.07249091565608978, -0.08781567215919495, -0.09132539480924606, -0.09192448109388351, -0.0999976247549057, -0.11696386337280273, -0.11705026030540466], "norm_scores": [0.10601592808961868, 0.10546797513961792, 0.10511079430580139, 0.1000201478600502, 0.09849905222654343, 0.09815395623445511, 0.09809517860412598, 0.09730642288923264, 0.09566942602396011, 0.09566115587949753]}}
{"id": "WebQTest-162", "question": "what countries does greece share borders with", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["media_common.netflix_genre.titles", "film.film.country"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["time.event.locations", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.location.containedby", "base.locations.continents.countries_within"], ["location.country.official_language", "location.country.languages_spoken"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["media_common.netflix_title.netflix_genres", "film.film.country"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.country.form_of_government", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.containedby", "location.location.containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.containedby", "base.locations.countries.continent"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.containedby", "location.location.partially_containedby"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.events", "location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does greece share borders with [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.028878163546323776, -0.03580200672149658, -0.0381975919008255, -0.040604837238788605, -0.04258222132921219, -0.046099647879600525, -0.04750918969511986, -0.0480382926762104, -0.048469845205545425, -0.04885014519095421], "norm_scores": [0.10136981308460236, 0.10067036002874374, 0.1004294902086258, 0.10018802434206009, 0.0999901071190834, 0.09963901340961456, 0.09949866682291031, 0.09944603592157364, 0.09940313547849655, 0.09936533868312836]}}
{"id": "WebQTest-163", "question": "what year did the seahawks win the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship"], ["sports.sports_championship_event.season"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the seahawks win the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship </PATH>", "<PATH> sports.sports_championship_event.season </PATH>"], "scores": [-0.011130345985293388, -0.014425634406507015, -0.046311862766742706, -0.06740085780620575, -0.07021011412143707, -0.08532462269067764, -0.10970071703195572, -0.13525108993053436, -0.1399059146642685, -0.1427452117204666], "norm_scores": [0.10725238174200058, 0.10689954459667206, 0.1035446897149086, 0.10138390213251114, 0.10109948366880417, 0.09958291053771973, 0.09718480706214905, 0.09473315626382828, 0.09429322183132172, 0.0940258726477623]}}
{"id": "WebQTest-164", "question": "what did stephen hawking become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["people.profession.people_with_this_profession"], ["symbols.name_source.namesakes"], ["influence.influence_node.influenced_by", "people.person.profession"], ["base.argumentmaps.innovator.original_ideas"], ["medicine.notable_person_with_medical_condition.condition"], ["symbols.namesake.named_after"], ["base.popstra.celebrity.notable_person_with_this_condition"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.popstra.celebrity.notable_person_with_this_condition </PATH>"], "scores": [-0.002122650621458888, -0.05014552175998688, -0.07373373210430145, -0.08008905500173569, -0.10075898468494415, -0.1026686280965805, -0.10720015317201614, -0.10797380656003952, -0.11329706013202667, -0.1200246810913086], "norm_scores": [0.10866264253854752, 0.10356765985488892, 0.10115327686071396, 0.10051245242357254, 0.09845619648694992, 0.09826835989952087, 0.09782405197620392, 0.09774840623140335, 0.09722944349050522, 0.09657751768827438]}}
{"id": "WebQTest-165", "question": "who plays donna noble", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["fictional_universe.fictional_character.ethnicity", "people.person.ethnicity"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays donna noble [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.022286761552095413, -0.027864970266819, -0.03484354540705681, -0.03885255008935928, -0.04392911493778229, -0.04415431246161461, -0.04623113572597504, -0.0544440783560276, -0.05864730104804039, -0.0695764347910881], "norm_scores": [0.10219423472881317, 0.10162575542926788, 0.10091902315616608, 0.10051524639129639, 0.10000627487897873, 0.09998375177383423, 0.09977632015943527, 0.09896022081375122, 0.098545141518116, 0.09747399389743805]}}
{"id": "WebQTest-166", "question": "what was dr seuss education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.degree"], ["book.written_work.author"], ["people.education.education", "education.education.institution"], ["book.book_subject.works", "education.education.student", "education.education.institution"], ["people.person.profession", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> people.education.education <SEP> education.education.institution </PATH>", "<PATH> book.book_subject.works <SEP> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.profession <SEP> education.education.institution </PATH>"], "scores": [-0.012480638921260834, -0.013544279150664806, -0.04445462301373482, -0.05742309242486954, -0.09668178111314774, -0.10317917913198471, -0.12614180147647858, -0.13210457563400269, -0.13279762864112854, -0.14258623123168945], "norm_scores": [0.10752090066671371, 0.10740659385919571, 0.10413741320371628, 0.10279562324285507, 0.0988381952047348, 0.09819809347391129, 0.09596889466047287, 0.09539835900068283, 0.09533225744962692, 0.09440364688634872]}}
{"id": "WebQTest-167", "question": "where did queensland get its name from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.armorial_grant.coat_of_arms", "symbols.coat_of_arms_bearer.coat_of_arms_used"], ["symbols.armorial_grant.coat_of_arms", "symbols.armorial_grant.armiger"], ["symbols.name_source.namesakes", "symbols.namesake.named_after"], ["symbols.armorial_grant.coat_of_arms", "symbols.armorial_grant.coat_of_arms"], ["symbols.armorial_grant.armiger", "symbols.armorial_grant.armiger"], ["location.location.containedby"], ["symbols.namesake.named_after", "symbols.namesake.named_after"], ["symbols.coat_of_arms.bearers", "symbols.coat_of_arms_bearer.coat_of_arms_used"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did queensland get its name from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.armorial_grant.coat_of_arms <SEP> symbols.coat_of_arms_bearer.coat_of_arms_used </PATH>", "<PATH> symbols.armorial_grant.coat_of_arms <SEP> symbols.armorial_grant.armiger </PATH>", "<PATH> symbols.name_source.namesakes <SEP> symbols.namesake.named_after </PATH>", "<PATH> symbols.armorial_grant.coat_of_arms <SEP> symbols.armorial_grant.coat_of_arms </PATH>", "<PATH> symbols.armorial_grant.armiger <SEP> symbols.armorial_grant.armiger </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> symbols.namesake.named_after </PATH>", "<PATH> symbols.coat_of_arms.bearers <SEP> symbols.coat_of_arms_bearer.coat_of_arms_used </PATH>"], "scores": [-0.0066577414982020855, -0.027140827849507332, -0.0761827826499939, -0.07688095420598984, -0.07952986657619476, -0.08057153970003128, -0.08094820380210876, -0.08393026888370514, -0.08401860296726227, -0.08851291239261627], "norm_scores": [0.10633523762226105, 0.10417931526899338, 0.09919342398643494, 0.09912418574094772, 0.09886197000741959, 0.09875904023647308, 0.09872183948755264, 0.09842789173126221, 0.0984191969037056, 0.09797785431146622]}}
{"id": "WebQTest-168", "question": "what document did james madison write", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["government.government_position_held.office_holder", "government.government_position_held.basic_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["book.written_work.subjects"], ["government.politician.government_positions_held", "government.political_district.representatives"], ["people.person.nationality", "book.written_work.subjects"]], "ground_paths": [["book.written_work.author"], ["book.book_subject.works"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did james madison write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.basic_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.basic_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.political_district.representatives </PATH>", "<PATH> people.person.nationality <SEP> book.written_work.subjects </PATH>"], "scores": [-0.012870005331933498, -0.017041794955730438, -0.05672980099916458, -0.06747782230377197, -0.07662195712327957, -0.08569469302892685, -0.08741582185029984, -0.08743216097354889, -0.09446745365858078, -0.09735054522752762], "norm_scores": [0.10565541684627533, 0.10521556437015533, 0.10112154483795166, 0.1000405102968216, 0.09912989288568497, 0.09823458641767502, 0.09806565195322037, 0.09806405007839203, 0.09737656265497208, 0.09709621965885162]}}
{"id": "WebQTest-169", "question": "what are the three official languages of belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["book.newspaper.circulation_areas", "book.periodical.language"], ["base.rosetta.languoid.languoid_class"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["book.newspaper.circulation_areas"], ["country.languages_spoken.countries_spoken_in"], ["country.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the three official languages of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> country.languages_spoken.countries_spoken_in </PATH>", "<PATH> country.country.languages_spoken </PATH>"], "scores": [-0.009643974713981152, -0.020842935889959335, -0.03879765048623085, -0.08148348331451416, -0.13051077723503113, -0.14019200205802917, -0.14238767325878143, -0.15379150211811066, -0.1576356142759323, -0.15833328664302826], "norm_scores": [0.10964548587799072, 0.10842442512512207, 0.10649506002664566, 0.10204488784074783, 0.09716256707906723, 0.09622645378112793, 0.09601540863513947, 0.09492668509483337, 0.09456247091293335, 0.09449651837348938]}}
{"id": "WebQTest-170", "question": "who played bilbo in lord of the rings", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["people.person.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["film.film.starring", "film.actor.film"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>"], "scores": [-0.031198909506201744, -0.03233659639954567, -0.03432517126202583, -0.035321563482284546, -0.04026803374290466, -0.04042927548289299, -0.041356850415468216, -0.041370976716279984, -0.04560752958059311, -0.0479639507830143], "norm_scores": [0.10078355669975281, 0.10066895931959152, 0.10046897083520889, 0.1003689169883728, 0.09987366944551468, 0.09985756874084473, 0.09976498037576675, 0.09976357221603394, 0.09934181720018387, 0.0991080030798912]}}
{"id": "WebQTest-171", "question": "what airport do you fly into to get to destin fl", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["location.location.nearby_airports", "location.location.containedby"], ["location.location.nearby_airports", "location.location.nearby_airports"], ["location.location.nearby_airports", "aviation.airport.serves"], ["common.topic.notable_types"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into to get to destin fl [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> location.location.nearby_airports <SEP> location.location.containedby </PATH>", "<PATH> location.location.nearby_airports <SEP> location.location.nearby_airports </PATH>", "<PATH> location.location.nearby_airports <SEP> aviation.airport.serves </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.010987840592861176, -0.017896044999361038, -0.04682708904147148, -0.07058532536029816, -0.07887332886457443, -0.08367873728275299, -0.08561301231384277, -0.08843682706356049, -0.0902346670627594, -0.09379200637340546], "norm_scores": [0.10568345338106155, 0.10495588183403015, 0.10196290910243988, 0.09956900030374527, 0.09874717891216278, 0.09827379882335663, 0.09808389097452164, 0.09780731052160263, 0.09763163328170776, 0.09728493541479111]}}
{"id": "WebQTest-172", "question": "what do you call members of the senate", "prediction": [["government.government_office_or_title.governmental_body_if_any", "common.topic.notable_types", "common.topic.notable_types"], ["government.governmental_body.offices_positions", "common.topic.notable_types", "common.topic.notable_types"], ["government.government_position_held.governmental_body", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.governmental_body_if_any", "government.government_position_held.office_holder", "government.government_position_held.basic_title"], ["government.government_position_held.governmental_body", "government.government_position_held.basic_title"], ["government.government_office_or_title.governmental_body_if_any", "government.government_office_or_title.office_holders", "government.government_position_held.basic_title"], ["government.government_office_or_title.governmental_body_if_any", "government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.governmental_body", "government.government_office_or_title.office_holders"], ["government.government_office_or_title.governmental_body_if_any", "government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_office_or_title.governmental_body_if_any", "government.government_position_held.office_holder", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call members of the senate [/INST]", "raw_output": {"paths": ["<PATH> government.government_office_or_title.governmental_body_if_any <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.governmental_body.offices_positions <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.governmental_body_if_any <SEP> government.government_position_held.office_holder <SEP> government.government_position_held.basic_title </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.basic_title </PATH>", "<PATH> government.government_office_or_title.governmental_body_if_any <SEP> government.government_office_or_title.office_holders <SEP> government.government_position_held.basic_title </PATH>", "<PATH> government.government_office_or_title.governmental_body_if_any <SEP> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.government_office_or_title.governmental_body_if_any <SEP> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.governmental_body_if_any <SEP> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.012302505783736706, -0.02012854442000389, -0.02092928998172283, -0.03293284401297569, -0.038123972713947296, -0.04067445173859596, -0.0408245287835598, -0.041108906269073486, -0.04583768919110298, -0.04963061958551407], "norm_scores": [0.10221188515424728, 0.10141509026288986, 0.10133391618728638, 0.10012482106685638, 0.09960640966892242, 0.09935268759727478, 0.09933777153491974, 0.0993095338344574, 0.09884102642536163, 0.09846684336662292]}}
{"id": "WebQTest-173", "question": "what countries are part of the baltic", "prediction": [["location.location.containedby"], ["location.location.contains"], ["common.topic.notable_types"], ["geography.body_of_water.islands"], ["base.locations.continents.countries_within"], ["geography.island.body_of_water", "location.location.containedby"], ["geography.island.island_group"], ["location.adjoining_relationship.adjoins"], ["location.location.events"], ["book.book_subject.works"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the baltic [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> geography.body_of_water.islands </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> geography.island.body_of_water <SEP> location.location.containedby </PATH>", "<PATH> geography.island.island_group </PATH>", "<PATH> location.adjoining_relationship.adjoins </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.01124953106045723, -0.019350668415427208, -0.052026744931936264, -0.09691426903009415, -0.10210692882537842, -0.11921009421348572, -0.12401378899812698, -0.126618891954422, -0.12701116502285004, -0.13854296505451202], "norm_scores": [0.10826905816793442, 0.10739549994468689, 0.10394295305013657, 0.09938038140535355, 0.09886566549539566, 0.09718912839889526, 0.09672338515520096, 0.09647173434495926, 0.09643390029668808, 0.09532823413610458]}}
{"id": "WebQTest-174", "question": "what team does colin kaepernick play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does colin kaepernick play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.013102194294333458, -0.0254374947398901, -0.025936618447303772, -0.03061269223690033, -0.04080197960138321, -0.049386315047740936, -0.050413161516189575, -0.05780015513300896, -0.058781616389751434, -0.05885285511612892], "norm_scores": [0.10282810032367706, 0.10156747698783875, 0.10151679813861847, 0.101043201982975, 0.10001886636018753, 0.0991639494895935, 0.09906218200922012, 0.09833310544490814, 0.09823664277791977, 0.09822963923215866]}}
{"id": "WebQTest-175", "question": "where did joe flacco attend college", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did joe flacco attend college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.016492782160639763, -0.020255059003829956, -0.022456716746091843, -0.03419667109847069, -0.052667684853076935, -0.054111383855342865, -0.07151878625154495, -0.07634537667036057, -0.09176327288150787, -0.1219075545668602], "norm_scores": [0.10399215668439865, 0.10360164195299149, 0.10337379574775696, 0.1021672934293747, 0.1002974882721901, 0.10015279054641724, 0.09842447191476822, 0.09795057028532028, 0.09645195305347443, 0.09358786791563034]}}
{"id": "WebQTest-176", "question": "who is emma stone father", "prediction": [["people.person.parents"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.gender"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is emma stone father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>"], "scores": [-0.007975379936397076, -0.02409227378666401, -0.10836561024188995, -0.11110864579677582, -0.12840794026851654, -0.13414083421230316, -0.13660290837287903, -0.1389889419078827, -0.13968057930469513, -0.14135242998600006], "norm_scores": [0.11029308289289474, 0.10852975398302078, 0.09975837171077728, 0.09948510676622391, 0.09777888655662537, 0.09721993654966354, 0.09698086977005005, 0.09674974530935287, 0.09668285399675369, 0.09652135521173477]}}
{"id": "WebQTest-177", "question": "where was theodore roosevelt buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["base.usnris.nris_listing.significant_person", "base.usnris.nris_listing.significant_level"], ["base.usnris.nris_listing.significant_person", "base.usnris.nris_listing.significant_person"], ["people.person.place_of_interment"], ["symbols.name_source.namesakes"], ["people.person.national_parks", "location.location.containedby"], ["base.usnris.nris_listing.significance_level", "base.usnris.nris_listing.significance_level"], ["base.usnris.nris_listing.significant_person", "base.usnris.nris_listing.significant_of"], ["base.inaugurations.inauguration.president", "time.event.locations"]], "ground_paths": [["people.place_of_interment.interred_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was theodore roosevelt buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> base.usnris.nris_listing.significant_person <SEP> base.usnris.nris_listing.significant_level </PATH>", "<PATH> base.usnris.nris_listing.significant_person <SEP> base.usnris.nris_listing.significant_person </PATH>", "<PATH> people.person.place_of_interment </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.person.national_parks <SEP> location.location.containedby </PATH>", "<PATH> base.usnris.nris_listing.significance_level <SEP> base.usnris.nris_listing.significance_level </PATH>", "<PATH> base.usnris.nris_listing.significant_person <SEP> base.usnris.nris_listing.significant_of </PATH>", "<PATH> base.inaugurations.inauguration.president <SEP> time.event.locations </PATH>"], "scores": [-0.01027149148285389, -0.023700322955846786, -0.05914318561553955, -0.0598803348839283, -0.06147691607475281, -0.06283149868249893, -0.06359535455703735, -0.07630807161331177, -0.07832756638526917, -0.08466488867998123], "norm_scores": [0.10486432909965515, 0.10346553474664688, 0.09986265748739243, 0.09978905320167542, 0.09962986409664154, 0.09949500858783722, 0.09941904246807098, 0.09816315025091171, 0.09796509891748428, 0.0973462387919426]}}
{"id": "WebQTest-178", "question": "who was the leader of the us during wwii", "prediction": [["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speaker_s"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["military.military_conflict.commanders", "military.military_command.military_commander"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["military.military_conflict.commanders", "military.military_commander.military_commands"], ["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["military.military_command.military_conflict", "military.military_command.military_commander"], ["military.military_command.military_conflict", "military.military_commander.military_commands"], ["military.military_person.participated_in_conflicts"]], "ground_paths": [["military.military_command.military_conflict", "military.military_commander.military_commands"], ["event.speech_topic.speeches_or_presentations_on_this_topic", "event.speech_or_presentation.speaker_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the us during wwii [/INST]", "raw_output": {"paths": ["<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speaker_s </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> military.military_conflict.commanders <SEP> military.military_command.military_commander </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> military.military_conflict.commanders <SEP> military.military_commander.military_commands </PATH>", "<PATH> freebase.type_profile.equivalent_topic <SEP> common.topic.notable_types </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_command.military_commander </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_commander.military_commands </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>"], "scores": [-0.016044622287154198, -0.025728749111294746, -0.027926962822675705, -0.028206367045640945, -0.03403950482606888, -0.034297674894332886, -0.034934695810079575, -0.05602934956550598, -0.06068491190671921, -0.06656941026449203], "norm_scores": [0.10225257277488708, 0.10126712173223495, 0.10104475915431976, 0.10101653635501862, 0.100429005920887, 0.1004030853509903, 0.1003391370177269, 0.09824468940496445, 0.09778837114572525, 0.09721462428569794]}}
{"id": "WebQTest-179", "question": "what artistic movement did henri matisse belong to", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"], ["visual_art.artist.associated_periods_or_movements"], ["film.film_genre.films_in_this_genre"], ["visual_art.visual_art_period_movement.associated_artists"], ["art.visual_artist.associated_periods_or_movements"], ["influence.influence_node.influenced_by"], ["art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic movement did henri matisse belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.artist.associated_periods_or_movements </PATH>", "<PATH> film.film_genre.films_in_this_genre </PATH>", "<PATH> visual_art.visual_art_period_movement.associated_artists </PATH>", "<PATH> art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.associated_artists </PATH>"], "scores": [-0.0013157122302800417, -0.04161994531750679, -0.10092470049858093, -0.10587829351425171, -0.1093171238899231, -0.11093494296073914, -0.11363919079303741, -0.11641710251569748, -0.12541620433330536, -0.13639385998249054], "norm_scores": [0.10986381024122238, 0.10552387684583664, 0.09944776445627213, 0.09895636141300201, 0.09861664474010468, 0.0984572321176529, 0.09819134324789047, 0.09791895002126694, 0.09704171121120453, 0.09598226100206375]}}
{"id": "WebQTest-180", "question": "where the missouri river ends", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"], ["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.partially_contains"], ["geography.river.origin"], ["location.location.containedby", "location.location.containedby"], ["geography.lake.basin_countries"], ["geography.body_of_water.mouth"], ["location.location.mouth"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where the missouri river ends [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> geography.body_of_water.mouth </PATH>", "<PATH> location.location.mouth </PATH>"], "scores": [-0.0036134885158389807, -0.05738382413983345, -0.05907552316784859, -0.0698915496468544, -0.07221289724111557, -0.1101222112774849, -0.13879090547561646, -0.14209109544754028, -0.1444820612668991, -0.14909572899341583], "norm_scores": [0.10941291600465775, 0.10368512570858002, 0.10350986570119858, 0.10239633172750473, 0.10215891897678375, 0.0983586311340332, 0.09557884931564331, 0.09526394307613373, 0.09503643959760666, 0.09459898620843887]}}
{"id": "WebQTest-181", "question": "what type of currency do they use in thailand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["royalty.kingdom.rulers", "location.country.currency_used"], ["royalty.monarch.kingdom", "location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in thailand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> royalty.kingdom.rulers <SEP> location.country.currency_used </PATH>", "<PATH> royalty.monarch.kingdom <SEP> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>"], "scores": [-0.009796773083508015, -0.01625048741698265, -0.1045878455042839, -0.1061316728591919, -0.1162782534956932, -0.11929774284362793, -0.12182610481977463, -0.1357361376285553, -0.1364230364561081, -0.1379023790359497], "norm_scores": [0.10937242954969406, 0.10866884887218475, 0.09948110580444336, 0.09932763874530792, 0.09832490235567093, 0.09802845865488052, 0.09778092801570892, 0.09643019735813141, 0.09636398404836655, 0.09622154384851456]}}
{"id": "WebQTest-182", "question": "who is khloe kardashian 's husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["people.marriage.spouse", "people.marriage.spouse"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.spouse_s", "people.marriage.spouse"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is khloe kardashian 's husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.breakup <SEP> base.popstra.celebrity.breakup </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.019239986315369606, -0.020478878170251846, -0.03496181592345238, -0.03565236181020737, -0.04646707698702812, -0.04655991122126579, -0.0491083487868309, -0.04987422004342079, -0.04996619001030922, -0.05207083746790886], "norm_scores": [0.10213543474674225, 0.10200897604227066, 0.10054223239421844, 0.1004728302359581, 0.09939210116863251, 0.09938286989927292, 0.09912993013858795, 0.09905403107404709, 0.09904492646455765, 0.09883669018745422]}}
{"id": "WebQTest-184", "question": "what state does romney live in", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["people.person.nationality", "location.administrative_division.country"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "location.administrative_division.first_level_division_of"], ["people.person.nationality", "location.country.first_level_divisions"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state does romney live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.first_level_division_of </PATH>", "<PATH> people.person.nationality <SEP> location.country.first_level_divisions </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.029079655185341835, -0.030278904363512993, -0.03608247637748718, -0.03821489214897156, -0.04120442643761635, -0.045156486332416534, -0.045769184827804565, -0.05494489520788193, -0.05628674849867821, -0.05656992644071579], "norm_scores": [0.10143337398767471, 0.10131180286407471, 0.10072553157806396, 0.10051096975803375, 0.10021094232797623, 0.09981568157672882, 0.09975454956293106, 0.09884341061115265, 0.09871086478233337, 0.09868291765451431]}}
{"id": "WebQTest-185", "question": "what type of government does france use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["olympics.olympic_participating_country.form_of_government"], ["film.film.country", "government.form_of_government.countries"], ["people.person.nationality", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["base.locations.countries.continent"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["geography.river.basin_countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does france use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> olympics.olympic_participating_country.form_of_government </PATH>", "<PATH> film.film.country <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.010945424437522888, -0.013301398605108261, -0.13514114916324615, -0.1503981202840805, -0.1569693237543106, -0.15765731036663055, -0.1657567173242569, -0.16689060628414154, -0.17107558250427246, -0.1724165976047516], "norm_scores": [0.11244199424982071, 0.11217739433050156, 0.09930954873561859, 0.09780588746070862, 0.0971652939915657, 0.0970984697341919, 0.09631520509719849, 0.09620606154203415, 0.09580426663160324, 0.09567588567733765]}}
{"id": "WebQTest-186", "question": "who rules denmark right now", "prediction": [["royalty.monarch.kingdom"], ["people.person.nationality"], ["royalty.kingdom.rulers"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["royalty.monarch.kingdom", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["royalty.royal_line.monarchs_from_this_line", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.political_appointer.appointees"], ["common.topic.notable_types"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who rules denmark right now [/INST]", "raw_output": {"paths": ["<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> royalty.kingdom.rulers </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> royalty.royal_line.monarchs_from_this_line <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.political_appointer.appointees </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.010698243044316769, -0.019435718655586243, -0.0658595934510231, -0.09485048055648804, -0.09494288265705109, -0.10526783019304276, -0.11569739878177643, -0.11928220093250275, -0.12900538742542267, -0.12983694672584534], "norm_scores": [0.10799798369407654, 0.10705846548080444, 0.10220199823379517, 0.09928160905838013, 0.09927242994308472, 0.09825272858142853, 0.09723331779241562, 0.09688538312911987, 0.09594792127609253, 0.09586815536022186]}}
{"id": "WebQTest-187", "question": "what did stephen hawking study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"], ["people.person.profession", "people.profession.part_of_professional_field"], ["education.education.student", "education.education.degree"], ["education.field_of_study.students_majoring", "education.field_of_study.students_major_field"], ["education.education.student", "education.education.field_of_study"], ["people.person.education", "education.field_of_study.students_major_field"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.field_of_study.students_major_field_of_study"]], "ground_paths": [["book.author.works_written", "book.written_work.subjects"], ["book.written_work.author", "book.written_work.subjects"], ["education.education.student", "education.education.major_field_of_study"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["people.person.profession", "people.professional_field.professions_in_this_field"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.profession <SEP> people.profession.part_of_professional_field </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> education.field_of_study.students_majoring <SEP> education.field_of_study.students_major_field </PATH>", "<PATH> education.education.student <SEP> education.education.field_of_study </PATH>", "<PATH> people.person.education <SEP> education.field_of_study.students_major_field </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.field_of_study.students_major_field_of_study </PATH>"], "scores": [-0.01908263936638832, -0.0203836802393198, -0.025270000100135803, -0.05158944055438042, -0.059467863291502, -0.063478484749794, -0.06965800374746323, -0.06989587098360062, -0.07521072030067444, -0.0790020301938057], "norm_scores": [0.1034560576081276, 0.10332154482603073, 0.10281790792942047, 0.10014710575342178, 0.09936121106147766, 0.09896350651979446, 0.09835384041070938, 0.09833045303821564, 0.09780921787023544, 0.09743910282850266]}}
{"id": "WebQTest-188", "question": "where did bristol palin go to school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["education.education.student", "education.educational_institution.subsidiary_or_constituent_schools"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did bristol palin go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.008771231397986412, -0.023767240345478058, -0.02754795178771019, -0.03337566927075386, -0.1053784042596817, -0.12352870404720306, -0.12816429138183594, -0.13143837451934814, -0.13187646865844727, -0.13198529183864594], "norm_scores": [0.1077357828617096, 0.10613223165273666, 0.10573173314332962, 0.10511735081672668, 0.09781467914581299, 0.09605532884597778, 0.0956110805273056, 0.09529855102300644, 0.09525681287050247, 0.09524644911289215]}}
{"id": "WebQTest-189", "question": "what region of the world is egypt associated with", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"], ["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["base.locations.continents.countries_within"], ["language.human_language.countries_spoken_in"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region of the world is egypt associated with [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.018423298373818398, -0.018871093168854713, -0.03669649735093117, -0.05933765694499016, -0.06768873333930969, -0.07856518775224686, -0.07962772995233536, -0.0806603953242302, -0.09037204086780548, -0.09254521876573563], "norm_scores": [0.10444580018520355, 0.10439904034137726, 0.1025545746088028, 0.1002587080001831, 0.09942492097616196, 0.09834939241409302, 0.09824495017528534, 0.0981435552239418, 0.09719502180814743, 0.09698403626680374]}}
{"id": "WebQTest-190", "question": "what did dr. seuss do", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by"], ["fictional_universe.fictional_character.occupation"], ["book.written_work.author"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["freebase.valuenotation.is_reviewed"], ["book.book.interior_illustrations_by", "people.person.profession"], ["book.book_edition.author_editor"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr. seuss do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> freebase.valuenotation.is_reviewed </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.profession </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.008195602335035801, -0.02994786761701107, -0.057713691145181656, -0.06599471718072891, -0.07547736912965775, -0.07913204282522202, -0.08063574135303497, -0.08141830563545227, -0.08147989958524704, -0.08969959616661072], "norm_scores": [0.10580841451883316, 0.10353168845176697, 0.10069659352302551, 0.09986616671085358, 0.09892363846302032, 0.09856277704238892, 0.09841466695070267, 0.09833768755197525, 0.09833163022994995, 0.0975266844034195]}}
{"id": "WebQTest-191", "question": "where did drew brees go to college wikianswers", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["common.topic.notable_types"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.player"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did drew brees go to college wikianswers [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.player </PATH>"], "scores": [-0.017404519021511078, -0.0184750035405159, -0.024160519242286682, -0.027002213522791862, -0.04076329618692398, -0.05345269665122032, -0.07059177756309509, -0.08433373272418976, -0.10440357774496078, -0.11031071096658707], "norm_scores": [0.10378210246562958, 0.10367107391357422, 0.10308331996202469, 0.10279080271720886, 0.10138597339391708, 0.1001075804233551, 0.09840644896030426, 0.09706339985132217, 0.09513476490974426, 0.0945744514465332]}}
{"id": "WebQTest-193", "question": "what school did ben roethlisberger go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.place_of_birth", "location.location.containedby"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["sports.education.student", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did ben roethlisberger go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> sports.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01856100559234619, -0.022245660424232483, -0.022644806653261185, -0.026162462309002876, -0.04098702222108841, -0.05697905272245407, -0.06916709989309311, -0.08152615278959274, -0.12817338109016418, -0.1351020634174347], "norm_scores": [0.10416001826524734, 0.10377693176269531, 0.10373552143573761, 0.10337125509977341, 0.10185012966394424, 0.10023429244756699, 0.09902004152536392, 0.09780377894639969, 0.09334628283977509, 0.0927017480134964]}}
{"id": "WebQTest-194", "question": "where did the iroquois indians come from", "prediction": [["people.ethnicity.geographic_distribution"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["people.ethnicity.geographic_distribution", "base.locations.states_and_provences.country"], ["people.ethnicity.geographic_distribution", "base.locations.continents.countries_within"], ["people.ethnicity.geographic_distribution", "people.ethnicity.geographic_distribution"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["people.ethnicity.geographic_distribution", "base.locations.countries.continent"], ["people.ethnicity.people", "people.person.place_of_birth"], ["people.ethnicity.geographic_distribution", "location.location.containedby"], ["people.ethnicity.geographic_distribution", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the iroquois indians come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.states_and_provences.country </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.continents.countries_within </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> people.ethnicity.geographic_distribution </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.countries.continent </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.place_of_birth </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> location.location.containedby </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.00020216783741489053, -0.1001506894826889, -0.10027646273374557, -0.10571373999118805, -0.108303502202034, -0.10886052250862122, -0.11421620845794678, -0.11685533821582794, -0.11739874631166458, -0.1208544448018074], "norm_scores": [0.11035112291574478, 0.0998549610376358, 0.09984240680932999, 0.09930100291967392, 0.09904418140649796, 0.09898901730775833, 0.09846028685569763, 0.09820077568292618, 0.0981474295258522, 0.097808837890625]}}
{"id": "WebQTest-195", "question": "what kind of guitar did george harrison use", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"], ["music.instrument.family"], ["music.group_member.instruments_played", "music.group_member.instruments_played"], ["music.musical_group.member", "music.group_membership.role"], ["music.group_membership.group", "music.group_membership.role"], ["music.group_member.instruments_played", "common.topic.notable_types", "common.topic.notable_types"], ["music.musical_group.member", "music.group_membership.group"], ["music.group_membership.member", "music.group_membership.role"]], "ground_paths": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did george harrison use [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> music.group_member.instruments_played <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.group_membership.member <SEP> music.group_membership.role </PATH>"], "scores": [-0.00929927546530962, -0.03404564782977104, -0.036528330296278, -0.04039300978183746, -0.07739425450563431, -0.08054032921791077, -0.09227372705936432, -0.09734497964382172, -0.10048560053110123, -0.10811062157154083], "norm_scores": [0.10595078766345978, 0.10336106270551682, 0.10310477018356323, 0.10270708054304123, 0.0989762395620346, 0.09866533428430557, 0.09751442074775696, 0.09702115505933762, 0.09671692550182343, 0.09598226100206375]}}
{"id": "WebQTest-196", "question": "what language do navajo people speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"], ["people.person.ethnicity", "people.person.nationality", "location.country.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do navajo people speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.0006176456226967275, -0.0812709853053093, -0.08405551314353943, -0.08700001984834671, -0.09453009068965912, -0.09667138010263443, -0.10043288767337799, -0.11479735374450684, -0.12032318115234375, -0.12234093248844147], "norm_scores": [0.10931160300970078, 0.10084141790866852, 0.10056101530790329, 0.10026535391807556, 0.09951318055391312, 0.099300317466259, 0.09892749786376953, 0.09751661866903305, 0.09697924554347992, 0.09678375720977783]}}
{"id": "WebQTest-197", "question": "what places make up new england", "prediction": [["location.location.contains"], ["location.location.containedby"], ["book.book_subject.works"], ["book.written_work.subjects"], ["base.locations.countries.states_provinces_within"], ["base.biblioness.bibs_location.state"], ["location.administrative_division.first_level_division_of"], ["location.location.partially_containedby"], ["common.topic.notable_types"], ["location.location.partially_contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places make up new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.009241113439202309, -0.02021419256925583, -0.12535299360752106, -0.13223311305046082, -0.1351911723613739, -0.13884767889976501, -0.1391592025756836, -0.1559331864118576, -0.15910887718200684, -0.16703850030899048], "norm_scores": [0.1113535538315773, 0.11013834178447723, 0.09914649277925491, 0.09846669435501099, 0.09817585349082947, 0.09781753271818161, 0.09778705984354019, 0.09616046398878098, 0.09585557132959366, 0.09509847313165665]}}
{"id": "WebQTest-199", "question": "what type of government does iraq have now", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["people.person.nationality", "government.form_of_government.countries"], ["base.locations.countries.continent"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["country.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does iraq have now [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> country.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.005662835203111172, -0.021317102015018463, -0.1297086924314499, -0.15200909972190857, -0.15470601618289948, -0.1670357882976532, -0.16771385073661804, -0.16799397766590118, -0.1704544723033905, -0.1725342869758606], "norm_scores": [0.11313359439373016, 0.11137636005878448, 0.09993535280227661, 0.09773141890764236, 0.09746820479631424, 0.09627381712198257, 0.09620855748653412, 0.09618161618709564, 0.09594525396823883, 0.09574590623378754]}}
{"id": "WebQTest-200", "question": "where was the vietnam war location", "prediction": [["time.event.locations"], ["location.location.events"], ["book.written_work.subjects"], ["book.book_subject.works"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties"], ["military.military_conflict.combatants"], ["book.book_subject.works", "location.location.events"], ["military.casualties.military_conflict"], ["book.book_subject.works", "time.event.locations"]], "ground_paths": [["location.location.events"], ["book.book_subject.works"], ["book.written_work.subjects"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the vietnam war location [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties </PATH>", "<PATH> military.military_conflict.combatants </PATH>", "<PATH> book.book_subject.works <SEP> location.location.events </PATH>", "<PATH> military.casualties.military_conflict </PATH>", "<PATH> book.book_subject.works <SEP> time.event.locations </PATH>"], "scores": [-0.008291088975965977, -0.03014380857348442, -0.04802674800157547, -0.07874702662229538, -0.08672504127025604, -0.13261346518993378, -0.1393895000219345, -0.14487800002098083, -0.1504654586315155, -0.1533997505903244], "norm_scores": [0.10915939509868622, 0.10679984092712402, 0.10490692406892776, 0.10173315554857254, 0.10092475265264511, 0.09639813005924225, 0.09574714303016663, 0.095223069190979, 0.09469250589609146, 0.09441505372524261]}}
{"id": "WebQTest-201", "question": "who did gerald ford select as his vice president when he became president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"], ["people.person.gender", "people.person.gender"], ["people.person.nationality", "people.person.nationality"], ["government.political_appointer.appointees", "government.politician.government_positions_held"], ["government.politician.government_positions_held", "government.politician.government_positions_held"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.profession", "common.topic.notable_types"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did gerald ford select as his vice president when he became president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.politician.government_positions_held </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>"], "scores": [-0.005817053373903036, -0.0200814176350832, -0.06765345484018326, -0.07762965559959412, -0.07902757078409195, -0.07910174131393433, -0.08758113533258438, -0.09064853936433792, -0.09463242441415787, -0.09828437119722366], "norm_scores": [0.10658514499664307, 0.10507556796073914, 0.10019394010305405, 0.09919935464859009, 0.0990607813000679, 0.09905343502759933, 0.09821707010269165, 0.09791626036167145, 0.09752695262432098, 0.09717144817113876]}}
{"id": "WebQTest-202", "question": "what is the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["finance.currency.countries_formerly_used"], ["location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["government.form_of_government.countries"], ["base.coinsdaily.coin_type.country"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.005144922062754631, -0.026958869770169258, -0.14260433614253998, -0.14341330528259277, -0.14776162803173065, -0.1564750373363495, -0.15740011632442474, -0.1756623387336731, -0.17633403837680817, -0.1789187490940094], "norm_scores": [0.11321625113487244, 0.11077330261468887, 0.09867586195468903, 0.09859607368707657, 0.0981682687997818, 0.09731660038232803, 0.09722662717103958, 0.09546715766191483, 0.09540305286645889, 0.09515678882598877]}}
{"id": "WebQTest-204", "question": "what is the capital of modern egypt", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.capital"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.country.administrative_divisions"], ["location.administrative_division.country"], ["location.administrative_division.first_level_division_of"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.010101121850311756, -0.02432060055434704, -0.046007633209228516, -0.06500044465065002, -0.08007141202688217, -0.09278669208288193, -0.09689117968082428, -0.10993290692567825, -0.11041730642318726, -0.1234075278043747], "norm_scores": [0.10672881454229355, 0.10522192716598511, 0.10296454280614853, 0.10102739930152893, 0.09951624274253845, 0.0982588678598404, 0.09785640239715576, 0.09658847004175186, 0.09654169529676437, 0.09529571235179901]}}
{"id": "WebQTest-205", "question": "what school did sir ernest rutherford go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did sir ernest rutherford go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.01359947957098484, -0.018504846841096878, -0.029337089508771896, -0.03495866805315018, -0.0523800328373909, -0.06365998089313507, -0.06371985375881195, -0.08169190585613251, -0.11591071635484695, -0.11793412268161774], "norm_scores": [0.1045975312590599, 0.10408569872379303, 0.10296429693698883, 0.10238710790872574, 0.10061882436275482, 0.0994902178645134, 0.09948427230119705, 0.09771230071783066, 0.094425268471241, 0.094234399497509]}}
{"id": "WebQTest-206", "question": "what position does brandon roy play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position_s"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.position_s"], ["sports.sports_position.sport", "sports.sports_team.sport", "basketball.basketball_player.position_s"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"], ["sports.sports_position.sport", "sports.sports_position.sport", "basketball.basketball_player.position_s"]], "ground_paths": [["basketball.basketball_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position does brandon roy play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position_s </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport <SEP> basketball.basketball_player.position_s </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_position.sport <SEP> basketball.basketball_player.position_s </PATH>"], "scores": [-0.009381763637065887, -0.03175715357065201, -0.03937731310725212, -0.0514480359852314, -0.05834176018834114, -0.05837039276957512, -0.06032806262373924, -0.065215565264225, -0.06693639606237411, -0.06747090071439743], "norm_scores": [0.1042187511920929, 0.10191270709037781, 0.10113906860351562, 0.09992558509111404, 0.09923909604549408, 0.09923625737428665, 0.09904216974973679, 0.09855929017066956, 0.09838982671499252, 0.09833725541830063]}}
{"id": "WebQTest-207", "question": "where is harvard university", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["education.educational_institution.parent_institution", "location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harvard university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.contains </PATH>"], "scores": [-0.003468790790066123, -0.048178210854530334, -0.06433700025081635, -0.07665057480335236, -0.07997572422027588, -0.08721337467432022, -0.10084721446037292, -0.1031523272395134, -0.10695735365152359, -0.11103852093219757], "norm_scores": [0.1077040359377861, 0.10299470275640488, 0.10134381055831909, 0.10010355710983276, 0.09977125376462936, 0.09905175119638443, 0.09771046042442322, 0.0974854826927185, 0.09711525589227676, 0.09671971946954727]}}
{"id": "WebQTest-209", "question": "where is laos in world map", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["geography.river.basin_countries"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.schemastaging.disputed_location_claimant.disputed_territories"], ["location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_children"], ["common.topic.notable_types"]], "ground_paths": [["book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is laos in world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.015237840823829174, -0.02753855288028717, -0.03437335044145584, -0.05338777229189873, -0.06881319731473923, -0.08292169123888016, -0.0904376432299614, -0.0918365865945816, -0.09505177289247513, -0.09628935903310776], "norm_scores": [0.10511893779039383, 0.10383381694555283, 0.10312654823064804, 0.1011841893196106, 0.09963535517454147, 0.09823951870203018, 0.09750392287969589, 0.09736761450767517, 0.09705506265163422, 0.09693502634763718]}}
{"id": "WebQTest-210", "question": "where is tyrese gibson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality"], ["music.artist.origin"], ["place.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here", "location.location.containedby"], ["location.place_lived.person", "location.place_lived.location"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tyrese gibson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> place.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> location.place_lived.person <SEP> location.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.005990439560264349, -0.02462421916425228, -0.07992815226316452, -0.08053212612867355, -0.10841333866119385, -0.11767449229955673, -0.1353176087141037, -0.14398761093616486, -0.15163911879062653, -0.15618793666362762], "norm_scores": [0.10976710170507431, 0.10774066299200058, 0.1019439548254013, 0.10188239067792892, 0.09908102452754974, 0.09816765785217285, 0.09645085781812668, 0.09561824798583984, 0.09488941729068756, 0.09445875883102417]}}
{"id": "WebQTest-211", "question": "what countries are near italy", "prediction": [["location.location.partially_containedby", "location.location.partially_contains"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_containedby"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["location.country.official_language", "location.country.official_language"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.locations.countries.continent", "location.location.contains_major_portion_of"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "geography.river.origin"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.country.official_language", "language.human_language.main_country"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.country.currency_used", "finance.currency.countries_used"], ["location.country.form_of_government", "government.form_of_government.countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["base.locations.countries.continent", "location.location.containedby"], ["travel.guidebook.travel_destinations_covered", "travel.travel_destination.guidebooks"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["organization.organization.founders", "base.nations.supranational_body.member"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "base.locations.countries.continent"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["fictional_universe.fictional_setting.works_set_here", "fictional_universe.work_of_fiction.setting"], ["location.location.containedby", "geography.river.basin_countries"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["location.country.currency_used", "location.country.currency_used"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["military.military_combatant.includes_allies", "military.military_combatant.includes_allies"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.time_zones", "location.location.time_zones"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["geography.river.basin_countries", "location.location.partially_contains"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are near italy [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.035676684230566025, -0.0357346348464489, -0.036712102591991425, -0.03742842748761177, -0.04017697274684906, -0.041535742580890656, -0.042736418545246124, -0.043261345475912094, -0.043341390788555145, -0.04504154250025749], "norm_scores": [0.10044923424720764, 0.10044340789318085, 0.10034528374671936, 0.10027343034744263, 0.09999819844961166, 0.09986241161823273, 0.0997425839304924, 0.09969024360179901, 0.0996822640299797, 0.09951293468475342]}}
{"id": "WebQTest-212", "question": "which states does the colorado river run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.contains"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_parent"], ["government.government_agency.jurisdiction"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby", "location.administrative_division.first_level_division_of"], ["location.location.partially_contains", "location.administrative_division.first_level_division_of"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which states does the colorado river run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> government.government_agency.jurisdiction </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.containedby <SEP> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.partially_contains <SEP> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.012339739128947258, -0.019164148718118668, -0.04009126126766205, -0.06958499550819397, -0.13572177290916443, -0.1435031294822693, -0.155327707529068, -0.1559164822101593, -0.1559373140335083, -0.15929675102233887], "norm_scores": [0.10948372632265091, 0.10873910784721375, 0.10648715496063232, 0.10339231789112091, 0.09677549451589584, 0.09602538496255875, 0.09489660710096359, 0.09484075009822845, 0.09483878314495087, 0.094520702958107]}}
{"id": "WebQTest-213", "question": "who did scarlett johansson date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.languages", "people.person.languages"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["film.film.executive_produced_by", "film.producer.films_executive_produced"], ["people.person.languages", "people.person.languages"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.children", "people.person.children"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did scarlett johansson date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.languages <SEP> people.person.languages </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.popstra.dated.participant <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> celebrities.romantic_relationship.celebrity <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.celebrity.sexual_relationships </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.romantic_relationship.celebrity </PATH>"], "scores": [-0.01703919656574726, -0.021895525977015495, -0.033827681094408035, -0.05060003325343132, -0.05166749656200409, -0.053753405809402466, -0.058873120695352554, -0.06113759055733681, -0.0615118071436882, -0.06153147295117378], "norm_scores": [0.10304717719554901, 0.10254795104265213, 0.10133160650730133, 0.09964621812105179, 0.09953990578651428, 0.09933248907327652, 0.09882523119449615, 0.09860169887542725, 0.09856480360031128, 0.09856287389993668]}}
{"id": "WebQTest-214", "question": "which continents were part of the roman empire", "prediction": [["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"], ["location.location.containedby", "location.location.containedby"], ["fictional_universe.fictional_language.where_spoken", "language.human_language.region"], ["location.location.containedby"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contains"], ["location.location.partially_containedby", "location.partial_containment_relationship.partially_contained_by"], ["location.location.contains"]], "ground_paths": [["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"], ["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.location.partiallycontains"], ["book.book_subject.works", "book.written_work.subjects"], ["location.country.official_language", "language.human_language.region"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which continents were part of the roman empire [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.region </PATH>", "<PATH> location.location.partially_contained_by <SEP> location.partial_containment_relationship.partially_contained_by </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_language.where_spoken <SEP> language.human_language.region </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contained_by <SEP> location.partial_containment_relationship.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.partial_containment_relationship.partially_contained_by </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01769939251244068, -0.03072095848619938, -0.03627609461545944, -0.039949238300323486, -0.04217972233891487, -0.055010225623846054, -0.05623907968401909, -0.05702971667051315, -0.057240720838308334, -0.05867408215999603], "norm_scores": [0.10276903212070465, 0.10143949091434479, 0.1008775532245636, 0.1005076915025711, 0.10028376430273056, 0.09900528937578201, 0.09888370335102081, 0.09880554676055908, 0.09878470748662949, 0.0986432135105133]}}
{"id": "WebQTest-215", "question": "where was elvis costello born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["music.recording.artist"], ["location.location.people_born_here", "location.location.containedby"], ["music.artist.label", "music.artist.label"], ["music.producer.releases_produced"], ["music.artist.label"], ["music.artist.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was elvis costello born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> music.artist.label </PATH>", "<PATH> music.artist.place_of_birth </PATH>"], "scores": [-0.016378356143832207, -0.01805264875292778, -0.03358692675828934, -0.10918435454368591, -0.11896394938230515, -0.13215439021587372, -0.13289673626422882, -0.13310399651527405, -0.13536378741264343, -0.13827663660049438], "norm_scores": [0.10824017226696014, 0.1080591008067131, 0.10639344900846481, 0.09864687919616699, 0.09768685698509216, 0.09640678018331528, 0.09633523970842361, 0.09631527215242386, 0.09609787166118622, 0.09581835567951202]}}
{"id": "WebQTest-216", "question": "where are the gobi desert located on a map", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"], ["location.location.partially_contains"], ["geography.river.basin_countries"], ["common.topic.notable_types"], ["base.locations.continents.countries_within"], ["location.location.containedby", "location.location.containedby"], ["common.image.appears_in_topic_gallery"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the gobi desert located on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> common.image.appears_in_topic_gallery </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.001540906960144639, -0.05883537977933884, -0.0767347440123558, -0.08342107385396957, -0.10404202342033386, -0.1052442118525505, -0.10847145318984985, -0.1165439561009407, -0.11717996746301651, -0.11882329732179642], "norm_scores": [0.10908200591802597, 0.10300787538290024, 0.10118051618337631, 0.10050623118877411, 0.09845492243766785, 0.09833663702011108, 0.0980197936296463, 0.09723172336816788, 0.097169890999794, 0.09701034426689148]}}
{"id": "WebQTest-217", "question": "what type of government does the us follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet", "government.form_of_government.countries"], ["base.locations.countries.planet", "base.aareas.schema.administrative_area.administrative_parent"], ["form.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["base.locations.countries.planet", "location.country.form_of_government"], ["base.locations.countries.planet"], ["language.human_language.countries_spoken_in"], ["locations.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does the us follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.planet <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> locations.country.form_of_government </PATH>"], "scores": [-0.006595117971301079, -0.020342227071523666, -0.1428987979888916, -0.14297828078269958, -0.15003208816051483, -0.15433934330940247, -0.15979479253292084, -0.16755810379981995, -0.16775429248809814, -0.16812151670455933], "norm_scores": [0.11271743476390839, 0.11117848753929138, 0.09835471212863922, 0.09834689646959305, 0.09765560925006866, 0.09723588824272156, 0.09670686721801758, 0.09595900028944016, 0.09594018757343292, 0.09590496122837067]}}
{"id": "WebQTest-218", "question": "in which continent is germany", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["location.location.events", "time.event.locations"], ["location.location.partially_containedby"], ["time.event.locations", "time.event.locations"], ["location.location.primarily_containedby"], ["geography.river.basin_countries"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which continent is germany [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> location.location.events <SEP> time.event.locations </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.007053358014672995, -0.03241494297981262, -0.05596962571144104, -0.06303208321332932, -0.09987910836935043, -0.10696378350257874, -0.11049903929233551, -0.1258278340101242, -0.12696047127246857, -0.12705695629119873], "norm_scores": [0.1080765500664711, 0.10537002980709076, 0.10291706770658493, 0.10219278186559677, 0.09849581867456436, 0.09780047088861465, 0.09745533019304276, 0.09597285091876984, 0.0958642065525055, 0.09585495293140411]}}
{"id": "WebQTest-220", "question": "what language turkey people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["food.beer_country_region.languages_spoken"], ["food.beer_country_region.beers_from_here"], ["base.rosetta.languoid.languoid_class"], ["film.film.featured_film_locations"], ["people.ethnicity.languages_spoken"], ["food.beer_country_country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language turkey people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> food.beer_country_region.languages_spoken </PATH>", "<PATH> food.beer_country_region.beers_from_here </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> food.beer_country_country.languages_spoken </PATH>"], "scores": [-0.007194220554083586, -0.030252965167164803, -0.052518997341394424, -0.057194724678993225, -0.10501586645841599, -0.11009906232357025, -0.1684611737728119, -0.18073631823062897, -0.18341855704784393, -0.18652811646461487], "norm_scores": [0.11038565635681152, 0.10786942392587662, 0.10549414157867432, 0.1050020381808281, 0.10009888559579849, 0.09959135204553604, 0.09394534677267075, 0.09279920905828476, 0.09255063533782959, 0.09226329624652863]}}
{"id": "WebQTest-221", "question": "who does jeremy lin play for", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["people.person.nationality", "sports.sports_team_location.teams"]], "ground_paths": [["people.person.place_of_birth", "sports.sports_team.location"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy lin play for [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>"], "scores": [-0.01395031251013279, -0.02490583062171936, -0.02558055706322193, -0.033169571310281754, -0.03654072433710098, -0.040002454072237015, -0.06744588166475296, -0.06901402771472931, -0.07057160884141922, -0.07587528973817825], "norm_scores": [0.1032024472951889, 0.10207798331975937, 0.1020091325044632, 0.10123790800571442, 0.10089720040559769, 0.10054852068424225, 0.09782664477825165, 0.09767335653305054, 0.0975213423371315, 0.09700549393892288]}}
{"id": "WebQTest-222", "question": "where is the seychelles on world map", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.contains"], ["base.locations.countries.continent"], ["geography.island.island_group"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["common.topic.notable_types"], ["book.written_work.subjects"], ["geography.island.body_of_water"]], "ground_paths": [["location.location.contains"], ["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seychelles on world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> geography.island.island_group </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> geography.island.body_of_water </PATH>"], "scores": [-0.0038939290679991245, -0.047802429646253586, -0.05337410792708397, -0.059307921677827835, -0.08299665153026581, -0.09919674694538116, -0.11121372133493423, -0.11208229511976242, -0.1166018471121788, -0.11789650470018387], "norm_scores": [0.10788340866565704, 0.10324889421463013, 0.10267522931098938, 0.10206777602434158, 0.09967833757400513, 0.09807654470205307, 0.09690502285957336, 0.0968208834528923, 0.09638428688049316, 0.09625957906246185]}}
{"id": "WebQTest-224", "question": "where did william morris go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.students_graduates"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did william morris go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.015377971343696117, -0.01547866128385067, -0.026971088722348213, -0.05809130147099495, -0.06021538004279137, -0.08607643097639084, -0.0919712632894516, -0.10424823313951492, -0.10517609119415283, -0.10599914938211441], "norm_scores": [0.10522809624671936, 0.10521750152111053, 0.10401521623134613, 0.10082809627056122, 0.10061415284872055, 0.09804552793502808, 0.097469262778759, 0.09627995640039444, 0.09619066119194031, 0.09611152112483978]}}
{"id": "WebQTest-225", "question": "what state is mount st. helens in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.schemastaging.disputed_location_claimant.disputed_territories"], ["geography.mountain.mountain_type"], ["location.mountain.mountain_type"], ["geography.mountain.mountain_range"], ["location.location.mountain"], ["base.locations.countries.states_provinces_within"], ["location.mountain.mountain_range"], ["base.schemastaging.disputed_location.claimed_by"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is mount st. helens in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories </PATH>", "<PATH> geography.mountain.mountain_type </PATH>", "<PATH> location.mountain.mountain_type </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> location.location.mountain </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> location.mountain.mountain_range </PATH>", "<PATH> base.schemastaging.disputed_location.claimed_by </PATH>"], "scores": [-0.003993851598352194, -0.032950595021247864, -0.13450658321380615, -0.14018478989601135, -0.14438068866729736, -0.14802084863185883, -0.14897660911083221, -0.15426315367221832, -0.16066232323646545, -0.16260983049869537], "norm_scores": [0.11247973144054413, 0.1092693880200386, 0.09871730953454971, 0.09815836697816849, 0.09774736315011978, 0.09739219397306442, 0.09729915857315063, 0.09678613394498825, 0.09616875648498535, 0.0959816575050354]}}
{"id": "WebQTest-226", "question": "who is jimmy savile", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["common.topic.image"], ["common.image.appears_in_topic_gallery"], ["influence.influence_node.influenced_by"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by", "people.person.profession"], ["influence.influence_node.influenced"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jimmy savile [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> common.image.appears_in_topic_gallery </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0007477255421690643, -0.07349993288516998, -0.07771053910255432, -0.15606579184532166, -0.15681865811347961, -0.16199392080307007, -0.16446082293987274, -0.1665896326303482, -0.1688627302646637, -0.17026962339878082], "norm_scores": [0.11358511447906494, 0.10561498999595642, 0.10517121851444244, 0.09724508225917816, 0.09717190265655518, 0.09667031466960907, 0.09643212705850601, 0.09622705727815628, 0.09600857645273209, 0.09587358683347702]}}
{"id": "WebQTest-227", "question": "where were the seneca indians located", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.contains"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.biblioness.bibs_location.country"], ["location.location.people_ethnicities"], ["people.ethnicity.geographic_distribution", "base.locations.continents.countries_within"], ["location.ethnicity.geographic_distribution"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the seneca indians located [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_ethnicities </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.continents.countries_within </PATH>", "<PATH> location.ethnicity.geographic_distribution </PATH>"], "scores": [-0.0007209903560578823, -0.084286168217659, -0.09761931747198105, -0.09857715666294098, -0.10048642754554749, -0.10129519551992416, -0.1032385304570198, -0.11937323212623596, -0.11970220506191254, -0.12813177704811096], "norm_scores": [0.1098598912358284, 0.10105254501104355, 0.09971415251493454, 0.09961868077516556, 0.09942866116762161, 0.0993482768535614, 0.09915541112422943, 0.09756839275360107, 0.09753631055355072, 0.09671757370233536]}}
{"id": "WebQTest-231", "question": "when did florida marlins join mlb", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.championships"], ["sports.sports_league_participation.league", "sports.sports_league_participation.season"], ["sports.sports_league_participation.league", "sports.sports_league_season.participation"], ["sports.sports_team_roster.team"], ["sports.sports_league_participation.league", "sports.sports_team.league"], ["sports.sports_team.previously_known_as"], ["sports.sports_team.sport"], ["sports.sports_team.location"], ["time.event.instance_of_recurring_event"]], "ground_paths": [["baseball.baseball_team.team_stats", "baseball.baseball_team_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did florida marlins join mlb [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_league_participation.season </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_league_season.participation </PATH>", "<PATH> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_team.league </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>", "<PATH> sports.sports_team.sport </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>"], "scores": [-0.015863869339227676, -0.019329925999045372, -0.03854743391275406, -0.05285672843456268, -0.06047530099749565, -0.061218149960041046, -0.06311585009098053, -0.06658170372247696, -0.0703209713101387, -0.0711078867316246], "norm_scores": [0.10365411639213562, 0.1032954677939415, 0.10132933408021927, 0.09988971054553986, 0.09913158416748047, 0.09905797243118286, 0.0988701730966568, 0.09852808713912964, 0.09816035628318787, 0.09808313846588135]}}
{"id": "WebQTest-232", "question": "what countries have spanish as the national language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["base.rosetta.local_name.locale"], ["location.country.currency_used"], ["base.rosetta.local_name.languoid"], ["location.country.internet_tld"], ["location.country.national_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have spanish as the national language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> base.rosetta.local_name.languoid </PATH>", "<PATH> location.country.internet_tld </PATH>", "<PATH> location.country.national_language </PATH>"], "scores": [-0.011093963868916035, -0.023107750341296196, -0.03062705509364605, -0.11097879707813263, -0.12728971242904663, -0.1296197772026062, -0.14943872392177582, -0.15118573606014252, -0.1626039743423462, -0.16361209750175476], "norm_scores": [0.1097668930888176, 0.10845606029033661, 0.10764360427856445, 0.09933263063430786, 0.09772556275129318, 0.09749812632799149, 0.09558483213186264, 0.09541799873113632, 0.0943346843123436, 0.09423962980508804]}}
{"id": "WebQTest-233", "question": "what to see in phoenix arizona", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["metropolitan_transit.transit_system.area_served"], ["film.film_festival.location"], ["film.film.featured_film_locations"], ["organization.organization.geographic_scope"], ["transportation.bridge.locale"], ["aviation.airport.serves"], ["location.location.contains"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in phoenix arizona [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> transportation.bridge.locale </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.013863972388207912, -0.014090311713516712, -0.035880859941244125, -0.13745632767677307, -0.14425083994865417, -0.14742131531238556, -0.1534591168165207, -0.16026188433170319, -0.16251610219478607, -0.17117522656917572], "norm_scores": [0.11032290011644363, 0.11029793322086334, 0.10792047530412674, 0.09749675542116165, 0.09683655947446823, 0.09653003513813019, 0.0959489569067955, 0.09529844671487808, 0.09508386999368668, 0.09426408261060715]}}
{"id": "WebQTest-234", "question": "what countries does the panama canal go through", "prediction": [["location.location.containedby"], ["geography.body_of_water.bridges", "location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["government.form_of_government.countries", "location.country.form_of_government"], ["base.locations.countries.continent"], ["book.written_work.subjects"], ["location.country.administrative_divisions"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does the panama canal go through [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.body_of_water.bridges <SEP> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.0010410364484414458, -0.0713367834687233, -0.08252882212400436, -0.0862865149974823, -0.09666428714990616, -0.09778343886137009, -0.10442467778921127, -0.10498195141553879, -0.10732722282409668, -0.10972131788730621], "norm_scores": [0.10883775353431702, 0.1014496386051178, 0.10032054036855698, 0.09994427859783173, 0.09891244769096375, 0.09880181401968002, 0.09814781695604324, 0.09809312969446182, 0.0978633463382721, 0.09762933105230331]}}
{"id": "WebQTest-235", "question": "what is charlie chaplin famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["people.profession.people_with_this_profession"], ["film.film.directed_by"], ["freebase.valuenotation.is_reviewed"], ["influence.influence_node.influenced_by"], ["base.activism.activist.area_of_activism"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["fictional_universe.fictional_character.character_created_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is charlie chaplin famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> film.film.directed_by </PATH>", "<PATH> freebase.valuenotation.is_reviewed </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>"], "scores": [-0.0038598948158323765, -0.03380937501788139, -0.10543335229158401, -0.11309056729078293, -0.13051626086235046, -0.13144297897815704, -0.13775396347045898, -0.1380137801170349, -0.14041580259799957, -0.14175482094287872], "norm_scores": [0.11081098765134811, 0.10754146426916122, 0.10010828077793121, 0.0993446633219719, 0.09762851148843765, 0.09753807634115219, 0.09692446142435074, 0.09689927846193314, 0.09666679799556732, 0.09653744846582413]}}
{"id": "WebQTest-237", "question": "what did fred durst do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre", "music.artist.genre"], ["music.recording.artist"], ["music.artist.origin", "people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types", "type.type.expected_by"], ["music.composition.composer"], ["common.topic.notable_types", "type.type.properties"], ["music.producer.releases_produced"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did fred durst do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.artist.origin <SEP> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.properties </PATH>", "<PATH> music.producer.releases_produced </PATH>"], "scores": [-0.0034836060367524624, -0.05763857811689377, -0.06893930584192276, -0.08693301677703857, -0.08901406079530716, -0.09300574660301208, -0.09343114495277405, -0.09723728895187378, -0.09799672663211823, -0.10127658396959305], "norm_scores": [0.10778887569904327, 0.10210680961608887, 0.10095943510532379, 0.09915903955698013, 0.09895290434360504, 0.0985587015748024, 0.09851677715778351, 0.09814252704381943, 0.09806802868843079, 0.09774689376354218]}}
{"id": "WebQTest-239", "question": "where do american bulldogs originate from", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here", "location.location.containedby"], ["biology.breed_origin.breeds_originating_here", "base.biblioness.bibs_location.country"], ["biology.breed_origin.breeds_originating_here", "base.breed_origin.breeds_originating_here"], ["biology.animal_breed.breed_of", "base.animalbreed.place_of_origin"], ["biology.animal_breed.animal_breed_of"], ["biology.breed_origin.breeds_originating_here", "base.breeds.breed_origin.breeds_originating_here"], ["biology.breed_origin.breeds_originating_here", "base.locations.continents.countries_within"]], "ground_paths": [["biology.animal_breed.place_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do american bulldogs originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.place_of_origin </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> location.location.containedby </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> base.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.breed_of <SEP> base.animalbreed.place_of_origin </PATH>", "<PATH> biology.animal_breed.animal_breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> base.breeds.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> base.locations.continents.countries_within </PATH>"], "scores": [-0.008202753029763699, -0.016471214592456818, -0.0813557580113411, -0.09128798544406891, -0.09570083767175674, -0.09598778933286667, -0.10319223999977112, -0.10651236772537231, -0.10811805725097656, -0.1090313196182251], "norm_scores": [0.10754506289958954, 0.10665949434041977, 0.09995868057012558, 0.09897077828645706, 0.09853500127792358, 0.09850673377513885, 0.09779959917068481, 0.09747543185949326, 0.09731903672218323, 0.0972302034497261]}}
{"id": "WebQTest-240", "question": "what colleges did albert einstein teach at", "prediction": [["people.person.employment_history", "business.employer.employees"], ["education.education.student", "education.education.institution"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"], ["common.topic.notable_types"], ["people.person.education", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["education.academic.departments_old", "education.department.institution"], ["people.person.nationality", "organization.organization.founders"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["people.person.nationality", "organization.organization.founders"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.employment_history", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"], ["education.department.academics_old", "education.university.departments"], ["business.employment_tenure.person", "business.employer.employees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did albert einstein teach at [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> education.academic.departments_old <SEP> education.department.institution </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.founders </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.017330175265669823, -0.028741944581270218, -0.029930606484413147, -0.031760819256305695, -0.036149587482213974, -0.04739680886268616, -0.05828991159796715, -0.07540049403905869, -0.07673808932304382, -0.07888966798782349], "norm_scores": [0.10309690237045288, 0.10192706435918808, 0.10180598497390747, 0.10161982476711273, 0.10117481648921967, 0.10004325956106186, 0.09895940124988556, 0.09728054702281952, 0.09715051203966141, 0.09694170951843262]}}
{"id": "WebQTest-241", "question": "who plays the voice of brian on family guy", "prediction": [["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program_writer_relationship.tv_program", "tv.tv_writer.tv_programs"], ["tv.tv_program_creator.programs_created"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.tv_program.languages", "people.person.languages"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of brian on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program_writer_relationship.tv_program <SEP> tv.tv_writer.tv_programs </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.tv_program.languages <SEP> people.person.languages </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>"], "scores": [-0.022682446986436844, -0.022771380841732025, -0.030361300334334373, -0.03112778440117836, -0.042911212891340256, -0.04406588524580002, -0.04742204397916794, -0.05842117592692375, -0.05943136289715767, -0.06568695604801178], "norm_scores": [0.10198928415775299, 0.10198021680116653, 0.1012091264128685, 0.10113158077001572, 0.09994689375162125, 0.09983155131340027, 0.09949706494808197, 0.09840867668390274, 0.09830932319164276, 0.09769625216722488]}}
{"id": "WebQTest-243", "question": "where did george w bush live as a child", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.parents", "people.person.place_of_birth"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.children", "people.person.place_of_birth"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george w bush live as a child [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.01099404040724039, -0.025395207107067108, -0.039733052253723145, -0.04886507987976074, -0.05335279181599617, -0.06344758719205856, -0.0675632432103157, -0.07408232986927032, -0.07457317411899567, -0.07505485415458679], "norm_scores": [0.10429874807596207, 0.10280748456716537, 0.10134395956993103, 0.10042271018028259, 0.09997305274009705, 0.09896890819072723, 0.09856243431568146, 0.09792197495698929, 0.09787392616271973, 0.09782680124044418]}}
{"id": "WebQTest-245", "question": "what year was the first miss america pageant held", "prediction": [["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"], ["time.recurring_event.current_frequency"], ["time.event.locations"], ["organization.organization.founders"], ["award.competitor.competitions_won", "time.event.instance_of_recurring_event"], ["award.competitor.competitions", "time.event.instance_of_recurring_event"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["award.competition.competitors", "time.event.instance_of_recurring_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was the first miss america pageant held [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> award.competitor.competitions_won <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> award.competitor.competitions <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> award.competition.competitors <SEP> time.event.instance_of_recurring_event </PATH>"], "scores": [-0.00788598507642746, -0.02387809008359909, -0.0524728037416935, -0.07386799901723862, -0.08327927440404892, -0.08505084365606308, -0.0875227153301239, -0.08788618445396423, -0.0990685224533081, -0.10603876411914825], "norm_scores": [0.10643142461776733, 0.10474289953708649, 0.10179022699594498, 0.0996355339884758, 0.09870223701000214, 0.09852753579616547, 0.0982842892408371, 0.09824856370687485, 0.09715604037046432, 0.09648119658231735]}}
{"id": "WebQTest-246", "question": "what was nikola tesla inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"], ["law.inventor.us_patents"], ["base.inventor.inventions"], ["symbols.name_source.namesakes"], ["common.topic.notable_types"], ["base.invention.inventor"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.inventions"], ["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> base.inventor.inventions </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.invention.inventor </PATH>"], "scores": [-0.006025008391588926, -0.039239201694726944, -0.054652370512485504, -0.05596890300512314, -0.05819118767976761, -0.07198697328567505, -0.08417106419801712, -0.09000759571790695, -0.10357119888067245, -0.10994157195091248], "norm_scores": [0.10627980530261993, 0.10280779004096985, 0.10123534500598907, 0.10110215097665787, 0.10087772458791733, 0.09949558973312378, 0.09829067438840866, 0.09771867096424103, 0.09640220552682877, 0.09579004347324371]}}
{"id": "WebQTest-247", "question": "where did flemish people come from", "prediction": [["people.ethnicity.geographic_distribution"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["people.ethnicity.included_in_group"], ["base.popstra.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "people.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "base.locations.continents.countries_within"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["people.ethnicity.geographic_distribution", "base.locations.countries.continent"], ["people.person.ethnicity"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did flemish people come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> base.popstra.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.continents.countries_within </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.countries.continent </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-7.766458293190226e-05, -0.12752412259578705, -0.1305759698152542, -0.1307494342327118, -0.13075169920921326, -0.13997198641300201, -0.14697933197021484, -0.1472724825143814, -0.1473243534564972, -0.15684954822063446], "norm_scores": [0.1132894977927208, 0.09973334521055222, 0.09942943602800369, 0.09941218048334122, 0.0994119644165039, 0.09849956631660461, 0.097811758518219, 0.09778308868408203, 0.09777802228927612, 0.09685109555721283]}}
{"id": "WebQTest-248", "question": "what state new york city belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["base.biblioness.bibs_location.state"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.contains", "location.location.containedby"], ["location.administrative_division.country", "base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state new york city belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> location.administrative_division.country <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.011616011150181293, -0.04915871471166611, -0.04996848478913307, -0.05027220770716667, -0.05697779357433319, -0.0651470348238945, -0.06956403702497482, -0.06970400363206863, -0.07054482400417328, -0.07143289595842361], "norm_scores": [0.10456844419240952, 0.10071544349193573, 0.10063391923904419, 0.1006033644080162, 0.0999310165643692, 0.09911797195672989, 0.09868113696575165, 0.09866733103990555, 0.09858439862728119, 0.09849689155817032]}}
{"id": "WebQTest-249", "question": "what was the cause of death for bruce lee", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["people.people.deceased_person.cause_of_death"], ["deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["people.person.deceased_of"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for bruce lee [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_of </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>"], "scores": [-0.0010785929625853896, -0.04777473583817482, -0.10792287439107895, -0.12326261401176453, -0.12795104086399078, -0.13494670391082764, -0.14019407331943512, -0.14130429923534393, -0.14830854535102844, -0.14877204596996307], "norm_scores": [0.1116238385438919, 0.10653126984834671, 0.10031250864267349, 0.09878547489643097, 0.09832341223955154, 0.09763798117637634, 0.09712697565555573, 0.09701920300722122, 0.09634203463792801, 0.09629739075899124]}}
{"id": "WebQTest-250", "question": "what type of government does usa follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet", "base.aareas.schema.administrative_area.administrative_parent"], ["base.locations.countries.planet", "government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["base.locations.countries.planet"], ["form.form_of_government.countries"], ["organization.organization.founders"], ["base.locations.countries.planet", "location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does usa follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.locations.countries.planet <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> base.locations.countries.planet <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007876981049776077, -0.017785998061299324, -0.14033761620521545, -0.14853167533874512, -0.15355078876018524, -0.15669116377830505, -0.16034822165966034, -0.16359350085258484, -0.16497984528541565, -0.16624866425991058], "norm_scores": [0.11256735771894455, 0.1114574447274208, 0.09860195964574814, 0.09779731184244156, 0.09730768948793411, 0.09700258076190948, 0.09664849191904068, 0.09633534401655197, 0.09620188921689987, 0.09607989341020584]}}
{"id": "WebQTest-251", "question": "what type of currency do they use in england", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.form_of_government"], ["location.country.currency_formerly_used"], ["language.human_language.main_country", "finance.currency.countries_used"], ["language.human_language.main_country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in england [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.main_country <SEP> finance.currency.countries_used </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> finance.currency.countries_used <PATH>"], "scores": [-0.0023111782502382994, -0.03943957760930061, -0.11660178750753403, -0.1245909258723259, -0.16063448786735535, -0.1645563691854477, -0.16577203571796417, -0.1669830083847046, -0.1707950383424759, -0.17397691309452057], "norm_scores": [0.11326520889997482, 0.10913696885108948, 0.10103242844343185, 0.10022846609354019, 0.09668020904064178, 0.09630178660154343, 0.09618479013442993, 0.0960683822631836, 0.09570285677909851, 0.09539882838726044]}}
{"id": "WebQTest-252", "question": "where was benjamin franklin educated", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["book.written_work.subjects", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["influence.influence_node.influenced_by", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was benjamin franklin educated [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> education.education.institution </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>"], "scores": [-0.012631786055862904, -0.025404449552297592, -0.027940785512328148, -0.02903042547404766, -0.054202888160943985, -0.0859372541308403, -0.10428981482982635, -0.10838015377521515, -0.11413656920194626, -0.11519332975149155], "norm_scores": [0.10557924211025238, 0.10423929244279861, 0.10397523641586304, 0.10386200994253159, 0.10128018260002136, 0.09811657667160034, 0.09633231908082962, 0.09593909233808517, 0.09538840502500534, 0.09528765827417374]}}
{"id": "WebQTest-253", "question": "what countries included in the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.country.first_level_divisions"], ["location.administrative_division.first_level_division_of"], ["base.locations.countries.continent"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.023033464327454567, -0.02528645098209381, -0.030497852712869644, -0.0363931842148304, -0.040625039488077164, -0.04922853410243988, -0.059207987040281296, -0.10298430919647217, -0.11932001262903214, -0.11944468319416046], "norm_scores": [0.10375943779945374, 0.10352593660354614, 0.10298781841993332, 0.10238246619701385, 0.10195010900497437, 0.10107674449682236, 0.10007306933403015, 0.09578674286603928, 0.09423471242189407, 0.09422296285629272]}}
{"id": "WebQTest-254", "question": "what countries in the world speak chinese", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["location.country.administrative_divisions"], ["base.rosetta.local_name.languoid"], ["base.rosetta.local_name.locale"], ["location.country.form_of_government"], ["base.rosetta.languoid.local_name"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries in the world speak chinese [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> base.rosetta.local_name.languoid </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.languoid.local_name </PATH>"], "scores": [-0.018824469298124313, -0.0212409645318985, -0.031784556806087494, -0.03421583026647568, -0.11846704035997391, -0.14135532081127167, -0.14832358062267303, -0.15890447795391083, -0.16310666501522064, -0.16834935545921326], "norm_scores": [0.10829707235097885, 0.10803569108247757, 0.1069025918841362, 0.10664299130439758, 0.09802627563476562, 0.09580811113119125, 0.09514281153678894, 0.0941414162516594, 0.09374664723873138, 0.09325645118951797]}}
{"id": "WebQTest-255", "question": "where obama went to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["people.person.education", "education.educational_institution.parent_institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where obama went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>"], "scores": [-0.011115296743810177, -0.02866489253938198, -0.030039900913834572, -0.0335431843996048, -0.056934840977191925, -0.06325663626194, -0.1034192144870758, -0.10545971244573593, -0.11506173759698868, -0.11512104421854019], "norm_scores": [0.10559239983558655, 0.10375545918941498, 0.10361289232969284, 0.10325054079294205, 0.10086336731910706, 0.10022774338722229, 0.09628210216760635, 0.09608583897352219, 0.09516763687133789, 0.09516199678182602]}}
{"id": "WebQTest-256", "question": "what is the primary language of china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["location.country.administrative_divisions"], ["language.human_language.language_family"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.writing_system"], ["language.human_language.region"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the primary language of china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.writing_system </PATH>", "<PATH> language.human_language.region </PATH>"], "scores": [-0.007188539486378431, -0.035862330347299576, -0.037129320204257965, -0.05733779817819595, -0.13261158764362335, -0.15425021946430206, -0.15707653760910034, -0.170171320438385, -0.17305152118206024, -0.17606452107429504], "norm_scores": [0.110608771443367, 0.10748223215341568, 0.10734613984823227, 0.10519860684871674, 0.09757061302661896, 0.0954819917678833, 0.09521251171827316, 0.09397386014461517, 0.09370358288288116, 0.09342167526483536]}}
{"id": "WebQTest-257", "question": "who was the soviet leader during world war ii", "prediction": [["military.military_person.participated_in_conflicts"], ["people.person.nationality"], ["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speaker_s"], ["military.military_command.military_conflict", "military.military_commander.military_commands"], ["military.military_conflict.military_personnel_involved"], ["military.military_commander.military_commands", "military.military_commander.military_commands"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["military.military_command.military_conflict", "military.military_command.military_commander"], ["military.military_command.military_conflict", "military.military_person.military_commands"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the soviet leader during world war ii [/INST]", "raw_output": {"paths": ["<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speaker_s </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_commander.military_commands </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_commander.military_commands </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_command.military_commander </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_person.military_commands </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.010703080333769321, -0.01340024545788765, -0.08521715551614761, -0.08681873977184296, -0.08934090286493301, -0.09259328991174698, -0.0946582779288292, -0.09606050699949265, -0.0963771790266037, -0.10003136098384857], "norm_scores": [0.10674576461315155, 0.10645823925733566, 0.09908081591129303, 0.09892226755619049, 0.09867307543754578, 0.09835267812013626, 0.09814979135990143, 0.09801225364208221, 0.09798122197389603, 0.09762383997440338]}}
{"id": "WebQTest-258", "question": "which of the following does australia export the most", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["common.topic.notable_types", "common.topic.notable_types"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["location.country.form_of_government", "location.country.form_of_government"]], "ground_paths": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which of the following does australia export the most [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> location.imports_and_exports.imported_from <SEP> location.imports_and_exports.exported_to </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> location.country.form_of_government <SEP> location.country.form_of_government </PATH>"], "scores": [-0.008833074942231178, -0.030096065253019333, -0.03348047658801079, -0.035412080585956573, -0.04037361964583397, -0.044753365218639374, -0.05325363948941231, -0.05722850188612938, -0.057564496994018555, -0.05953839793801308], "norm_scores": [0.10336601734161377, 0.10119135677814484, 0.1008494570851326, 0.10065484046936035, 0.10015667974948883, 0.09971897304058075, 0.09887493401765823, 0.09848269075155258, 0.09844960272312164, 0.09825547784566879]}}
{"id": "WebQTest-260", "question": "where was the city of david", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["location.location.contains", "location.location.containedby"], ["architecture.architect.structures_designed", "location.location.containedby"], ["base.biblioness.bibs_location.country"], ["symbols.namesake.named_after", "symbols.namesake.named_after"], ["location.location.people_born_here"], ["time.event.locations"], ["symbols.namesake.named_after", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the city of david [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> architecture.architect.structures_designed <SEP> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> symbols.namesake.named_after <SEP> symbols.namesake.named_after </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>"], "scores": [-0.005738943815231323, -0.0348413847386837, -0.08133122324943542, -0.09701777994632721, -0.09907660633325577, -0.10261522978544235, -0.10356814414262772, -0.10453107208013535, -0.10673712939023972, -0.10957762598991394], "norm_scores": [0.10813302546739578, 0.10503143817186356, 0.10026030987501144, 0.09869984537363052, 0.09849684685468674, 0.09814891964197159, 0.09805543720722198, 0.09796106070280075, 0.09774518758058548, 0.09746794402599335]}}
{"id": "WebQTest-261", "question": "what is cindy sherman known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["visual_art.visual_artist.art_forms"], ["film.film_crewmember.films_crewed", "film.film_job.films_with_this_crew_job"], ["film.film_crew_gig.crewmember", "film.film_job.films_with_this_crew_job"], ["visual_art.art_series.artist"], ["people.profession.people_with_this_profession"], ["common.image.appears_in_topic_gallery"], ["common.topic.image"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is cindy sherman known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> film.film_crewmember.films_crewed <SEP> film.film_job.films_with_this_crew_job </PATH>", "<PATH> film.film_crew_gig.crewmember <SEP> film.film_job.films_with_this_crew_job </PATH>", "<PATH> visual_art.art_series.artist </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.image.appears_in_topic_gallery </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>"], "scores": [-0.006205006968230009, -0.03237880766391754, -0.051134463399648666, -0.06746303290128708, -0.07933244109153748, -0.08518510311841965, -0.08894839882850647, -0.09232132136821747, -0.1057119145989418, -0.10617421567440033], "norm_scores": [0.10669425874948502, 0.10393788665533066, 0.10200662910938263, 0.10035454481840134, 0.09917043149471283, 0.09859172254800797, 0.09822137653827667, 0.09789065271615982, 0.09658856689929962, 0.09654392302036285]}}
{"id": "WebQTest-262", "question": "what tv shows did shawnee smith play in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.performance.actor", "film.film.starring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv shows did shawnee smith play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_program.regular_cast </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.022143738344311714, -0.023025857284665108, -0.03192473202943802, -0.0321044847369194, -0.0366048626601696, -0.038399115204811096, -0.047420650720596313, -0.054668769240379333, -0.05586536228656769, -0.05628814175724983], "norm_scores": [0.10177802294492722, 0.10168828070163727, 0.10078738629817963, 0.10076926648616791, 0.10031678527593613, 0.10013695806264877, 0.09923762828111649, 0.09852094203233719, 0.09840312600135803, 0.0983615294098854]}}
{"id": "WebQTest-264", "question": "what does australia use for currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["common.topic.notable_types"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does australia use for currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.011654749512672424, -0.015328626148402691, -0.12146572768688202, -0.1371915638446808, -0.14292162656784058, -0.1438327133655548, -0.14638535678386688, -0.14862500131130219, -0.15283717215061188, -0.15287716686725616], "norm_scores": [0.11098652333021164, 0.11057952046394348, 0.09944430738687515, 0.09789269417524338, 0.09733337163925171, 0.0972447320818901, 0.09699682146310806, 0.09677981585264206, 0.09637302160263062, 0.096369169652462]}}
{"id": "WebQTest-265", "question": "which airport to fly into rome", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"], ["film.film_location.featured_in_films", "film.film.featured_film_locations"], ["base.aareas.schema.administrative_area.administrative_parent", "location.location.nearby_airports"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["common.topic.notable_types"], ["base.aareas.schema.administrative_area.administrative_children", "location.location.nearby_airports"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>", "<PATH> film.film_location.featured_in_films <SEP> film.film.featured_film_locations </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.location.nearby_airports </PATH>", "<PATH> film.film.featured_film_locations <SEP> film.film.featured_film_locations </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> location.location.nearby_airports </PATH>"], "scores": [-0.006069873925298452, -0.030147245153784752, -0.06044676527380943, -0.06742746382951736, -0.08651543408632278, -0.1053575649857521, -0.10598307847976685, -0.1072591096162796, -0.10871405899524689, -0.11018576472997665], "norm_scores": [0.10747827589511871, 0.10492139309644699, 0.10179000347852707, 0.10108190774917603, 0.0991707518696785, 0.09731966257095337, 0.0972588062286377, 0.0971347764134407, 0.09699355810880661, 0.0968509167432785]}}
{"id": "WebQTest-266", "question": "what is there to do for fun in kansas city", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["base.usnationalparks.us_national_park.state"], ["metropolitan_transit.transit_system.area_served"], ["film.film_festival.location"], ["film.film.featured_film_locations"], ["organization.organization.geographic_scope"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.place_with_neighborhoods.neighborhoods"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do for fun in kansas city [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> travel.travel_destination.tourist_attractions <PATH>"], "scores": [-0.011198751628398895, -0.016883419826626778, -0.0329909548163414, -0.12622544169425964, -0.12625610828399658, -0.13771456480026245, -0.15779638290405273, -0.15967844426631927, -0.16307908296585083, -0.1698669046163559], "norm_scores": [0.11019742488861084, 0.10957276821136475, 0.10782195627689362, 0.09822363406419754, 0.09822062402963638, 0.097101591527462, 0.09517106413841248, 0.09499210864305496, 0.09466962516307831, 0.09402919560670853]}}
{"id": "WebQTest-267", "question": "where was martin luther king jr raised", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced"], ["people.person.nationality"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr raised [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0006611900753341615, -0.06148397922515869, -0.1116718128323555, -0.11513993889093399, -0.13185788691043854, -0.13240575790405273, -0.13392744958400726, -0.13581465184688568, -0.14366959035396576, -0.15239770710468292], "norm_scores": [0.11165529489517212, 0.10506651550531387, 0.09992359578609467, 0.0995776429772377, 0.09792675077915192, 0.09787310659885406, 0.0977242961525917, 0.09754004329442978, 0.09677687287330627, 0.0959358662366867]}}
{"id": "WebQTest-268", "question": "what year was lebron james rookie season", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.sports_award.award_winner", "sports.sports_league_season.awards"], ["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was lebron james rookie season [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_league_season.awards </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_league_season.awards </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.010113132186233997, -0.014908872544765472, -0.035515282303094864, -0.04849699139595032, -0.050517428666353226, -0.051433466374874115, -0.05321728065609932, -0.06477309763431549, -0.06936496496200562, -0.07002059370279312], "norm_scores": [0.10372001677751541, 0.10322380065917969, 0.10111849009990692, 0.0998142808675766, 0.09961282461881638, 0.09952161461114883, 0.09934424608945847, 0.09820284694433212, 0.09775294363498688, 0.09768887609243393]}}
{"id": "WebQTest-269", "question": "where does delaware river start", "prediction": [["geography.river.origin"], ["location.location.containedby"], ["transportation.bridge.body_of_water_spanned"], ["location.location.partial_containedby"], ["geography.lake.outflow"], ["location.location.partial_contains"], [], ["location.location.containedby", "location.location.containedby"], ["location.location.partially_containedby"], ["geography.lake.basin_countries"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does delaware river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>", "<PATH> location.location.partial_containedby </PATH>", "<PATH> geography.lake.outflow </PATH>", "<PATH> location.location.partial_contains </PATH>", "<PATH> </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> geography.lake.basin_countries </PATH>"], "scores": [-0.0009603092330507934, -0.0804266557097435, -0.10254580527544022, -0.10555536299943924, -0.10920454561710358, -0.1244768500328064, -0.12794089317321777, -0.13036192953586578, -0.1329423040151596, -0.14651606976985931], "norm_scores": [0.11099779605865479, 0.10251858085393906, 0.10027585178613663, 0.09997451305389404, 0.0996103584766388, 0.09810063987970352, 0.09776140004396439, 0.09752500057220459, 0.09727367013692856, 0.09596221894025803]}}
{"id": "WebQTest-270", "question": "what kind of monarchy does japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["royalty.monarch.kingdom"], ["royalty.kingdom.rulers"], ["royalty.form_of_government.countries"], ["royalty.monarch.form"], ["language.human_language.countries_spoken_in"], ["people.person.nationality"], ["royalty.kingdom.form"], ["royalty.monarchical_form.countries"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of monarchy does japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.form_of_government.countries </PATH>", "<PATH> royalty.monarch.form </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> royalty.kingdom.form </PATH>", "<PATH> royalty.monarchical_form.countries </PATH>"], "scores": [-0.007673174142837524, -0.02395893819630146, -0.04579358547925949, -0.06857690960168839, -0.0888560488820076, -0.09555523842573166, -0.12106539309024811, -0.15158116817474365, -0.16173163056373596, -0.16431063413619995], "norm_scores": [0.10873991996049881, 0.10698334872722626, 0.10467272996902466, 0.10231488943099976, 0.10026092827320099, 0.09959150850772858, 0.09708304703235626, 0.09416522830724716, 0.0932142436504364, 0.09297414869070053]}}
{"id": "WebQTest-271", "question": "where did mitt romney 's parents come from", "prediction": [["people.person.place_of_birth"], ["people.person.parents"], ["people.person.children"], ["people.person.nationality"], ["people.person.religion"], ["people.person.ethnicity"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mitt romney 's parents come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.religion </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.014943204820156097, -0.026960626244544983, -0.029303936287760735, -0.04390637204051018, -0.09273296594619751, -0.10115458071231842, -0.12095066159963608, -0.1233542338013649, -0.12494617700576782, -0.12531065940856934], "norm_scores": [0.10665732622146606, 0.1053832471370697, 0.10513658821582794, 0.1036124974489212, 0.09867497533559799, 0.09784746170043945, 0.09592951089143753, 0.09569921344518661, 0.09554699063301086, 0.09551216661930084]}}
{"id": "WebQTest-273", "question": "when did michael jordan return to the nba", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.season"], ["sports.sports_award.award_winner", "sports.sports_league_season.awards"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael jordan return to the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_league_season.awards </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.season </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_league_season.awards </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.season </PATH>"], "scores": [-0.011593044735491276, -0.02350492589175701, -0.0260894987732172, -0.02700495906174183, -0.05293460935354233, -0.06022404506802559, -0.061478979885578156, -0.06802012771368027, -0.07025273144245148, -0.07245845347642899], "norm_scores": [0.10361665487289429, 0.10238971561193466, 0.10212542116641998, 0.10203196853399277, 0.09942032396793365, 0.09869824349880219, 0.0985744521021843, 0.09793177247047424, 0.09771336615085602, 0.09749807417392731]}}
{"id": "WebQTest-276", "question": "where was mission san rafael located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "location.administrative_division.second_level_division_of"], ["religion.place_of_worship.religion"], ["symbols.namesake.named_after"], ["architecture.building.building_function"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_parent"], ["architecture.building.building_complex"], ["location.location.containedby", "location.us_county.county_seat"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mission san rafael located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.administrative_division.second_level_division_of </PATH>", "<PATH> religion.place_of_worship.religion </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> architecture.building.building_function </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.containedby <SEP> location.us_county.county_seat </PATH>"], "scores": [-0.0006351229385472834, -0.07562680542469025, -0.10847774893045425, -0.11740849167108536, -0.12870588898658752, -0.1299741566181183, -0.13412125408649445, -0.13863393664360046, -0.14318342506885529, -0.1434519737958908], "norm_scores": [0.11168228089809418, 0.10361336916685104, 0.10026487708091736, 0.09937342256307602, 0.09825707226991653, 0.09813253581523895, 0.09772641211748123, 0.0972864106297493, 0.09684480726718903, 0.09681880474090576]}}
{"id": "WebQTest-277", "question": "who was esther 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.children", "people.person.children"], ["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.children"]], "ground_paths": [["visual_art.art_subject.artwork_on_the_subject", "visual_art.art_subject.artwork_on_the_subject"], ["common.topic.notable_types", "common.topic.notable_types"], ["visual_art.art_subject.artwork_on_the_subject", "visual_art.artwork.art_subject"], ["people.person.spouse_s", "people.person.spouse_s"], ["book.translated_work.translations", "book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was esther 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> freebase.valuenotation.has_value <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.028766341507434845, -0.03288234770298004, -0.03709513694047928, -0.04217817634344101, -0.04832758754491806, -0.048599980771541595, -0.04886503890156746, -0.0496969036757946, -0.051401007920503616, -0.053405892103910446], "norm_scores": [0.10154411196708679, 0.1011270210146904, 0.10070188343524933, 0.10019131004810333, 0.09957708418369293, 0.09954996407032013, 0.09952357411384583, 0.09944082796573639, 0.0992715060710907, 0.09907268732786179]}}
{"id": "WebQTest-278", "question": "who 's dating claire danes", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"]], "ground_paths": [["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who 's dating claire danes [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> celebrities.romantic_relationship.celebrity <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.celebrity.sexual_relationships </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> celebrities.romantic_relationship.celebrity <SEP> celebrities.celebrity.sexual_relationships </PATH>"], "scores": [-0.017887188121676445, -0.027411315590143204, -0.030270718038082123, -0.042298365384340286, -0.047748759388923645, -0.04801034927368164, -0.049402620643377304, -0.05830225721001625, -0.05902417004108429, -0.05961902067065239], "norm_scores": [0.10263573378324509, 0.10166285187005997, 0.10137257725000381, 0.10016060620546341, 0.09961617738008499, 0.09959012269973755, 0.09945156425237656, 0.09857039898633957, 0.09849927574396133, 0.09844069927930832]}}
{"id": "WebQTest-279", "question": "what county is st paul va in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.contains"], ["location.citytown.postal_codes"], ["common.topic.notable_types"], ["location.us_county.capital"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul va in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.us_county.capital </PATH>"], "scores": [-0.01843361184000969, -0.024753613397479057, -0.029999732971191406, -0.032507844269275665, -0.07812023162841797, -0.08494436740875244, -0.08728837221860886, -0.10746394097805023, -0.11062603443861008, -0.11437695473432541], "norm_scores": [0.10510093718767166, 0.10443879663944244, 0.10389232635498047, 0.10363209247589111, 0.0990113615989685, 0.09833799302577972, 0.09810775518417358, 0.09614821523427963, 0.09584466367959976, 0.09548583626747131]}}
{"id": "WebQTest-281", "question": "what type of sports do japanese play", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["cricket.cricket_team.country"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["people.person.nationality", "sports.sports_team.location"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.sport"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of sports do japanese play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> cricket.cricket_team.country </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.imports_and_exports.exported_to </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team.location </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.sport </PATH>"], "scores": [-0.009796010330319405, -0.015212221071124077, -0.11672604829072952, -0.13154669106006622, -0.13668277859687805, -0.1400984674692154, -0.1459254026412964, -0.15047185122966766, -0.15276534855365753, -0.1545347273349762], "norm_scores": [0.11097856611013412, 0.11037910729646683, 0.09972406923770905, 0.09825699031352997, 0.0977536290884018, 0.09742030501365662, 0.09685429185628891, 0.0964149460196495, 0.0961940810084343, 0.09602402150630951]}}
{"id": "WebQTest-282", "question": "which countries share a border with russia", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_containedby"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["time.event.locations", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.containedby"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["location.location.time_zones", "time.time_zone.locations_in_this_time_zone"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "geography.river.origin"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_contains", "geography.river.basin_countries"], ["base.locations.continents.countries_within", "base.locations.continents.countries_within"], ["location.country.languages_spoken", "location.country.official_language"], ["language.human_language.main_country", "language.human_language.countries_spoken_in"], ["time.event.locations", "time.event.locations"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["language.human_language.main_country", "location.country.languages_spoken"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["location.location.partially_containedby", "base.schemastaging.disputed_location.claimed_by"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.locations.continents.countries_within", "location.location.partially_contains"], ["location.location.partially_containedby", "base.locations.continents.countries_within"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["base.rivalries.rival.rivals", "base.rivalries.rivalry.rival"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.country.languages_spoken", "language.human_language.main_country"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.location.events", "location.location.events"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["base.rivalries.rival.rivals", "base.rivalries.rival.rivals"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.containedby", "location.location.containedby"], ["base.locations.continents.countries_within", "base.locations.countries.continent"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["location.location.partially_containedby", "location.location.containedby"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "geography.lake.basin_countries"], ["location.location.partially_containedby", "base.locations.countries.continent"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "geography.river.origin"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["geography.river.basin_countries", "location.location.partially_contains"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries share a border with russia [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.03518274426460266, -0.03642285242676735, -0.03741887956857681, -0.03777424246072769, -0.04143171384930611, -0.04161404073238373, -0.04602785408496857, -0.04657731577754021, -0.047763217240571976, -0.04785240441560745], "norm_scores": [0.1006634533405304, 0.1005387082695961, 0.10043861716985703, 0.10040292143821716, 0.1000363752245903, 0.10001813620328903, 0.0995776504278183, 0.09952294826507568, 0.09940499067306519, 0.09939613193273544]}}
{"id": "WebQTest-284", "question": "where did the latin language originate from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"], ["language.language_family.languages"], ["symbols.namesake.named_after"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["base.rosetta.languoid.place_of_origin"], ["base.rosetta.languoid.parent_set"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the latin language originate from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.languoid.place_of_origin </PATH>", "<PATH> base.rosetta.languoid.parent_set </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.004712370224297047, -0.03339089825749397, -0.050793107599020004, -0.1100933849811554, -0.1285930722951889, -0.1362612545490265, -0.13852986693382263, -0.14179643988609314, -0.14202545583248138, -0.14285939931869507], "norm_scores": [0.1101781353354454, 0.10706326365470886, 0.10521624237298965, 0.09915828704833984, 0.09734075516462326, 0.09659717977046967, 0.09637829661369324, 0.09606398642063141, 0.09604198485612869, 0.09596192091703415]}}
{"id": "WebQTest-286", "question": "what school did karl benz go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.place_of_birth", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"], ["business.employment_tenure.person", "business.employer.employees"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did karl benz go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>"], "scores": [-0.017911678180098534, -0.020725024864077568, -0.0211409293115139, -0.03441360965371132, -0.05678977072238922, -0.0658913403749466, -0.06665480136871338, -0.06887073069810867, -0.08919888734817505, -0.09724508970975876], "norm_scores": [0.10362381488084793, 0.10333269089460373, 0.10328973084688187, 0.10192784667015076, 0.09967242926359177, 0.09876935929059982, 0.09869398921728134, 0.09847553074359894, 0.0964939147233963, 0.09572062641382217]}}
{"id": "WebQTest-287", "question": "what movies has carmen electra been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["film.film_character.portrayed_in_films", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has carmen electra been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.021002568304538727, -0.02667522057890892, -0.038160789757966995, -0.0385875478386879, -0.03914562985301018, -0.039830874651670456, -0.0502142496407032, -0.050363585352897644, -0.054648641496896744, -0.05474895238876343], "norm_scores": [0.1020483523607254, 0.10147110372781754, 0.10031231492757797, 0.10026951879262924, 0.10021357983350754, 0.10014493018388748, 0.09911046177148819, 0.0990956649184227, 0.09867194294929504, 0.09866204857826233]}}
{"id": "WebQTest-289", "question": "when do world war ii end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["time.event.locations"], ["military.military_conflict.casualties", "military.military_conflict.casualties"], ["event.disaster.included_in_event"], ["military.military_conflict.casualties", "military.casualties.military_conflict"], ["military.military_conflict.casualties", "time.event.included_in_event"], ["time.event.instance_of_recurring_event"], ["military.military_conflict.casualties", "military.military_conflict.time_of_casualties"], ["military.military_conflict.casualties", "military.military_casualties.military_conflict"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when do world war ii end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>", "<PATH> event.disaster.included_in_event </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.military_conflict </PATH>", "<PATH> military.military_conflict.casualties <SEP> time.event.included_in_event </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.time_of_casualties </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_casualties.military_conflict </PATH>"], "scores": [-0.009845959953963757, -0.03574849292635918, -0.04329632595181465, -0.06954570859670639, -0.07339828461408615, -0.07773270457983017, -0.08050904422998428, -0.0858711525797844, -0.08705047518014908, -0.09159453213214874], "norm_scores": [0.10568428039550781, 0.10298193991184235, 0.10220757871866226, 0.09955959767103195, 0.09917677938938141, 0.09874783456325531, 0.09847405552864075, 0.09794744104146957, 0.09783200174570084, 0.09738844633102417]}}
{"id": "WebQTest-291", "question": "where did morgan freeman graduate", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.honorary_degree_recipient.honorary_degrees", "education.honorary_degree.institution"], ["education.honorary_degree_recipient.honorary_degrees", "education.educational_institution.honorary_degrees_awarded"], ["education.honorary_degree.recipient", "education.educational_institution.honorary_degrees_awarded"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did morgan freeman graduate [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.honorary_degree_recipient.honorary_degrees <SEP> education.honorary_degree.institution </PATH>", "<PATH> education.honorary_degree_recipient.honorary_degrees <SEP> education.educational_institution.honorary_degrees_awarded </PATH>", "<PATH> education.honorary_degree.recipient <SEP> education.educational_institution.honorary_degrees_awarded </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.012772436253726482, -0.012792190536856651, -0.042361803352832794, -0.0630791112780571, -0.07802197337150574, -0.07867075502872467, -0.09552149474620819, -0.10665014386177063, -0.11843492835760117, -0.13311971724033356], "norm_scores": [0.10624577850103378, 0.10624367743730545, 0.10314808040857315, 0.10103312134742737, 0.09953462332487106, 0.09947006404399872, 0.09780795872211456, 0.09672553092241287, 0.09559232741594315, 0.09419882297515869]}}
{"id": "WebQTest-294", "question": "who was carolus linnaeus", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["fictional_universe.fictional_character.occupation"], ["symbols.name_source.namesakes"], ["common.topic.image"], ["influence.influence_node.influenced_by", "people.person.profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was carolus linnaeus [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>"], "scores": [-0.0008927154121920466, -0.07008488476276398, -0.07838620245456696, -0.10764537751674652, -0.1140117347240448, -0.11839550733566284, -0.12736570835113525, -0.12822352349758148, -0.13524003326892853, -0.13547320663928986], "norm_scores": [0.11050309240818024, 0.10311566293239594, 0.10226321965456009, 0.09931442141532898, 0.098684161901474, 0.09825250506401062, 0.09737509489059448, 0.09729159623384476, 0.09661134332418442, 0.09658882021903992]}}
{"id": "WebQTest-295", "question": "what type of government does germany have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["form_of_government.countries.country"], ["book.newspaper.circulation_areas", "government.form_of_government.countries"], ["form_of_government.countries.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does germany have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> form_of_government.countries.country </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> government.form_of_government.countries </PATH>", "<PATH> form_of_government.countries.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.009665397927165031, -0.014389198273420334, -0.1403445154428482, -0.154216006398201, -0.15537326037883759, -0.15853823721408844, -0.16229026019573212, -0.1651570200920105, -0.17018724977970123, -0.17058566212654114], "norm_scores": [0.11258988827466965, 0.11205928772687912, 0.09879755228757858, 0.09743654727935791, 0.09732384979724884, 0.09701630473136902, 0.09665298461914062, 0.09637629985809326, 0.09589272737503052, 0.09585452824831009]}}
{"id": "WebQTest-296", "question": "what kind of government does libya have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["base.biblioness.bibs_location.country", "government.form_of_government.countries"], ["people.person.nationality", "people.person.nationality", "government.form_of_government.countries"], ["base.biblioness.bibs_location.country", "location.country.form_of_government"], ["base.biblioness.bibs_location.country"], ["people.person.nationality", "people.person.nationality", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does libya have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality <SEP> location.country.form_of_government </PATH>", "</PATH> government.form_of_government.countries </PATH>"], "scores": [-0.005729174707084894, -0.021115457639098167, -0.1490781009197235, -0.160551980137825, -0.16324163973331451, -0.1633373200893402, -0.16594946384429932, -0.16668613255023956, -0.17218288779258728, -0.17465218901634216], "norm_scores": [0.11349806934595108, 0.11176512390375137, 0.09834060072898865, 0.09721869975328445, 0.09695756435394287, 0.0969482883810997, 0.09669537097215652, 0.09662417322397232, 0.09609450399875641, 0.09585750848054886]}}
{"id": "WebQTest-297", "question": "what time zone is anaheim california", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["base.biblioness.bibs_location.country", "location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby", "location.location.time_zones"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.contains", "location.location.time_zones"], ["location.city.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is anaheim california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.contains <SEP> location.location.time_zones </PATH>", "<PATH> location.city.time_zones </PATH>"], "scores": [-1.0411530638521072e-05, -0.17110949754714966, -0.1759878695011139, -0.18289393186569214, -0.1853117197751999, -0.18938027322292328, -0.19329772889614105, -0.19495975971221924, -0.19997499883174896, -0.2018173187971115], "norm_scores": [0.11826330423355103, 0.09966498613357544, 0.09917997568845749, 0.09849737584590912, 0.09825953096151352, 0.09786055982112885, 0.09747794270515442, 0.09731607139110565, 0.09682923555374146, 0.09665101021528244]}}
{"id": "WebQTest-298", "question": "what year did the orioles go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_league_season.champion"], ["time.event.instance_of_recurring_event"], ["sports.mascot.team"], ["sports.sports_teams.champion"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the orioles go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_league_season.champion </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.mascot.team </PATH>", "<PATH> sports.sports_teams.champion </PATH>", "<PATH> sports.sports_championship_event.champion <PATH>", "</PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.006865667179226875, -0.017933694645762444, -0.07608035206794739, -0.11473241448402405, -0.12738387286663055, -0.13433344662189484, -0.16266335546970367, -0.1652415245771408, -0.16633988916873932, -0.16983836889266968], "norm_scores": [0.1111343577504158, 0.10991109907627106, 0.10370239615440369, 0.09977056086063385, 0.09851627051830292, 0.09783399850130081, 0.0951012521982193, 0.0948563888669014, 0.09475225955247879, 0.09442134946584702]}}
{"id": "WebQTest-300", "question": "who is sir francis bacon", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["symbols.namesake.named_after"], ["common.topic.image"], ["symbols.name_source.namesakes"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir francis bacon [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.00010861126793315634, -0.11791864037513733, -0.1273135244846344, -0.14241480827331543, -0.14345334470272064, -0.1465521901845932, -0.1477847695350647, -0.16691607236862183, -0.1741684377193451, -0.19097119569778442], "norm_scores": [0.11438285559415817, 0.10167090594768524, 0.10072018951177597, 0.09921061247587204, 0.09910763800144196, 0.09880099445581436, 0.09867928922176361, 0.09680937230587006, 0.09610980749130249, 0.09450839459896088]}}
{"id": "WebQTest-301", "question": "what were amelia earhart 's achievements", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["base.activism.activist.area_of_activism"], ["fictional_universe.fictional_character.occupation"], ["base.activism.activism_issue.activists"], ["symbols.name_source.namesakes"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["base.americancomedy.impersonated_person.impersonated_by"], ["people.profession.people_with_this_profession"], ["symbols.namesake.named_after"]], "ground_paths": [["fictional_universe.fictional_character.occupation"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were amelia earhart 's achievements [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> base.americancomedy.impersonated_person.impersonated_by </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.006219479721039534, -0.03408852964639664, -0.051681339740753174, -0.05775955319404602, -0.08749930560588837, -0.0911300852894783, -0.0988537147641182, -0.10099625587463379, -0.10243832319974899, -0.10495609790086746], "norm_scores": [0.1069093719124794, 0.10397104173898697, 0.10215789824724197, 0.10153883695602417, 0.09856356680393219, 0.09820634871721268, 0.09745076298713684, 0.09724219143390656, 0.09710206836462021, 0.09685789048671722]}}
{"id": "WebQTest-302", "question": "what are the official colors of the dallas cowboys", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.sports_team_colors.team"], ["sports.professional_sports_team.owner_s", "sports.sports_team.colors"], ["sports.professional_sports_team.colors"], ["sports.sports_team_color.team"], ["sports.sports_team.team_colors"], ["sports.sports_team.color"], ["sports.sports_team_colors.teams"], ["sports.sports_team_colors.sport", "sports.sports_team.colors"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official colors of the dallas cowboys [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team_colors.team </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.colors </PATH>", "<PATH> sports.sports_team_color.team </PATH>", "<PATH> sports.sports_team.team_colors </PATH>", "<PATH> sports.sports_team.color </PATH>", "<PATH> sports.sports_team_colors.teams </PATH>", "<PATH> sports.sports_team_colors.sport <SEP> sports.sports_team.colors </PATH>"], "scores": [-0.00010760638542706147, -0.09581343084573746, -0.11416106671094894, -0.11520812660455704, -0.12101930379867554, -0.12180033326148987, -0.13657546043395996, -0.13719387352466583, -0.1402077078819275, -0.14216987788677216], "norm_scores": [0.11179531365633011, 0.10159189999103546, 0.09974493086338043, 0.09964054822921753, 0.0990631952881813, 0.09898585826158524, 0.09753407537937164, 0.09747377783060074, 0.09718044847249985, 0.0969899520277977]}}
{"id": "WebQTest-303", "question": "where did margaret hoover go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.parent_institution"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["people.person.education", "education.educational_institution.newspaper"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did margaret hoover go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.newspaper </PATH>"], "scores": [-0.00968153029680252, -0.024428581818938255, -0.027814622968435287, -0.04470603168010712, -0.05814744532108307, -0.0803501084446907, -0.08926237374544144, -0.09081020206212997, -0.10503166168928146, -0.11474385112524033], "norm_scores": [0.10557059198617935, 0.10402516275644302, 0.10367351770401001, 0.10193704068660736, 0.1005760207772255, 0.09836757928133011, 0.09749479591846466, 0.09734400361776352, 0.09596943110227585, 0.09504187107086182]}}
{"id": "WebQTest-304", "question": "what is jamie oliver children names", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"], ["influence.influence_node.influenced_by", "people.person.children"], ["influence.influence_node.influenced_by", "people.person.parents"], ["influence.influence_node.influenced"], ["people.person.nationality"], ["influence.influence_node.influenced_by"], ["people.person.profession"], ["people.person.gender"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie oliver children names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.children </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.parents </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.0117145124822855, -0.016079870983958244, -0.15437255799770355, -0.17199155688285828, -0.174554705619812, -0.17510277032852173, -0.1765679568052292, -0.17736047506332397, -0.17900454998016357, -0.1837923675775528], "norm_scores": [0.11367741227149963, 0.11318224668502808, 0.09856405854225159, 0.09684266149997711, 0.09659476578235626, 0.09654183685779572, 0.09640049189329147, 0.09632411599159241, 0.09616588801145554, 0.09570655971765518]}}
{"id": "WebQTest-305", "question": "where does bradley walsh live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.children", "people.person.place_of_birth"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does bradley walsh live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.017652153968811035, -0.017665348947048187, -0.036252930760383606, -0.0394129678606987, -0.05103220045566559, -0.06503820419311523, -0.07747242599725723, -0.08382514119148254, -0.099461130797863, -0.10900972038507462], "norm_scores": [0.10424305498600006, 0.10424168407917023, 0.10232197493314743, 0.10199914872646332, 0.10082085430622101, 0.09941859543323517, 0.0981900617480278, 0.09756825864315033, 0.0960545465350151, 0.09514173865318298]}}
{"id": "WebQTest-306", "question": "who is the head coach of inter milan", "prediction": [["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["base.schemastaging.sports_team_extra.managers", "base.schemastaging.sports_team_manager_tenure.manager"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["soccer.football_team_management_tenure.team", "soccer.football_team_manager.team"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"], ["soccer.football_team.manager", "soccer.football_team_management_tenure.manager"], ["soccer.football_team.manager"], ["base.schemastaging.athlete_salary.team", "base.schemastaging.athlete_salary.athlete"], ["soccer.football_team_coach_tenure.team", "soccer.football_team_coach.teams_coached"]], "ground_paths": [["soccer.football_team_management_tenure.team", "soccer.football_team_manager.team"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["base.schemastaging.sports_team_manager_tenure.team", "base.schemastaging.sports_team_manager_tenure.manager"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the head coach of inter milan [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> base.schemastaging.sports_team_extra.managers <SEP> base.schemastaging.sports_team_manager_tenure.manager </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> soccer.football_team_management_tenure.team <SEP> soccer.football_team_manager.team </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> soccer.football_team.manager <SEP> soccer.football_team_management_tenure.manager </PATH>", "<PATH> soccer.football_team.manager </PATH>", "<PATH> base.schemastaging.athlete_salary.team <SEP> base.schemastaging.athlete_salary.athlete </PATH>", "<PATH> soccer.football_team_coach_tenure.team <SEP> soccer.football_team_coach.teams_coached </PATH>"], "scores": [-0.019846756011247635, -0.022122107446193695, -0.024861223995685577, -0.033129505813121796, -0.0378187820315361, -0.04637762904167175, -0.04988991469144821, -0.059762805700302124, -0.06199146434664726, -0.06435488164424896], "norm_scores": [0.10222847759723663, 0.10199613869190216, 0.10171714425086975, 0.10087957978248596, 0.10040763765573502, 0.09955193102359772, 0.09920288622379303, 0.09822829067707062, 0.0980096161365509, 0.09777824580669403]}}
{"id": "WebQTest-308", "question": "who did reese witherspoon get married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.marriage.spouse", "people.marriage.spouse"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did reese witherspoon get married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.celebrity.sexual_relationships </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.017155760899186134, -0.028784193098545074, -0.03194968402385712, -0.046032898128032684, -0.047928620129823685, -0.048705942928791046, -0.04871664196252823, -0.0487418957054615, -0.049704715609550476, -0.05046910047531128], "norm_scores": [0.10249075293540955, 0.10130584239959717, 0.10098566859960556, 0.09957343339920044, 0.0993848443031311, 0.09930762648582458, 0.09930656850337982, 0.09930405765771866, 0.09920848906040192, 0.09913268685340881]}}
{"id": "WebQTest-310", "question": "what kind of money should i take to costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["language.human_language.countries_spoken_in"], ["government.form_of_government.countries", "location.country.currency_used"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["common.topic.notable_types"], ["government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money should i take to costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.currency_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.007656437810510397, -0.019025029614567757, -0.11775192618370056, -0.12048371136188507, -0.13047349452972412, -0.1333489716053009, -0.13781465590000153, -0.1433062106370926, -0.1443188339471817, -0.1504361480474472], "norm_scores": [0.11068834364414215, 0.10943710058927536, 0.09914892911911011, 0.09887844324111938, 0.09789558500051498, 0.09761448949575424, 0.09717954695224762, 0.09664735198020935, 0.09654953330755234, 0.09596071392297745]}}
{"id": "WebQTest-311", "question": "where is the ufc headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["sports.sports_league.sport", "sports.sports_team.location"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["sports.sports_organization.headquarters", "location.mailing_address.citytown"], ["sports.sports_league_draft_pick.league", "sports.sports_league_draft_pick.location"], ["sports.sports_league_participation.league", "sports.sports_team.location"], ["sports.sports_organization.headquarters", "sports.sports_team.location"], ["sports.sports_league.sport", "sports.sports_team_location.teams"], ["organization.organization.place_founded"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["common.topic.subject_of", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the ufc headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> sports.sports_league.sport <SEP> sports.sports_team.location </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> sports.sports_organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> sports.sports_league_draft_pick.league <SEP> sports.sports_league_draft_pick.location </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_team.location </PATH>", "<PATH> sports.sports_organization.headquarters <SEP> sports.sports_team.location </PATH>", "<PATH> sports.sports_league.sport <SEP> sports.sports_team_location.teams </PATH>", "<PATH> organization.organization.place_founded </PATH>"], "scores": [-0.009703146293759346, -0.03791796416044235, -0.04589402675628662, -0.05648115649819374, -0.060468729585409164, -0.06529827415943146, -0.0663347914814949, -0.06801654398441315, -0.06997189670801163, -0.07272723317146301], "norm_scores": [0.10464533418416977, 0.10173404961824417, 0.10092584788799286, 0.0998629704117775, 0.09946554154157639, 0.09898632764816284, 0.09888378530740738, 0.09871762245893478, 0.0985247865319252, 0.09825368970632553]}}
{"id": "WebQTest-312", "question": "who plays captain kirk in star trek", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["media_common.quotation.spoken_by_character", "media_common.quotation.author"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"]], "ground_paths": [["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays captain kirk in star trek [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> media_common.quotation.spoken_by_character <SEP> media_common.quotation.author </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> fictional_universe.fictional_character.quotations <SEP> media_common.quotation.author </PATH>"], "scores": [-0.02567320317029953, -0.028123052790760994, -0.02860678732395172, -0.029444757848978043, -0.0315314456820488, -0.04150395840406418, -0.04177060350775719, -0.05264906957745552, -0.06698766350746155, -0.06940346211194992], "norm_scores": [0.10159021615982056, 0.10134164243936539, 0.10129263252019882, 0.10120779275894165, 0.10099681466817856, 0.09999462962150574, 0.09996797144412994, 0.09888635575771332, 0.09747859090566635, 0.09724338352680206]}}
{"id": "WebQTest-313", "question": "who does albert pujols play for 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.sports_award.award_winner", "sports.sports_award.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does albert pujols play for 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.team </PATH>"], "scores": [-0.011058447882533073, -0.023905739188194275, -0.02608691155910492, -0.029785923659801483, -0.03508303314447403, -0.0426236130297184, -0.04908766597509384, -0.06178806349635124, -0.06743034720420837, -0.0680379718542099], "norm_scores": [0.10307177901268005, 0.10175605863332748, 0.10153434425592422, 0.10115946829319, 0.10062503069639206, 0.09986911714076996, 0.09922563284635544, 0.09797339886426926, 0.09742216765880585, 0.09736298769712448]}}
{"id": "WebQTest-314", "question": "what kind of government is sweden", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["people.person.nationality", "government.form_of_government.countries"], ["language.human_language.main_country", "government.form_of_government.countries"], ["base.locations.countries.continent"], ["geography.river.basin_countries"], ["common.topic.notable_types"], ["form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> form_of_government.countries </PATH>"], "scores": [-0.011533726006746292, -0.013090638443827629, -0.14513026177883148, -0.15962249040603638, -0.16088326275348663, -0.16429826617240906, -0.1721065491437912, -0.18226851522922516, -0.18230658769607544, -0.18245582282543182], "norm_scores": [0.11317411810159683, 0.11299806088209152, 0.09902090579271317, 0.09759621322154999, 0.09747324883937836, 0.09714094549417496, 0.0963853970170021, 0.09541089832782745, 0.09540726244449615, 0.09539301693439484]}}
{"id": "WebQTest-315", "question": "where was country singer george jones born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["music.artist.origin", "location.location.people_born_here"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality"], ["music.artist.label", "music.artist.label"], ["base.biblioness.bibs_location.country"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was country singer george jones born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> music.artist.origin <SEP> location.location.people_born_here </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.010372337885200977, -0.019130317494273186, -0.050328873097896576, -0.09652910381555557, -0.13272523880004883, -0.14494206011295319, -0.14931879937648773, -0.15322701632976532, -0.15442559123039246, -0.15852844715118408], "norm_scores": [0.1099652424454689, 0.1090063750743866, 0.10565803945064545, 0.10088765621185303, 0.09730120748281479, 0.09611973911523819, 0.0956999659538269, 0.09532666951417923, 0.09521248936653137, 0.09482263773679733]}}
{"id": "WebQTest-316", "question": "what did shakespeare become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["freebase.valuenotation.is_reviewed"], ["theater.theater_character.plays_appears_in"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["people.profession.people_with_this_profession"], ["film.film_story_contributor.film_story_credits"], ["influence.influence_node.influenced_by"]], "ground_paths": [["fictional_universe.character_occupation.characters_with_this_occupation"], ["fictional_universe.fictional_character.occupation"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shakespeare become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> freebase.valuenotation.is_reviewed </PATH>", "<PATH> theater.theater_character.plays_appears_in </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.005104952957481146, -0.03418770059943199, -0.05595617741346359, -0.07030782848596573, -0.09463413804769516, -0.1028873473405838, -0.10631372779607773, -0.11610773205757141, -0.12579824030399323, -0.12773089110851288], "norm_scores": [0.10811486095190048, 0.10501586645841599, 0.10275453329086304, 0.10129036009311676, 0.09885606914758682, 0.0980435460805893, 0.09770820289850235, 0.09675591439008713, 0.09582282602787018, 0.09563781321048737]}}
{"id": "WebQTest-317", "question": "where does russia import from", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["location.statistical_region.places_imported_from", "location.imports_and_exports.imported_from"], ["location.country.form_of_government", "location.country.form_of_government"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does russia import from [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.statistical_region.places_imported_from <SEP> location.statistical_region.places_exported_to </PATH>", "<PATH> location.imports_and_exports.imported_from <SEP> location.statistical_region.places_imported_from </PATH>", "<PATH> location.statistical_region.places_imported_from <SEP> location.imports_and_exports.imported_from </PATH>", "<PATH> location.country.form_of_government <SEP> location.country.form_of_government </PATH>"], "scores": [-0.027850110083818436, -0.02866261452436447, -0.031042229384183884, -0.03493966907262802, -0.035216908901929855, -0.04320671781897545, -0.047983504831790924, -0.049264419823884964, -0.04949519410729408, -0.05369977653026581], "norm_scores": [0.10123191773891449, 0.10114970803260803, 0.1009092926979065, 0.10051676630973816, 0.10048890113830566, 0.09968921542167664, 0.09921415150165558, 0.09908715635538101, 0.09906429797410965, 0.09864863753318787]}}
{"id": "WebQTest-318", "question": "what is the money of switzerland called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["common.topic.notable_types"], ["location.country.currency_used", "location.country.currency_used"], ["location.country.currency"], ["base.coinsdaily.coin_type.country"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the money of switzerland called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.004134023562073708, -0.03059396706521511, -0.14026479423046112, -0.15362116694450378, -0.15933524072170258, -0.16460540890693665, -0.1655413955450058, -0.16943858563899994, -0.17039960622787476, -0.17095479369163513], "norm_scores": [0.11353953927755356, 0.11057468503713608, 0.09908919781446457, 0.09777451306581497, 0.09721741825342178, 0.09670641273260117, 0.09661594033241272, 0.09624014049768448, 0.09614770114421844, 0.09609434008598328]}}
{"id": "WebQTest-321", "question": "what do people in australia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["film.film.featured_film_locations"], ["languages.human_language.countries_spoken_in"], ["book.newspaper.circulation_areas"], ["film.film.country"], ["country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in australia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> languages.human_language.countries_spoken_in </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> film.film.country </PATH>", "<PATH> country.languages_spoken </PATH>"], "scores": [-0.006882531102746725, -0.022528652101755142, -0.07080850750207901, -0.10617408156394958, -0.15611396729946136, -0.18138070404529572, -0.18144452571868896, -0.18246296048164368, -0.1976146101951599, -0.1996019035577774], "norm_scores": [0.11287779361009598, 0.11112543195486069, 0.10588777810335159, 0.10220843553543091, 0.09722951799631119, 0.09480361640453339, 0.09479756653308868, 0.09470107406377792, 0.09327701479196548, 0.09309183061122894]}}
{"id": "WebQTest-322", "question": "what has ian somerhalder acted in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.producer.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.film.produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ian somerhalder acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_program.regular_cast </PATH>"], "scores": [-0.018833670765161514, -0.030923916026949883, -0.032849885523319244, -0.03436852991580963, -0.03615448251366615, -0.04447142779827118, -0.05073947831988335, -0.06571868062019348, -0.0793740302324295, -0.08529478311538696], "norm_scores": [0.1029239073395729, 0.10168702900409698, 0.10149136930704117, 0.10133735835552216, 0.10115653276443481, 0.10031870007514954, 0.09969187527894974, 0.0982096940279007, 0.09687772393226624, 0.09630582481622696]}}
{"id": "WebQTest-323", "question": "what movies did ron howard director", "prediction": [["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"], ["film.producer.film"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["film.director.film"], ["film.producer.film"], ["film.film.directed_by"], ["film.film.produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did ron howard director [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.director.film </PATH>", "<PATH> film.producer.film </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.02225477248430252, -0.03128189593553543, -0.03344522416591644, -0.04467490687966347, -0.0454401858150959, -0.04613222926855087, -0.07083266973495483, -0.07136549055576324, -0.07312679290771484, -0.0737617239356041], "norm_scores": [0.10292240977287292, 0.1019975021481514, 0.101777084171772, 0.10064055025577545, 0.10056356340646744, 0.10049399733543396, 0.09804215282201767, 0.09798993170261383, 0.09781748801469803, 0.09775540232658386]}}
{"id": "WebQTest-326", "question": "what did gerald r ford die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.person.cause_of_death"], ["people.person.deceased_cause_of_death"], ["deceased_person.deceased_person.cause_of_death"], ["people.people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["medicine.notable_person_with_medical_condition.condition"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gerald r ford die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.person.deceased_cause_of_death </PATH>", "<PATH> deceased_person.deceased_person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>"], "scores": [-0.0012332036858424544, -0.048777610063552856, -0.10340868681669235, -0.1231679618358612, -0.13504265248775482, -0.14107872545719147, -0.14671772718429565, -0.14787307381629944, -0.15595434606075287, -0.15733511745929718], "norm_scores": [0.11202938854694366, 0.10682765394449234, 0.1011480912566185, 0.09916910529136658, 0.0979984700679779, 0.0974087193608284, 0.09686098247766495, 0.09674912691116333, 0.09597042948007584, 0.09583800286054611]}}
{"id": "WebQTest-327", "question": "what two continents is turkey on", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["base.locations.continents.countries_within"], ["location.location.contains"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.contains", "location.location.containedby"]], "ground_paths": [["base.locations.countries.continent"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two continents is turkey on [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.adjoining_relationship.adjoins </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.location.adjoin_s </PATH>", "<PATH> location.location.adjoin_s <SEP> location.adjoining_relationship.adjoins </PATH>", "<PATH> location.location.contains <SEP> location.location.containedby </PATH>"], "scores": [-0.012209422886371613, -0.02935369685292244, -0.04604324698448181, -0.05443902686238289, -0.05985060706734657, -0.06438490003347397, -0.06696014851331711, -0.07638533413410187, -0.08262787759304047, -0.08912481367588043], "norm_scores": [0.10467308014631271, 0.10289382934570312, 0.10119083523750305, 0.10034482181072235, 0.09980325400829315, 0.09935174137353897, 0.09909621626138687, 0.09816660732030869, 0.09755570441484451, 0.09692393988370895]}}
{"id": "WebQTest-328", "question": "where did kevin love go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["people.person.education", "education.educational_institution.parent_institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin love go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>"], "scores": [-0.015708008781075478, -0.0211145281791687, -0.027414189651608467, -0.032784849405288696, -0.04494098201394081, -0.04955076798796654, -0.07029641419649124, -0.07038220763206482, -0.08890746533870697, -0.09467315673828125], "norm_scores": [0.10361504554748535, 0.10305636376142502, 0.10240918397903442, 0.1018606498837471, 0.10062991827726364, 0.10016710311174393, 0.09811047464609146, 0.09810205549001694, 0.09630142152309418, 0.09574777632951736]}}
{"id": "WebQTest-329", "question": "what type of government was formed when italy unified", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent", "government.form_of_government.countries"], ["government.form_of_government.countries", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["symbols.namesake.named_after", "government.form_of_government.countries"], ["people.person.nationality", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government was formed when italy unified [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> symbols.namesake.named_after <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.010263312608003616, -0.012886197306215763, -0.11097810417413712, -0.11342934519052505, -0.1235022321343422, -0.12861421704292297, -0.13107579946517944, -0.1361807882785797, -0.13742905855178833, -0.14015868306159973], "norm_scores": [0.10975101590156555, 0.10946352779865265, 0.09923586994409561, 0.09899291396141052, 0.09800077229738235, 0.09750107675790787, 0.09726135432720184, 0.09676610678434372, 0.09664539247751236, 0.09638195484876633]}}
{"id": "WebQTest-331", "question": "where was the temple of karnak built", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["architecture.structure.architect"], ["architecture.architect.structures_designed"], ["architecture.building_complex.building_sites"], ["architecture.building_complex.location"], ["location.country.administrative_divisions"], ["fictional_universe.fictional_setting.contained_by"], ["location.location.contains"], ["architecture.building_complex.buildings_sites"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the temple of karnak built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> architecture.building_complex.building_sites </PATH>", "<PATH> architecture.building_complex.location </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> fictional_universe.fictional_setting.contained_by </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> architecture.building_complex.buildings_sites </PATH>"], "scores": [-0.0017739773029461503, -0.06533795595169067, -0.07440810650587082, -0.10399999469518661, -0.10828043520450592, -0.1099320724606514, -0.11073845624923706, -0.11224675923585892, -0.11703696846961975, -0.12050949782133102], "norm_scores": [0.10942136496305466, 0.10268255323171616, 0.10175541788339615, 0.09878839552402496, 0.09836644679307938, 0.0982041135430336, 0.09812495112419128, 0.09797706454992294, 0.09750886261463165, 0.09717083722352982]}}
{"id": "WebQTest-333", "question": "what movies did morgan freeman star in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "film.film.language"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.producer.films_executive_produced"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.producer.film"], ["theater.theater_actor.theater_roles", "theater.theater_role.play"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.film.executive_produced_by"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did morgan freeman star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>"], "scores": [-0.018669195473194122, -0.03737962618470192, -0.038085974752902985, -0.038421016186475754, -0.04069840908050537, -0.04137937352061272, -0.04183683171868324, -0.0438879132270813, -0.044035959988832474, -0.051826078444719315], "norm_scores": [0.10211410373449326, 0.10022126138210297, 0.10015050321817398, 0.10011694580316544, 0.09988921135663986, 0.09982120990753174, 0.09977555274963379, 0.0995711162686348, 0.09955637156963348, 0.09878382831811905]}}
{"id": "WebQTest-334", "question": "who did john kennedy have affairs with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.sexual_relationships"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["base.americancomedy.impersonated_celebrity.sexual_relationships", "base.americancomedy.impersonated_celebrity.sexual_relationships"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["music.recording.featured_artists", "music.recording.artist"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did john kennedy have affairs with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.americancomedy.impersonated_celebrity.impersonated_by <SEP> base.americancomedy.impersonated_celebrity.impersonated_by </PATH>", "<PATH> base.americancomedy.impersonated_celebrity.impersonated_by <SEP> base.americancomedy.impersonated_celebrity.sexual_relationships </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.area_of_activism </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> base.americancomedy.impersonated_celebrity.sexual_relationships <SEP> base.americancomedy.impersonated_celebrity.sexual_relationships </PATH>"], "scores": [-0.020458422601222992, -0.03317932412028313, -0.03363341465592384, -0.04334757477045059, -0.04608920216560364, -0.04809623956680298, -0.04827030748128891, -0.04976075515151024, -0.0512734018266201, -0.05416353791952133], "norm_scores": [0.10225698351860046, 0.10096441954374313, 0.10091858357191086, 0.09994298964738846, 0.09966935962438583, 0.09946951270103455, 0.09945220500230789, 0.09930408746004105, 0.09915398806333542, 0.09886783361434937]}}
{"id": "WebQTest-335", "question": "what type of money to take to cuba", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.tourist_attraction.near_travel_destination"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tourist_attraction.near_travel_destination", "location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["base.coinsdaily.coin_type.country"], ["travel.tourist_attraction.near_travel_destination", "finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money to take to cuba [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.010851345956325531, -0.015274344943463802, -0.10172929614782333, -0.10935957729816437, -0.11512885242700577, -0.11687146872282028, -0.12228749692440033, -0.12516538798809052, -0.1280946284532547, -0.1372242420911789], "norm_scores": [0.10902192443609238, 0.10854078829288483, 0.09955109655857086, 0.09879438579082489, 0.09822604805231094, 0.09805502742528915, 0.09752540290355682, 0.09724514186382294, 0.09696070104837418, 0.0960795059800148]}}
{"id": "WebQTest-336", "question": "what currency should you take to morocco", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["base.coinsdaily.coin_type.country"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["common.topic.notable_types"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to morocco [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.011675450019538403, -0.014263792894780636, -0.1083662137389183, -0.12496870756149292, -0.12699347734451294, -0.13397976756095886, -0.1387660801410675, -0.15182484686374664, -0.1611599326133728, -0.16773764789104462], "norm_scores": [0.11061161756515503, 0.11032568663358688, 0.10041727870702744, 0.09876387566328049, 0.09856410324573517, 0.09787790477275848, 0.09741054475307465, 0.09614675492048264, 0.09525339305400848, 0.09462889283895493]}}
{"id": "WebQTest-337", "question": "who is mary mcleod bethune for kids", "prediction": [["people.person.profession"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.profession.people_with_this_profession"], ["education.education.student", "education.education.student"], ["symbols.namesake.named_after"], ["fictional_universe.fictional_character.occupation"], ["common.topic.notable_types"], ["symbols.name_source.namesakes"], ["book.author.works_written"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is mary mcleod bethune for kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> education.education.student <SEP> education.education.student </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0005262293852865696, -0.08392059057950974, -0.0855652242898941, -0.09277540445327759, -0.09298688918352127, -0.10788043588399887, -0.10926293581724167, -0.10984719544649124, -0.113728828728199, -0.11656258255243301], "norm_scores": [0.10944442451000214, 0.10068758577108383, 0.10052213817834854, 0.09979995340108871, 0.09977885335683823, 0.09830379486083984, 0.09816799312829971, 0.09811065346002579, 0.09773056954145432, 0.0974540039896965]}}
{"id": "WebQTest-338", "question": "what do they call money in japan", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"], ["language.human_language.main_country"], ["language.human_language.countries_spoken_in", "location.country.currency_used"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country"], ["language.human_language.main_country", "location.country.currency_used"], ["common.topic.notable_types"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they call money in japan [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.013274077326059341, -0.01761489361524582, -0.039120979607105255, -0.10277057439088821, -0.11175879091024399, -0.12334911525249481, -0.13199582695960999, -0.13836702704429626, -0.14618027210235596, -0.15431028604507446], "norm_scores": [0.10868388414382935, 0.10821312665939331, 0.10591073334217072, 0.09937961399555206, 0.09849037230014801, 0.0973554253578186, 0.09651725739240646, 0.09590426832437515, 0.09515786916017532, 0.09438736736774445]}}
{"id": "WebQTest-339", "question": "what nationality is mother teresa", "prediction": [["people.person.nationality"], ["people.person.ethnicity"], ["people.ethnicity.people"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["film.film_subject.films"], ["location.location.people_born_here"], ["film.film_subject.films", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is mother teresa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> film.film_subject.films </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> film.film_subject.films <SEP> people.person.nationality </PATH>"], "scores": [-0.005917608272284269, -0.03655898943543434, -0.06338430196046829, -0.08191327750682831, -0.08683218061923981, -0.08738194406032562, -0.09277526289224625, -0.09617810696363449, -0.11621078103780746, -0.1292850226163864], "norm_scores": [0.10758651047945023, 0.10433991253376007, 0.10157816857099533, 0.09971335530281067, 0.09922407567501068, 0.0991695374250412, 0.09863612800836563, 0.09830106049776077, 0.09635141491889954, 0.09509988874197006]}}
{"id": "WebQTest-340", "question": "who is richard pryor son", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"], ["people.person.parent"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.sibling_s", "people.person.sibling_s"], ["people.person.sibling_s"], ["people.person.place_of_birth"], ["people.person.profession"], ["people.person.spouse_s"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is richard pryor son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> people.person.sibling_s <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.person.sibling_s <SEP> people.person.sibling_s </PATH>", "<PATH> people.person.sibling_s </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.spouse_s </PATH>"], "scores": [-0.003381785238161683, -0.03793482854962349, -0.09715099632740021, -0.1430891752243042, -0.14858563244342804, -0.15388986468315125, -0.1593249887228012, -0.1638246327638626, -0.1647501438856125, -0.16856689751148224], "norm_scores": [0.11264624446630478, 0.10882045328617096, 0.10256360471248627, 0.09795860201120377, 0.09742165356874466, 0.09690626710653305, 0.09638100117444992, 0.09594830125570297, 0.09585953503847122, 0.0954943597316742]}}
{"id": "WebQTest-341", "question": "what countries share borders with spain", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.country.form_of_government", "location.country.form_of_government"], ["common.topic.notable_types", "common.topic.notable_types"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["film.film.country", "film.film.country"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["finance.currency.countries_used", "location.country.currency_used"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.country.form_of_government", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.location.primarily_containedby", "location.location.containedby"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["location.location.primarily_containedby", "location.location.contains_major_portion_of"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.primarily_containedby", "base.locations.continents.countries_within"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.country.currency_formerly_used", "finance.currency.countries_formerly_used"], ["location.location.primarily_containedby", "location.location.primarily_containedby"], ["location.location.partially_containedby", "location.location.containedby"], ["location.location.partially_contains", "location.location.contains"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["royalty.royal_line.kingdom_s_ruled", "people.family.country"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.partially_contains", "location.location.primarily_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.location.time_zones", "location.location.time_zones"], ["location.country.internet_tld", "location.country.internet_tld"], ["location.location.containedby", "location.location.partially_containedby"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["geography.river.basin_countries", "location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries share borders with spain [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.country.form_of_government <SEP> location.country.form_of_government </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> biology.animal_breed.place_of_origin <SEP> biology.breed_origin.breeds_originating_here </PATH>"], "scores": [-0.03571981564164162, -0.04065834358334541, -0.04252171516418457, -0.045130662620067596, -0.04659111052751541, -0.04663274064660072, -0.04666149988770485, -0.046665795147418976, -0.04874895513057709, -0.04925575479865074], "norm_scores": [0.10091729462146759, 0.10042013972997665, 0.10023318976163864, 0.09997203201055527, 0.0998261347413063, 0.09982196986675262, 0.09981910139322281, 0.09981867671012878, 0.09961095452308655, 0.09956049174070358]}}
{"id": "WebQTest-342", "question": "what countries have english as their official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["base.rosetta.local_name.locale"], ["country.official_language.countries_spoken_in"], ["base.rosetta.languoid.languoid_class"], ["base.rosetta.local_name.languoid"], ["location.country.internet_tld"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have english as their official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> country.official_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.local_name.languoid </PATH>", "<PATH> location.country.internet_tld </PATH>"], "scores": [-0.01708352379500866, -0.020115472376346588, -0.022738805040717125, -0.0869186595082283, -0.13872165977954865, -0.1512681394815445, -0.15933026373386383, -0.16154353320598602, -0.16707779467105865, -0.17331291735172272], "norm_scores": [0.10949602723121643, 0.10916454344987869, 0.10887853801250458, 0.10211025178432465, 0.096955306828022, 0.09574645757675171, 0.09497764706611633, 0.09476766735315323, 0.09424465149641037, 0.09365884214639664]}}
{"id": "WebQTest-343", "question": "what international organizations is china part of", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization_member.member_of", "organization.organization_membership.organization"], ["organization.organization_membership.member", "organization.organization_membership.organization"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["government.government_position_held.governmental_body", "government.government_position_held.governmental_body"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.scope"], ["organization.organization.founders"], ["organization.organization_member.member_of", "organization.organization_member.member_of"]], "ground_paths": [["organization.organization_member.member_of", "organization.organization_membership.organization"], ["organization.organization.founders"], ["organization.organization_membership.member", "organization.organization_membership.organization"], ["organization.organization_founder.organizations_founded"], ["organization.organization_membership.member", "organization.membership_organization.members"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what international organizations is china part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_member.member_of <SEP> organization.organization_membership.organization </PATH>", "<PATH> organization.organization_membership.member <SEP> organization.organization_membership.organization </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.governmental_body </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.scope </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_member.member_of <SEP> organization.organization_member.member_of </PATH>"], "scores": [-0.02454606629908085, -0.025665242224931717, -0.029030954465270042, -0.03445162624120712, -0.03748449683189392, -0.03972427919507027, -0.04621834307909012, -0.04643843322992325, -0.0580625981092453, -0.06129208579659462], "norm_scores": [0.10157958418130875, 0.10146596282720566, 0.10112503916025162, 0.10057835280895233, 0.10027376562356949, 0.10004943609237671, 0.0994018018245697, 0.09937993437051773, 0.09823141247034073, 0.0979146882891655]}}
{"id": "WebQTest-344", "question": "what county is west st paul in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"], ["location.citytown.postal_codes"], ["location.place_with_neighborhoods.neighborhoods"], ["location.location.containedby", "location.location.containedby"], ["location.hud_county_place.county", "location.hud_county_place.county"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is west st paul in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.place_with_neighborhoods.neighborhoods </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county <SEP> location.hud_county_place.county </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.013121828436851501, -0.025652198120951653, -0.04157101362943649, -0.049419622868299484, -0.05048590525984764, -0.07857254892587662, -0.0799146369099617, -0.10970398038625717, -0.11000309139490128, -0.11935609579086304], "norm_scores": [0.10555174946784973, 0.104237399995327, 0.10259120166301727, 0.10178915411233902, 0.1016806811094284, 0.09886454045772552, 0.09873194247484207, 0.0958341658115387, 0.09580549597740173, 0.09491360932588577]}}
{"id": "WebQTest-346", "question": "who does donnie wahlberg play in the sixth sense", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.performance.character"], ["film.actor.film", "film.performance.character"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does donnie wahlberg play in the sixth sense [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.02030467614531517, -0.030461756512522697, -0.03555961698293686, -0.037162911146879196, -0.03724105656147003, -0.037603359669446945, -0.040165532380342484, -0.04348495230078697, -0.045017343014478683, -0.07691776752471924], "norm_scores": [0.1020193099975586, 0.10098833590745926, 0.10047481954097748, 0.10031385719776154, 0.10030601918697357, 0.10026969015598297, 0.1000131145119667, 0.09968166798353195, 0.09952903538942337, 0.09640412777662277]}}
{"id": "WebQTest-347", "question": "what did baron de montesquie influence", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "organization.organization.founders"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "organization.organization_founder.organizations_founded"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["organization.organization_founder.organizations_founded"], ["influence.influence_node.influenced_by", "organization.organization.founders"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did baron de montesquie influence [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> organization.organization.founders </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> organization.organization_founder.organizations_founded </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> organization.organization.founders </PATH>"], "scores": [-0.004884494934231043, -0.023059239611029625, -0.10787609219551086, -0.1085292249917984, -0.10989782959222794, -0.11168955266475677, -0.13007156550884247, -0.1312616765499115, -0.1344282180070877, -0.143647238612175], "norm_scores": [0.10992360860109329, 0.1079438105225563, 0.0991658866405487, 0.09910113364458084, 0.0989656001329422, 0.09878843277692795, 0.09698908776044846, 0.09687373042106628, 0.09656746685504913, 0.09568130224943161]}}
{"id": "WebQTest-349", "question": "who did carlos boozer play for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did carlos boozer play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.009556286036968231, -0.015356126241385937, -0.052307065576314926, -0.052925944328308105, -0.05404562130570412, -0.05649527162313461, -0.05972598120570183, -0.06639520078897476, -0.06702791899442673, -0.07232847064733505], "norm_scores": [0.10417018085718155, 0.10356775671243668, 0.09981067478656769, 0.09974891692399979, 0.09963729977607727, 0.09939352422952652, 0.09907293319702148, 0.09841439127922058, 0.09835213422775269, 0.09783219546079636]}}
{"id": "WebQTest-351", "question": "who owns the portland press herald", "prediction": [["book.newspaper.owner"], ["periodicals.newspaper_circulation_area.newspapers", "business.business_operation.industry"], ["book.newspaper_owner.newspapers_owned"], ["periodicals.newspaper_circulation_area.newspapers", "book.periodical.language", "people.person.languages"], ["business.business_operation.industry", "business.business_operation.industry"], ["periodicals.newspaper_circulation_area.newspapers", "book.periodical.owner"], ["book.newspaper.circulation_areas"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.owner"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization.parent", "organization.organization_relationship.parent"]], "ground_paths": [["book.newspaper_owner.newspapers_owned"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns the portland press herald [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.owner </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> business.business_operation.industry </PATH>", "<PATH> book.newspaper_owner.newspapers_owned </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> book.periodical.language <SEP> people.person.languages </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> book.periodical.owner </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> book.newspaper.owner </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization_relationship.parent </PATH>"], "scores": [-0.02259877324104309, -0.03754095733165741, -0.038489457219839096, -0.04326990991830826, -0.04622599482536316, -0.04707060009241104, -0.05389039218425751, -0.054812293499708176, -0.06187134236097336, -0.06527525931596756], "norm_scores": [0.10247348248958588, 0.10095368325710297, 0.10085797309875488, 0.10037697851657867, 0.1000806987285614, 0.09999620914459229, 0.0993165671825409, 0.09922505170106888, 0.09852708876132965, 0.09819227457046509]}}
{"id": "WebQTest-352", "question": "what did albert speer design", "prediction": [["architecture.architect.structures_designed"], ["architecture.structure.architect"], ["architecture.architecture.architect"], ["architecture.building.architect"], ["architecture.architect.structures_designed", "architecture.architect.structures_designed"], ["architecture.architect.structures_designed", "architecture.structure.architect"], ["architect.architect.structures_designed"], ["symbols.namesake.named_after"], ["common.topic.notable_types"], ["film.film.story_by"]], "ground_paths": [["architecture.architect.structures_designed"], ["architecture.structure.architect"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did albert speer design [/INST]", "raw_output": {"paths": ["<PATH> architecture.architect.structures_designed </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.architecture.architect </PATH>", "<PATH> architecture.building.architect </PATH>", "<PATH> architecture.architect.structures_designed <SEP> architecture.architect.structures_designed </PATH>", "<PATH> architecture.architect.structures_designed <SEP> architecture.structure.architect </PATH>", "<PATH> architect.architect.structures_designed </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.0006097509758546948, -0.08461900800466537, -0.10743182897567749, -0.11624223738908768, -0.11722201108932495, -0.13570456206798553, -0.1393490433692932, -0.15008780360221863, -0.15387727320194244, -0.15573197603225708], "norm_scores": [0.11212903261184692, 0.10309398174285889, 0.10076873749494553, 0.09988483041524887, 0.09978701174259186, 0.09795963019132614, 0.09760327637195587, 0.09656073898077011, 0.0961955189704895, 0.0960172638297081]}}
{"id": "WebQTest-354", "question": "what art movement did leonardo da vinci belong to", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "visual_art.visual_artist.associated_periods_or_movements"], ["interests.hobby.people_with_this_hobby"], ["film.film.subjects"], ["symbols.namesake.named_after"], ["interests.hobbyist.hobbies"], ["influence.influence_node.influenced_by", "visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art movement did leonardo da vinci belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> interests.hobby.people_with_this_hobby </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> interests.hobbyist.hobbies </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.004188236314803362, -0.02512100711464882, -0.06079835072159767, -0.09063684195280075, -0.092458076775074, -0.09303627163171768, -0.09630326926708221, -0.09872927516698837, -0.10465993732213974, -0.10667766630649567], "norm_scores": [0.10751857608556747, 0.10529130697250366, 0.1016010195016861, 0.0986141785979271, 0.09843474626541138, 0.09837784618139267, 0.09805696457624435, 0.09781936556100845, 0.09724095463752747, 0.0970449447631836]}}
{"id": "WebQTest-356", "question": "what airport is near arlington tx", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"], ["geography.river.cities", "location.location.nearby_airports"], ["geography.river.cities", "location.location.containedby"], ["travel.travel_destination.near_airports"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport is near arlington tx [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>", "<PATH> geography.river.cities <SEP> location.location.nearby_airports </PATH>", "<PATH> geography.river.cities <SEP> location.location.containedby </PATH>", "<PATH> travel.travel_destination.near_airports </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.010934028774499893, -0.021480588242411613, -0.04326939955353737, -0.056021325290203094, -0.06984679400920868, -0.1037568673491478, -0.10792039334774017, -0.10992351174354553, -0.11409979313611984, -0.1153528094291687], "norm_scores": [0.10656555742025375, 0.10544756799936295, 0.1031748428940773, 0.10186751186847687, 0.10046884417533875, 0.0971190482378006, 0.0967155322432518, 0.09652199596166611, 0.09611973166465759, 0.09599936008453369]}}
{"id": "WebQTest-359", "question": "what team does heskey play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does heskey play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.02773098088800907, -0.028864743188023567, -0.02923636883497238, -0.03243019059300423, -0.035022418946027756, -0.03662809357047081, -0.03903590887784958, -0.03910142183303833, -0.04033998027443886, -0.04190179705619812], "norm_scores": [0.1007312759757042, 0.10061714053153992, 0.10057975351810455, 0.10025902837514877, 0.09999946504831314, 0.09983903169631958, 0.09959892928600311, 0.09959240257740021, 0.0994691252708435, 0.0993138998746872]}}
{"id": "WebQTest-360", "question": "where did emperor hirohito live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.deceased_person.place_of_death"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.country.administrative_divisions"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor hirohito live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.country.administrative_divisions </PATH>"], "scores": [-0.023266829550266266, -0.025410909205675125, -0.028548091650009155, -0.038644593209028244, -0.049220405519008636, -0.050692617893218994, -0.05261645466089249, -0.058451712131500244, -0.05950263887643814, -0.06684606522321701], "norm_scores": [0.10221884399652481, 0.1019999161362648, 0.10168042033910751, 0.10065896809101105, 0.09960002452135086, 0.09945349395275116, 0.09926235675811768, 0.09868481755256653, 0.09858117252588272, 0.09785988926887512]}}
{"id": "WebQTest-361", "question": "what to do and see in vienna austria", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["film.film.featured_film_locations"], ["organization.organization.geographic_scope"], ["base.arthist.helynevek.intezmeny"], ["metropolitan_transit.transit_system.area_served"], ["film.film_festival.location"], ["olympics.olympic_games.host_city"], ["transportation.bridge.locale"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["base.arthist.int_zm_nyek.helynev"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do and see in vienna austria [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> olympics.olympic_games.host_city </PATH>", "<PATH> transportation.bridge.locale </PATH>"], "scores": [-0.013291362673044205, -0.018958805128932, -0.024749064818024635, -0.10067732632160187, -0.1091068759560585, -0.11026560515165329, -0.11928558349609375, -0.12050818651914597, -0.13104422390460968, -0.13603395223617554], "norm_scores": [0.10768134891986847, 0.10707280039787292, 0.10645460337400436, 0.09867093712091446, 0.09784267842769623, 0.09772937744855881, 0.09685181826353073, 0.09673348069190979, 0.09571965038776398, 0.09524322301149368]}}
{"id": "WebQTest-362", "question": "when did conflict start in ireland", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.events"], ["people.person.nationality", "people.person.nationality"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.nationality", "people.person.place_of_birth"], ["time.event.included_in_event"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["people.person.place_of_birth", "people.person.gender", "time.event.locations"], ["time.event.includes_event"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did conflict start in ireland [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> people.person.place_of_birth </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> people.person.place_of_birth <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.gender <SEP> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.005076076835393906, -0.04320831969380379, -0.04903826862573624, -0.08776645362377167, -0.08889804035425186, -0.0893721804022789, -0.09179836511611938, -0.09229922294616699, -0.09992371499538422, -0.10253235697746277], "norm_scores": [0.10719234496355057, 0.10318181663751602, 0.10258201509714127, 0.09868514537811279, 0.09857353568077087, 0.09852681308984756, 0.09828805923461914, 0.09823884814977646, 0.09749267250299454, 0.0972386822104454]}}
{"id": "WebQTest-365", "question": "what is there to do in peoria illinois", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["organization.organization.geographic_scope"], ["film.film.featured_film_locations"], ["tv.tv_program.filming_locations"], ["film.film_festival.location"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do in peoria illinois [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> travel.travel_destination.tourist_attractions <PATH>", "<PATH> travel.tourist_attraction.near_travel_destination <PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> tv.tv_program.filming_locations </PATH>", "<PATH> film.film_festival.location </PATH>"], "scores": [-0.010502696968615055, -0.017529761418700218, -0.04271847754716873, -0.05908745154738426, -0.17065894603729248, -0.17074157297611237, -0.17323480546474457, -0.17635329067707062, -0.18195821344852448, -0.1822470873594284], "norm_scores": [0.11111795157194138, 0.11033985763788223, 0.10759525001049042, 0.10584835708141327, 0.09467367082834244, 0.09466585516929626, 0.09443012624979019, 0.09413610398769379, 0.09360995888710022, 0.09358291327953339]}}
{"id": "WebQTest-366", "question": "what language do british speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"], ["base.rosetta.languoid.languoid_class"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.person.nationality", "people.ethnicity.people", "people.ethnicity.languages_spoken"], ["people.ethnicity.people", "people.person.nationality", "location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do british speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> people.ethnicity.people <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.015978336334228516, -0.020675403997302055, -0.03731883689761162, -0.044850945472717285, -0.11689029633998871, -0.12784317135810852, -0.13505056500434875, -0.1446840465068817, -0.1492428183555603, -0.1683650016784668], "norm_scores": [0.10816910862922668, 0.10766222327947617, 0.10588519275188446, 0.10509064793586731, 0.09778624027967453, 0.09672105312347412, 0.09602644294500351, 0.09510581940412521, 0.09467323869466782, 0.09288007766008377]}}
{"id": "WebQTest-367", "question": "what makes elvis presley famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"], ["music.artist.origin"], ["fictional_universe.fictional_character.occupation"], ["music.artist.origin", "people.person.profession"], ["people.profession.people_with_this_profession"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["music.artist.origin", "people.deceased_person.place_of_death"], ["music.artist.origin", "people.person.place_of_birth"]], "ground_paths": [["fictional_universe.fictional_character.occupation"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what makes elvis presley famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> music.artist.origin <SEP> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> music.artist.origin <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> music.artist.origin <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.004410063847899437, -0.042555589228868484, -0.06720089912414551, -0.0858854353427887, -0.09505126625299454, -0.09545692801475525, -0.09767888486385345, -0.1005658209323883, -0.11076343059539795, -0.11430152505636215], "norm_scores": [0.10794360190629959, 0.10390358418226242, 0.10137414187192917, 0.09949759393930435, 0.09858979284763336, 0.09854980558156967, 0.09833107143640518, 0.09804759919643402, 0.09705283492803574, 0.09671005606651306]}}
{"id": "WebQTest-370", "question": "what language do people speak in the netherlands", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["book.newspaper.circulation_areas"], ["book.written_work.subjects"], ["book.written_work.original_language"], ["film.film.language"], ["book.book_subject.works"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the netherlands [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.original_language </PATH>", "<PATH> film.film.language </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.013577250763773918, -0.016609303653240204, -0.04105917364358902, -0.05137727037072182, -0.13886819779872894, -0.14522936940193176, -0.17144139111042023, -0.17256426811218262, -0.17597433924674988, -0.17832545936107635], "norm_scores": [0.10992828756570816, 0.10959547758102417, 0.1069483757019043, 0.10585054755210876, 0.09698314964771271, 0.09636817872524261, 0.09387499839067459, 0.09376964718103409, 0.09345042705535889, 0.09323097765445709]}}
{"id": "WebQTest-371", "question": "what are the major languages spoken in greece", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.language_family"], ["location.country.currency_formerly_used"], ["language.human_language.country"], ["language.human_language.region"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in greece [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> language.human_language.region </PATH>"], "scores": [-0.016853203997015953, -0.01883767731487751, -0.026250533759593964, -0.053385406732559204, -0.14788368344306946, -0.14978475868701935, -0.15801826119422913, -0.18111534416675568, -0.18137609958648682, -0.182967409491539], "norm_scores": [0.1096777394413948, 0.10946030169725418, 0.10865188390016556, 0.10574328154325485, 0.09620832651853561, 0.09602560847997665, 0.09523823112249374, 0.09306370466947556, 0.09303943812847137, 0.09289150685071945]}}
{"id": "WebQTest-374", "question": "what type of artist is henri matisse", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.art_form.artists"], ["visual_art.visual_artists.art_forms"], ["visual_art.visual_art_form.visual_artists"], ["visual_art.artist.art_forms"], ["film.film_genre.films_in_this_genre"], ["visual_art.visual_art_form.artworks"], ["visual_art.visual_artist.visual_art_forms"]], "ground_paths": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of artist is henri matisse [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.art_form.artists </PATH>", "<PATH> visual_art.visual_artists.art_forms </PATH>", "<PATH> visual_art.visual_art_form.visual_artists </PATH>", "<PATH> visual_art.artist.art_forms </PATH>", "<PATH> film.film_genre.films_in_this_genre </PATH>", "<PATH> visual_art.visual_art_form.artworks </PATH>", "<PATH> visual_art.visual_artist.visual_art_forms </PATH>"], "scores": [-0.0006290591554716229, -0.07363718748092651, -0.08118320256471634, -0.08798753470182419, -0.11350808292627335, -0.12801425158977509, -0.1303691864013672, -0.13791979849338531, -0.14416900277137756, -0.14760781824588776], "norm_scores": [0.11084214597940445, 0.10303811728954315, 0.10226351022720337, 0.10157003998756409, 0.09901071339845657, 0.0975848063826561, 0.09735527634620667, 0.09662295132875443, 0.09602101892232895, 0.09569139033555984]}}
{"id": "WebQTest-375", "question": "what was the first book charles dickens wrote", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"], ["film.film_story_contributor.film_story_credits"], ["fictional_universe.fictional_character_creator.fictional_characters_created", "book.book_character.appears_in_book"], ["book.book_edition.author_editor"], ["fictional_universe.fictional_character_creator.fictional_characters_created", "book.book_edition.author_editor"], ["tv.tv_program_creator.programs_created"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first book charles dickens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created <SEP> book.book_character.appears_in_book </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created <SEP> book.book_edition.author_editor </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>"], "scores": [-0.0029565885197371244, -0.03543562442064285, -0.05703197047114372, -0.06092558801174164, -0.0724545568227768, -0.08321715891361237, -0.08363550901412964, -0.08817567676305771, -0.0919857993721962, -0.09871196001768112], "norm_scores": [0.10661973804235458, 0.10321246832609177, 0.10100734233856201, 0.10061483085155487, 0.09946149587631226, 0.09839677810668945, 0.09835562109947205, 0.0979100838303566, 0.09753774106502533, 0.09688389301300049]}}
{"id": "WebQTest-376", "question": "what books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"], ["book.author.book_editions_published"], ["tv.tv_program.program_creator"], ["book.writer.works_written"], ["common.topic.notable_types"]], "ground_paths": [["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> tv.tv_program.program_creator </PATH>", "<PATH> book.writer.works_written </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.014961491338908672, -0.01846657320857048, -0.04019350931048393, -0.054271820932626724, -0.0716179758310318, -0.09287495166063309, -0.1160471960902214, -0.12747766077518463, -0.13716904819011688, -0.13728934526443481], "norm_scores": [0.10672061145305634, 0.10634720325469971, 0.10406152158975601, 0.10260678082704544, 0.10084228962659836, 0.09872131049633026, 0.0964600145816803, 0.09536371380090714, 0.09444396942853928, 0.09443260729312897]}}
{"id": "WebQTest-377", "question": "what school did michael jordan attend", "prediction": [["people.person.education", "education.education.institution"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.school"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_ pick.school"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did michael jordan attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_ pick.school </PATH>"], "scores": [-0.01845378614962101, -0.022554980590939522, -0.025566674768924713, -0.02720516175031662, -0.03048296459019184, -0.05797016993165016, -0.09670641273260117, -0.12766580283641815, -0.12998618185520172, -0.13224424421787262], "norm_scores": [0.10484836995601654, 0.10441924631595612, 0.10410524159669876, 0.10393480956554413, 0.10359469056129456, 0.10078593343496323, 0.0969565212726593, 0.09400079399347305, 0.0937829315662384, 0.09357140958309174]}}
{"id": "WebQTest-379", "question": "what year does hitler die", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["base.activism.activist.area_of_activism", "time.event.locations"], ["base.activism.activist.area_of_activism", "base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism", "base.activism.activist.activism_issue"], ["military.military_person.participated_in_conflicts"], ["people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year does hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> time.event.locations </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activism_issue.activists </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.activism_issue </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.012888921424746513, -0.021344004198908806, -0.03898685798048973, -0.06681260466575623, -0.07357054948806763, -0.07471528649330139, -0.08002223819494247, -0.08286331593990326, -0.08563730120658875, -0.09169865399599075], "norm_scores": [0.1050858125090599, 0.1042010560631752, 0.10237877070903778, 0.09956926852464676, 0.09889865666627884, 0.09878551214933395, 0.09826265275478363, 0.09798387438058853, 0.097712442278862, 0.097121961414814]}}
{"id": "WebQTest-380", "question": "where is mount st helens volcano", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"], ["geography.mountain.mountain_range"], ["geography.mountain_range.mountains"], ["geography.mountain.mountain_mountain_type"], ["geography.mountain.mountain_class"], ["location.mountain.mountain_type"], ["geography.mountain.mountain_region"], ["geography.mountain.mountain_body"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount st helens volcano [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> geography.mountain.mountain_mountain_type </PATH>", "<PATH> geography.mountain.mountain_class </PATH>", "<PATH> location.mountain.mountain_type </PATH>", "<PATH> geography.mountain.mountain_region </PATH>", "<PATH> geography.mountain.mountain_body </PATH>"], "scores": [-0.007643659599125385, -0.02888791263103485, -0.04654154181480408, -0.10067451745271683, -0.12526606023311615, -0.14568772912025452, -0.1484181135892868, -0.15371409058570862, -0.1557786762714386, -0.155964195728302], "norm_scores": [0.11026163399219513, 0.10794390738010406, 0.10605503618717194, 0.10046657919883728, 0.0980260893702507, 0.09604453295469284, 0.09578265249729156, 0.09527673572301865, 0.09508022665977478, 0.09506259113550186]}}
{"id": "WebQTest-381", "question": "what shows are shot in new york", "prediction": [["film.film.featured_film_locations"], ["tv.tv_program.filming_locations"], ["tv.tv_location.tv_shows_filmed_here"], ["media_common.netflix_title.netflix_genres"], ["tv.regular_tv_appearance.series", "tv.tv_program.regular_cast"], ["location.location.featured_in_films"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.series"], ["tv.tv_producer_term.producer", "tv.tv_program.tv_producer"], ["film.producer.films_executive_produced"], ["location.location.containedby"]], "ground_paths": [["film.film.featured_film_locations"], ["tv.tv_location.tv_shows_filmed_here"], ["tv.tv_program.filming_locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what shows are shot in new york [/INST]", "raw_output": {"paths": ["<PATH> film.film.featured_film_locations </PATH>", "<PATH> tv.tv_program.filming_locations </PATH>", "<PATH> tv.tv_location.tv_shows_filmed_here </PATH>", "<PATH> media_common.netflix_title.netflix_genres </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_program.regular_cast </PATH>", "<PATH> location.location.featured_in_films </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> tv.tv_producer_term.producer <SEP> tv.tv_program.tv_producer </PATH>", "<PATH> film.producer.films_executive_produced </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01575624756515026, -0.022068610414862633, -0.022084316238760948, -0.08664368093013763, -0.10173819959163666, -0.10802815109491348, -0.1147698387503624, -0.11920288950204849, -0.12288068234920502, -0.12291639298200607], "norm_scores": [0.10692118108272552, 0.10624837130308151, 0.1062467023730278, 0.09960421919822693, 0.09811203181743622, 0.09749684482812881, 0.09684175997972488, 0.09641341120004654, 0.09605947136878967, 0.09605604410171509]}}
{"id": "WebQTest-382", "question": "what book did tupac write", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"], ["book.author.book_editions_published"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "book.book_edition.author_editor"], ["influence.influence_node.influenced", "book.book_edition.author_editor"], []], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did tupac write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.book_edition.author_editor </PATH>", "<PATH> influence.influence_node.influenced <SEP> book.book_edition.author_editor </PATH>", "<PATH> </PATH>"], "scores": [-0.01447670254856348, -0.01870409958064556, -0.03956817090511322, -0.07845259457826614, -0.12991708517074585, -0.1308542937040329, -0.13147728145122528, -0.1376725286245346, -0.13778047263622284, -0.14363938570022583], "norm_scores": [0.10838176310062408, 0.10792455822229385, 0.10569614171981812, 0.10166509449481964, 0.09656530618667603, 0.09647484868764877, 0.09641475975513458, 0.09581929445266724, 0.09580894559621811, 0.09524925798177719]}}
{"id": "WebQTest-383", "question": "what language do chinese people write in", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.writing_system"], ["language.human_language.language_family"], ["language.language_family.languages"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.mainland_used"], ["language.human_language.written_system"]], "ground_paths": [["language.language_writing_system.languages"], ["language.human_language.writing_system"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do chinese people write in [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.writing_system </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.mainland_used </PATH>", "<PATH> language.human_language.written_system </PATH>"], "scores": [-0.008316801860928535, -0.033910803496837616, -0.039472546428442, -0.050255391746759415, -0.07682756334543228, -0.09192609786987305, -0.10501013696193695, -0.12120818346738815, -0.12344583123922348, -0.13526540994644165], "norm_scores": [0.10718492418527603, 0.1044764444231987, 0.10389699041843414, 0.10278269648551941, 0.10008751600980759, 0.09858767688274384, 0.09730616211891174, 0.09574268758296967, 0.09552869200706482, 0.09440622478723526]}}
{"id": "WebQTest-384", "question": "what was robert burns famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"], ["influence.influence_node.influenced", "people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["influence.influence_node.influenced_by", "common.topic.notable_types"], ["influence.influence_node.influenced"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced", "common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> common.topic.notable_types </PATH>"], "scores": [-0.0059728375636041164, -0.02744194306433201, -0.07155696302652359, -0.08837628364562988, -0.08863461762666702, -0.10118823498487473, -0.1073443815112114, -0.11714920401573181, -0.11764103174209595, -0.12374331057071686], "norm_scores": [0.10813571512699127, 0.10583887994289398, 0.10127127915620804, 0.09958222508430481, 0.09955649077892303, 0.09831451624631882, 0.09771113842725754, 0.09675777703523636, 0.0967101976275444, 0.09612184762954712]}}
{"id": "WebQTest-385", "question": "when did william mckinley died", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"], ["base.culturalevent.event.entity_involved"], ["people.cause_of_death.people"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["base.americancause.deceased_person.cause_of_death"], ["people.person.nationality", "location.location.containedby"], ["base.americancivilwar.president.vice_president"], ["people.person.deceased_place_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did william mckinley died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> base.americancause.deceased_person.cause_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> base.americancivilwar.president.vice_president </PATH>", "<PATH> people.person.deceased_place_of_death </PATH>"], "scores": [-0.003751633455976844, -0.036722928285598755, -0.052010003477334976, -0.07686391472816467, -0.08920440077781677, -0.08958127349615097, -0.09012310206890106, -0.09557520598173141, -0.11188909411430359, -0.11472629010677338], "norm_scores": [0.10743696987628937, 0.10395239293575287, 0.10237535089254379, 0.09986228495836258, 0.09863750636577606, 0.0986003428697586, 0.09854693710803986, 0.09801109880208969, 0.0964251309633255, 0.09615194797515869]}}
{"id": "WebQTest-386", "question": "how deep is lake merritt oakland", "prediction": [["geography.lake.basin_countries"], ["location.location.containedby"], ["geography.lake.depth"], ["location.location.contains"], ["geography.lake.lake_type"], ["geography.lake.lake_basin_countries"], ["geography.lake.basin_count"], ["geography.lake.basin_county"], ["location.location.containedby", "location.location.containedby"], ["geography.lake.water_level"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how deep is lake merritt oakland [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.lake.depth </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.lake.lake_type </PATH>", "<PATH> geography.lake.lake_basin_countries </PATH>", "<PATH> geography.lake.basin_count </PATH>", "<PATH> geography.lake.basin_county </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> geography.lake.water_level </PATH>"], "scores": [-0.011554241180419922, -0.03950248658657074, -0.04628792777657509, -0.055598508566617966, -0.058909084647893906, -0.06606178730726242, -0.06825677305459976, -0.07453703135251999, -0.07805785536766052, -0.08976583182811737], "norm_scores": [0.10481986403465271, 0.1019308865070343, 0.1012415885925293, 0.10030334442853928, 0.09997183829545975, 0.09925930947065353, 0.09904168546199799, 0.0984216257929802, 0.09807571023702621, 0.09693413227796555]}}
{"id": "WebQTest-388", "question": "what are the important holidays of islam", "prediction": [["time.holiday.featured_in_religions"], ["religion.religion.holidays"], ["time.recurring_event.holidays"], ["location.location.events", "time.holiday.featured_in_religions"], ["location.location.events"], ["time.recurring_event.current_frequency"], ["symbols.namesake.named_after"], ["language.human_language.countries_spoken_in"], ["religion.religion.religions"], ["film.film.subjects"]], "ground_paths": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the important holidays of islam [/INST]", "raw_output": {"paths": ["<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> religion.religion.holidays </PATH>", "<PATH> time.recurring_event.holidays </PATH>", "<PATH> location.location.events <SEP> time.holiday.featured_in_religions </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> religion.religion.religions </PATH>", "<PATH> film.film.subjects </PATH>"], "scores": [-0.00799244549125433, -0.016868729144334793, -0.1361979842185974, -0.13673733174800873, -0.14471425116062164, -0.14585661888122559, -0.1472492665052414, -0.15289655327796936, -0.1556122601032257, -0.15726955235004425], "norm_scores": [0.11169860512018204, 0.11071151494979858, 0.09825818985700607, 0.09820521622896194, 0.0974249616265297, 0.09731373190879822, 0.09717828780412674, 0.09663105010986328, 0.09636899083852768, 0.09620939940214157]}}
{"id": "WebQTest-389", "question": "what type of music did claude debussy play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["book.written_work.author"], ["music.composer.genre"], ["music.genre.artist"], ["influence.influence_node.influenced_by"], ["music.composer.compositions"], ["influence.influence_node.influenced"], ["music.music_genre.artists"]], "ground_paths": [["music.artist.genre"], ["music.genre.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did claude debussy play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> music.composer.genre </PATH>", "<PATH> music.genre.artist </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> music.music_genre.artists </PATH>"], "scores": [-0.004150953143835068, -0.030395440757274628, -0.0966467335820198, -0.1181100606918335, -0.12408144026994705, -0.1267033964395523, -0.12924142181873322, -0.12984046339988708, -0.13036896288394928, -0.1371031254529953], "norm_scores": [0.11024237424135208, 0.1073867678642273, 0.10050279647111893, 0.09836866706609726, 0.09778301417827606, 0.09752696007490158, 0.09727975726127625, 0.0972215011715889, 0.0971701368689537, 0.09651797264814377]}}
{"id": "WebQTest-390", "question": "when was the last time the toronto maple leafs were in the stanley cup finals", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["sports.professional_sports_team.owner_s", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up", "sports.sports_championship_event.runner_up"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs were in the stanley cup finals [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up <SEP> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.010596872307360172, -0.01490738894790411, -0.02560313791036606, -0.07644452154636383, -0.08798190206289291, -0.11126492917537689, -0.11665280163288116, -0.12543363869190216, -0.1287040412425995, -0.13168928027153015], "norm_scores": [0.10738505423069, 0.1069231703877449, 0.10578563809394836, 0.10054177790880203, 0.09938845783472061, 0.0971011221408844, 0.09657936543226242, 0.09573503583669662, 0.09542245417833328, 0.09513802081346512]}}
{"id": "WebQTest-392", "question": "what team is hank baskett on 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.player_games.player", "american_football.player_game_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is hank baskett on 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> american_football.player_games.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.015590819530189037, -0.019429953768849373, -0.020508255809545517, -0.024003131315112114, -0.034892693161964417, -0.041854891926050186, -0.09038688242435455, -0.09765579551458359, -0.10126690566539764, -0.10226289182901382], "norm_scores": [0.10392999649047852, 0.10353176295757294, 0.1034201830625534, 0.10305936634540558, 0.10194318741559982, 0.10123591125011444, 0.09644004702568054, 0.09574156999588013, 0.09539645910263062, 0.09530149400234222]}}
{"id": "WebQTest-393", "question": "what are the two official languages of paraguay", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["government.governmental_jurisdiction.government", "language.human_language.countries_spoken_in"], ["government.governmental_jurisdiction.government", "location.country.languages_spoken"], ["government.governmental_jurisdiction.government"], ["country.official_language.countries_spoken_in"], ["book.newspaper.circulation_areas", "book.periodical.language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the two official languages of paraguay [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> government.governmental_jurisdiction.government <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> government.governmental_jurisdiction.government <SEP> location.country.languages_spoken </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>", "<PATH> country.official_language.countries_spoken_in </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.language </PATH>"], "scores": [-0.01497744582593441, -0.025932002812623978, -0.02798069640994072, -0.0347415991127491, -0.15738435089588165, -0.15750660002231598, -0.1585327684879303, -0.16080938279628754, -0.16756446659564972, -0.1681525856256485], "norm_scores": [0.10943210124969482, 0.10823985934257507, 0.10801833868026733, 0.10729050636291504, 0.0949069932103157, 0.0948953926563263, 0.09479805827140808, 0.09458249062299728, 0.09394573420286179, 0.09389049559831619]}}
{"id": "WebQTest-395", "question": "what state is the steelers from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.location", "location.location.containedby"], ["sports.professional_sports_team.owner_s", "sports.sports_team.location"], ["sports.sports_team_location.teams", "location.location.containedby"], ["sports.professional_sports_team.owner_s", "sports.sports_team_location.teams"], ["sports.sports_facility.teams"], ["sports.sports_team_location_symbol_relationship.administrative_division", "location.location_symbol_relationship.administrative_division"], ["sports.sports_team_location_symbol_relationship.team", "sports.sports_team_location_symbol_relationship.administrative_division"], ["sports.sports_team.previously_known_as"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is the steelers from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.location <SEP> location.location.containedby </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams <SEP> location.location.containedby </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.administrative_division </PATH>", "<PATH> sports.sports_team_location_symbol_relationship.team <SEP> sports.sports_team_location_symbol_relationship.administrative_division </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.004064981825649738, -0.031253352761268616, -0.09514365345239639, -0.10291922837495804, -0.11009298264980316, -0.11638883501291275, -0.11689835786819458, -0.12065768986940384, -0.12250036001205444, -0.13044783473014832], "norm_scores": [0.10943330079317093, 0.10649807006120682, 0.09990668296813965, 0.09913286566734314, 0.09842425584793091, 0.09780654311180115, 0.09775671362876892, 0.09738990664482117, 0.09721061587333679, 0.09644109010696411]}}
{"id": "WebQTest-396", "question": "what college did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.employment_history", "business.employment_tenure.company"], ["influence.influence_node.influenced_by", "education.education.student", "education.education.institution"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> education.education.student <SEP> education.education.institution </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.010216325521469116, -0.018775159493088722, -0.02582581341266632, -0.07709468901157379, -0.0874718576669693, -0.10245385020971298, -0.10543667525053024, -0.11447553336620331, -0.11723142117261887, -0.127796933054924], "norm_scores": [0.10699094831943512, 0.10607913136482239, 0.1053338423371315, 0.10006958991289139, 0.09903652966022491, 0.09756382554769516, 0.09727323800325394, 0.09639795869588852, 0.09613265842199326, 0.0951223149895668]}}
{"id": "WebQTest-397", "question": "what are the school colors for harvard university", "prediction": [["education.educational_institution.colors"], ["education.school_mascot.school"], ["education.educational_institution.colors", "education.educational_institution.colors"], ["education.educational_institution.colors", "education.color.colors"], ["education.educational_institution.colors", "education.school_colors.school"], ["education.educational_institution.parent_institution", "education.educational_institution.colors"], ["education.school_colors.school"], ["education.educational_institution.colors", "education.colors.colors"], ["education.educational_institution.colors", "education.school_color.school"], ["education.educational_institution.colors", "education.school_color.colors"]], "ground_paths": [["education.educational_institution.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the school colors for harvard university [/INST]", "raw_output": {"paths": ["<PATH> education.educational_institution.colors </PATH>", "<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.colors <SEP> education.educational_institution.colors </PATH>", "<PATH> education.educational_institution.colors <SEP> education.color.colors </PATH>", "<PATH> education.educational_institution.colors <SEP> education.school_colors.school </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> education.educational_institution.colors </PATH>", "<PATH> education.school_colors.school </PATH>", "<PATH> education.educational_institution.colors <SEP> education.colors.colors </PATH>", "<PATH> education.educational_institution.colors <SEP> education.school_color.school </PATH>", "<PATH> education.educational_institution.colors <SEP> education.school_color.colors </PATH>"], "scores": [-0.0032970330212265253, -0.04782150685787201, -0.06332643330097198, -0.06653367727994919, -0.06755141168832779, -0.07192506641149521, -0.07326703518629074, -0.08319886773824692, -0.08641206473112106, -0.08985820412635803], "norm_scores": [0.10636822134256363, 0.10173612087965012, 0.10017087310552597, 0.0998501107096672, 0.09974854439496994, 0.09931322932243347, 0.09918005019426346, 0.09819988161325455, 0.09788485616445541, 0.09754811227321625]}}
{"id": "WebQTest-398", "question": "where is the university of the rockies located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.contains"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the university of the rockies located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.0027910235803574324, -0.049697116017341614, -0.06156026944518089, -0.06298558413982391, -0.06881122291088104, -0.07336101680994034, -0.07971248030662537, -0.08800503611564636, -0.08823764324188232, -0.09272239357233047], "norm_scores": [0.1065753623843193, 0.10169176012277603, 0.10049249976873398, 0.10034937411546707, 0.09976646304130554, 0.09931357949972153, 0.09868480265140533, 0.09786982834339142, 0.09784706681966782, 0.09740923345088959]}}
{"id": "WebQTest-399", "question": "what disease does robin roberts have", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"], ["medicine.cancer_patient.notable_people_with_this_condition"], ["people.person.medical_condition"], ["medicine.disease_notable_person_with_this_condition"], ["sports.sports_team_coach.sports_coached", "medicine.disease.notable_people_with_this_condition"], ["medicine.cancer_patient.notable_person_with_this_condition"], ["sports.sports_team_coach.sports_coached", "medicine.notable_person_with_medical_condition.condition"], ["medicine.medical_condition.notable_people_with_this_condition"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease does robin roberts have [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.cancer_patient.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition </PATH>", "<PATH> medicine.disease_notable_person_with_this_condition </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.cancer_patient.notable_person_with_this_condition </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.medical_condition.notable_people_with_this_condition </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.010588244535028934, -0.012200264260172844, -0.09053897112607956, -0.09664804488420486, -0.11370358616113663, -0.11424156278371811, -0.11543050408363342, -0.1228090450167656, -0.12407510727643967, -0.12855181097984314], "norm_scores": [0.10847858339548111, 0.10830385982990265, 0.10014329105615616, 0.09953337907791138, 0.09785017371177673, 0.09779754281044006, 0.09768132865428925, 0.09696324914693832, 0.09684056788682938, 0.09640800952911377]}}
{"id": "WebQTest-400", "question": "what was the capital city of the east roman empire", "prediction": [["location.location.containedby"], ["location.country.capital"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["fictional_universe.fictional_setting.languages", "language.human_language.countries_spoken_in"], ["location.location.people_born_here", "people.deceased_person.place_of_death"], ["location.location.nearby_airports", "location.location.containedby"], ["fictional_universe.fictional_setting.languages", "language.human_language.main_country"], ["location.location.containedby", "base.biblioness.bibs_location.country"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the capital city of the east roman empire [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.people_born_here <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.nearby_airports <SEP> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.main_country </PATH>", "<PATH> location.location.containedby <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.006816633511334658, -0.028928222134709358, -0.051372092217206955, -0.07595217972993851, -0.07817614078521729, -0.08452636003494263, -0.10153994709253311, -0.10216210037469864, -0.10274096578359604, -0.10440726578235626], "norm_scores": [0.10685643553733826, 0.10451960563659668, 0.10219991207122803, 0.09971844404935837, 0.09949692338705063, 0.09886709600687027, 0.09719923883676529, 0.09713879227638245, 0.09708257764577866, 0.0969209372997284]}}
{"id": "WebQTest-401", "question": "who plays edward scissorhands", "prediction": [["film.film.country", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film.starring", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.performance.film", "film.actor.film"], ["film.film.language", "people.person.languages"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays edward scissorhands [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> film.film.language <SEP> people.person.languages </PATH>"], "scores": [-0.01688280701637268, -0.0391928069293499, -0.04235633462667465, -0.046546585857868195, -0.04927036166191101, -0.05119717866182327, -0.054892171174287796, -0.05502060055732727, -0.059623368084430695, -0.06192774325609207], "norm_scores": [0.10312090069055557, 0.10084574669599533, 0.10052722692489624, 0.1001068726181984, 0.09983456879854202, 0.09964239597320557, 0.0992748960852623, 0.09926214069128036, 0.09880632162094116, 0.09857889264822006]}}
{"id": "WebQTest-402", "question": "what does joey jordison play in slipknot", "prediction": [["music.group_membership.group", "music.group_membership.role"], ["music.group_member.instruments_played"], ["music.group_member.instruments_played", "music.group_member.instruments_played"], ["music.instrument.instrumentalists", "music.group_member.instruments_played"], ["music.musical_group.member", "music.group_membership.role"], ["music.instrument.instrumentalists", "music.group_member.instruments_played", "music.group_member.instruments_played"], ["music.track_contribution.role", "music.track_contribution.role"], ["music.instrument.instrumentalists"], ["music.artist.genre", "music.artist.genre"], ["music.instrument.instrumentalists", "common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["music.group_member.instruments_played"], ["music.group_membership.group", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does joey jordison play in slipknot [/INST]", "raw_output": {"paths": ["<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.instrumentalists <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.instrument.instrumentalists <SEP> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.track_contribution.role <SEP> music.track_contribution.role </PATH>", "<PATH> music.instrument.instrumentalists </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.instrument.instrumentalists <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.01803933084011078, -0.018744466826319695, -0.04230431094765663, -0.04745195806026459, -0.04749547317624092, -0.05017353966832161, -0.051594022661447525, -0.052856072783470154, -0.056249476969242096, -0.05787180736660957], "norm_scores": [0.10264902561903, 0.10257666558027267, 0.1001882255077362, 0.0996738150715828, 0.09966947883367538, 0.09940291941165924, 0.0992618128657341, 0.09913661330938339, 0.09880078583955765, 0.09864062070846558]}}
{"id": "WebQTest-403", "question": "what kind of language does turkey speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["food.beer_country_region.languages_spoken"], ["food.beer_country_region.beers_from_here"], ["food.beer_country_country.languages_spoken"], ["base.rosetta.languoid.languoid_class"], ["film.film.country", "film.film.language"], ["government.form_of_government.countries"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does turkey speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> food.beer_country_region.languages_spoken </PATH>", "<PATH> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer_country_country.languages_spoken </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> film.film.country <SEP> film.film.language </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.00684687914326787, -0.030163846909999847, -0.04786338284611702, -0.05638189986348152, -0.0963875874876976, -0.11084386706352234, -0.1423594206571579, -0.15684393048286438, -0.175742045044899, -0.17769336700439453], "norm_scores": [0.10958251357078552, 0.10705694556236267, 0.10517875105142593, 0.10428658872842789, 0.10019688308238983, 0.09875883162021637, 0.09569492191076279, 0.09431882202625275, 0.09255310893058777, 0.09237268567085266]}}
{"id": "WebQTest-404", "question": "what was lebron james first team", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lebron james first team [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>"], "scores": [-0.013628965243697166, -0.016456594690680504, -0.029681745916604996, -0.04691554605960846, -0.04776289686560631, -0.05215095728635788, -0.05286325886845589, -0.06004645302891731, -0.06153767928481102, -0.061949606984853745], "norm_scores": [0.10309933125972748, 0.10280821472406387, 0.10145751386880875, 0.09972399473190308, 0.09963952749967575, 0.09920327365398407, 0.09913262724876404, 0.09842309355735779, 0.09827643632888794, 0.09823595732450485]}}
{"id": "WebQTest-405", "question": "who fought the battle of gettysburg", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"], ["military.military_person.participated_in_conflicts"], ["military.military_conflict.casualties", "military.military_person.casualties"], ["military.military_command.military_conflict", "military.military_command.military_commander"], ["military.military_command.military_conflict", "military.military_commander.military_commands"], ["military.military_conflict.casualties", "military.casualties.combatant"], ["military.military_command.military_conflict"], ["military.military_command.military_conflict", "military.military_person.participated_in_conflicts"], ["military.military_conflict.commanders", "military.military_command.military_commander"]], "ground_paths": [["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["book.written_work.subjects", "book.written_work.subjects"], ["military.military_command.military_conflict", "military.military_combatant.military_commanders"], ["military.military_conflict.military_personnel_involved", "people.person.nationality"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["location.location.events", "location.administrative_division.second_level_division_of"], ["time.event.included_in_event", "time.event.locations"], ["location.location.events", "location.administrative_division.first_level_division_of"], ["military.military_person.participated_in_conflicts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought the battle of gettysburg [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_person.casualties </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_command.military_commander </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_commander.military_commands </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.combatant </PATH>", "<PATH> military.military_command.military_conflict </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_person.participated_in_conflicts </PATH>", "<PATH> military.military_conflict.commanders <SEP> military.military_command.military_commander </PATH>"], "scores": [-0.006715656723827124, -0.027048269286751747, -0.046175334602594376, -0.05640074983239174, -0.05865151435136795, -0.06608138233423233, -0.06652295589447021, -0.07160814106464386, -0.08390337228775024, -0.08548430353403091], "norm_scores": [0.10511327534914017, 0.10299762338399887, 0.10104631632566452, 0.10001833736896515, 0.0997934639453888, 0.09905476868152618, 0.09901104122400284, 0.09850882738828659, 0.0973050445318222, 0.09715133905410767]}}
{"id": "WebQTest-406", "question": "who is the coach of the sf giants", "prediction": [["baseball.baseball_team.current_coaches"], ["baseball.baseball_team.current_manager"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["baseball.baseball_team.current_coach"], ["baseball.baseball_manager.current_team_managed"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"], ["sports.sports_team.coaches", "baseball.baseball_team.current_manager"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"]], "ground_paths": [["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"], ["baseball.current_coaching_tenure.baseball_team", "baseball.current_coaching_tenure.baseball_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach of the sf giants [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> baseball.baseball_team.current_manager </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> baseball.baseball_team.current_coach </PATH>", "<PATH> baseball.baseball_manager.current_team_managed </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team.coaches <SEP> baseball.baseball_team.current_manager </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>"], "scores": [-0.010966983623802662, -0.029284309595823288, -0.046844806522130966, -0.04703933000564575, -0.047965288162231445, -0.051111094653606415, -0.057538971304893494, -0.06911541521549225, -0.07109230756759644, -0.07228387892246246], "norm_scores": [0.10399675369262695, 0.1021091565489769, 0.10033171623945236, 0.10031220316886902, 0.10021936893463135, 0.09990458935499191, 0.0992644727230072, 0.09812197089195251, 0.09792818129062653, 0.09781156480312347]}}
{"id": "WebQTest-408", "question": "who was the first president of the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first president of the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.000600591825786978, -0.08116006851196289, -0.08634714782238007, -0.09471636265516281, -0.09720095992088318, -0.09916342049837112, -0.10087871551513672, -0.1025770753622055, -0.10319872945547104, -0.107084259390831], "norm_scores": [0.10900634527206421, 0.10056926310062408, 0.1000489592552185, 0.09921511262655258, 0.09896890819072723, 0.09877487272024155, 0.09860560297966003, 0.09843827784061432, 0.09837709367275238, 0.0979955866932869]}}
{"id": "WebQTest-409", "question": "where did giuliana rancic grow up", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.second_level_division_of"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["people.person.place_of_birth", "location.location.containedby"], ["influence.influence_node.influenced_by", "location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did giuliana rancic grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.second_level_division_of </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> location.location.people_born_here </PATH>"], "scores": [-0.0034169810824096203, -0.04137752205133438, -0.048150815069675446, -0.116019107401371, -0.13638386130332947, -0.13952694833278656, -0.14245741069316864, -0.1449490487575531, -0.15002942085266113, -0.1551641821861267], "norm_scores": [0.11084256321191788, 0.10671378672122955, 0.10599341988563538, 0.0990385115146637, 0.097042016685009, 0.09673748165369034, 0.09645440429449081, 0.09621437638998032, 0.09572681039571762, 0.09523653239011765]}}
{"id": "WebQTest-410", "question": "what are the major cities in ukraine", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.biblioness.bibs_location.country"], ["location.country.languages_spoken"], ["location.country.administrative_divisions"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major cities in ukraine [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.02077271230518818, -0.0230789203196764, -0.03197108581662178, -0.044303227216005325, -0.05516022816300392, -0.06274263560771942, -0.06501488387584686, -0.0796576514840126, -0.08346087485551834, -0.08681336790323257], "norm_scores": [0.10348466783761978, 0.1032462939620018, 0.10233227908611298, 0.10107804834842682, 0.09998657554388046, 0.0992313027381897, 0.09900608658790588, 0.09756692498922348, 0.097196564078331, 0.09687124937772751]}}
{"id": "WebQTest-414", "question": "who does brian dawkins play for 2011", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does brian dawkins play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.0126486262306571, -0.012795956805348396, -0.04316573217511177, -0.046131689101457596, -0.04860099405050278, -0.048857394605875015, -0.051402729004621506, -0.05141328647732735, -0.052264437079429626, -0.05398458614945412], "norm_scores": [0.10298001021146774, 0.1029648408293724, 0.09988483041524887, 0.09958900511264801, 0.09934339672327042, 0.09931793063879013, 0.09906545281410217, 0.099064402282238, 0.0989801213145256, 0.09881000965833664]}}
{"id": "WebQTest-415", "question": "what movies did tupac star in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["music.producer.releases_produced"], ["music.recording.artist", "music.recording.releases"], ["music.featured_artist.recordings", "music.recording.releases"], ["film.personal_film_appearance.person", "film.film.personal_appearances"]], "ground_paths": [["people.person.languages", "film.film.language"], ["music.recording.artist", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["music.featured_artist.recordings", "music.recording.releases"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["music.producer.releases_produced"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did tupac star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> music.recording.artist <SEP> music.recording.releases </PATH>", "<PATH> music.featured_artist.recordings <SEP> music.recording.releases </PATH>", "<PATH> film.personal_film_appearance.person <SEP> film.film.personal_appearances </PATH>"], "scores": [-0.021231770515441895, -0.023491758853197098, -0.03943784534931183, -0.040778305381536484, -0.04859554395079613, -0.050040021538734436, -0.052497394382953644, -0.0542757622897625, -0.06783649325370789, -0.08466323465108871], "norm_scores": [0.10272560268640518, 0.10249371081590652, 0.10087229311466217, 0.10073716938495636, 0.09995274990797043, 0.099808469414711, 0.09956351667642593, 0.09938661009073257, 0.09804794937372208, 0.09641192853450775]}}
{"id": "WebQTest-416", "question": "what form of currency does china have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does china have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.currency_used </PATH>"], "scores": [-0.011153372004628181, -0.015324241481721401, -0.120286725461483, -0.13611331582069397, -0.14146985113620758, -0.1441812962293625, -0.16165229678153992, -0.16493460536003113, -0.16978031396865845, -0.1723966747522354], "norm_scores": [0.11172713339328766, 0.11126210540533066, 0.10017576068639755, 0.098602794110775, 0.09807603806257248, 0.09781047701835632, 0.09611646831035614, 0.09580150246620178, 0.09533838927745819, 0.09508928656578064]}}
{"id": "WebQTest-417", "question": "who plays kenneth", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["people.person.gender", "people.person.gender"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays kenneth [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> tv.tv_guest_role.character <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>"], "scores": [-0.02303030900657177, -0.024949422106146812, -0.031245168298482895, -0.03180895373225212, -0.0343380905687809, -0.03510477393865585, -0.04840131849050522, -0.06462910026311874, -0.07214569300413132, -0.07477826625108719], "norm_scores": [0.10210585594177246, 0.10191009193658829, 0.10127050429582596, 0.10121342539787292, 0.10095777362585068, 0.10088039934635162, 0.09954791516065598, 0.09794551879167557, 0.097212053835392, 0.09695646166801453]}}
{"id": "WebQTest-418", "question": "what league are the indianapolis colts in", "prediction": [["american_football.football_team.division"], ["sports.sports_team.league"], ["sports.sports_team.sport"], ["american_football.football_team.league"], ["american_football.football_team.current_division"], ["american_football.football_team.current_head_coach"], ["sports.sports_team.championships"], ["sports.sports_team_roster.team"], ["sports.sports_league_participation.league", "sports.sports_league_participation.team"], ["sports.sports_team_roster.team", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team.sport", "sports.sports_league.sport"], ["american_football.football_team.division"], ["american_football.football_team.conference"], ["american_football.football_team.conference", "american_football.football_conference.league"], ["sports.sports_league_participation.team", "sports.sports_league_participation.league"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what league are the indianapolis colts in [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_team.division </PATH>", "<PATH> sports.sports_team.league </PATH>", "<PATH> sports.sports_team.sport </PATH>", "<PATH> american_football.football_team.league </PATH>", "<PATH> american_football.football_team.current_division </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_league_participation.team </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01713145151734352, -0.03802937641739845, -0.043461110442876816, -0.04675685241818428, -0.048876259475946426, -0.05230177938938141, -0.052723873406648636, -0.05312756076455116, -0.056462500244379044, -0.059444256126880646], "norm_scores": [0.10300764441490173, 0.10087733715772629, 0.1003308817744255, 0.100000761449337, 0.09978904575109482, 0.09944780170917511, 0.09940583258867264, 0.0993657186627388, 0.099034883081913, 0.09874003380537033]}}
{"id": "WebQTest-419", "question": "what country did magellan represent", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_command.military_combatant"], ["location.location.people_born_here", "location.location.containedby"], ["military.military_command.military_commander", "military.military_command.military_combatant"], ["symbols.coat_of_arms.bearers", "symbols.coat_of_arms_bearer.coat_of_arms_used"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_command.military_commander", "military.military_combatant.military_commanders"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did magellan represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_command.military_combatant </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> military.military_command.military_commander <SEP> military.military_command.military_combatant </PATH>", "<PATH> symbols.coat_of_arms.bearers <SEP> symbols.coat_of_arms_bearer.coat_of_arms_used </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> military.military_command.military_commander <SEP> military.military_combatant.military_commanders </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0033407986629754305, -0.04583030194044113, -0.07510223984718323, -0.07614004611968994, -0.07671476155519485, -0.07688198238611221, -0.07743407040834427, -0.07775456458330154, -0.08286845684051514, -0.08512535691261292], "norm_scores": [0.10661904513835907, 0.1021837517619133, 0.09923598170280457, 0.09913305193185806, 0.09907609224319458, 0.09905952215194702, 0.09900485724210739, 0.09897312521934509, 0.09846828132867813, 0.09824629127979279]}}
{"id": "WebQTest-420", "question": "where is perpignan located", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.locations.continents.countries_within"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.subdivides_place"], ["geography.river.cities"], ["base.aareas.schema.administrative_area_type.subdivides_place"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is perpignan located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.subdivides_place </PATH>", "<PATH> geography.river.cities </PATH>", "<PATH> base.aareas.schema.administrative_area_type.subdivides_place </PATH>"], "scores": [-0.007517759222537279, -0.026219835504889488, -0.05820994824171066, -0.07047560811042786, -0.07844609022140503, -0.08299383521080017, -0.08535902947187424, -0.11193312704563141, -0.11431959271430969, -0.11945151537656784], "norm_scores": [0.10696757584810257, 0.10498564690351486, 0.10168030112981796, 0.10044074803590775, 0.09964336454868317, 0.09919124096632004, 0.09895691275596619, 0.09636186063289642, 0.09613216668367386, 0.09564008563756943]}}
{"id": "WebQTest-421", "question": "who did kim richards marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kim richards marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.017147673293948174, -0.026124507188796997, -0.046373043209314346, -0.04665514454245567, -0.04810633510351181, -0.05101388320326805, -0.05271149054169655, -0.054339997470378876, -0.05624114349484444, -0.06146311014890671], "norm_scores": [0.10292018949985504, 0.10200042277574539, 0.09995583444833755, 0.0999276414513588, 0.09978273510932922, 0.09949303418397903, 0.09932427108287811, 0.09916266053915024, 0.0989743173122406, 0.09845881909132004]}}
{"id": "WebQTest-422", "question": "what happened in bosnia in the 90s", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["time.event.included_in_event"], ["book.written_work.subjects"], ["event.disaster.areas_affected"], ["book.book_subject.works"], ["time.event.includes_event"], ["military.military_conflict.casualties", "military.military_conflict.casualties"], ["military.military_conflict.casualties", "military.casualties.military_conflict"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened in bosnia in the 90s [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.military_conflict </PATH>"], "scores": [-0.006312603130936623, -0.035204797983169556, -0.052795540541410446, -0.056800730526447296, -0.07869292050600052, -0.08560357987880707, -0.08814404159784317, -0.08859015256166458, -0.09584780782461166, -0.1034206748008728], "norm_scores": [0.1064387708902359, 0.10340751707553864, 0.1016044095158577, 0.10119827836751938, 0.09900689870119095, 0.09832505881786346, 0.09807558357715607, 0.09803184121847153, 0.09732294082641602, 0.09658870846033096]}}
{"id": "WebQTest-423", "question": "when 's the last time the steelers won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the steelers won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>"], "scores": [-0.010200855322182178, -0.013714049011468887, -0.05259646102786064, -0.07623317837715149, -0.08406717330217361, -0.09314341843128204, -0.10095228254795074, -0.11321581900119781, -0.1260877251625061, -0.13120977580547333], "norm_scores": [0.10715536773204803, 0.1067795604467392, 0.1027074009180069, 0.10030820220708847, 0.09952545911073685, 0.09862622618675232, 0.09785906970500946, 0.09666629880666733, 0.09542998671531677, 0.09494244307279587]}}
{"id": "WebQTest-425", "question": "where english is spoken", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["language.human_language.region"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.dialects"], ["language.human_language.language_family"], ["film.film.language"], ["location.country.language_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where english is spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.dialects </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> film.film.language </PATH>", "<PATH> location.country.language_spoken </PATH>"], "scores": [-0.013951233588159084, -0.024612169712781906, -0.029479674994945526, -0.07815811783075333, -0.1578882336616516, -0.16131936013698578, -0.17451728880405426, -0.18540050089359283, -0.18677914142608643, -0.19260837137699127], "norm_scores": [0.1109570562839508, 0.1097804382443428, 0.1092473715543747, 0.10405673831701279, 0.096082404255867, 0.09575330466032028, 0.09449785947799683, 0.09347499161958694, 0.09334621578454971, 0.09280366450548172]}}
{"id": "WebQTest-426", "question": "where is giza pyramids located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["location.location.contains"], ["location.country.administrative_divisions"], ["travel.tourist_attraction.near_travel_destination"], ["base.schemastaging.disputed_location_claimant.disputed_territories"], ["architecture.structure.architect"], ["base.locations.countries.continent"], ["architecture.architect.structures_designed"], ["location.location.partially_contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is giza pyramids located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.0024680308997631073, -0.04665585234761238, -0.07975614815950394, -0.1141841784119606, -0.11722772568464279, -0.1219879612326622, -0.13509435951709747, -0.13926997780799866, -0.141007199883461, -0.14997172355651855], "norm_scores": [0.11065562069416046, 0.10587245225906372, 0.10242540389299393, 0.09895911067724228, 0.09865838289260864, 0.09818985313177109, 0.09691134840250015, 0.09650752693414688, 0.09634001553058624, 0.09548022598028183]}}
{"id": "WebQTest-427", "question": "what is the zip code for midland tx", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.citytown.postal_ codes"], ["location.Citytown.postal_codes"], ["location.citytown.postal_code"], ["location.city.postal_codes"], ["location.citytown.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for midland tx [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.citytown.postal_ codes </PATH>", "</PATH> location.citytown.postal_codes </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_code </PATH>", "<PATH> location.city.postal_codes </PATH>", "<PATH> location.citytown.postal_codes </PATH>"], "scores": [-7.624483259860426e-05, -0.10957995802164078, -0.15002331137657166, -0.1779881715774536, -0.17990633845329285, -0.18559183180332184, -0.18749958276748657, -0.1941118985414505, -0.19713079929351807, -0.2042008340358734], "norm_scores": [0.11696776747703552, 0.10483573377132416, 0.10068041831254959, 0.09790390729904175, 0.09771629422903061, 0.09716230630874634, 0.0969771221280098, 0.09633798897266388, 0.09604759514331818, 0.09537092596292496]}}
{"id": "WebQTest-428", "question": "what states make up the midwest us", "prediction": [["location.location.contains"], ["location.location.containedby"], ["location.location.partially_containedby"], ["base.locations.countries.states_provinces_within"], ["location.location.partially_contains"], ["location.administrative_division.first_level_division_of"], ["book.written_work.subjects"], ["book.book_subject.works"], ["location.location.primarily_containedby"], ["location.location.time_zones"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states make up the midwest us [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> location.location.time_zones </PATH>"], "scores": [-0.011610076762735844, -0.015970762819051743, -0.12932339310646057, -0.12988664209842682, -0.13980437815189362, -0.14022085070610046, -0.14852075278759003, -0.16464416682720184, -0.1666690856218338, -0.16794231534004211], "norm_scores": [0.11143505573272705, 0.11095017939805984, 0.0990602895617485, 0.09900450706481934, 0.09802746027708054, 0.09798664599657059, 0.09717673063278198, 0.0956224724650383, 0.09542904049158096, 0.0953076109290123]}}
{"id": "WebQTest-430", "question": "what instruments does justin bieber use", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_member.instruments_played"], ["music.recording_contribution.contributor", "music.track_contribution.role"], ["music.musical_group.member", "music.group_membership.role"], ["music.instrument.family"], ["music.instrument.instrumentalists"], ["music.producer.releases_produced"], ["music.album.artist"], ["music.artist.album", "music.album.artist"]], "ground_paths": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.membership", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments does justin bieber use [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.recording_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.instrument.instrumentalists </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.artist.album <SEP> music.album.artist </PATH>"], "scores": [-0.004598156549036503, -0.03858045116066933, -0.05098509043455124, -0.06450047343969345, -0.0741543173789978, -0.07575636357069016, -0.08227363973855972, -0.08711500465869904, -0.0952206701040268, -0.1012834683060646], "norm_scores": [0.10644520819187164, 0.10288872569799423, 0.10162031650543213, 0.10025611519813538, 0.09929291158914566, 0.09913397580385208, 0.09848998486995697, 0.09801431745290756, 0.09722305834293365, 0.09663539379835129]}}
{"id": "WebQTest-431", "question": "what language does australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["people.ethnicity.languages_spoken"], ["language.human_language.region"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> language.human_language.region </PATH>"], "scores": [-0.0077166748233139515, -0.022534534335136414, -0.061527058482170105, -0.10131234675645828, -0.16137057542800903, -0.19211499392986298, -0.20097051560878754, -0.20099426805973053, -0.2020825296640396, -0.20461519062519073], "norm_scores": [0.11330310255289078, 0.11163656413555145, 0.10736735165119171, 0.10317957401275635, 0.09716520458459854, 0.09422336518764496, 0.09339264780282974, 0.09339043498039246, 0.0932888612151146, 0.09305288642644882]}}
{"id": "WebQTest-432", "question": "who did annie oakley married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.person.spouse_s"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did annie oakley married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.02569805085659027, -0.0372958667576313, -0.0386858694255352, -0.04016540199518204, -0.043496180325746536, -0.04385920614004135, -0.04431651532649994, -0.04500690475106239, -0.052002739161252975, -0.05314946547150612], "norm_scores": [0.10167814791202545, 0.10050570964813232, 0.10036610811948776, 0.10021772235631943, 0.09988447278738022, 0.09984822571277618, 0.09980256110429764, 0.09973368793725967, 0.09903839975595474, 0.0989248976111412]}}
{"id": "WebQTest-433", "question": "who has played lex luthor", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["people.person.gender", "people.person.gender"], ["film.film_character.portrayed_in_films_dubbed", "film.actor.dubbing_performances"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films_dubbed", "film.dubbing_performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has played lex luthor [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films_dubbed <SEP> film.actor.dubbing_performances </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films_dubbed <SEP> film.dubbing_performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.01962851919233799, -0.02560279704630375, -0.028169048950076103, -0.03391794487833977, -0.04028921574354172, -0.060748353600502014, -0.07499520480632782, -0.07541590929031372, -0.07582081109285355, -0.08322541415691376], "norm_scores": [0.1032392829656601, 0.10262434184551239, 0.10236132144927979, 0.10177454352378845, 0.10112817585468292, 0.09908019751310349, 0.09767862409353256, 0.09763753414154053, 0.09759800881147385, 0.09687800705432892]}}
{"id": "WebQTest-434", "question": "where does the zambezi river start", "prediction": [["geography.river.origin"], ["location.location.containedby"], [], ["location.location.partial_containedby"], ["geography.river.basin_countries"], ["geography.river.source"], ["geography.river.origin", "geography.river.origin"], ["geography.river.mouth"], ["geography.lake.outflow"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> </PATH>", "<PATH> location.location.partial_containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> geography.river.source </PATH>", "<PATH> geography.river.origin <SEP> geography.river.origin </PATH>", "<PATH> geography.river.mouth </PATH>", "<PATH> geography.lake.outflow </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.00036270657437853515, -0.10292787849903107, -0.11872517317533493, -0.12827017903327942, -0.13358904421329498, -0.13796299695968628, -0.14097431302070618, -0.14129464328289032, -0.14672821760177612, -0.15239334106445312], "norm_scores": [0.11264151334762573, 0.10166114568710327, 0.10006779432296753, 0.09911718964576721, 0.0985914021730423, 0.09816110134124756, 0.09786596149206161, 0.09783460944890976, 0.09730446338653564, 0.0967547744512558]}}
{"id": "WebQTest-436", "question": "what is the nigeria time", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"], ["location.country.form_of_government"], ["common.topic.notable_types"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["base.locations.countries.continent"], ["location.country.time_zones"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the nigeria time [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.country.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00017937636584974825, -0.08329865336418152, -0.11921412497758865, -0.1487780064344406, -0.15869110822677612, -0.1648196429014206, -0.17029830813407898, -0.17087745666503906, -0.18209534883499146, -0.1826910376548767], "norm_scores": [0.11461276561021805, 0.1054714024066925, 0.10175056755542755, 0.09878645837306976, 0.09781201928853989, 0.09721440821886063, 0.09668325632810593, 0.09662728011608124, 0.09554938226938248, 0.09549248218536377]}}
{"id": "WebQTest-437", "question": "where did they find jenni rivera 's body", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.nationality", "location.administrative_division.first_level_division_of"], ["location.location.people_born_here"], ["people.marriage.spouse", "people.marriage.location_of_ceremony"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they find jenni rivera 's body [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.location_of_ceremony </PATH>"], "scores": [-0.0005048344610258937, -0.08288420736789703, -0.08406784385442734, -0.09062125533819199, -0.09663532674312592, -0.09770496189594269, -0.09952060133218765, -0.10108384490013123, -0.107168048620224, -0.10971502959728241], "norm_scores": [0.1089833602309227, 0.10036522895097733, 0.10024650394916534, 0.0995917022228241, 0.09899453818798065, 0.09888871014118195, 0.09870932996273041, 0.09855514764785767, 0.09795733541250229, 0.09770815819501877]}}
{"id": "WebQTest-438", "question": "what university did romney graduated from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.religion", "religion.religious_organization.associated_with"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.religion", "religion.religion.organizations"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did romney graduated from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.religion <SEP> religion.religious_organization.associated_with </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.014489749446511269, -0.02167651057243347, -0.023881686851382256, -0.046636998653411865, -0.06614024192094803, -0.0770847350358963, -0.08607970178127289, -0.10842086374759674, -0.1111825704574585, -0.11479707807302475], "norm_scores": [0.10532423108816147, 0.10457000136375427, 0.10433966666460037, 0.10199218988418579, 0.10002228617668152, 0.09893356263637543, 0.0980476438999176, 0.09588143229484558, 0.09561701118946075, 0.0952720195055008]}}
{"id": "WebQTest-439", "question": "where is the time zone line in south dakota", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.time_zones"], ["base.aareas.schema.administrative_area.administrative_parent"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.partially_contains"], ["location.location.partially_containedby"], ["location.adjoining_relationship.adjoins", "location.location.time_zones"], ["location.location.contains", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the time zone line in south dakota [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.time_zones </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.location.time_zones </PATH>", "<PATH> location.location.contains <SEP> location.location.time_zones </PATH>"], "scores": [-0.0018822172423824668, -0.04710739478468895, -0.08009828627109528, -0.09418880194425583, -0.10591704398393631, -0.10741163045167923, -0.11111674457788467, -0.11659684032201767, -0.11802441626787186, -0.12257896363735199], "norm_scores": [0.10919167846441269, 0.10436347126960754, 0.10097659379243851, 0.09956376999616623, 0.09840288013219833, 0.09825590997934341, 0.09789253771305084, 0.09735754877328873, 0.09721866250038147, 0.09677687287330627]}}
{"id": "WebQTest-440", "question": "where is jason mraz from", "prediction": [["music.artist.origin"], ["location.location.people_born_here"], ["people.person.place_of_birth"], ["music.recording.artist"], ["music.album.artist"], ["music.producer.tracks_produced"], ["people.person.nationality"], ["music.record_label.artist", "music.artist.origin"], ["music.producer.travel_produced"], ["music.recording.artist", "music.artist.origin"]], "ground_paths": [["music.artist.origin"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jason mraz from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.producer.tracks_produced </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.record_label.artist <SEP> music.artist.origin </PATH>", "<PATH> music.producer.travel_produced </PATH>", "<PATH> music.recording.artist <SEP> music.artist.origin </PATH>"], "scores": [-0.004818407818675041, -0.03907981887459755, -0.0516795814037323, -0.09261550009250641, -0.1126289963722229, -0.11888807266950607, -0.12369181215763092, -0.1295788586139679, -0.12972140312194824, -0.13486400246620178], "norm_scores": [0.10919718444347382, 0.10551929473876953, 0.10419811308383942, 0.10001879930496216, 0.09803697466850281, 0.09742527455091476, 0.09695839136838913, 0.09638926386833191, 0.09637552499771118, 0.09588117897510529]}}
{"id": "WebQTest-441", "question": "what killed john bonham", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["music.group_member.instruments_played", "people.deceased_person.cause_of_death"], ["music.group_member.instruments_played", "music.group_member.instruments_played"], ["medicine.disease.notable_people_with_this_condition"], ["people.person.cause_of_death"], ["deceased_person.cause_of_death"], ["music.group_member.instruments_played", "music.group_member.instruments_played", "music.group_member.instruments_played"], ["people.people.deceased_person.cause_of_death"], ["music.group_member.instruments_played", "music.group_member.instruments_played", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what killed john bonham [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> music.group_member.instruments_played <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0017968002939596772, -0.044189855456352234, -0.09396980702877045, -0.12740057706832886, -0.12893177568912506, -0.1294371783733368, -0.13570502400398254, -0.1371137946844101, -0.14198754727840424, -0.14480756223201752], "norm_scores": [0.11114524304866791, 0.10653192549943924, 0.10135860741138458, 0.09802612662315369, 0.09787613898515701, 0.09782668948173523, 0.0972154438495636, 0.0970785915851593, 0.09660661220550537, 0.0963345542550087]}}
{"id": "WebQTest-442", "question": "what industry does walmart operate in", "prediction": [["business.business_operation.industry"], ["business.industry.companies"], ["business.business_operation.industry", "business.business_operation.industry"], ["business.business_operation.industry", "business.industry.parent_industry"], ["organization.organization.industry"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["business.company.industry"], ["organization.organization.sectors"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_language"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["business.industry.companies"], ["organization.organization.sectors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what industry does walmart operate in [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> business.business_operation.industry <SEP> business.industry.parent_industry </PATH>", "<PATH> organization.organization.industry </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> business.company.industry </PATH>", "<PATH> organization.organization.sectors </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_language </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.005587760824710131, -0.0483894981443882, -0.05149258300662041, -0.06973875313997269, -0.074318066239357, -0.07979113608598709, -0.08670024573802948, -0.08880965411663055, -0.09156531095504761, -0.0967884436249733], "norm_scores": [0.1065434142947197, 0.10207939147949219, 0.10176311433315277, 0.09992316365242004, 0.09946663677692413, 0.09892373532056808, 0.0982426106929779, 0.09803560376167297, 0.09776581823825836, 0.09725651144981384]}}
{"id": "WebQTest-443", "question": "what county is texarkana arkansas in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.hud_county_place.county"], ["location.location.contains"], ["location.us_county.hud_county_place"], ["location.citytown.postal_codes"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.location.people_born_here"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is texarkana arkansas in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.008351783268153667, -0.021021032705903053, -0.05759744718670845, -0.07267336547374725, -0.09000957757234573, -0.1040748879313469, -0.11238521337509155, -0.11261981725692749, -0.11403457820415497, -0.12165581434965134], "norm_scores": [0.10750224441289902, 0.106148861348629, 0.10233646631240845, 0.10080522298812866, 0.09907270222902298, 0.09768896549940109, 0.09688050299882889, 0.09685777872800827, 0.09672084450721741, 0.095986507833004]}}
{"id": "WebQTest-446", "question": "what music period did beethoven live in", "prediction": [["music.artist.genre"], ["music.artist.period"], ["music.artist.periods"], ["influence.influence_node.influenced_by"], ["music.genre.artists"], ["influence.influence_node.influenced"], ["visual_art.visual_artist.associated_periods_or_movements"], ["book.written_work.author"], ["music.period.artist"], ["music.music_period.artist"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music period did beethoven live in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.artist.period </PATH>", "<PATH> music.artist.periods </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> music.period.artist </PATH>", "<PATH> music.music_period.artist </PATH>"], "scores": [-0.0016631126636639237, -0.0688074454665184, -0.08992443978786469, -0.09120211005210876, -0.09551957994699478, -0.09634733945131302, -0.09723205119371414, -0.09869275987148285, -0.10425674915313721, -0.10519175231456757], "norm_scores": [0.10863027721643448, 0.10157584398984909, 0.09945335984230042, 0.09932637959718704, 0.09889846295118332, 0.0988166332244873, 0.09872924536466599, 0.09858513623476028, 0.09803813695907593, 0.09794650971889496]}}
{"id": "WebQTest-447", "question": "what did laura ingalls wilder wrote", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"], ["book.author.book_editions_published"], ["influence.influence_node.influenced", "book.written_work.author"], ["influence.influence_node.influenced", "book.book_edition.author_editor"], ["tv.tv_program_creator.programs_created"]], "ground_paths": [["book.literary_series.author_s"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did laura ingalls wilder wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> influence.influence_node.influenced <SEP> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced <SEP> book.book_edition.author_editor </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>"], "scores": [-0.010633432306349277, -0.02545604109764099, -0.036843620240688324, -0.061820585280656815, -0.08414740115404129, -0.09811708331108093, -0.10882382839918137, -0.11497824639081955, -0.11971212178468704, -0.12266208976507187], "norm_scores": [0.10691821575164795, 0.10534509271383286, 0.10415227711200714, 0.1015830785036087, 0.09934018552303314, 0.09796208143234253, 0.09691882878541946, 0.09632418304681778, 0.0958692729473114, 0.0955868735909462]}}
{"id": "WebQTest-448", "question": "what type of government system does italy have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["olympics.olympic_participating_country.form_of_government"], ["people.person.nationality", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_games.participating_countries"], ["locations.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government system does italy have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> olympics.olympic_participating_country.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_games.participating_countries </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> locations.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.009394954890012741, -0.014824938029050827, -0.144078329205513, -0.15860994160175323, -0.1619824767112732, -0.1625986546278, -0.1664588898420334, -0.17632202804088593, -0.17639675736427307, -0.18057502806186676], "norm_scores": [0.11317109316587448, 0.11255823820829391, 0.09891069680452347, 0.09748376160860062, 0.09715554863214493, 0.09709569811820984, 0.09672161191701889, 0.09577231854200363, 0.09576515853404999, 0.09536586701869965]}}
{"id": "WebQTest-449", "question": "what did john irving wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["book.author.book_editions_published"], ["influence.influence_node.influenced", "book.book_edition.author_editor"], ["influence.influence_node.influenced_by", "book.book_edition.author_editor"], ["influence.influence_node.influenced", "book.written_work.subjects"], ["tv.tv_program.program_creator"]], "ground_paths": [["film.film.story_by"], ["book.author.book_editions_published"], ["film.film_story_contributor.film_story_credits"], ["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john irving wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> influence.influence_node.influenced <SEP> book.book_edition.author_editor </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.book_edition.author_editor </PATH>", "<PATH> influence.influence_node.influenced <SEP> book.written_work.subjects </PATH>", "<PATH> tv.tv_program.program_creator </PATH>"], "scores": [-0.01515952032059431, -0.025599773973226547, -0.029647691175341606, -0.06569452583789825, -0.07557571679353714, -0.09732191264629364, -0.13002991676330566, -0.1312384009361267, -0.1370774656534195, -0.13822971284389496], "norm_scores": [0.10706918686628342, 0.10595717281103134, 0.10552913695573807, 0.10179289430379868, 0.10079201310873032, 0.09862382709980011, 0.09545022249221802, 0.09533494710922241, 0.09477989375591278, 0.09467075020074844]}}
{"id": "WebQTest-450", "question": "what is the national flower of hawaii", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"], ["location.location.containedby", "location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["base.aareas.schema.administrative_area.administrative_parent", "location.location_symbol_relationship.administrative_parent", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national flower of hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.location.containedby <SEP> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.location_symbol_relationship.administrative_parent <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>"], "scores": [-0.010315005667507648, -0.01971006765961647, -0.022638922557234764, -0.030338104814291, -0.0503605380654335, -0.05278685688972473, -0.05384742468595505, -0.057287391275167465, -0.05844822898507118, -0.06133251637220383], "norm_scores": [0.10317230224609375, 0.10220752656459808, 0.1019086167216301, 0.1011270135641098, 0.09912235289812088, 0.09888213872909546, 0.09877732396125793, 0.09843810647726059, 0.09832391142845154, 0.09804071485996246]}}
{"id": "WebQTest-451", "question": "what did alice paul accomplish", "prediction": [["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"], ["common.topic.notable_types"], ["people.person.profession"], ["base.activism.activists.area_of_activism"], ["organization.organization_founder.organizations_founded"], ["base.activism.activist.activism_issue"], ["fictional_universe.fictional_character.occupation"], ["fight.activist.area_of_activism"], ["organization.activist.area_of_activism"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did alice paul accomplish [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> base.activism.activists.area_of_activism </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activist.activism_issue </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fight.activist.area_of_activism </PATH>", "<PATH> organization.activist.area_of_activism </PATH>"], "scores": [-0.005656159017235041, -0.027324169874191284, -0.07263550162315369, -0.07552394270896912, -0.11399281769990921, -0.11512483656406403, -0.1290263831615448, -0.12931150197982788, -0.1295153647661209, -0.13314099609851837], "norm_scores": [0.10903503745794296, 0.10669787228107452, 0.10197114944458008, 0.10167703777551651, 0.09783991426229477, 0.09772922098636627, 0.09638003259897232, 0.09635256230831146, 0.09633292257785797, 0.09598428010940552]}}
{"id": "WebQTest-453", "question": "what languages do they speak in costa rica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["book.newspaper.circulation_areas", "book.periodical.language"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["book.newspaper.circulation_areas"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do they speak in costa rica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>"], "scores": [-0.004447008483111858, -0.02791968733072281, -0.08857760578393936, -0.09122616052627563, -0.16567859053611755, -0.17500795423984528, -0.1773151159286499, -0.17758981883525848, -0.1830245554447174, -0.20257100462913513], "norm_scores": [0.11304197460412979, 0.11041947454214096, 0.10392075031995773, 0.10364587604999542, 0.09620945155620575, 0.09531605988740921, 0.0950964018702507, 0.09507028013467789, 0.09455500543117523, 0.09272473305463791]}}
{"id": "WebQTest-454", "question": "where did hank marvin come from", "prediction": [["people.person.place_of_birth"], ["music.artist.origin"], ["location.location.people_born_here"], ["music.recording.artist"], ["music.recording_artist.origin"], ["music.recording_contribution.contributor", "music.artist.origin"], ["people.deceased_person.place_of_death"], ["music.recording_contribution.contributor", "location.location.people_born_here"], ["music.artist.track", "people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hank marvin come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.recording_artist.origin </PATH>", "<PATH> music.recording_contribution.contributor <SEP> music.artist.origin </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> music.recording_contribution.contributor <SEP> location.location.people_born_here </PATH>", "<PATH> music.artist.track <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.01143875252455473, -0.02495371736586094, -0.05184872820973396, -0.05739883705973625, -0.0703076496720314, -0.09197426587343216, -0.09308522194623947, -0.09620555490255356, -0.09995786100625992, -0.10019972920417786], "norm_scores": [0.10595265030860901, 0.10453034192323685, 0.10175646841526031, 0.10119326412677765, 0.09989538043737411, 0.09775426238775253, 0.09764572232961655, 0.09734150767326355, 0.09697693586349487, 0.09695348888635635]}}
{"id": "WebQTest-455", "question": "when did chipper jones get drafted", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_league_draft.picks", "sports.sports_league_draft_pick.draft"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"], ["sports.pro_athlete.teams", "sports.sports_league_draft_pick.draft"], ["sports.sports_league_draft.picks", "sports.sports_league_draft.picks"], ["sports.pro_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did chipper jones get drafted [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_league_draft.picks <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_league_draft.picks <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.pro_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.021659737452864647, -0.022793857380747795, -0.022898998111486435, -0.02491525560617447, -0.03842216730117798, -0.0385868139564991, -0.04494785517454147, -0.05446959286928177, -0.06590688228607178, -0.06591103225946426], "norm_scores": [0.10184227675199509, 0.10172683745622635, 0.10171614587306976, 0.10151126980781555, 0.10014937072992325, 0.10013289004564285, 0.0994979590177536, 0.0985550656914711, 0.09743428230285645, 0.09743388742208481]}}
{"id": "WebQTest-456", "question": "what part did winona ryder play in star trek", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["film.actor.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did winona ryder play in star trek [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.020391540601849556, -0.028146252036094666, -0.02932962216436863, -0.03001045435667038, -0.03827955201268196, -0.04801104590296745, -0.04994936287403107, -0.057265765964984894, -0.059987619519233704, -0.06280120462179184], "norm_scores": [0.1022164598107338, 0.1014268696308136, 0.10130690783262253, 0.1012379601597786, 0.10040426254272461, 0.09943192452192307, 0.09923937916755676, 0.09851595014333725, 0.09824817627668381, 0.09797212481498718]}}
{"id": "WebQTest-459", "question": "where did jovan belcher kill himself", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["people.person.nationality", "location.location.containedby"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["location.location.people_born_here"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jovan belcher kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.0019241479458287358, -0.06916357576847076, -0.07707121968269348, -0.08044900745153427, -0.08160359412431717, -0.08198169618844986, -0.08227911591529846, -0.08277634531259537, -0.08537238091230392, -0.09846918284893036], "norm_scores": [0.10745109617710114, 0.10046368837356567, 0.09967239201068878, 0.09933628886938095, 0.09922166168689728, 0.09918415546417236, 0.09915465861558914, 0.0991053655743599, 0.09884841740131378, 0.0975622609257698]}}
{"id": "WebQTest-461", "question": "where did clay matthews go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.place_of_birth", "location.location.containedby"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.education.student", "education.education.institution"], ["sports.pro_athlete.teams", "sports.sports_league_draft_pick.school"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did clay matthews go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.education.student <SEP> education.education.institution </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_draft_pick.school </PATH>"], "scores": [-0.0125666419044137, -0.014717096462845802, -0.03428199514746666, -0.04877380654215813, -0.05203760415315628, -0.06235286220908165, -0.07216183096170425, -0.08876952528953552, -0.08902598917484283, -0.10059049725532532], "norm_scores": [0.10455389320850372, 0.10432930290699005, 0.10230793803930283, 0.10083600133657455, 0.10050743073225021, 0.09947600215673447, 0.09850502014160156, 0.09688258916139603, 0.09685774147510529, 0.0957440733909607]}}
{"id": "WebQTest-462", "question": "what time is it in texas houston right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby"], ["base.locations.countries.states_provinces_within"], ["time.location.time_zones"], ["base.biblioness.bibs_location.state"], ["location.location.time_Zones"], ["location.city.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time is it in texas houston right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> time.location.time_zones </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.city.time_zones </PATH>"], "scores": [-1.6193931514862925e-05, -0.13054625689983368, -0.158266082406044, -0.175454780459404, -0.17680644989013672, -0.17826928198337555, -0.18120944499969482, -0.18457815051078796, -0.1897091567516327, -0.19336825609207153], "norm_scores": [0.11679255962371826, 0.10250066220760345, 0.09969838708639145, 0.09799934178590775, 0.0978669747710228, 0.09772391617298126, 0.09743700921535492, 0.09710933268070221, 0.0966123417019844, 0.0962594673037529]}}
{"id": "WebQTest-463", "question": "who is princess leia in star wars", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.dubbing_performance.character", "film.actor.dubbing_performances"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is princess leia in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.dubbing_performance.character <SEP> film.actor.dubbing_performances </PATH>"], "scores": [-0.011720455251634121, -0.0289674773812294, -0.03591447323560715, -0.04815872013568878, -0.048407554626464844, -0.0490368977189064, -0.0497320257127285, -0.05120144039392471, -0.05581364408135414, -0.06724004447460175], "norm_scores": [0.10333341360092163, 0.10156650096178055, 0.10086336731910706, 0.09963589161634445, 0.09961111098527908, 0.09954844415187836, 0.09947925806045532, 0.09933319687843323, 0.09887610375881195, 0.09775273501873016]}}
{"id": "WebQTest-464", "question": "what is the political structure of china", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["government.form_of_government.countries", "location.country.form_of_government"], ["organization.organization.founders"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political structure of china [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.006846623029559851, -0.019771602004766464, -0.14438514411449432, -0.14526605606079102, -0.14709073305130005, -0.15089130401611328, -0.15802790224552155, -0.158064603805542, -0.16364364326000214, -0.164535790681839], "norm_scores": [0.11245137453079224, 0.11100729554891586, 0.09800146520137787, 0.0979151725769043, 0.0977366641163826, 0.09736591577529907, 0.09667352586984634, 0.09666997939348221, 0.09613215178251266, 0.09604642540216446]}}
{"id": "WebQTest-465", "question": "what is new york city airport", "prediction": [["aviation.airport.serves"], ["location.location.containedby"], ["location.location.nearby_airports"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["common.topic.notable_types"], ["aviation.airport.hub_for"], ["travel.transportation.mode_of_transportation"]], "ground_paths": [["aviation.airport.serves"], ["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is new york city airport [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> aviation.airport.hub_for </PATH>", "<PATH> travel.transportation.mode_of_transportation </PATH>"], "scores": [-0.007173658348619938, -0.03135257586836815, -0.04631490632891655, -0.06511985510587692, -0.08236505836248398, -0.10948352515697479, -0.10960264503955841, -0.11348612606525421, -0.11773661524057388, -0.12654347717761993], "norm_scores": [0.10756924748420715, 0.10499953478574753, 0.10344018787145615, 0.10151317715644836, 0.09977757185697556, 0.09710811823606491, 0.09709654748439789, 0.09672021120786667, 0.09630996733903885, 0.0954655110836029]}}
{"id": "WebQTest-466", "question": "what language brazil speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spokencommon"], ["location. country.languages_spoken"], ["location.country.languages_spokenlanguage"], ["location.country.languages_spokenperiod"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language brazil speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken <PATH>", "<PATH> location.country.languages_spokencommon </PATH>", "<PATH> location. country.languages_spoken </PATH>", "<PATH> location.country.languages_spokenlanguage </PATH>", "<PATH> location.country.languages_spokenperiod </PATH>"], "scores": [-0.00019102486839983612, -0.10104986280202866, -0.11050766706466675, -0.1282747983932495, -0.1945715695619583, -0.19971613585948944, -0.24276131391525269, -0.24467051029205322, -0.24797414243221283, -0.2537781298160553], "norm_scores": [0.11839759349822998, 0.10703860968351364, 0.10603102296590805, 0.10416380316019058, 0.09748201072216034, 0.09698179364204407, 0.09289577603340149, 0.09271857887506485, 0.0924127846956253, 0.09187797456979752]}}
{"id": "WebQTest-467", "question": "who played obi wan in episode 2", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.performance.character", "film.actor.film"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["film.film.country", "people.person.nationality"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played obi wan in episode 2 [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.027489563450217247, -0.02891170233488083, -0.033140163868665695, -0.033906497061252594, -0.03940736502408981, -0.04080565273761749, -0.04113570228219032, -0.04374862089753151, -0.04439554363489151, -0.04664195701479912], "norm_scores": [0.10105033963918686, 0.10090673714876175, 0.10048095136880875, 0.10040397942066193, 0.09985318779945374, 0.09971366077661514, 0.09968075156211853, 0.09942062944173813, 0.09935633838176727, 0.09913339465856552]}}
{"id": "WebQTest-468", "question": "where did venus williams come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["people.ethnicity.people"], ["sports.sports_team_roster.player", "sports.sports_team_roster.place_of_birth"], ["people.person.ethnicity"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did venus williams come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.place_of_birth </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.004225816577672958, -0.03791430965065956, -0.05045419931411743, -0.10546557605266571, -0.12350323796272278, -0.13900774717330933, -0.1430133879184723, -0.14670999348163605, -0.15904390811920166, -0.16466276347637177], "norm_scores": [0.11070717871189117, 0.10703974962234497, 0.1057058572769165, 0.10004789382219315, 0.09825942665338516, 0.09674772620201111, 0.0963609516620636, 0.0960054025053978, 0.09482856094837189, 0.09429721534252167]}}
{"id": "WebQTest-469", "question": "what currency is used in panama", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["finance.currency.countries_used", "location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["common.topic.notable_types"], ["location.country.currency_used", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is used in panama [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>"], "scores": [-0.009434365667402744, -0.018212471157312393, -0.15062358975410461, -0.15445488691329956, -0.16958369314670563, -0.1776120811700821, -0.17964886128902435, -0.1850641816854477, -0.18730348348617554, -0.18772412836551666], "norm_scores": [0.11392149329185486, 0.11292584985494614, 0.09892087429761887, 0.09854261577129364, 0.09706299006938934, 0.09628685563802719, 0.09609093517065048, 0.09557197988033295, 0.09535820782184601, 0.09531810879707336]}}
{"id": "WebQTest-470", "question": "where does the zambezi river originate", "prediction": [["geography.river.origin"], ["location.location.containedby"], ["location.location.partial_containedby"], [], ["geography.river.basin_countries"], ["location.location.partially_containedby"], ["location.location.partial_contains"], ["geography.lake.outflow"], ["symbols.namesake.named_after"], ["geography.river.mouth"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river originate [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partial_containedby </PATH>", "<PATH> </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partial_contains </PATH>", "<PATH> geography.lake.outflow </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> geography.river.mouth </PATH>"], "scores": [-0.0005508718895725906, -0.09356693923473358, -0.09418711811304092, -0.10855881869792938, -0.1198040246963501, -0.1284864991903305, -0.13700881600379944, -0.13830287754535675, -0.1384720355272293, -0.1486896574497223], "norm_scores": [0.1115555614233017, 0.10164706408977509, 0.1015840545296669, 0.10013455897569656, 0.09901483356952667, 0.09815885871648788, 0.0973258763551712, 0.09720000624656677, 0.09718357026576996, 0.09619563817977905]}}
{"id": "WebQTest-472", "question": "what team did ronaldo play for in 2003", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did ronaldo play for in 2003 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.019626356661319733, -0.023857414722442627, -0.030048483982682228, -0.03386799618601799, -0.03573298826813698, -0.036545101553201675, -0.03741028532385826, -0.037913545966148376, -0.04027682542800903, -0.042573198676109314], "norm_scores": [0.10142355412244797, 0.10099533200263977, 0.10037199407815933, 0.09998935461044312, 0.09980304539203644, 0.09972202777862549, 0.0996357798576355, 0.0995856449007988, 0.09935058653354645, 0.09912269562482834]}}
{"id": "WebQTest-473", "question": "where does robin williams live 2011", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.children", "people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["film.producer.film", "film.film.featured_film_locations"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.place_of_birth"], ["people.person.nationality", "location.country.administrative_divisions"], ["people.person.children", "people.deceased_person.place_of_death"]], "ground_paths": [["people.person.parents", "people.person.place_of_birth"], ["film.film.produced_by", "film.film_location.featured_in_films"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "location.administrative_division.second_level_division_of"], ["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robin williams live 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> film.producer.film <SEP> film.film.featured_film_locations </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.country.administrative_divisions </PATH>", "<PATH> people.person.children <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.014711637981235981, -0.022941501811146736, -0.030208250507712364, -0.03380156308412552, -0.047176554799079895, -0.05053328350186348, -0.05661151558160782, -0.06816788017749786, -0.08435872197151184, -0.08476609736680984], "norm_scores": [0.1034943237900734, 0.10264606773853302, 0.10190287977457047, 0.10153736174106598, 0.10018834471702576, 0.09985260665416718, 0.0992475152015686, 0.09810717403888702, 0.09653153270483017, 0.0964922085404396]}}
{"id": "WebQTest-474", "question": "who played jacob black in twilight", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["film.film.country", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film.country", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film.starring", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jacob black in twilight [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_winning_work.awards_won <SEP> award.award_winner.awards_won </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_honor.award_winner </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02961183525621891, -0.031106846407055855, -0.032109335064888, -0.032462798058986664, -0.03487490117549896, -0.038921553641557693, -0.04158911481499672, -0.042238783091306686, -0.04750576242804527, -0.04805522784590721], "norm_scores": [0.10082487761974335, 0.10067425668239594, 0.10057338327169418, 0.10053783655166626, 0.10029561817646027, 0.0998905822634697, 0.09962446987628937, 0.09955976903438568, 0.09903676807880402, 0.09898236393928528]}}
{"id": "WebQTest-477", "question": "which airport to fly into in buenos aires", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["film.film_location.featured_in_films", "film.film.featured_film_locations"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_children", "location.location.nearby_airports"]], "ground_paths": [["aviation.airport.serves"], ["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into in buenos aires [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> film.film_location.featured_in_films <SEP> film.film.featured_film_locations </PATH>", "<PATH> film.film.featured_film_locations <SEP> film.film.featured_film_locations </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> location.location.nearby_airports </PATH>"], "scores": [-0.0073252576403319836, -0.026146644726395607, -0.045969851315021515, -0.05801950395107269, -0.07165518403053284, -0.07980582863092422, -0.08553993701934814, -0.08759055286645889, -0.10072894394397736, -0.1017441600561142], "norm_scores": [0.10604273527860641, 0.10406553000211716, 0.1020229235291481, 0.10080096125602722, 0.09943580627441406, 0.09862863272428513, 0.09806470572948456, 0.0978638157248497, 0.0965864509344101, 0.09648844599723816]}}
{"id": "WebQTest-479", "question": "what team did adrian peterson play for in college", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["people.person.education", "education.education.institution"], ["education.educational_institution.sports_teams"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did adrian peterson play for in college [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.educational_institution.sports_teams </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>"], "scores": [-0.010116581805050373, -0.01695467159152031, -0.038619473576545715, -0.04944216459989548, -0.052816715091466904, -0.05392380431294441, -0.05848861113190651, -0.061096444725990295, -0.06119526922702789, -0.06437583267688751], "norm_scores": [0.1037093997001648, 0.1030026450753212, 0.10079511255025864, 0.0997101292014122, 0.0993742123246193, 0.09926425665616989, 0.09881217032670975, 0.0985548123717308, 0.09854507446289062, 0.09823214262723923]}}
{"id": "WebQTest-480", "question": "where does name pennsylvania come from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes", "symbols.namesake.named_after"], ["symbols.namesake.named_after", "symbols.namesake.named_after"], ["symbols.armorial_grant.coat_of_arms", "symbols.coat_of_arms_bearer.coat_of_arms_used"], ["symbols.symbol_of_administrative_division.official_symbol_of", "symbols.namesake.named_after"], ["symbols.symbol_of_administrative_division.official_symbol_of", "symbols.symbol_of_administrative_division.official_symbol_of"], ["symbols.symbol_of_administrative_division.official_symbol_of", "location.location_symbol_relationship.administrative_division"], ["symbols.symbol_of_administrative_division.official_symbol_of", "government.governmental_jurisdiction.official_symbols"], ["symbols.symbol_of_administrative_division.official_symbol_of", "symbols.name_source.namesakes"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does name pennsylvania come from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes <SEP> symbols.namesake.named_after </PATH>", "<PATH> symbols.namesake.named_after <SEP> symbols.namesake.named_after </PATH>", "<PATH> symbols.armorial_grant.coat_of_arms <SEP> symbols.coat_of_arms_bearer.coat_of_arms_used </PATH>", "<PATH> symbols.symbol_of_administrative_division.official_symbol_of <SEP> symbols.namesake.named_after </PATH>", "<PATH> symbols.symbol_of_administrative_division.official_symbol_of <SEP> symbols.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> symbols.symbol_of_administrative_division.official_symbol_of <SEP> location.location_symbol_relationship.administrative_division </PATH>", "<PATH> symbols.symbol_of_administrative_division.official_symbol_of <SEP> government.governmental_jurisdiction.official_symbols </PATH>", "<PATH> symbols.symbol_of_administrative_division.official_symbol_of <SEP> symbols.name_source.namesakes </PATH>"], "scores": [-0.0045002056285738945, -0.031122295185923576, -0.09446673840284348, -0.10049104690551758, -0.10260166227817535, -0.1051812469959259, -0.11006046086549759, -0.110170878469944, -0.11090990155935287, -0.1111811175942421], "norm_scores": [0.1086442619562149, 0.10579008609056473, 0.0992967039346695, 0.09870031476020813, 0.09849221259355545, 0.09823846817016602, 0.09776031225919724, 0.09774951636791229, 0.0976773053407669, 0.09765081852674484]}}
{"id": "WebQTest-481", "question": "what kind of money do they use in germany", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country"], ["finance.currency.countries_used", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do they use in germany [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> finance.currency.countries_used <PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>"], "scores": [-0.005204995162785053, -0.025629963725805283, -0.11272697895765305, -0.13537701964378357, -0.1579611897468567, -0.16317950189113617, -0.16380853950977325, -0.178374782204628, -0.18231156468391418, -0.18571509420871735], "norm_scores": [0.1131880134344101, 0.11089960485696793, 0.10164926201105118, 0.09937277436256409, 0.0971536785364151, 0.09664802253246307, 0.09658724069595337, 0.09519053250551224, 0.09481652826070786, 0.09449435770511627]}}
{"id": "WebQTest-482", "question": "who plays blaine in batman", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays blaine in batman [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.014851096086204052, -0.0253952294588089, -0.033217594027519226, -0.036724768579006195, -0.042729396373033524, -0.044476158916950226, -0.055759236216545105, -0.0596645213663578, -0.06850539147853851, -0.08055766671895981], "norm_scores": [0.10316456854343414, 0.1020825058221817, 0.10128708928823471, 0.10093247890472412, 0.100328229367733, 0.1001531332731247, 0.09902945160865784, 0.0986434742808342, 0.09777522087097168, 0.0966038703918457]}}
{"id": "WebQTest-484", "question": "what language does australia use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["book.newspaper.circulation_areas"], ["country.languages_spoken"], ["book.newspaper.circulation_areas", "book.periodical.language"], ["film.film.featured_film_locations"], ["film.film.country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> country.languages_spoken </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.language </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> film.film.country </PATH>"], "scores": [-0.00833928119391203, -0.021112019196152687, -0.06480303406715393, -0.09214375913143158, -0.15440556406974792, -0.16859205067157745, -0.17713962495326996, -0.17729389667510986, -0.18401992321014404, -0.18406908214092255], "norm_scores": [0.11191936582326889, 0.11049893498420715, 0.10577508062124252, 0.10292228311300278, 0.09670957177877426, 0.09534728527069092, 0.09453577548265457, 0.09452119469642639, 0.0938875675201416, 0.0938829556107521]}}
{"id": "WebQTest-487", "question": "who did veronica lake mary", "prediction": [["people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents"], ["people.person.parents", "people.person.children"], ["people.marriage.type_of_union", "people.person.spouse_s"], ["people.marriage.type_of_union", "people.marriage.spouse"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did veronica lake mary [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.marriage.type_of_union <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.type_of_union <SEP> people.marriage.spouse </PATH>"], "scores": [-0.028422802686691284, -0.030311809852719307, -0.03801529482007027, -0.045954007655382156, -0.04718470573425293, -0.047433774918317795, -0.05547685921192169, -0.056900229305028915, -0.06085290014743805, -0.06435563415288925], "norm_scores": [0.10191813856363297, 0.10172579437494278, 0.10094515979290009, 0.10014696419239044, 0.1000237837433815, 0.09999887645244598, 0.09919780492782593, 0.09905670583248138, 0.09866594523191452, 0.09832094609737396]}}
{"id": "WebQTest-488", "question": "who did sir francis drake marry", "prediction": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.religion", "people.person.religion"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sir francis drake marry [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>"], "scores": [-0.026120690628886223, -0.028951216489076614, -0.03746125102043152, -0.04025917500257492, -0.040417760610580444, -0.04200784116983414, -0.042683474719524384, -0.052936725318431854, -0.05364956706762314, -0.08282677829265594], "norm_scores": [0.10186687856912613, 0.10157895088195801, 0.1007181778550148, 0.10043676942586899, 0.10042084753513336, 0.10026129335165024, 0.1001935824751854, 0.09917151927947998, 0.09910085052251816, 0.09625113755464554]}}
{"id": "WebQTest-489", "question": "who plays stephanie plum in one for the money", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.film.starring", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.film.executive_produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stephanie plum in one for the money [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.022652091458439827, -0.033951178193092346, -0.03603246808052063, -0.042261507362127304, -0.04556233808398247, -0.04706595465540886, -0.047470152378082275, -0.04922482743859291, -0.05175962299108505, -0.053296297788619995], "norm_scores": [0.10204411298036575, 0.10089759528636932, 0.10068781673908234, 0.10006257891654968, 0.09973283112049103, 0.0995829775929451, 0.09954274445772171, 0.09936822950839996, 0.09911666065454483, 0.09896447509527206]}}
{"id": "WebQTest-490", "question": "what did the ancient romans speak", "prediction": [["fictional_universe.fictional_setting.languages"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["fictional_universe.fictional_language.where_spoken"], ["people.ethnicity.languages_spoken"], ["fictional_universe.fictional_character.ethnicity", "people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the ancient romans speak [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> fictional_universe.fictional_language.where_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> fictional_universe.fictional_character.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.012809433043003082, -0.030932268127799034, -0.04109605774283409, -0.04266619309782982, -0.05092361196875572, -0.0633525624871254, -0.07588572800159454, -0.08442717045545578, -0.08505267649888992, -0.08560997247695923], "norm_scores": [0.10451614856719971, 0.1026390790939331, 0.10160115361213684, 0.10144175589084625, 0.10060755908489227, 0.09936484694480896, 0.09812726825475693, 0.09729268401861191, 0.09723184257745743, 0.09717767685651779]}}
{"id": "WebQTest-491", "question": "what does ringo sing", "prediction": [["music.recording.artist"], ["music.artist.track"], ["music.album.artist"], ["music.composition.composer"], ["music.producer.releases_produced"], ["music.producer.tracks_produced"], ["music.composer.compositions"], ["music.musical_group.member", "music.group_membership.group"], ["music.recording.producer"], ["music.artist.album"]], "ground_paths": [["music.artist.label", "music.record_label.releases"], ["music.artist.album"], ["music.recording.featured_artists"], ["music.album.artist"], ["music.album.artist", "music.album.compositions"], ["music.composer.compositions"], ["music.lyricist.lyrics_written"], ["music.recording.artist"], ["organization.organization.founders", "film.film.production_companies"], ["music.composition.composer"], ["theater.theatrical_composer.plays_composed", "film.film.sequel"], ["people.person.nationality", "film.film.country"], ["music.featured_artist.recordings"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does ringo sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> music.producer.tracks_produced </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.recording.producer </PATH>", "<PATH> music.artist.album </PATH>"], "scores": [-0.020289696753025055, -0.023368021473288536, -0.0596950389444828, -0.0603967159986496, -0.06776377558708191, -0.07323631644248962, -0.07550308853387833, -0.07572326809167862, -0.07834109663963318, -0.08150371164083481], "norm_scores": [0.10419248044490814, 0.10387222468852997, 0.10016657412052155, 0.10009632259607315, 0.0993616133928299, 0.09881933778524399, 0.09859558939933777, 0.09857387840747833, 0.09831617772579193, 0.09800572693347931]}}
{"id": "WebQTest-492", "question": "where was st. lucy born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["symbols.name_source.namesakes", "location.location.people_born_here"], ["people.deceased_person.place_of_death"], ["symbols.name_source.namesakes", "location.location.containedby"], ["symbols.namesake.named_after", "location.location.people_born_here"], ["organization.organization_founder.organizations_founded"], ["book.written_work.subjects"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st. lucy born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.name_source.namesakes <SEP> location.location.people_born_here </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> symbols.name_source.namesakes <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.people_born_here </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.006021431181579828, -0.02564806118607521, -0.08483649045228958, -0.09139405190944672, -0.11942485719919205, -0.12155921757221222, -0.1430596262216568, -0.14947594702243805, -0.15971536934375763, -0.16031602025032043], "norm_scores": [0.11038168519735336, 0.1082363873720169, 0.10201594978570938, 0.10134916007518768, 0.09854770451784134, 0.09833760559558868, 0.09624586999416351, 0.09563030302524567, 0.0946560949087143, 0.09459925442934036]}}
{"id": "WebQTest-493", "question": "what type of government does australia have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["type.type_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent"], ["form_of_government.countries.form_of_government"], ["type.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does australia have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> type.type_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> form_of_government.countries.form_of_government </PATH>", "<PATH> type.form_of_government.countries </PATH>"], "scores": [-0.0070436811074614525, -0.019130680710077286, -0.13447432219982147, -0.1398438662290573, -0.14910316467285156, -0.15364214777946472, -0.153888538479805, -0.15427537262439728, -0.1610337197780609, -0.16539740562438965], "norm_scores": [0.11220145225524902, 0.11085343360900879, 0.09877704828977585, 0.09824809432029724, 0.0973425805568695, 0.09690173715353012, 0.09687786549329758, 0.09684039652347565, 0.09618812799453735, 0.09576930850744247]}}
{"id": "WebQTest-497", "question": "who was the father of king george vi", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["royalty.monarch.kingdom"], ["people.person.sibling_s", "people.person.sibling_s"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["royalty.kingdom.rulers"], ["people.person.religion"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the father of king george vi [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> people.person.sibling_s <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.deceased_person.place_of_burial <SEP> people.deceased_person.place_of_burial </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.sibling_s <SEP> people.person.sibling_s </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> royalty.kingdom.rulers </PATH>", "<PATH> people.person.religion </PATH>"], "scores": [-0.012016371823847294, -0.018687140196561813, -0.05210346728563309, -0.11183962225914001, -0.11688131839036942, -0.11830879002809525, -0.12209608405828476, -0.12726962566375732, -0.13709406554698944, -0.14008724689483643], "norm_scores": [0.10860375314950943, 0.10788170248270035, 0.10433626174926758, 0.09828611463308334, 0.09779182821512222, 0.09765233099460602, 0.09728319942951202, 0.09678119421005249, 0.09583503007888794, 0.0955486074090004]}}
{"id": "WebQTest-500", "question": "what is the zip code for trenton", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["base.biblioness.bibs_location.country", "location.location.containedby"], ["location.location.contains"], ["base.biblioness.bibs_location.state"], ["location.Citytown.postal_codes"], ["base.biblioness.bibs_location.state", "location.location.containedby"], ["location.citytown.postal_codes", "location.location.containedby"], ["location.citytown.postal_codes", "location.postal_code.country"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for trenton [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> location.location.containedby </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.citytown.postal_codes <SEP> location.location.containedby </PATH>", "<PATH> location.citytown.postal_codes <SEP> location.postal_code.country </PATH>"], "scores": [-9.648317791288719e-05, -0.10415656864643097, -0.15756618976593018, -0.1697542518377304, -0.17266210913658142, -0.17577211558818817, -0.17672492563724518, -0.17708511650562286, -0.18030785024166107, -0.18256916105747223], "norm_scores": [0.11595544219017029, 0.10449569672346115, 0.0990610420703888, 0.09786101430654526, 0.09757686406373978, 0.09727387130260468, 0.09718123078346252, 0.09714622795581818, 0.09683365374803543, 0.09661492705345154]}}
{"id": "WebQTest-501", "question": "what has ashley greene been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ashley greene been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.01730135641992092, -0.03231364116072655, -0.03553635627031326, -0.0422070138156414, -0.046730585396289825, -0.048492398113012314, -0.04892880842089653, -0.04919297993183136, -0.04989172890782356, -0.05195464938879013], "norm_scores": [0.10252124071121216, 0.10099366307258606, 0.10066871345043182, 0.09999941289424896, 0.09954807907342911, 0.09937284886837006, 0.09932950139045715, 0.09930326789617538, 0.09923388808965683, 0.09902939945459366]}}
{"id": "WebQTest-502", "question": "where did al qaeda attack", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"], ["organization.organization_founder.organizations_founded", "base.terrorism.terrorist_attack.organizations_involved"], ["organization.organization.founders", "base.terrorism.terrorist_attack.organizations_involved"], ["organization.organization_founder.organizations_founded"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["organization.organization.founders"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.conflict"], ["organization.organization.founders", "base.culturalevent.event.entity_involved"]], "ground_paths": [["military.military_combatant.military_conflicts", "military.military_combatant_group.conflict"], ["film.film.subjects", "film.film.subjects"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.terrorism.terrorist_organization.involved_in_attacks"], ["base.disaster2.attack_process.attacker", "base.disaster2.attack.attack"], ["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did al qaeda attack [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.terrorism.terrorist_attack.organizations_involved </PATH>", "<PATH> organization.organization.founders <SEP> base.terrorism.terrorist_attack.organizations_involved </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> military.military_combatant.military_conflicts <SEP> military.military_combatant_group.conflict </PATH>", "<PATH> organization.organization.founders <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.018836794421076775, -0.032031528651714325, -0.03475722298026085, -0.038862280547618866, -0.042655639350414276, -0.043235473334789276, -0.048984915018081665, -0.05145568028092384, -0.05212148278951645, -0.052161216735839844], "norm_scores": [0.1022879034280777, 0.10094710439443588, 0.10067232698202133, 0.10025990754365921, 0.09988030791282654, 0.09982240945100784, 0.09925013780593872, 0.09900520741939545, 0.09893932193517685, 0.09893538802862167]}}
{"id": "WebQTest-503", "question": "what language is spoken in switzerland", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["book.newspaper.circulation_areas"], ["film.film.featured_film_locations"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["location.country.currency_used"], ["location.country.language_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.language_spoken </PATH>"], "scores": [-0.01745724305510521, -0.020960252732038498, -0.023444684222340584, -0.11100409924983978, -0.1508345752954483, -0.16218797862529755, -0.16657155752182007, -0.17574115097522736, -0.18148332834243774, -0.19519942998886108], "norm_scores": [0.1105879619717598, 0.11020125448703766, 0.10992780327796936, 0.10071194171905518, 0.09677937626838684, 0.09568681567907333, 0.09526827931404114, 0.09439869970083237, 0.09385819733142853, 0.09257962554693222]}}
{"id": "WebQTest-504", "question": "who was philip in acts chapter 8", "prediction": [["people.person.gender", "people.person.gender"], ["people.person.profession"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.profession", "people.person.profession"], ["film.film_subject.films", "film.film_character.portrayed_in_films"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.film_subject.films", "film.film_story_contributor.film_story_credits"], ["book.book_subject.works", "people.person.profession"], ["religion.religion.notable_figures", "religion.religion.types_of_this_religion"], ["film.film_subject.films", "film.film_subject.films"]], "ground_paths": [["fictional_universe.person_in_fiction.representations_in_fiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was philip in acts chapter 8 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> film.film_subject.films <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> film.film_subject.films <SEP> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.book_subject.works <SEP> people.person.profession </PATH>", "<PATH> religion.religion.notable_figures <SEP> religion.religion.types_of_this_religion </PATH>", "<PATH> film.film_subject.films <SEP> film.film_subject.films </PATH>"], "scores": [-0.014323078095912933, -0.01685483567416668, -0.05703243613243103, -0.06556709110736847, -0.07170145213603973, -0.07855355739593506, -0.08059590309858322, -0.08754763007164001, -0.08816999942064285, -0.09054051339626312], "norm_scores": [0.10516982525587082, 0.10490389913320541, 0.1007726639509201, 0.0999162569642067, 0.09930521994829178, 0.09862708300352097, 0.09842587262392044, 0.09774401038885117, 0.09768319129943848, 0.09745191037654877]}}
{"id": "WebQTest-506", "question": "what the time zone in england", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["locations.location.time_zones"], ["base.aareas.schema.administrative_area.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in england [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> base.aareas.schema.administrative_area.locations_in_this_time_zone </PATH>"], "scores": [-0.001270974287763238, -0.04616151750087738, -0.1482839584350586, -0.14948494732379913, -0.16312843561172485, -0.1638818383216858, -0.171884223818779, -0.1730823665857315, -0.18559005856513977, -0.1894318014383316], "norm_scores": [0.11457964777946472, 0.10954985022544861, 0.09891463816165924, 0.09879591315984726, 0.0974571481347084, 0.09738375246524811, 0.09660755842924118, 0.09649187326431274, 0.09529250115156174, 0.0949271097779274]}}
{"id": "WebQTest-508", "question": "what is the best currency to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the best currency to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.016897480934858322, -0.017363224178552628, -0.037372373044490814, -0.037597086280584335, -0.10931016504764557, -0.11127950996160507, -0.12246062606573105, -0.12408103793859482, -0.12814275920391083, -0.12952648103237152], "norm_scores": [0.10675977170467377, 0.10671006143093109, 0.1045960932970047, 0.1045726016163826, 0.09733596444129944, 0.09714445471763611, 0.09606432914733887, 0.09590879082679749, 0.0955200269818306, 0.09538795053958893]}}
{"id": "WebQTest-509", "question": "what system of government is practiced in nigeria", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["geography.river.basin_countries", "location.country.form_of_government"], ["base.locations.countries.continent"], ["base.aareas.schema.administrative_area.administrative_parent", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent"], ["people.person.nationality", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government is practiced in nigeria [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> geography.river.basin_countries <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.0054327030666172504, -0.021335477009415627, -0.1329740732908249, -0.13677102327346802, -0.13949142396450043, -0.14128896594047546, -0.14278608560562134, -0.14885663986206055, -0.14907948672771454, -0.15247714519500732], "norm_scores": [0.1116519346833229, 0.1098904013633728, 0.09828239679336548, 0.09790992736816406, 0.09764394164085388, 0.09746857732534409, 0.09732276201248169, 0.09673374891281128, 0.09671219438314438, 0.09638416022062302]}}
{"id": "WebQTest-512", "question": "who were mary shelley", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced_by", "people.person.profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were mary shelley [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>"], "scores": [-0.00182019779458642, -0.07026420533657074, -0.07434990257024765, -0.07845741510391235, -0.08159898966550827, -0.08662781119346619, -0.0974331796169281, -0.0974879041314125, -0.09920404106378555, -0.10862211138010025], "norm_scores": [0.10804255306720734, 0.10089507699012756, 0.10048370063304901, 0.10007180273532867, 0.09975791722536087, 0.09925750643014908, 0.09819076955318451, 0.09818539768457413, 0.09801703691482544, 0.09709823876619339]}}
{"id": "WebQTest-513", "question": "what language do most australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.006627068854868412, -0.024129321798682213, -0.06339042633771896, -0.09329355508089066, -0.16364486515522003, -0.17374001443386078, -0.18255232274532318, -0.18394067883491516, -0.18468205630779266, -0.1874174028635025], "norm_scores": [0.11244864761829376, 0.11049766838550568, 0.10624346882104874, 0.10311347991228104, 0.09610860049724579, 0.09514325112104416, 0.0943085104227066, 0.09417766332626343, 0.09410787373781204, 0.09385079890489578]}}
{"id": "WebQTest-514", "question": "where is denmark situated", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent"], ["common.topic.notable_types"], ["geography.river.basin_countries"], ["location.country.languages_spoken"], ["location.location.contains", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark situated [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.location.contains <SEP> location.location.containedby </PATH>"], "scores": [-0.01982497237622738, -0.021442659199237823, -0.03488820046186447, -0.05259811878204346, -0.09092634916305542, -0.09544770419597626, -0.09751269221305847, -0.10892195999622345, -0.1112932339310646, -0.11193572729825974], "norm_scores": [0.10554837435483932, 0.10537777096033096, 0.10397038608789444, 0.10214529186487198, 0.09830430895090103, 0.09786085039377213, 0.09765898436307907, 0.09655109792947769, 0.09632241725921631, 0.09626055508852005]}}
{"id": "WebQTest-515", "question": "where was teddy roosevelt 's house", "prediction": [["symbols.name_source.namesakes", "location.location.containedby"], ["symbols.namesake.named_after", "location.location.containedby"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.parents", "people.person.place_of_birth"], ["people.person.children", "people.person.place_of_birth"], ["people.person.nationality", "location.country.administrative_divisions"], ["book.written_work.subjects", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["book.written_work.author", "people.deceased_person.place_of_death"], ["people.person.parents", "people.person.place_of_birth"], ["book.book_edition.author_editor", "book.book_edition.place_of_publication"], ["people.person.parents", "people.deceased_person.place_of_death"], ["book.written_work.author", "base.biblioness.bibs_location.state"], ["base.usnris.nris_listing.significant_person", "travel.travel_destination.tourist_attractions"], ["symbols.namesake.named_after", "location.location.containedby"], ["symbols.namesake.named_after", "transportation.bridge.locale"], ["people.person.children", "people.person.place_of_birth"], ["people.person.place_of_birth", "base.aareas.schema.administrative_area.administrative_children"], ["book.written_work.subjects", "location.administrative_division.country"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["people.person.children", "people.deceased_person.place_of_death"], ["book.written_work.subjects", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["symbols.name_source.namesakes", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was teddy roosevelt 's house [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> base.inaugurations.inauguration.president <SEP> time.event.locations </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.country.administrative_divisions </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.023530326783657074, -0.028384502977132797, -0.02910337969660759, -0.03176305443048477, -0.037635911256074905, -0.039785418659448624, -0.04640289396047592, -0.06694212555885315, -0.06876271218061447, -0.07167890667915344], "norm_scores": [0.10209350287914276, 0.10159911960363388, 0.10152611136436462, 0.10125644505023956, 0.10066352039575577, 0.10044737160205841, 0.09978485852479935, 0.09775625914335251, 0.09757845103740692, 0.09729430824518204]}}
{"id": "WebQTest-516", "question": "what is the government system of malaysia", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["government.form_of_government.countries", "location.country.form_of_government"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["law.court.jurisdiction", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of malaysia [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> law.court.jurisdiction <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.010205844417214394, -0.01376034040004015, -0.11243090778589249, -0.11806317418813705, -0.14015911519527435, -0.1440473198890686, -0.1530769020318985, -0.15566758811473846, -0.15667887032032013, -0.17159907519817352], "norm_scores": [0.11115935444831848, 0.11076495051383972, 0.10035759210586548, 0.099793940782547, 0.09761308878660202, 0.09723428636789322, 0.09636025130748749, 0.09611093252897263, 0.09601379185914993, 0.09459187835454941]}}
{"id": "WebQTest-517", "question": "what do people speak in canada", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["book.newspaper.circulation_areas"], ["location.country.language_spoken"], ["film.film.featured_film_locations"], ["location.country.form_of_government"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in canada [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> location.country.language_spoken </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.016248561441898346, -0.017431804910302162, -0.038406092673540115, -0.04991867393255234, -0.15744230151176453, -0.16741208732128143, -0.17805536091327667, -0.1815890520811081, -0.18498346209526062, -0.18935710191726685], "norm_scores": [0.11042632162570953, 0.11029574275016785, 0.10800646245479584, 0.10677015036344528, 0.0958855003118515, 0.09493429213762283, 0.0939292386174202, 0.093597911298275, 0.09328074008226395, 0.09287365525960922]}}
{"id": "WebQTest-518", "question": "what type of money do they use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries", "location.country.currency_used"], ["government.form_of_government.countries"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country"], ["finance.currency.countries_used", "location.country.currency_used"], ["government.form_of_government.countries", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money do they use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.currency_used </PATH>"], "scores": [-0.006147301290184259, -0.022286154329776764, -0.11028892546892166, -0.11847401410341263, -0.13023824989795685, -0.15074831247329712, -0.1518961489200592, -0.161603182554245, -0.16505873203277588, -0.16751523315906525], "norm_scores": [0.1117074117064476, 0.10991904884576797, 0.10065928846597672, 0.0998387411236763, 0.09867110103368759, 0.09666796773672104, 0.09655707329511642, 0.09562432020902634, 0.09529444575309753, 0.09506064653396606]}}
{"id": "WebQTest-519", "question": "what timezone is nashville tn", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.time_Zones"], ["location.location.time_zonessymbol"], ["locations.location.time_zones"], ["location.location.time_zoneslanguage"], ["location.location.time_zonescommon"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is nashville tn [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.location.time_zonessymbol </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> location.location.time_zoneslanguage </PATH>", "<PATH> location.location.time_zonescommon </PATH>"], "scores": [-8.672210242366418e-06, -0.17945505678653717, -0.18942494690418243, -0.19016283750534058, -0.20776644349098206, -0.20961952209472656, -0.23152929544448853, -0.23583173751831055, -0.2399291694164276, -0.24382665753364563], "norm_scores": [0.1209646463394165, 0.10109411925077438, 0.10009123384952545, 0.10001739859580994, 0.09827214479446411, 0.09809020906686783, 0.09596443921327591, 0.09555245190858841, 0.09516172856092453, 0.09479155391454697]}}
{"id": "WebQTest-521", "question": "who was anakin skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.gender", "fictional_universe.gender.gender"], ["fictional_universe.fictional_character.occupation", "people.person.profession"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was anakin skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.gender.gender </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.004075220320373774, -0.03989866375923157, -0.04815007373690605, -0.05329909175634384, -0.06111779436469078, -0.06698401272296906, -0.06917094439268112, -0.07092775404453278, -0.08219602704048157, -0.08530466258525848], "norm_scores": [0.1055254265666008, 0.10181204974651337, 0.10097542405128479, 0.10045682638883591, 0.09967444837093353, 0.09909145534038544, 0.09887497872114182, 0.09870143234729767, 0.09759548306465149, 0.09729256480932236]}}
{"id": "WebQTest-522", "question": "where did adolf hitler die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people"], ["deceased_person.place_of_death"], ["people.place_of_death.people", "location.location.containedby"], ["base.activism.activist.area_of_death"], ["people.place_of_death.people", "people.place_of_death.location"], ["people.place_of_interment.interred_here"], ["location.deceased_person.place_of_death"], ["people.person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did adolf hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> location.location.containedby </PATH>", "<PATH> base.activism.activist.area_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_death </PATH>"], "scores": [-0.0005309276166372001, -0.08337670564651489, -0.1028074249625206, -0.10621676594018936, -0.11217742413282394, -0.11362646520137787, -0.11654039472341537, -0.11802595853805542, -0.1221596747636795, -0.12356999516487122], "norm_scores": [0.11037863790988922, 0.10160277038812637, 0.09964761883020401, 0.09930846095085144, 0.09871827811002731, 0.09857534617185593, 0.09828852117061615, 0.098142609000206, 0.09773775190114975, 0.09760000556707382]}}
{"id": "WebQTest-523", "question": "what is the largest nation in europe", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"], ["base.locations.countries.continent"], ["location.location.partially_contains"], ["location.location.primarily_containedby"], ["location.location.contains"], ["geography.river.basin_countries"], ["common.topic.notable_types"], ["location.location.primacy_containedby"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the largest nation in europe [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.primacy_containedby </PATH>"], "scores": [-0.012463584542274475, -0.024037770926952362, -0.04095802828669548, -0.05793624371290207, -0.06086661294102669, -0.08013849705457687, -0.10559111833572388, -0.10658086091279984, -0.13497218489646912, -0.1434161514043808], "norm_scores": [0.10653631389141083, 0.1053103506565094, 0.10354346036911011, 0.10180032253265381, 0.1015024408698082, 0.09956502914428711, 0.09706281125545502, 0.09696679562330246, 0.09425249695777893, 0.09345997869968414]}}
{"id": "WebQTest-524", "question": "where was the first gulf war fought", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["film.film_genre.films_in_this_genre"], ["geography.river.basin_countries"], ["military.military_conflict.casualties"], ["book.written_work.subjects"], ["language.human_language.countries_spoken_in"], ["military.military_combatant_group.conflict"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first gulf war fought [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> film.film_genre.films_in_this_genre </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> military.military_conflict.casualties </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> military.military_combatant_group.conflict </PATH>"], "scores": [-0.00449568685144186, -0.03209074214100838, -0.11478894203901291, -0.1381574124097824, -0.14319294691085815, -0.1485614776611328, -0.16249623894691467, -0.1687139868736267, -0.1711491495370865, -0.1744985729455948], "norm_scores": [0.11271258443593979, 0.10964479297399521, 0.10094217211008072, 0.09861066192388535, 0.09811534732580185, 0.09759002178907394, 0.09623955935239792, 0.09564302861690521, 0.09541040658950806, 0.09509136527776718]}}
{"id": "WebQTest-525", "question": "who are the senators of virginia 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.election.district", "government.election.winner"], ["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of virginia 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.01639503985643387, -0.02479889988899231, -0.024987315759062767, -0.026838434860110283, -0.03186626732349396, -0.03752480074763298, -0.03917461261153221, -0.04202717915177345, -0.04528013989329338, -0.0457158200442791], "norm_scores": [0.10171665996313095, 0.1008654311299324, 0.10084642469882965, 0.10065992176532745, 0.10015509277582169, 0.09958995878696442, 0.0994257926940918, 0.09914257377386093, 0.09882058948278427, 0.09877754747867584]}}
{"id": "WebQTest-526", "question": "what band was george clinton in", "prediction": [["music.artist.genre", "music.artist.genre"], ["music.artist.label", "music.artist.label"], ["music.recording.artist"], ["music.recording.artist", "music.recording.artist"], ["music.group_membership.member", "music.musical_group.member"], ["broadcast.artist.content", "broadcast.artist.content"], ["music.group_membership.member", "music.group_membership.group"], ["music.artist.origin", "music.artist.origin"], ["music.musical_group.member", "music.group_membership.group"], ["music.recording.artist", "music.artist.album"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["music.recording.featured_artists", "music.artist.album"], ["broadcast.artist.content", "broadcast.artist.content"], ["base.saturdaynightlive.snl_episode.musical_guest", "base.saturdaynightlive.snl_musical_guest.episodes_as_musical_guest"], ["music.recording.featured_artists", "music.recording.featured_artists"], ["music.producer.releases_produced", "music.album.artist"], ["music.recording.producer", "music.album.artist"], ["music.composer.compositions", "music.artist.track"], ["music.recording.featured_artists", "music.featured_artist.recordings"], ["music.recording.artist", "music.recording.artist"], ["music.composition.composer", "music.artist.track"], ["music.composer.compositions", "music.recording.artist"], ["music.group_membership.member", "music.group_membership.group"], ["music.group_member.membership", "music.musical_group.member"], ["music.composition.composer", "music.recording.artist"], ["music.producer.tracks_produced", "music.recording.artist"], ["music.featured_artist.recordings", "music.recording.artist"], ["music.artist.genre", "music.artist.genre"], ["music.artist.genre", "music.genre.artists"], ["music.recording.artist", "music.album.artist"], ["music.group_membership.member", "music.musical_group.member"], ["music.album.artist", "music.artist.album"], ["music.album.artist", "music.album.artist"], ["music.recording.producer", "music.recording.artist"], ["music.record_label.artist", "music.record_label.artist"], ["music.artist.origin", "music.artist.origin"], ["people.person.nationality", "music.artist.origin"], ["music.artist.album", "music.artist.album"], ["music.featured_artist.recordings", "music.recording.featured_artists"], ["music.artist.album", "music.album.artist"], ["music.artist.label", "music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what band was george clinton in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>", "<PATH> music.group_membership.member <SEP> music.musical_group.member </PATH>", "<PATH> broadcast.artist.content <SEP> broadcast.artist.content </PATH>", "<PATH> music.group_membership.member <SEP> music.group_membership.group </PATH>", "<PATH> music.artist.origin <SEP> music.artist.origin </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.recording.artist <SEP> music.artist.album </PATH>"], "scores": [-0.014946434646844864, -0.03638236224651337, -0.0517699271440506, -0.05198259651660919, -0.05865045264363289, -0.0625096783041954, -0.06341588497161865, -0.06378240138292313, -0.06569778919219971, -0.07286661863327026], "norm_scores": [0.10398976504802704, 0.1017843708395958, 0.1002301424741745, 0.1002088338136673, 0.09954287856817245, 0.09915945678949356, 0.09906964004039764, 0.09903333336114883, 0.09884383529424667, 0.098137766122818]}}
{"id": "WebQTest-527", "question": "what country borders slovakia", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_contains", "location.location.partially_contains"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_contains", "location.location.partially_containedby"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["base.locations.countries.continent", "base.locations.continents.countries_within"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.location.containedby", "location.location.contains"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.location.partially_contains", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.events", "location.location.events"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["base.datedlocationtest.dated_location_break_up.new_locations", "base.datedlocationtest.dated_location_break_up.new_locations"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["location.location.containedby", "location.location.containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["finance.currency.countries_formerly_used", "location.country.currency_formerly_used"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["location.location.partially_containedby", "location.location.containedby"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.country.currency_used", "location.country.currency_used"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "geography.river.origin"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.location.time_zones", "location.location.time_zones"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["organization.organization.founders", "organization.organization.founders"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country borders slovakia [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03652501478791237, -0.03940131142735481, -0.04025297239422798, -0.04125981032848358, -0.04172255098819733, -0.04258439317345619, -0.04328575357794762, -0.04533348232507706, -0.04554518684744835, -0.04944952204823494], "norm_scores": [0.10060230642557144, 0.10031335800886154, 0.1002279594540596, 0.10012710094451904, 0.10008077323436737, 0.09999456256628036, 0.09992445260286331, 0.09972003847360611, 0.09969893097877502, 0.09931043535470963]}}
{"id": "WebQTest-528", "question": "what was james garfield most known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["people.profession.people_with_this_profession"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_office_category.officeholders"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_position_held.office_holder_position_or_title"], ["government.political_appointer.appointees", "government.government_position_held.office_holder"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was james garfield most known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_category.officeholders </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_holder_position_or_title </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.0011546500027179718, -0.0588049441576004, -0.08067813515663147, -0.08500794321298599, -0.08641441911458969, -0.08821365237236023, -0.09266842901706696, -0.09357757121324539, -0.10799509286880493, -0.11651725322008133], "norm_scores": [0.10827222466468811, 0.10220681875944138, 0.09999550133943558, 0.09956347197294235, 0.09942354261875153, 0.09924481064081192, 0.09880367666482925, 0.09871389716863632, 0.09730090200901031, 0.09647520631551743]}}
{"id": "WebQTest-529", "question": "where did paula deen go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.subsidiary_or_constituent_schools"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["location.location.people_born_here", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did paula deen go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.017919642850756645, -0.018461842089891434, -0.027402635663747787, -0.03133179619908333, -0.03893261030316353, -0.057667169719934464, -0.12031897157430649, -0.12370669841766357, -0.12929482758045197, -0.1311800628900528], "norm_scores": [0.10518850386142731, 0.10513148456811905, 0.10419571399688721, 0.10378710925579071, 0.10300124436616898, 0.10108952224254608, 0.09495040029287338, 0.09462928026914597, 0.09410195797681808, 0.09392471611499786]}}
{"id": "WebQTest-530", "question": "where does robert irvine come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["influence.influence_node.influenced", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robert irvine come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.0040850467048585415, -0.0377531424164772, -0.049157727509737015, -0.10791585594415665, -0.1248350590467453, -0.13915979862213135, -0.13962018489837646, -0.14121516048908234, -0.14204780757427216, -0.1458001434803009], "norm_scores": [0.11027535051107407, 0.10662440210580826, 0.10541529953479767, 0.09939975291490555, 0.09773213416337967, 0.09634213149547577, 0.09629778563976288, 0.09614431858062744, 0.09606429189443588, 0.09570450335741043]}}
{"id": "WebQTest-531", "question": "what style of music does michael jackson perform", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.artist.album", "music.album.genre"], ["book.author.works_written"], ["influence.influence_node.influenced"], ["book.written_work.author"], ["influence.influence_node.influenced_by"], ["music.album.genre"], ["music.music_artist.genre"], ["music.composer.compositions"]], "ground_paths": [["music.artist.genre"], ["music.genre.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does michael jackson perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.artist.album <SEP> music.album.genre </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> music.album.genre </PATH>", "<PATH> music.music_artist.genre </PATH>", "<PATH> music.composer.compositions </PATH>"], "scores": [-0.0008912067278288305, -0.06025386601686478, -0.1314188838005066, -0.1329915076494217, -0.1362466812133789, -0.13781622052192688, -0.13785824179649353, -0.13981777429580688, -0.14106358587741852, -0.14168669283390045], "norm_scores": [0.1120842769742012, 0.10562430322170258, 0.09836877137422562, 0.09821420162916183, 0.09789501130580902, 0.09774148464202881, 0.0977373793721199, 0.0975460484623909, 0.09742459654808044, 0.0973639115691185]}}
{"id": "WebQTest-532", "question": "what does david beckham play", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.position"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_position.sport"]], "ground_paths": [["soccer.football_player.position_s", "sports.sport.positions"], ["symbols.namesake.named_after", "cvg.computer_videogame.cvg_genre"], ["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"], ["book.written_work.subjects", "book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does david beckham play [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.position </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_position.sport </PATH>"], "scores": [-0.02771911956369877, -0.0303681418299675, -0.032363634556531906, -0.03507319837808609, -0.037710703909397125, -0.04227909445762634, -0.0432899072766304, -0.04829639196395874, -0.048700541257858276, -0.05280987545847893], "norm_scores": [0.10121827572584152, 0.10095050185918808, 0.10074926167726517, 0.10047663748264313, 0.1002119854092598, 0.09975522011518478, 0.0996544361114502, 0.09915675967931747, 0.09911669790744781, 0.09871023148298264]}}
{"id": "WebQTest-533", "question": "what does julian lennon do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["music.artist.genre"], ["music.artist.album"], ["music.recording.artist"], ["common.topic.notable_types", "type.type.properties"], ["music.artist.origin"], ["music.artist.track"], ["music.producer.releases_produced"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does julian lennon do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.properties </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.producer.releases_produced </PATH>"], "scores": [-0.0010342829627916217, -0.06243225187063217, -0.10140514373779297, -0.11653704196214676, -0.11952169984579086, -0.12889644503593445, -0.13048197329044342, -0.14594851434230804, -0.14619018137454987, -0.1462380737066269], "norm_scores": [0.11138937622308731, 0.10475601255893707, 0.10075189918279648, 0.0992388129234314, 0.09894305467605591, 0.09801982343196869, 0.0978645384311676, 0.09636256098747253, 0.09633927047252655, 0.09633465856313705]}}
{"id": "WebQTest-534", "question": "what is the milwaukee brewers mascot", "prediction": [["sports.mascot.team"], ["sports.sports_team.team_mascot"], ["sports.professional_sports_team.owner_s", "sports.sports_team.team_mascot"], ["sports.defunct_sports_team.later_known_as"], ["sports.mascot.team", "sports.mascot.team"], ["sports.sports_team.team_mascot", "sports.sports_team.team_mascot"], ["sports.sports_team.team_mascot", "sports.mascot.team"], ["sports.mascot.team", "sports.sports_team.team_mascot"], ["sports.professional_sports_team.owner_s", "sports.mascot.team"]], "ground_paths": [["sports.sports_team.team_mascot"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers mascot [/INST]", "raw_output": {"paths": ["<PATH> sports.mascot.team </PATH>", "<PATH> sports.sports_team.team_mascot </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.team_mascot </PATH>", "<PATH> sports.defunct_sports_team.later_known_as </PATH>", "<PATH> sports.mascot.team <SEP> sports.mascot.team </PATH>", "<PATH> sports.sports_team.team_mascot <SEP> sports.sports_team.team_mascot </PATH>", "<PATH> sports.sports_team.team_mascot <SEP> sports.mascot.team </PATH>", "<PATH> sports.mascot.team <SEP> sports.sports_team.team_mascot </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.mascot.team </PATH>", "<PATH> sports.mascot.team <PATH>"], "scores": [-0.0011793284211307764, -0.04541978985071182, -0.1368146538734436, -0.14102394878864288, -0.1412159949541092, -0.14162883162498474, -0.14417879283428192, -0.14466607570648193, -0.15436355769634247, -0.16956660151481628], "norm_scores": [0.11269183456897736, 0.1078149750828743, 0.0983981117606163, 0.09798480570316315, 0.09796598553657532, 0.09792554378509521, 0.09767615795135498, 0.09762857854366302, 0.09668640047311783, 0.09522759169340134]}}
{"id": "WebQTest-535", "question": "what is the second biggest state in the united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.containedby", "location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.partially_contains"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_parent"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"]], "ground_paths": [["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the second biggest state in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.adjoining_relationship.adjoins </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.location.adjoin_s </PATH>"], "scores": [-0.0016563396202400327, -0.056987322866916656, -0.07482700794935226, -0.07651960104703903, -0.07794521749019623, -0.08080147206783295, -0.09431838989257812, -0.09639175981283188, -0.09948720782995224, -0.1014750674366951], "norm_scores": [0.10767904669046402, 0.10188289731740952, 0.10008145123720169, 0.09991219639778137, 0.09976986050605774, 0.09948530048131943, 0.0981496199965477, 0.09794632345438004, 0.09764360636472702, 0.09744969755411148]}}
{"id": "WebQTest-537", "question": "what has charlie hunnam been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has charlie hunnam been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> people.person.nationality <SEP> media_common.netflix_title.netflix_genres </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>"], "scores": [-0.022704267874360085, -0.026484645903110504, -0.031606096774339676, -0.033106133341789246, -0.036663804203271866, -0.04036012291908264, -0.06016037240624428, -0.08192647248506546, -0.0910416692495346, -0.09440057724714279], "norm_scores": [0.10292146354913712, 0.10253311693668365, 0.10200934112071991, 0.10185643285512924, 0.10149470716714859, 0.1011202409863472, 0.09913773089647293, 0.09700320661067963, 0.09612302482128143, 0.09580068290233612]}}
{"id": "WebQTest-538", "question": "what are all the movies taylor swift has been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["people.person.languages", "film.film.language"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["music.album.genre", "media_common.netflix_title.netflix_genres"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the movies taylor swift has been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.026942340657114983, -0.03416350856423378, -0.037724241614341736, -0.038343336433172226, -0.038644466549158096, -0.04089735448360443, -0.04361354932188988, -0.04615084081888199, -0.04799295589327812, -0.048407841473817825], "norm_scores": [0.10134148597717285, 0.10061232000589371, 0.10025470703840256, 0.10019265860319138, 0.10016248375177383, 0.0999370813369751, 0.09966599941253662, 0.0994134470820427, 0.09923048317432404, 0.09918932616710663]}}
{"id": "WebQTest-540", "question": "what college did brett favre", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["sports.education.student", "education.education.institution"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["common.topic.notable_types"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did brett favre [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> sports.education.student <SEP> education.education.institution </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.016496723517775536, -0.019275233149528503, -0.02804572694003582, -0.03284134343266487, -0.04810311645269394, -0.07261863350868225, -0.07517501711845398, -0.08170474320650101, -0.093245729804039, -0.11188839375972748], "norm_scores": [0.1041790023446083, 0.10388994216918945, 0.10298275202512741, 0.10249007493257523, 0.10093776881694794, 0.0984933152794838, 0.09824185073375702, 0.09760244190692902, 0.09648249298334122, 0.09470045566558838]}}
{"id": "WebQTest-542", "question": "where did alex chilton die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"], ["location.deceased_person.place_of_death"], ["music.artist.origin"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.place_of_death.people", "people.place_of_death.location"], ["people.person.place_of_birth"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alex chilton die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.0002976026735268533, -0.09530451148748398, -0.10018201172351837, -0.11189192533493042, -0.1167801097035408, -0.12322236597537994, -0.12672249972820282, -0.12757477164268494, -0.12801003456115723, -0.13029752671718597], "norm_scores": [0.111073799431324, 0.10100681334733963, 0.10051534324884415, 0.09934519231319427, 0.09886075556278229, 0.0982259139418602, 0.0978827103972435, 0.09779932349920273, 0.0977567657828331, 0.09753339737653732]}}
{"id": "WebQTest-543", "question": "what college did martin luther king jr go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did martin luther king jr go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>"], "scores": [-0.01450392510741949, -0.017995323985815048, -0.02745359018445015, -0.032200466841459274, -0.0620749406516552, -0.07351070642471313, -0.08985556662082672, -0.09274321794509888, -0.09867963194847107, -0.1060250997543335], "norm_scores": [0.10475237667560577, 0.10438728332519531, 0.10340461134910583, 0.10291492938995361, 0.09988587349653244, 0.09875011444091797, 0.09714917093515396, 0.09686903655529022, 0.09629568457603455, 0.09559094905853271]}}
{"id": "WebQTest-545", "question": "what kind of cancer did farrah fawcett die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.people", "people.deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["media_common.deceased_person.cause_of_death"], ["film.actor.film", "film.performance.character", "people.deceased_person.cause_of_death"], ["people.cause_of_death.people", "medicine.disease.notable_people_with_this_condition"], ["film.film.produced_by", "film.film.subjects"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did farrah fawcett die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.people <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> media_common.deceased_person.cause_of_death </PATH>", "<PATH> film.actor.film <SEP> film.performance.character <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> film.film.produced_by <SEP> film.film.subjects </PATH>"], "scores": [-0.008079377003014088, -0.018457965925335884, -0.043002545833587646, -0.06671156734228134, -0.07920727133750916, -0.10825639218091965, -0.10868654400110245, -0.10932746529579163, -0.11772259324789047, -0.11983907222747803], "norm_scores": [0.10714854300022125, 0.10604224354028702, 0.10347115993499756, 0.10104682296514511, 0.09979201853275299, 0.09693485498428345, 0.09689316153526306, 0.09683108329772949, 0.09602157771587372, 0.09581856429576874]}}
{"id": "WebQTest-546", "question": "who is michael j fox 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.spouse_s", "people.marriage.spouse"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.parents", "people.person.parents"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.021135590970516205, -0.03334684669971466, -0.037098757922649384, -0.03856155648827553, -0.04237908869981766, -0.04497510939836502, -0.04534856602549553, -0.04617106541991234, -0.048501886427402496, -0.05140814930200577], "norm_scores": [0.10199176520109177, 0.10075388848781586, 0.10037657618522644, 0.10022985190153122, 0.0998479574918747, 0.09958907961845398, 0.09955189377069473, 0.09947004914283752, 0.09923847019672394, 0.09895047545433044]}}
{"id": "WebQTest-547", "question": "what money do i use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries", "location.country.currency_used"], ["government.form_of_government.countries"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["common.topic.notable_types"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do i use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005965732503682375, -0.024182312190532684, -0.11681359261274338, -0.12778714299201965, -0.12978911399841309, -0.1438630223274231, -0.14727503061294556, -0.15762746334075928, -0.16225604712963104, -0.16480876505374908], "norm_scores": [0.11169447749853134, 0.10967820137739182, 0.09997492283582687, 0.09888383746147156, 0.09868607670068741, 0.09730691462755203, 0.09697546064853668, 0.09597671031951904, 0.09553349763154984, 0.0952899307012558]}}
{"id": "WebQTest-548", "question": "what guitar did joe walsh play", "prediction": [["music.guitarist.guitars_played"], ["music.group_member.instruments_played"], ["music.guitar.guitarists"], ["music.instrument.family"], ["music.musical_group.member", "music.group_membership.group"], ["music.music_group.member", "music.group_membership.group"], ["music.instrument.guitarists"], ["music.artist.guitars_played"], ["music.producer.releases_produced"], ["music.recording.artist"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did joe walsh play [/INST]", "raw_output": {"paths": ["<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.music_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.instrument.guitarists </PATH>", "<PATH> music.artist.guitars_played </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.018841665238142014, -0.020550085231661797, -0.03183542191982269, -0.04525742679834366, -0.07481595128774643, -0.09788962453603745, -0.10263313353061676, -0.10451176017522812, -0.10727404057979584, -0.10769998282194138], "norm_scores": [0.10529937595129013, 0.10511963069438934, 0.10393999516963959, 0.10255422443151474, 0.0995672419667244, 0.09729615598917007, 0.09683571755886078, 0.09665398299694061, 0.0963873565196991, 0.09634631127119064]}}
{"id": "WebQTest-549", "question": "what are the official languages of the eu", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "language.human_language.countries_spoken_in"], ["organization.organization.geographic_scope", "location.country.languages_spoken"], ["organization.organization.geographic_scope", "location.country.official_language"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization.founders"], ["organization.organization.geographic_scope", "language.human_language.region"], ["organization.organization.geographic_scope", "language.human_language.main_country"], ["government.governmental_jurisdiction.governing_officials", "language.human_language.countries_spoken_in"]], "ground_paths": [["organization.organization.founders", "location.country.languages_spoken"], ["organization.organization.geographic_scope", "language.human_language.region"], ["organization.organization.founders", "location.country.official_language"], ["organization.organization_founder.organizations_founded", "location.country.official_language"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.country.languages_spoken </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.country.official_language </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_membership.member </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.geographic_scope <SEP> language.human_language.region </PATH>", "<PATH> organization.organization.geographic_scope <SEP> language.human_language.main_country </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.024381807073950768, -0.02979067526757717, -0.03762112557888031, -0.03845440596342087, -0.04076476767659187, -0.04196919873356819, -0.04422898218035698, -0.04726304113864899, -0.05037509649991989, -0.05388478562235832], "norm_scores": [0.1016591414809227, 0.10111076384782791, 0.10032211244106293, 0.10023854672908783, 0.10000722855329514, 0.09988685697317123, 0.09966138005256653, 0.09935946017503738, 0.09905073046684265, 0.09870370477437973]}}
{"id": "WebQTest-550", "question": "what is ryan seacrest radio show", "prediction": [["broadcast.content.producer"], ["media_common.netflix_title.netflix_genres"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["internet.website.owner"], ["tv.tv_program.program_creator"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["broadcast.producer.content", "broadcast.content.producer"], ["broadcast.artist.content", "broadcast.artist.content"], ["internet.website_owner.sites", "internet.website_owner.sites"]], "ground_paths": [["tv.tv_program_creator.programs_created"], ["broadcast.artist.content"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ryan seacrest radio show [/INST]", "raw_output": {"paths": ["<PATH> broadcast.content.producer </PATH>", "<PATH> media_common.netflix_title.netflix_genres </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_program.regular_cast </PATH>", "<PATH> internet.website.owner </PATH>", "<PATH> tv.tv_program.program_creator </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> broadcast.producer.content <SEP> broadcast.content.producer </PATH>", "<PATH> broadcast.artist.content <SEP> broadcast.artist.content </PATH>", "<PATH> internet.website_owner.sites <SEP> internet.website_owner.sites </PATH>"], "scores": [-0.03585498407483101, -0.037766411900520325, -0.05236576125025749, -0.054450906813144684, -0.05561511591076851, -0.056295592337846756, -0.05723949149250984, -0.05783228576183319, -0.05988612771034241, -0.06467773765325546], "norm_scores": [0.10174553841352463, 0.10155124217271805, 0.10007943212985992, 0.09987097233533859, 0.09975476562976837, 0.09968690574169159, 0.09959285706281662, 0.09953384101390839, 0.0993296205997467, 0.09885480999946594]}}
{"id": "WebQTest-552", "question": "what language do most italians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["base.rosetta.languoid.languoid_class"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "location.country.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most italians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>"], "scores": [-0.016650451347231865, -0.020413734018802643, -0.02787303738296032, -0.05346420407295227, -0.12125278264284134, -0.13116060197353363, -0.13999217748641968, -0.1471996009349823, -0.15139077603816986, -0.15182557702064514], "norm_scores": [0.10810220241546631, 0.10769615322351456, 0.1068957969546318, 0.10419491678476334, 0.09736577421426773, 0.09640585631132126, 0.09555818140506744, 0.09487193077802658, 0.0944751426577568, 0.09443407505750656]}}
{"id": "WebQTest-554", "question": "what money do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country"], ["finance.currency.countries_used", "location.country.currency_used"], ["location.country.form_of_government"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used <PATH>", "</PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.004435276612639427, -0.029846008867025375, -0.13879695534706116, -0.15117697417736053, -0.17168450355529785, -0.1732531040906906, -0.1905677765607834, -0.19323736429214478, -0.19426345825195312, -0.19999460875988007], "norm_scores": [0.1147981584072113, 0.11191780120134354, 0.10036502033472061, 0.09913016110658646, 0.09711794555187225, 0.0969657227396965, 0.09530124813318253, 0.09504717588424683, 0.09494969248771667, 0.0944070890545845]}}
{"id": "WebQTest-555", "question": "what did drew barrymore go to rehab for", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.person.profession", "people.profession.specializations"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["people.person.profession", "people.profession.specializations_of"], ["people.person.profession", "people.person.profession"], ["people.person.profession", "people.profession.specialization_of"], ["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"], ["medicine.notable_person_with_medical_condition.condition"], ["base.popstra.rehab_stay.patient", "base.popstra.rehab_stay.substance_abused"], ["people.person.profession", "people.profession.part_of_professional_field"]], "ground_paths": [["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did drew barrymore go to rehab for [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> base.popstra.celebrity.substance_abuse <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations_of </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.profession <SEP> people.profession.specialization_of </PATH>", "<PATH> base.popstra.substance_abuse.abuser <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> base.popstra.rehab_stay.patient <SEP> base.popstra.rehab_stay.substance_abused </PATH>", "<PATH> people.person.profession <SEP> people.profession.part_of_professional_field </PATH>"], "scores": [-0.021607058122754097, -0.025536121800541878, -0.0375504270195961, -0.041107114404439926, -0.04751451686024666, -0.053376179188489914, -0.05347617343068123, -0.053856898099184036, -0.05951329320669174, -0.06460156291723251], "norm_scores": [0.10244091600179672, 0.10203920304775238, 0.1008206158876419, 0.1004626601934433, 0.09982100874185562, 0.09923761337995529, 0.09922768920660019, 0.09918992221355438, 0.09863044321537018, 0.09812986105680466]}}
{"id": "WebQTest-556", "question": "when is saint george day celebrated", "prediction": [["time.recurring_event.current_frequency"], ["time.day_of_year.holidays"], ["time.holiday.featured_in_religions"], ["time.recurring_event.instances"], ["time.event.included_in_event"], ["time.recurring_event.holidays"], ["time.day_of_the_year.holidays"], ["time.day_of_year.featured_in_religions"], ["time.event.includes_event"], ["time.day_of_honor.honored_for"]], "ground_paths": [["time.holiday.day_of_year"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is saint george day celebrated [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.day_of_year.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> time.recurring_event.holidays </PATH>", "<PATH> time.day_of_the_year.holidays </PATH>", "<PATH> time.day_of_year.featured_in_religions </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.day_of_honor.honored_for </PATH>"], "scores": [-0.01547024305909872, -0.026144595816731453, -0.03402148559689522, -0.05944770202040672, -0.08139737695455551, -0.08349618315696716, -0.08630792051553726, -0.08790646493434906, -0.08927800506353378, -0.10418100655078888], "norm_scores": [0.10521761327981949, 0.10410045832395554, 0.10328368842601776, 0.10069068521261215, 0.09850463271141052, 0.0982981026172638, 0.09802210330963135, 0.09786553680896759, 0.09773140400648117, 0.09628571569919586]}}
{"id": "WebQTest-557", "question": "what government did the roman empire have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["fictional_universe.fictional_setting.languages", "location.country.form_of_government"], ["fictional_universe.fictional_setting.languages", "government.form_of_government.countries"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation", "government.form_of_government.countries"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation", "location.country.form_of_government"], ["fictional_universe.fictional_language.where_spoken", "location.country.form_of_government"], ["fictional_universe.fictional_setting.languages", "language.human_language.main_country"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation"], ["government.form_of_government.countries", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government did the roman empire have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> location.country.form_of_government </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> government.form_of_government.countries </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation <SEP> government.form_of_government.countries </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation <SEP> location.country.form_of_government </PATH>", "<PATH> fictional_universe.fictional_language.where_spoken <SEP> location.country.form_of_government </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.main_country </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.005005497485399246, -0.02506214939057827, -0.08281966298818588, -0.0878809317946434, -0.09189717471599579, -0.10164069384336472, -0.10688874870538712, -0.10718507319688797, -0.10968052595853806, -0.11140284687280655], "norm_scores": [0.10803691297769547, 0.1058916375041008, 0.09994886815547943, 0.09944427758455276, 0.09904568642377853, 0.09808532148599625, 0.09757191687822342, 0.09754300862550735, 0.09729988873004913, 0.09713245183229446]}}
{"id": "WebQTest-559", "question": "what instrument did robin gibb play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.group_membership.group", "music.group_membership.role"], ["music.musical_group.member", "music.group_membership.role"], ["music.instrument.instrumentalists"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_membership.member", "music.group_membership.role"], ["music.track_contribution.role", "music.track_contribution.role"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.artist.instruments_played"]], "ground_paths": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_member.instruments_played"], ["music.group_membership.member", "music.performance_role.regular_performances"], ["music.group_membership.member", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument did robin gibb play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.instrument.instrumentalists </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_membership.member <SEP> music.group_membership.role </PATH>", "<PATH> music.track_contribution.role <SEP> music.track_contribution.role </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> music.artist.instruments_played </PATH>"], "scores": [-0.0008504593861289322, -0.07527504861354828, -0.081051766872406, -0.08616405725479126, -0.09769986569881439, -0.10199018567800522, -0.10769997537136078, -0.11063622683286667, -0.11239947378635406, -0.1127772405743599], "norm_scores": [0.10912055522203445, 0.1012941524386406, 0.10071069747209549, 0.10019714385271072, 0.0990479364991188, 0.09862389415502548, 0.09806238114833832, 0.09777486324310303, 0.09760262072086334, 0.09756575524806976]}}
{"id": "WebQTest-560", "question": "who voiced darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_guest_role.special_performance_type", "tv.tv_actor.guest_roles"], ["film.performance.character", "film.actor.film"], ["tv.tv_guest_role.special_performance_type", "tv.tv_guest_role.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.special_performance_type", "film.actor.film"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_guest_role.special_performance_type <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_guest_role.special_performance_type <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> tv.tv_guest_role.character <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.025654975324869156, -0.02849588356912136, -0.030708830803632736, -0.03692813217639923, -0.04084727540612221, -0.04632439464330673, -0.04702431708574295, -0.047190360724925995, -0.047408577054739, -0.06622795015573502], "norm_scores": [0.10160897672176361, 0.10132072865962982, 0.1010967567563057, 0.10046995431184769, 0.10007696598768234, 0.09953033924102783, 0.09946069866418839, 0.099444180727005, 0.09942248463630676, 0.09756890684366226]}}
{"id": "WebQTest-561", "question": "when was abe lincoln president", "prediction": [["government.us_president.vice_president"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "location.location.events"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "location.location.containedby"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "location.location.containedby", "time.event.locations"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["base.culturalevent.event.entity_involved"], ["government.us_vice_president.to_president"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "location.location.events"]], "ground_paths": [["base.inaugurations.inauguration.president"], ["base.inaugurations.inauguration_speaker.inauguration"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was abe lincoln president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> location.location.events </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> location.location.containedby </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> location.location.containedby <SEP> time.event.locations </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location.events </PATH>"], "scores": [-0.025845592841506004, -0.027544142678380013, -0.03979244828224182, -0.04126685857772827, -0.045221518725156784, -0.045734062790870667, -0.04796012118458748, -0.04815243184566498, -0.050403546541929245, -0.053891029208898544], "norm_scores": [0.10168365389108658, 0.10151109099388123, 0.10027533769607544, 0.10012759268283844, 0.0997323989868164, 0.0996812954545021, 0.09945964813232422, 0.09944052994251251, 0.09921692311763763, 0.09887150675058365]}}
{"id": "WebQTest-562", "question": "where are the texas rangers playing", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.professional_sports_team.owner_s", "sports.sports_team.arena_stadium"], ["sports.professional_sports_team.owner_s", "sports.sports_facility.teams"], ["sports.professional_sports_team.owner_s"], ["sports.sports_team.venue", "sports.sports_facility.home_venue_for"], ["sports.sports_venue_relationship.team", "sports.sports_facility.teams"], ["sports.sports_venue_relationship.team", "sports.sports_team.arena_stadium"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the texas rangers playing [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.professional_sports_team.owner_s </PATH>", "<PATH> sports.sports_team.venue <SEP> sports.sports_facility.home_venue_for </PATH>", "<PATH> sports.sports_venue_relationship.team <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_venue_relationship.team <SEP> sports.sports_team.arena_stadium </PATH>"], "scores": [-0.0011992906220257282, -0.05347785726189613, -0.09053738415241241, -0.0966145470738411, -0.1007552221417427, -0.1044594869017601, -0.12028634548187256, -0.12935999035835266, -0.13194935023784637, -0.13215725123882294], "norm_scores": [0.1098676323890686, 0.10427145659923553, 0.10047794133424759, 0.09986916929483414, 0.09945650398731232, 0.09908877313137054, 0.09753285348415375, 0.09665187448263168, 0.09640192985534668, 0.09638188779354095]}}
{"id": "WebQTest-564", "question": "what is arkansas state capitol", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.state"], ["location.us_state.capital"], ["government.governmental_jurisdiction.government_bodies"], ["base.aareas.schema.administrative_area.capital"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.capital"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.district_represented"], ["government.government_position_held.jurisdiction_of_office", "government.governmental_jurisdiction.governing_officials"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is arkansas state capitol [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.us_state.capital </PATH>", "<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.capital </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.governmental_jurisdiction.governing_officials </PATH>"], "scores": [-0.020073620602488518, -0.024554703384637833, -0.02791418507695198, -0.04781828820705414, -0.04989473149180412, -0.05007744953036308, -0.05656204745173454, -0.06130799278616905, -0.06335636973381042, -0.07210006564855576], "norm_scores": [0.10275238752365112, 0.1022929772734642, 0.10194990783929825, 0.09994073957204819, 0.09973343461751938, 0.0997152104973793, 0.09907069802284241, 0.09860162436962128, 0.09839985519647598, 0.09754322469234467]}}
{"id": "WebQTest-567", "question": "what is the zip code for nogales az", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.city.postal_codes"], ["location.us_county.hud_county_place"], ["location.Citytown.postal_codes"], ["location.citytown.postal_codes", "location.location.containedby"], ["location.us_county.county_seat"], ["location.citytown.postal_codes", "location.postal_code.country"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for nogales az [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.city.postal_codes </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_codes <SEP> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.citytown.postal_codes <SEP> location.postal_code.country </PATH>"], "scores": [-0.00034179294016212225, -0.08188524097204208, -0.11204617470502853, -0.12854094803333282, -0.1641061007976532, -0.17108041048049927, -0.17210569977760315, -0.17985054850578308, -0.1848180741071701, -0.18508167564868927], "norm_scores": [0.11456876993179321, 0.10559719055891037, 0.10245984047651291, 0.10078363865613937, 0.09726224839687347, 0.09658627212047577, 0.09648729860782623, 0.09574289619922638, 0.09526847302913666, 0.09524336457252502]}}
{"id": "WebQTest-568", "question": "when did michael vick start playing for the eagles", "prediction": [["american_football.football_player.rushing", "american_football.player_rushing_statistics.season"], ["american_football.football_player.games", "american_football.player_game_statistics.season"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.season"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.season"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.season"], ["american_football.football_player.passing", "american_football.player_passing_statistics.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.position_s", "sports.sports_league_season.position", "sports.sports_league_season.position"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael vick start playing for the eagles [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.season </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.season </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.season </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.season </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.player_passing_statistics.player <SEP> american_football.player_passing_statistics.season </PATH>", "<PATH> american_football.football_player.passing <SEP> american_football.player_passing_statistics.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.football_player.position_s <SEP> sports.sports_league_season.position <SEP> sports.sports_league_season.position </PATH>"], "scores": [-0.020613335072994232, -0.02205255813896656, -0.027287250384688377, -0.028745610266923904, -0.034777093678712845, -0.04001934081315994, -0.04005615785717964, -0.04359644651412964, -0.0488194040954113, -0.06814173609018326], "norm_scores": [0.10168467462062836, 0.10153842717409134, 0.10100829601287842, 0.100861094892025, 0.10025458037853241, 0.09973039478063583, 0.09972672909498215, 0.09937428683042526, 0.098856620490551, 0.09696481376886368]}}
{"id": "WebQTest-570", "question": "what is william taft famous for", "prediction": [["people.person.profession"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["common.topic.notable_types"], ["government.politician.government_positions_held", "government.government_office_category.officeholders"], ["government.politician.government_positions_held"], ["base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.government_office_category.officeholders"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is william taft famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_category.officeholders </PATH>", "<PATH> government.politician.government_positions_held </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_category.officeholders </PATH>"], "scores": [-0.00974931102246046, -0.0391465462744236, -0.04048145189881325, -0.04072682559490204, -0.04417721927165985, -0.04591033235192299, -0.05336850509047508, -0.06363196671009064, -0.06413877755403519, -0.072921521961689], "norm_scores": [0.10382477194070816, 0.10081703215837479, 0.10068254172801971, 0.10065784305334091, 0.10031113028526306, 0.10013744235038757, 0.09939336776733398, 0.09837847203016281, 0.0983286127448082, 0.09746880829334259]}}
{"id": "WebQTest-572", "question": "where did robin cook die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.place_of_death.people_deceased_here"], ["location.deceased_person.place_of_death"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.place_of_death.people_born_here"], ["people.place_of_death.people_deceased_person"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin cook die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_born_here </PATH>", "<PATH> people.place_of_death.people_deceased_person </PATH>"], "scores": [-0.00032396172173321247, -0.10255522280931473, -0.11007610708475113, -0.11037444323301315, -0.11145796626806259, -0.11231212317943573, -0.12852713465690613, -0.12947148084640503, -0.13516753911972046, -0.13708244264125824], "norm_scores": [0.11125832796096802, 0.10044632107019424, 0.09969371557235718, 0.09966397285461426, 0.09955604374408722, 0.09947104007005692, 0.09787112474441528, 0.09777874499559402, 0.09722337871789932, 0.09703738242387772]}}
{"id": "WebQTest-573", "question": "where did melba beals live", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.nationality", "location.administrative_division.second_level_division_of"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth", "location.us_state.capital"], ["people.person.nationality", "location.country.first_level_divisions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did melba beals live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.second_level_division_of </PATH>"], "scores": [-0.0170796737074852, -0.027391476556658745, -0.03345426917076111, -0.03563076630234718, -0.051509227603673935, -0.05512622743844986, -0.07287567853927612, -0.07765405625104904, -0.0812297910451889, -0.08385882526636124], "norm_scores": [0.10368963330984116, 0.10262589156627655, 0.10200557857751846, 0.10178381204605103, 0.10018039494752884, 0.09981870651245117, 0.09806260466575623, 0.09759514033794403, 0.09724678844213486, 0.09699145704507828]}}
{"id": "WebQTest-575", "question": "who is louisiana state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "ground_paths": [["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.locations.countries.states_provinces_within", "people.person.nationality"], ["government.election.district", "government.election.winner"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.containedby", "symbols.name_source.namesakes"], ["location.location.partially_containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "people.person.place_of_birth"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is louisiana state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.022576574236154556, -0.025792157277464867, -0.029586832970380783, -0.032842908054590225, -0.03670435771346092, -0.03695770353078842, -0.039421167224645615, -0.03965970501303673, -0.042941730469465256, -0.04449344426393509], "norm_scores": [0.1012575775384903, 0.10093250125646591, 0.10055021196603775, 0.1002233475446701, 0.09983708709478378, 0.09981179982423782, 0.09956622123718262, 0.09954246878623962, 0.09921630471944809, 0.0990624725818634]}}
{"id": "WebQTest-576", "question": "what are the 7 countries that are part of central america", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"], ["base.locations.countries.continent"], ["book.written_work.subjects"], ["book.book_subject.works", "book.written_work.subjects"], ["book.book_subject.works"], ["base.locations.continents.countries_within"], ["location.country.form_of_government"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the 7 countries that are part of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works <SEP> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.007674529682844877, -0.02796512097120285, -0.03492963686585426, -0.11042095720767975, -0.11283570528030396, -0.11555010825395584, -0.11838068068027496, -0.12304624170064926, -0.1265716850757599, -0.12743893265724182], "norm_scores": [0.10852383077144623, 0.10634400695562363, 0.10560594499111176, 0.09792710095643997, 0.09769091755151749, 0.09742610901594162, 0.09715072065591812, 0.09669851511716843, 0.09635820984840393, 0.09627468138933182]}}
{"id": "WebQTest-577", "question": "where did derek fisher go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["sports.sport.team_coaches", "sports.sports_team.sport"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did derek fisher go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.sport.team_coaches <SEP> sports.sports_team.sport </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.017872774973511696, -0.023554543033242226, -0.024508681148290634, -0.034715574234724045, -0.04002020135521889, -0.04426947608590126, -0.058545347303152084, -0.06100226938724518, -0.06428588926792145, -0.06875050812959671], "norm_scores": [0.10260581225156784, 0.10202448070049286, 0.10192718356847763, 0.10089211910963058, 0.10035833716392517, 0.09993279725313187, 0.0985163003206253, 0.09827455133199692, 0.09795238822698593, 0.09751603752374649]}}
{"id": "WebQTest-578", "question": "who was kobe bryant drafted by", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kobe bryant drafted by [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>"], "scores": [-0.025067683309316635, -0.02704426646232605, -0.03057028353214264, -0.031083421781659126, -0.03424132615327835, -0.03435027599334717, -0.03753536194562912, -0.0375434085726738, -0.04447229951620102, -0.04644818976521492], "norm_scores": [0.10097940266132355, 0.10078001022338867, 0.10042528063058853, 0.1003737598657608, 0.10005728900432587, 0.10004639625549316, 0.09972824156284332, 0.09972743690013885, 0.09903883188962936, 0.09884333610534668]}}
{"id": "WebQTest-579", "question": "where are the new orleans hornets moving to", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.previously_known_as"], ["sports.sports_team.arena_stadium"], ["sports.defunct_sports_team.later_known_as"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], [], ["sports.sports_team_roster.team", "sports.sports_team_roster.team"], ["sports.sports_team_relocation.team"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the new orleans hornets moving to [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.defunct_sports_team.later_known_as </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_relocation.team </PATH>"], "scores": [-0.011108477599918842, -0.02984420396387577, -0.044293876737356186, -0.0523984394967556, -0.0563867948949337, -0.05707535520195961, -0.06724236160516739, -0.0733887329697609, -0.08003373444080353, -0.0862608328461647], "norm_scores": [0.10454582422971725, 0.10260531306266785, 0.10113336145877838, 0.10031703114509583, 0.09991773217916489, 0.09984895586967468, 0.0988389328122139, 0.09823329001665115, 0.09758269786834717, 0.09697693586349487]}}
{"id": "WebQTest-580", "question": "what language does algerian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["base.rosetta.languoid.languoid_class"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["people.person.ethnicity", "people.person.nationality", "location.country.languages_spoken"], ["language.language_family.geographic_distribution"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does algerian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>"], "scores": [-0.011965794488787651, -0.017700115218758583, -0.051991403102874756, -0.05489038676023483, -0.12568111717700958, -0.13105186820030212, -0.1333645135164261, -0.133970707654953, -0.14894215762615204, -0.1556829810142517], "norm_scores": [0.10866939276456833, 0.10804802924394608, 0.10440573841333389, 0.10410350561141968, 0.096988745033741, 0.09646923840045929, 0.0962463915348053, 0.09618806838989258, 0.09475871920585632, 0.09412211179733276]}}
{"id": "WebQTest-581", "question": "where was anne boleyn executed", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["location.location.people_executed"], ["people.person.place_of_death"], ["location.location.events"], ["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_burial"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was anne boleyn executed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> location.location.people_executed </PATH>", "<PATH> people.person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0012099993182346225, -0.08056356012821198, -0.08258307725191116, -0.08291413635015488, -0.08391605317592621, -0.1044997051358223, -0.10538021475076675, -0.11299751698970795, -0.11610420793294907, -0.12741811573505402], "norm_scores": [0.10919661819934845, 0.10086636245250702, 0.10066286474466324, 0.10062955319881439, 0.1005287691950798, 0.09848067909479141, 0.0983940064907074, 0.09764735400676727, 0.09734446555376053, 0.09624932706356049]}}
{"id": "WebQTest-582", "question": "what did galileo do to become famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["law.inventor.inventions"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did galileo do to become famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.0017456653295084834, -0.05881063640117645, -0.07363203167915344, -0.08660518378019333, -0.08843071758747101, -0.09953939914703369, -0.10035084933042526, -0.10382937639951706, -0.11502783000469208, -0.11530610173940659], "norm_scores": [0.108551524579525, 0.10253046452999115, 0.10102201998233795, 0.09971991181373596, 0.09953803569078445, 0.09843841940164566, 0.09835857897996902, 0.09801702946424484, 0.0969255194067955, 0.09689854085445404]}}
{"id": "WebQTest-583", "question": "what school did martin luther king jr received his doctorate degree from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["common.topic.notable_types"], ["book.written_work.author", "education.dissertation.institution"], ["education.education.degree_recipient", "education.education.institution"], ["education.education.degree_recipient"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did martin luther king jr received his doctorate degree from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.author <SEP> education.dissertation.institution </PATH>", "<PATH> education.education.degree_recipient <SEP> education.education.institution </PATH>", "<PATH> education.education.degree_recipient </PATH>"], "scores": [-0.006852809805423021, -0.023420043289661407, -0.02969735860824585, -0.04292162507772446, -0.06725702434778214, -0.08748236298561096, -0.09693465381860733, -0.09965775161981583, -0.10433080792427063, -0.11788028478622437], "norm_scores": [0.10619384795427322, 0.10444900393486023, 0.10379539430141449, 0.10243181884288788, 0.09996918588876724, 0.09796758741140366, 0.09704592823982239, 0.09678202122449875, 0.09633079916238785, 0.09503438323736191]}}
{"id": "WebQTest-584", "question": "where buddha come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["people.person.nationality"], ["people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.004570694174617529, -0.044427838176488876, -0.04640442132949829, -0.08161351084709167, -0.10172851383686066, -0.11066899448633194, -0.13374175131320953, -0.1343367099761963, -0.1430855691432953, -0.1496439427137375], "norm_scores": [0.1093459203839302, 0.10507340729236603, 0.10486593097448349, 0.1012379452586174, 0.09922188520431519, 0.09833875298500061, 0.09609577804803848, 0.09603862464427948, 0.09520206600427628, 0.09457973390817642]}}
{"id": "WebQTest-585", "question": "what did king henry viii rule", "prediction": [["royalty.kingdom.rulers"], ["royalty.monarch.kingdom"], ["people.person.nationality"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.political_appointer.appointees", "government.governmental_jurisdiction.governing_officials"], ["people.person.nationality", "location.administrative_division.country"], ["government.political_appointer.appointees", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.appointed_by", "government.government_position_held.jurisdiction_of_office"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["royalty.monarch.kingdom"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did king henry viii rule [/INST]", "raw_output": {"paths": ["<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.015307169407606125, -0.029646214097738266, -0.030596040189266205, -0.04821242019534111, -0.04955252632498741, -0.05864901840686798, -0.05959314852952957, -0.06485942006111145, -0.06714606285095215, -0.0731973871588707], "norm_scores": [0.10347992926836014, 0.10200671851634979, 0.10190986841917038, 0.10013031214475632, 0.09999620914459229, 0.09909072518348694, 0.09899722039699554, 0.09847723692655563, 0.0982523113489151, 0.09765955805778503]}}
{"id": "WebQTest-586", "question": "what type of cancer did larry hagman have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.people_with_this_condition"], ["people.cause_of_death.people", "people.deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["film.performance.actor", "film.performance.character"], ["medicine.cancer_patient.cancer_type"]], "ground_paths": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did larry hagman have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> medicine.cancer_patient.cancer_type </PATH>"], "scores": [-0.014797278679907322, -0.015162459574639797, -0.03220745921134949, -0.03875775262713432, -0.10454589873552322, -0.10701596736907959, -0.11242233216762543, -0.12288397550582886, -0.1318041980266571, -0.1347300112247467], "norm_scores": [0.10677071660757065, 0.10673173516988754, 0.1049279049038887, 0.10424283891916275, 0.0976056233048439, 0.09736482799053192, 0.09683986008167267, 0.09583203494548798, 0.09498099237680435, 0.09470349550247192]}}
{"id": "WebQTest-588", "question": "what wars did robert e lee served in", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_command.military_commander", "military.military_conflict.commanders"], ["military.military_person.participated_in_conflicts"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["military.military_commander.military_commands", "military.military_command.military_conflict"], ["military.military_command.military_commander", "military.military_command.military_conflict"], ["military.military_conflict.military_personnel_involved"], ["people.person.nationality", "base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts", "time.event.included_in_event"]], "ground_paths": [["military.military_conflict.military_personnel_involved"], ["base.culturalevent.event.entity_involved"], ["military.military_command.military_commander", "military.military_conflict.commanders"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_person.participated_in_conflicts"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["people.person.nationality", "base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_command.military_conflict"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wars did robert e lee served in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> military.military_command.military_commander <SEP> military.military_conflict.commanders </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_conflict.commanders </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_command.military_conflict </PATH>", "<PATH> military.military_command.military_commander <SEP> military.military_command.military_conflict </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> people.person.nationality <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts <SEP> time.event.included_in_event </PATH>"], "scores": [-0.006413273047655821, -0.025822147727012634, -0.046505048871040344, -0.04756094142794609, -0.048388656228780746, -0.051065247505903244, -0.06001492589712143, -0.06124847009778023, -0.06769748032093048, -0.06808425486087799], "norm_scores": [0.10425800085067749, 0.1022539809346199, 0.10016079992055893, 0.10005509108304977, 0.09997230768203735, 0.09970507770776749, 0.09881673753261566, 0.09869491308927536, 0.0980604812502861, 0.09802256524562836]}}
{"id": "WebQTest-589", "question": "what russian language called", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.language_family"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["language.language_writing_system.languages"], ["language.human_language.region"], ["language.language_family.languages"], ["language.human_language.writing_system"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what russian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.language_writing_system.languages </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> language.human_language.writing_system </PATH>"], "scores": [-0.014625719748437405, -0.02549462765455246, -0.026780186221003532, -0.07998638600111008, -0.1085667535662651, -0.14141227304935455, -0.1495901346206665, -0.15042996406555176, -0.16162122786045074, -0.1618419885635376], "norm_scores": [0.1089530810713768, 0.10777529329061508, 0.10763683170080185, 0.10205957293510437, 0.09918396174907684, 0.09597913175821304, 0.09519743174314499, 0.09511750936508179, 0.09405896067619324, 0.09403820335865021]}}
{"id": "WebQTest-590", "question": "who is in charge of libya now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["government.governmental_jurisdiction.governing_officials", "government.political_appointer.appointees"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in charge of libya now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.appointed_by </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.political_appointer.appointees </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.appointed_by </PATH>"], "scores": [-0.010198063217103481, -0.02411908283829689, -0.036602336913347244, -0.037415437400341034, -0.040969084948301315, -0.05414257571101189, -0.057458579540252686, -0.057947199791669846, -0.06068403646349907, -0.06143718585371971], "norm_scores": [0.1034339964389801, 0.10200407356023788, 0.10073863714933395, 0.10065676271915436, 0.10029970109462738, 0.09898706525564194, 0.09865935891866684, 0.09861116856336594, 0.09834165126085281, 0.09826761484146118]}}
{"id": "WebQTest-592", "question": "when did shays rebellion start", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["time.recurring_event.included_in_event"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["time.recurring_event.instances"], ["time.event.locations"], ["time.event.instance_of_recurring_event"], ["base.culturalevent.event.entity_involved", "time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shays rebellion start [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> time.recurring_event.included_in_event </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.includes_event </PATH>"], "scores": [-0.014290327206254005, -0.020825257524847984, -0.03378952294588089, -0.07842987030744553, -0.08261154592037201, -0.08360068500041962, -0.0891026183962822, -0.0950048640370369, -0.09931328892707825, -0.11113658547401428], "norm_scores": [0.10575734078884125, 0.10506848245859146, 0.10371513664722443, 0.09918707609176636, 0.09877316653728485, 0.09867552667856216, 0.0981341078877449, 0.09755659848451614, 0.09713719040155411, 0.0959954708814621]}}
{"id": "WebQTest-593", "question": "what kind of disease does montel williams have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.person.medical_condition"], ["people.person.profession", "medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition", "medical_condition.notable_people_with_this_condition"], ["people.person.disease"], ["people.person.disease", "medicine.disease.notable_people_with_this_condition"], ["tv.tv_personality.tv_regular_appearances", "tv.tv_regular_personal_appearance.appearance_type"], ["media_common.quotation.author", "medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition", "medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of disease does montel williams have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.medical_condition </PATH>", "<PATH> people.person.profession <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition <SEP> medical_condition.notable_people_with_this_condition </PATH>", "<PATH> people.person.disease </PATH>", "<PATH> people.person.disease <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> tv.tv_personality.tv_regular_appearances <SEP> tv.tv_regular_personal_appearance.appearance_type </PATH>", "<PATH> media_common.quotation.author <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition <SEP> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.009445248171687126, -0.013007990084588528, -0.08288682252168655, -0.09090803563594818, -0.10969728231430054, -0.11099023371934891, -0.11249691247940063, -0.11304350197315216, -0.11577554792165756, -0.11616984009742737], "norm_scores": [0.10802612453699112, 0.10764194279909134, 0.10037684440612793, 0.09957492351531982, 0.09772144258022308, 0.09759518504142761, 0.09744825214147568, 0.09739500284194946, 0.09712927043437958, 0.09709098190069199]}}
{"id": "WebQTest-595", "question": "when is the last time the ravens won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.runner_up"], ["common.topic.notable_types"], ["time.recurring_event.current_frequency"], ["sports.sports_league_season.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the ravens won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> sports.sports_league_season.championship </PATH>"], "scores": [-0.010314271785318851, -0.020952241495251656, -0.03330262750387192, -0.04559876397252083, -0.0920294001698494, -0.09648790955543518, -0.1125471442937851, -0.12071205675601959, -0.13942772150039673, -0.1432906836271286], "norm_scores": [0.10725414007902145, 0.10611922293901443, 0.10481666773557663, 0.1035357192158699, 0.09883838891983032, 0.09839869290590286, 0.09683111310005188, 0.09604370594024658, 0.09426290541887283, 0.09389947354793549]}}
{"id": "WebQTest-596", "question": "where did kansas city royals come from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.previously_known_as"], ["sports.sports_team.place_founded"], ["sports.defunct_sports_team.later_known_as"], ["sports.sports_team.origin"], ["sports.sports_team.locations"], ["sports.sports_team_origin.teams"], ["sports.sports_team.location", "location.location.containedby"], ["sports.sports_team_roster.team", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kansas city royals come from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>", "<PATH> sports.sports_team.place_founded </PATH>", "<PATH> sports.defunct_sports_team.later_known_as </PATH>", "<PATH> sports.sports_team.origin </PATH>", "<PATH> sports.sports_team.locations </PATH>", "<PATH> sports.sports_team_origin.teams </PATH>", "<PATH> sports.sports_team.location <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014977683313190937, -0.021988127380609512, -0.036492880433797836, -0.044473063200712204, -0.06154420226812363, -0.07718919962644577, -0.08510588854551315, -0.08992940187454224, -0.09146323055028915, -0.09522939473390579], "norm_scores": [0.10475414246320724, 0.10402233898639679, 0.10252440720796585, 0.1017095074057579, 0.0999879390001297, 0.09843580424785614, 0.09765959531068802, 0.09718967229127884, 0.09704071283340454, 0.09667592495679855]}}
{"id": "WebQTest-597", "question": "what style of music does john mayer play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.musical_group.member", "music.group_membership.group"], ["music.album.genre"], ["music.artist.album", "music.artist.genre"], ["music.music_artist.genre"], ["music.artist.album", "music.album.genre"], ["music.composer.compositions"], ["music.genre.artist"], ["music.music_genre.artists"]], "ground_paths": [["music.artist.genre"], ["music.genre.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does john mayer play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.album.genre </PATH>", "<PATH> music.artist.album <SEP> music.artist.genre </PATH>", "<PATH> music.music_artist.genre </PATH>", "<PATH> music.artist.album <SEP> music.album.genre </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.genre.artist </PATH>", "<PATH> music.music_genre.artists </PATH>"], "scores": [-0.0006489988300018013, -0.06409439444541931, -0.13267922401428223, -0.14941851794719696, -0.1528134047985077, -0.15317022800445557, -0.15375542640686035, -0.15414902567863464, -0.16565950214862823, -0.16572512686252594], "norm_scores": [0.11356604844331741, 0.10658461600542068, 0.09951958060264587, 0.09786755591630936, 0.0975358709692955, 0.09750107675790787, 0.09744403511285782, 0.09740567952394485, 0.09629093110561371, 0.09628460556268692]}}
{"id": "WebQTest-599", "question": "what inventions did thomas edison invent list", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"], ["law.us_patent.inventor"], ["base.argumentmaps.original_idea.innovator"], ["law.us_patent.patent_office", "law.us_patent.patent_office"], ["common.topic.notable_types"], ["symbols.name_source.namesakes"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.inventions"], ["law.inventor.us_patents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions did thomas edison invent list [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> law.us_patent.inventor </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> law.us_patent.patent_office <SEP> law.us_patent.patent_office </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.00904012005776167, -0.022581327706575394, -0.05329039320349693, -0.055789075791835785, -0.08500750362873077, -0.09040527790784836, -0.0946086123585701, -0.1124429702758789, -0.12153469771146774, -0.12463844567537308], "norm_scores": [0.10694633424282074, 0.10550791025161743, 0.10231710970401764, 0.10206177085638046, 0.09912282973527908, 0.098589226603508, 0.09817568957805634, 0.09644031524658203, 0.095567487180233, 0.09527131915092468]}}
{"id": "WebQTest-600", "question": "where did djokovic come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.ethnicity.people"], ["base.biblioness.bibs_location.country"], ["people.ethnicity.people_with_this_ethnicity"], ["people.person.ethnicity"], ["people.ethnicity.people", "people.ethnicity.geographic_distribution"], ["people.ethnicity.people", "people.person.place_of_birth"], ["people.ethnicity.people", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did djokovic come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people <SEP> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.place_of_birth </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality </PATH>"], "scores": [-0.008754754438996315, -0.02134166844189167, -0.05577758699655533, -0.08249260485172272, -0.12361246347427368, -0.1275470107793808, -0.13102558255195618, -0.1395220011472702, -0.13964004814624786, -0.1467694193124771], "norm_scores": [0.10916205495595932, 0.10779665410518646, 0.1041477620601654, 0.10140229016542435, 0.09731720387935638, 0.09693507105112076, 0.09659844636917114, 0.09578118473291397, 0.09576988220214844, 0.09508953243494034]}}
{"id": "WebQTest-602", "question": "what part of the country is new england", "prediction": [["location.location.contains"], ["location.location.containedby"], ["base.locations.countries.states_provinces_within"], ["location.administrative_division.first_level_division_of"], ["location.location.partially_containedby"], ["base.biblioness.bibs_location.country"], ["location.location.partially_contains"], ["book.written_work.subjects"], ["book.book_subject.works"], ["location.administrative_division.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.0068350899964571, -0.023381192237138748, -0.11911993473768234, -0.13459758460521698, -0.1380414217710495, -0.13912051916122437, -0.14807239174842834, -0.1527065485715866, -0.156393364071846, -0.15867435932159424], "norm_scores": [0.11156558990478516, 0.10973480343818665, 0.0997161716222763, 0.09818467497825623, 0.09784713387489319, 0.09774160385131836, 0.0968705341219902, 0.09642266482114792, 0.09606782346963882, 0.095848947763443]}}
{"id": "WebQTest-603", "question": "what money to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_formerly_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.tour_operator.travel_destinations", "finance.currency.countries_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_formerly_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> finance.currency.countries_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.013670290820300579, -0.023543432354927063, -0.03584209084510803, -0.04600443318486214, -0.10257679224014282, -0.11804647743701935, -0.12080645561218262, -0.12086942791938782, -0.14085176587104797, -0.15026043355464935], "norm_scores": [0.10750491917133331, 0.10644873231649399, 0.10514757037162781, 0.10408443212509155, 0.0983595997095108, 0.0968497171998024, 0.09658277779817581, 0.09657669812440872, 0.0946660190820694, 0.09377951920032501]}}
{"id": "WebQTest-604", "question": "what kind of music did jimi hendrix play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["book.written_work.author"], ["influence.influence_node.influenced_by"], ["music.group_member.instruments_played"], ["influence.influence_node.influenced"], ["music.musical_group.member"], ["music.music_artist.genre"], ["book.book_subject.works"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did jimi hendrix play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> music.musical_group.member </PATH>", "<PATH> music.music_artist.genre </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.0008984622545540333, -0.056034691631793976, -0.14228849112987518, -0.14680881798267365, -0.15898863971233368, -0.1606130450963974, -0.16222083568572998, -0.1662750542163849, -0.17015577852725983, -0.17184028029441833], "norm_scores": [0.1140165627002716, 0.10790028423070908, 0.09898355603218079, 0.0985371321439743, 0.09734423458576202, 0.09718623757362366, 0.09703011065721512, 0.09663752466440201, 0.09626322239637375, 0.09610120952129364]}}
{"id": "WebQTest-605", "question": "what degrees did obama get in college", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"], ["book.written_work.subjects", "location.location.containedby"], ["common.topic.notable_types"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.educational_institution.degrees_awarded"], ["book.written_work.subjects", "book.written_work.subjects"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did obama get in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.degrees_awarded </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.subjects </PATH>"], "scores": [-0.011278610676527023, -0.01973506063222885, -0.02676076628267765, -0.06843023002147675, -0.08007577806711197, -0.0966583788394928, -0.10761871188879013, -0.11033740639686584, -0.12047039717435837, -0.12090356647968292], "norm_scores": [0.10662207752466202, 0.10572423040866852, 0.10498405247926712, 0.10069931298494339, 0.09953340888023376, 0.097896508872509, 0.0968293845653534, 0.09656649082899094, 0.0955929234623909, 0.09555152803659439]}}
{"id": "WebQTest-606", "question": "who were alexander graham bell 's siblings", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.parents", "people.person.parents"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.sibling_s", "people.person.sibling_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were alexander graham bell 's siblings [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.sibling_relationship.sibling <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.sibling_s <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.person.sibling_s <SEP> people.person.sibling_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.020808888599276543, -0.022734373807907104, -0.03901207074522972, -0.039523351937532425, -0.04166131094098091, -0.046534232795238495, -0.04751492664217949, -0.04870138689875603, -0.05040901154279709, -0.054529886692762375], "norm_scores": [0.10204833000898361, 0.10185202956199646, 0.10020752996206284, 0.10015631467103958, 0.0999424085021019, 0.09945657849311829, 0.09935909509658813, 0.09924127906560898, 0.09907195717096329, 0.0986645370721817]}}
{"id": "WebQTest-608", "question": "when did liverpool fc last win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["time.recurring_event.instances"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["sports.sports_championship.events"], ["sports.sports_club.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did liverpool fc last win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> sports.sports_club.championships </PATH>"], "scores": [-0.00456384988501668, -0.029409246519207954, -0.05703561007976532, -0.05724800378084183, -0.06694841384887695, -0.08606842905282974, -0.10618409514427185, -0.10631382465362549, -0.10870709270238876, -0.12539361417293549], "norm_scores": [0.10720204561948776, 0.1045713797211647, 0.10172199457883835, 0.10170038789510727, 0.10071862488985062, 0.09881117939949036, 0.09684339165687561, 0.0968308225274086, 0.09659935534000397, 0.09500081837177277]}}
{"id": "WebQTest-609", "question": "who played jeannie", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"]], "ground_paths": [["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jeannie [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> tv.tv_guest_role.character <SEP> tv.tv_guest_role.actor </PATH>"], "scores": [-0.01570522040128708, -0.031221456825733185, -0.03280797600746155, -0.03985476866364479, -0.046455562114715576, -0.04915882647037506, -0.05203657224774361, -0.05466430261731148, -0.06314601004123688, -0.06594064086675644], "norm_scores": [0.10297190397977829, 0.10138650238513947, 0.10122577846050262, 0.1005149707198143, 0.09985367208719254, 0.09958410263061523, 0.09929794073104858, 0.09903735667467117, 0.09820090979337692, 0.09792684763669968]}}
{"id": "WebQTest-611", "question": "where was st peter killed", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["location.location.people_born_here"], ["fictional_universe.fictional_character.place_of_death"], ["symbols.namesake.named_after"], ["people.place_of_interment.interred_here"], ["fictional_universe.fictional_setting.characters_that_have_lived_here"], ["location.deceased_person.place_of_death"], ["symbols.name_source.namesakes"], ["base.saints.saint.venerated_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st peter killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> fictional_universe.fictional_character.place_of_death </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> base.saints.saint.venerated_here </PATH>"], "scores": [-0.0009040497825480998, -0.08072368800640106, -0.09241556376218796, -0.09652349352836609, -0.10331922024488449, -0.11542829126119614, -0.12506818771362305, -0.12668748199939728, -0.12695644795894623, -0.128831148147583], "norm_scores": [0.11030690371990204, 0.10184447467327118, 0.10066065937280655, 0.10024800151586533, 0.09956905245780945, 0.09837063401937485, 0.09742690622806549, 0.09726926684379578, 0.09724310785531998, 0.09706097841262817]}}
{"id": "WebQTest-612", "question": "who does jeremy shockey play for in 2012", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_coach.sports_coached", "sports.sports_team.sport"], ["ice_hockey.hockey_player.hockey_position", "ice.hockey.hockey_team.captain"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sport.team_coaches", "sports.sports_team.sport"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy shockey play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team.sport </PATH>", "<PATH> ice_hockey.hockey_player.hockey_position <SEP> ice.hockey.hockey_team.captain </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sport.team_coaches <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.013658651150763035, -0.0174262635409832, -0.026875056326389313, -0.030571937561035156, -0.035579051822423935, -0.05406207591295242, -0.05458821728825569, -0.055286794900894165, -0.057720284909009933, -0.06637383252382278], "norm_scores": [0.10277780890464783, 0.1023913025856018, 0.10142838954925537, 0.1010541170835495, 0.10054939985275269, 0.09870800375938416, 0.0986560806632042, 0.09858718514442444, 0.09834756702184677, 0.09750019013881683]}}
{"id": "WebQTest-613", "question": "what college did deion sanders jr go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_athlete_affiliation.athlete"], ["sports.pro_athlete.teams", "sports.sports_athlete_affiliation.athlete_affiliation_institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did deion sanders jr go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_athlete_affiliation.athlete </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_athlete_affiliation.athlete_affiliation_institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.01967279240489006, -0.023859594017267227, -0.023953890427947044, -0.028502602130174637, -0.03708096221089363, -0.042681239545345306, -0.06322982907295227, -0.07061783224344254, -0.07067123800516129, -0.07367385178804398], "norm_scores": [0.10258328914642334, 0.1021546870470047, 0.10214506089687347, 0.10168148577213287, 0.10081295669078827, 0.10024994611740112, 0.09821098297834396, 0.09748806804418564, 0.09748286008834839, 0.09719059616327286]}}
{"id": "WebQTest-614", "question": "when did the battle of alamo start", "prediction": [["time.event.locations"], ["time.event.included_in_event"], ["time.event.includes_event"], ["base.culturalevent.event.entity_involved"], ["location.location.events"], ["military.military_conflict.military_personnel_involved"], ["event.disaster.areas_affected"], ["military.military_conflict.casualties", "military.military_conflict.time_of_casualties"], ["military.military_conflict.casualties", "military.military_conflict.casualties"], ["time.event.instance_of_recurring_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the battle of alamo start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.time_of_casualties </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>"], "scores": [-0.01120006199926138, -0.02820703200995922, -0.044256750494241714, -0.06459034979343414, -0.06963853538036346, -0.07163088768720627, -0.07692472636699677, -0.08424654603004456, -0.08745553344488144, -0.08835644274950027], "norm_scores": [0.10524658113718033, 0.1034717932343483, 0.10182435065507889, 0.0997748076915741, 0.09927238523960114, 0.09907481074333191, 0.09855170547962189, 0.09783275425434113, 0.09751933068037033, 0.09743151068687439]}}
{"id": "WebQTest-615", "question": "what were the names of henry viii 's three children", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"], ["fictional_universe.fictional_character.parents"], ["people.person.offspring"], ["people.deceased_person.place_of_burial"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.person.gender"], ["fictional_universe.fictional_character.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were the names of henry viii 's three children [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.person.offspring </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>"], "scores": [-0.012267695739865303, -0.012884904630482197, -0.13077259063720703, -0.1421692818403244, -0.14794011414051056, -0.14850276708602905, -0.15115612745285034, -0.1529119312763214, -0.15916858613491058, -0.16002537310123444], "norm_scores": [0.11139924079179764, 0.11133050918579102, 0.0989500880241394, 0.09782879799604416, 0.09726586937904358, 0.09721115976572037, 0.09695356339216232, 0.09678348153829575, 0.09617982804775238, 0.09609745442867279]}}
{"id": "WebQTest-616", "question": "what was queen elizabeth ii childhood nickname", "prediction": [["people.person.children"], ["people.person.parents"], ["royalty.monarch.kingdom"], ["people.person.nationality", "people.person.nationality"], ["royalty.kingdom.rulers"], ["royalty.monarch.kingdom", "people.person.nationality"], ["people.person.nationality", "royalty.monarch.kingdom"], ["royalty.monarch.kingdom", "people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.children"], ["symbols.name_source.namesakes"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was queen elizabeth ii childhood nickname [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> royalty.monarch.kingdom </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.023957256227731705, -0.0296035073697567, -0.034130558371543884, -0.03554312884807587, -0.04993894696235657, -0.05384581908583641, -0.059312161058187485, -0.06687616556882858, -0.07120958715677261, -0.07536506652832031], "norm_scores": [0.1026206761598587, 0.10204289108514786, 0.10158197581768036, 0.10143858194351196, 0.09998875111341476, 0.09959886968135834, 0.09905592352151871, 0.0983094796538353, 0.09788438677787781, 0.0974784791469574]}}
{"id": "WebQTest-617", "question": "where did tim tebow grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.location"], ["base.schemastaging.athlete_salary.athlete", "location.location.people_born_here"], ["location.location.people_born_here", "location.location.containedby"], ["base.schemastaging.athlete_extra.salary", "location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tim tebow grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.location </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> location.location.people_born_here </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> location.location.people_born_here </PATH>"], "scores": [-0.010052560828626156, -0.01937372237443924, -0.061337292194366455, -0.07302398234605789, -0.09369241446256638, -0.11573860049247742, -0.12579196691513062, -0.13148759305477142, -0.13540397584438324, -0.13648025691509247], "norm_scores": [0.10823740065097809, 0.1072331890463829, 0.10282640904188156, 0.10163170844316483, 0.0995526909828186, 0.09738195687532425, 0.09640784561634064, 0.09586029499769211, 0.09548560529947281, 0.09538289159536362]}}
{"id": "WebQTest-618", "question": "where is chesapeake bay bridge", "prediction": [["transportation.bridge.locale"], ["transportation.bridge.body_of_water_spanned"], ["location.location.containedby"], ["geography.body_of_water.bridges"], ["location.location.contains"], ["transportation.bridge.locale", "location.location.containedby"], ["transportation.transportation.bridge"], ["transportation.bridge.locations"], ["transportation.transportation.mode_of_transportation"], ["transportation.transportation.bridges"]], "ground_paths": [["transportation.bridge.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chesapeake bay bridge [/INST]", "raw_output": {"paths": ["<PATH> transportation.bridge.locale </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.body_of_water.bridges </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> transportation.bridge.locale <SEP> location.location.containedby </PATH>", "<PATH> transportation.transportation.bridge </PATH>", "<PATH> transportation.bridge.locations </PATH>", "<PATH> transportation.transportation.mode_of_transportation </PATH>", "<PATH> transportation.transportation.bridges </PATH>"], "scores": [-0.0009908973006531596, -0.0649932473897934, -0.0846487283706665, -0.09815265983343124, -0.11393396556377411, -0.12478428333997726, -0.12684832513332367, -0.1487942636013031, -0.15046584606170654, -0.15587832033634186], "norm_scores": [0.11106255650520325, 0.10417698323726654, 0.10214933007955551, 0.10077918320894241, 0.09920123964548111, 0.09813069552183151, 0.09792836010456085, 0.09580263495445251, 0.09564263373613358, 0.09512636810541153]}}
{"id": "WebQTest-619", "question": "who played victoria barkley", "prediction": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["people.person.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played victoria barkley [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>"], "scores": [-0.020120490342378616, -0.02100229263305664, -0.023856978863477707, -0.034146763384342194, -0.04687529802322388, -0.052298661321401596, -0.062060270458459854, -0.06817933917045593, -0.0767304003238678, -0.08154947310686111], "norm_scores": [0.10287237167358398, 0.10278169810771942, 0.10248870402574539, 0.10143952816724777, 0.10015653818845749, 0.09961482137441635, 0.09864714741706848, 0.09804536402225494, 0.09721054136753082, 0.09674321115016937]}}
{"id": "WebQTest-620", "question": "where did kennedy 's inaugural address take place", "prediction": [["time.event.locations"], ["location.location.events"], ["event.speech_or_presentation.speaker_s", "event.speech_or_presentation.speech_topic"], ["architecture.architect.structures_designed"], ["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speech_venue"], ["base.culturalevent.event.entity_involved"], ["base.inaugurations.inauguration.president"], ["base.inaugurations.inauguration_speaker.inauguration"], ["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speech_topic"], ["time.recurring_event.current_frequency"]], "ground_paths": [["people.person.nationality", "location.administrative_division.country"], ["symbols.name_source.namesakes", "people.person.place_of_birth"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["common.image.appears_in_topic_gallery", "common.image.image_location"], ["people.place_lived.person", "people.place_lived.location"], ["symbols.name_source.namesakes", "location.location.containedby"], ["base.inaugurations.inauguration.president", "location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kennedy 's inaugural address take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.speech_or_presentation.speaker_s <SEP> event.speech_or_presentation.speech_topic </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speech_venue </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.inaugurations.inauguration.president </PATH>", "<PATH> base.inaugurations.inauguration_speaker.inauguration </PATH>", "<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speech_topic </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.0007092279265634716, -0.06875322014093399, -0.09847497195005417, -0.09887289255857468, -0.09936480224132538, -0.10497112572193146, -0.10783735662698746, -0.11611387878656387, -0.11676036566495895, -0.12007836252450943], "norm_scores": [0.1096256673336029, 0.10241442918777466, 0.09941527992486954, 0.09937572479248047, 0.09932685643434525, 0.09877155721187592, 0.09848885238170624, 0.0976770743727684, 0.09761395305395126, 0.09729060530662537]}}
{"id": "WebQTest-621", "question": "what countries have german as the official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.local_name.locale"], ["location.country.form_of_government"], ["location.country.administrative_divisions"], ["location.country.currency_used"], ["film.film.featured_film_locations"], ["book.newspaper.circulation_areas"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have german as the official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>"], "scores": [-0.015387631952762604, -0.02068650908768177, -0.024197937920689583, -0.11565650254487991, -0.1235181912779808, -0.13656680285930634, -0.16411812603473663, -0.16479361057281494, -0.16533741354942322, -0.17473971843719482], "norm_scores": [0.10976476222276688, 0.10918467491865158, 0.1088019534945488, 0.0992925688624382, 0.09851502627134323, 0.09723788499832153, 0.09459541738033295, 0.09453154355287552, 0.09448015689849854, 0.09359598159790039]}}
{"id": "WebQTest-622", "question": "where did the taliban began", "prediction": [["organization.organization.place_founded"], ["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["organization.organization.geographic_scope", "location.location.containedby"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["organization.organization.geographic_scope", "base.culturalevent.event.entity_involved"], ["organization.organization.geographic_scope", "base.locations.countries.continent"], ["organization.organization.place_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the taliban began [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.continents.countries_within </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.countries.continent </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.01953197456896305, -0.025339458137750626, -0.03010839782655239, -0.054859891533851624, -0.055748675018548965, -0.05858194828033447, -0.06816843897104263, -0.07045401632785797, -0.07108468562364578, -0.07178356498479843], "norm_scores": [0.10333957523107529, 0.10274115949869156, 0.10225237160921097, 0.0997525304555893, 0.09966391324996948, 0.0993819385766983, 0.09843376278877258, 0.09820904582738876, 0.09814712405204773, 0.09807855635881424]}}
{"id": "WebQTest-624", "question": "who plays elena gilbert on vampire diaries", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "ground_paths": [["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays elena gilbert on vampire diaries [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>"], "scores": [-0.025654485449194908, -0.031016366556286812, -0.03212223947048187, -0.03571443259716034, -0.03646312654018402, -0.03703172504901886, -0.037170685827732086, -0.039129551500082016, -0.040222588926553726, -0.04108211770653725], "norm_scores": [0.1009945273399353, 0.10045445710420609, 0.10034343600273132, 0.09998362511396408, 0.09990879148244858, 0.09985200315713882, 0.09983812272548676, 0.09964275360107422, 0.09953390061855316, 0.09944837540388107]}}
{"id": "WebQTest-626", "question": "what does the missouri river bisect", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"], ["geography.river.basin_countries"], ["base.locations.countries.states_provinces_within"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby", "location.location.containedby"], ["location.location.partially_contains", "location.location.containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does the missouri river bisect [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.containedby </PATH>"], "scores": [-0.011181031353771687, -0.019604524597525597, -0.057289984077215195, -0.06299304217100143, -0.09308113902807236, -0.10475526005029678, -0.11329098045825958, -0.12330350279808044, -0.12333714216947556, -0.12546129524707794], "norm_scores": [0.10740135610103607, 0.10650046169757843, 0.10256163030862808, 0.1019783765077591, 0.09895574301481247, 0.09780723601579666, 0.09697593748569489, 0.09600981324911118, 0.09600658714771271, 0.09580286592245102]}}
{"id": "WebQTest-627", "question": "what nationality is taylor lautner jacob black", "prediction": [["people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.actor", "film.performance.character"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nominee"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.nationality"], ["fictional_universe.fictional_character.ethnicity"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is taylor lautner jacob black [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.nationality </PATH>", "<PATH> fictional_universe.fictional_character.ethnicity </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.021014302968978882, -0.022549670189619064, -0.04251747578382492, -0.04707935079932213, -0.04768303036689758, -0.0477413684129715, -0.049147892743349075, -0.04924282804131508, -0.054831892251968384, -0.05817272141575813], "norm_scores": [0.10231779515743256, 0.10216081887483597, 0.10014113038778305, 0.09968532621860504, 0.09962517023086548, 0.09961935877799988, 0.09947934001684189, 0.09946989268064499, 0.09891550987958908, 0.0985855981707573]}}
{"id": "WebQTest-628", "question": "what animal represents california", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "government.governmental_jurisdiction.official_symbols", "government.governmental_jurisdiction.official_symbols"], ["location.offical_symbol_variety.symbols_of_this_kind", "government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what animal represents california [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> government.governmental_jurisdiction.official_symbols <SEP> government.governmental_jurisdiction.official_symbols </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.018952859565615654, -0.02708732895553112, -0.027442114427685738, -0.02863585203886032, -0.03184289112687111, -0.03252401947975159, -0.035727351903915405, -0.035827215760946274, -0.03810666501522064, -0.04145479574799538], "norm_scores": [0.10128702968358994, 0.10046645998954773, 0.10043081641197205, 0.10031099617481232, 0.09998981654644012, 0.09992173314094543, 0.09960215538740158, 0.09959221631288528, 0.0993654653429985, 0.09903332591056824]}}
{"id": "WebQTest-629", "question": "what company does nike own", "prediction": [["business.business_operation.industry"], ["business.business_operation.industry", "business.business_operation.industry"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_language"], ["business.industry.companies"], ["organization.organization.parent", "organization.organization.child"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["organization.organization.spun_off_from", "organization.organization.spin_offs"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["organization.organization.spun_off_from", "organization.organization_spin_off.parent_company"]], "ground_paths": [["business.acquisition.acquiring_company", "business.acquisition.company_acquired"], ["business.business_operation.industry", "business.business_operation.industry"], ["business.industry.companies", "business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what company does nike own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_language </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>", "<PATH> organization.organization_relationship.child <SEP> organization.organization_relationship.parent </PATH>", "<PATH> organization.organization.spun_off_from <SEP> organization.organization.spin_offs </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization_relationship.parent </PATH>", "<PATH> organization.organization.spun_off_from <SEP> organization.organization_spin_off.parent_company </PATH>"], "scores": [-0.02919749915599823, -0.029323164373636246, -0.031804587692022324, -0.04535385221242905, -0.046883367002010345, -0.051439955830574036, -0.057790182530879974, -0.058247704058885574, -0.05983896926045418, -0.0625908374786377], "norm_scores": [0.10181352496147156, 0.10180073231458664, 0.10154843330383301, 0.10018181055784225, 0.10002870112657547, 0.09957394748926163, 0.09894362837076187, 0.09889836609363556, 0.09874112159013748, 0.09846977889537811]}}
{"id": "WebQTest-630", "question": "what language do colombian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["people.ethnicity.languages_spoken"], ["base.rosetta.languoid.languoid_class"], ["people.person.nationality", "people.person.languages"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do colombian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.person.nationality <SEP> people.person.languages </PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "</PATH> location.country.languages_spoken </PATH>"], "scores": [-0.008834348060190678, -0.01816607266664505, -0.07130558043718338, -0.08948498964309692, -0.15972577035427094, -0.1649472564458847, -0.19007764756679535, -0.19134625792503357, -0.1925329864025116, -0.1979392170906067], "norm_scores": [0.11241810023784637, 0.1113739162683487, 0.10561006516218185, 0.10370747745037079, 0.09667293727397919, 0.09616947919130325, 0.09378281980752945, 0.09366392344236374, 0.0935528352856636, 0.09304843097925186]}}
{"id": "WebQTest-631", "question": "what year did ray allen join the nba", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["sports.sports_award.award_winner", "sports.sports_league_season.awards"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"], ["common.topic.image", "common.image.appears_in_topic_gallery"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did ray allen join the nba [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_league_season.awards </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_league_season.awards </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>"], "scores": [-0.018130553886294365, -0.022556349635124207, -0.023564158007502556, -0.029879285022616386, -0.036698900163173676, -0.04394112527370453, -0.047517940402030945, -0.05090649425983429, -0.06174144148826599, -0.06822452694177628], "norm_scores": [0.10222994536161423, 0.10177849978208542, 0.10167597234249115, 0.10103589296340942, 0.10034921765327454, 0.09962508827447891, 0.09926939010620117, 0.09893357753753662, 0.09786742180585861, 0.09723498672246933]}}
{"id": "WebQTest-633", "question": "what is claire danes famous for", "prediction": [["people.person.profession"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["film.actor.film", "film.performance.character"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["common.topic.notable_types"], ["film.performance.actor", "film.performance.character"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is claire danes famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>"], "scores": [-0.000977091956883669, -0.07522954046726227, -0.07554848492145538, -0.08948086947202682, -0.08985951542854309, -0.09167347103357315, -0.09168411791324615, -0.09515374898910522, -0.09701180458068848, -0.1117967739701271], "norm_scores": [0.10837680846452713, 0.10062107443809509, 0.10058898478746414, 0.09919726103544235, 0.09915971010923386, 0.09898000210523605, 0.09897894412279129, 0.09863612055778503, 0.09845301508903503, 0.09700809419155121]}}
{"id": "WebQTest-634", "question": "what county is san antonio in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.citytown.postal_codes"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.us_county.capital"]], "ground_paths": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san antonio in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.us_county.capital </PATH>"], "scores": [-0.008463079109787941, -0.02677745930850506, -0.03900480642914772, -0.08815915882587433, -0.0984753891825676, -0.10246111452579498, -0.10589355230331421, -0.11245762556791306, -0.1129777655005455, -0.1379854530096054], "norm_scores": [0.10767687112092972, 0.10572277754545212, 0.1044379472732544, 0.09942849725484848, 0.0984080359339714, 0.09801658987998962, 0.09768073260784149, 0.09704165160655975, 0.09699118882417679, 0.09459573775529861]}}
{"id": "WebQTest-635", "question": "what characters does matt stone do", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.fictional_character.character_created_by"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.character"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["people.person.nationality", "people.person.nationality"], ["people.person.gender", "people.person.gender"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_no_value"], ["film.actor.film", "film.performance.character"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.ethnicity", "people.person.ethnicity"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["film.performance.actor", "film.performance.character"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what characters does matt stone do [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.016965726390480995, -0.03621361777186394, -0.03835698589682579, -0.03966914489865303, -0.04013637453317642, -0.042159225791692734, -0.04378475621342659, -0.04470913112163544, -0.05470098555088043, -0.05827173963189125], "norm_scores": [0.10247769951820374, 0.10052408277988434, 0.1003088504076004, 0.10017731785774231, 0.10013052076101303, 0.09992817789316177, 0.09976587444543839, 0.09967369586229324, 0.09868273138999939, 0.09833098948001862]}}
{"id": "WebQTest-636", "question": "what timezone is the state of michigan and", "prediction": [["location.location.time_zones"], ["base.locations.states_and_provences.country", "location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.biblioness.bibs_location.state", "location.location.time_zones"], ["base.locations.countries.states_provinces_within", "location.location.time_zones"], ["location.location.time_zones", "location.location.time_zones"], ["location.administrative_division.first_level_division_of", "location.location.time_zones"], ["location.administrative_division.country", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is the state of michigan and [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> base.locations.states_and_provences.country <SEP> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> location.location.time_zones </PATH>", "<PATH> base.locations.countries.states_provinces_within <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.administrative_division.first_level_division_of <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.administrative_division.country <SEP> location.location.time_zones </PATH>"], "scores": [-1.190434159070719e-05, -0.14555272459983826, -0.15441136062145233, -0.15872733294963837, -0.1676521748304367, -0.16934750974178314, -0.17822964489459991, -0.18678852915763855, -0.18830910325050354, -0.1969052255153656], "norm_scores": [0.11654138565063477, 0.10075639188289642, 0.09986776858568192, 0.0994376689195633, 0.09855415672063828, 0.098387211561203, 0.09751719236373901, 0.09668612480163574, 0.0965392142534256, 0.09571290761232376]}}
{"id": "WebQTest-637", "question": "what type of government did soviet union have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["people.person.nationality", "people.person.nationality", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.main_country", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent"], ["people.person.nationality", "people.person.nationality", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government did soviet union have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality <SEP> location.country.form_of_government </PATH>"], "scores": [-0.007543281652033329, -0.017810186371207237, -0.14520622789859772, -0.14528928697109222, -0.14878948032855988, -0.15221495926380157, -0.15331420302391052, -0.15602675080299377, -0.15816347301006317, -0.1606113761663437], "norm_scores": [0.11222467571496964, 0.11107838153839111, 0.09779173135757446, 0.09778361022472382, 0.09744194895029068, 0.09710872918367386, 0.09700203686952591, 0.0967392772436142, 0.09653279185295105, 0.0962967798113823]}}
{"id": "WebQTest-638", "question": "what languages are there in japan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["language.language_family.geographic_distribution"], ["language.human_language.language_family"], ["language.human_language.country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages are there in japan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.014546387828886509, -0.021956652402877808, -0.031858764588832855, -0.05272744596004486, -0.13505929708480835, -0.16593585908412933, -0.17323845624923706, -0.17970828711986542, -0.18139445781707764, -0.19306887686252594], "norm_scores": [0.1102800965309143, 0.10946590453386307, 0.10838731378316879, 0.1061488538980484, 0.09775950759649277, 0.09478716552257538, 0.09409748762845993, 0.09349066019058228, 0.0933331549167633, 0.09224987775087357]}}
{"id": "WebQTest-639", "question": "what year did the bulls get rodman", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"], ["sports.pro_athlete.teams", "sports.sports_league_draft_pick.draft"], ["sports.sports_team.championships"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the bulls get rodman [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>"], "scores": [-0.017381178215146065, -0.0340285450220108, -0.03656621649861336, -0.039559781551361084, -0.04074229672551155, -0.042779743671417236, -0.04363672807812691, -0.04387317970395088, -0.04851444438099861, -0.049753591418266296], "norm_scores": [0.10225137323141098, 0.10056325048208237, 0.1003083810210228, 0.10000854730606079, 0.09989035129547119, 0.09968703985214233, 0.09960164874792099, 0.09957809746265411, 0.09911700338125229, 0.09899425506591797]}}
{"id": "WebQTest-640", "question": "who was saint paul the apostle", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["religion.religion.notable_figures"], ["book.written_work.subjects"], ["common.topic.notable_types"], ["common.topic.image"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was saint paul the apostle [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.00309902336448431, -0.05081777274608612, -0.06936141848564148, -0.08037193864583969, -0.08554092794656754, -0.09410130232572556, -0.09558826684951782, -0.10335317999124527, -0.11049924790859222, -0.11105416715145111], "norm_scores": [0.10798079520463943, 0.1029490977525711, 0.10105763375759125, 0.09995103627443314, 0.0994357243180275, 0.09858815371990204, 0.09844166040420532, 0.0976802408695221, 0.09698469191789627, 0.09693089127540588]}}
{"id": "WebQTest-642", "question": "where did vince scully go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["sports.sports_broadcast.content", "sports.sports_broadcast.content"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did vince scully go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_broadcast.content <SEP> sports.sports_broadcast.content </PATH>"], "scores": [-0.014705788344144821, -0.020568570122122765, -0.023650407791137695, -0.03135737031698227, -0.051420047879219055, -0.07974276691675186, -0.08278617262840271, -0.09034574776887894, -0.09521915763616562, -0.11058000475168228], "norm_scores": [0.10457760095596313, 0.10396628081798553, 0.10364636778831482, 0.10285063832998276, 0.10080774128437042, 0.09799264371395111, 0.09769486635923386, 0.09695912897586823, 0.09648775309324265, 0.09501694142818451]}}
{"id": "WebQTest-643", "question": "where does princess leia live", "prediction": [["fictional_universe.fictional_character.places_lived"], ["fictional_universe.fictional_character.place_of_birth"], ["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["fictional_universe.fictional_character.places_lived", "fictional_universe.fictional_setting.fictional_characters_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["fictional_universe.character_occupation.characters_with_this_occupation", "fictional_universe.fictional_setting.characters_that_have_lived_here"], ["fictional_universe.fictional_character.places_lived", "fictional_universe.fictional_setting.characters_that_have_lived_here"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_location.tv_characters"]], "ground_paths": [["fictional_universe.fictional_character.places_lived"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does princess leia live [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.places_lived </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> fictional_universe.fictional_character.places_lived <SEP> fictional_universe.fictional_setting.fictional_characters_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation <SEP> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> fictional_universe.fictional_character.places_lived <SEP> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_location.tv_characters </PATH>"], "scores": [-0.0039133550599217415, -0.03685765340924263, -0.0677337720990181, -0.07130139321088791, -0.07335958629846573, -0.07770941406488419, -0.07829449325799942, -0.07853031158447266, -0.08523080497980118, -0.089820496737957], "norm_scores": [0.1064014658331871, 0.10295324772596359, 0.09982302784919739, 0.09946753829717636, 0.0992630198597908, 0.09883218258619308, 0.09877437353134155, 0.09875108301639557, 0.09809160977602005, 0.0976424366235733]}}
{"id": "WebQTest-644", "question": "what famous people are from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["location.location.containedby", "people.person.nationality"], ["location.location.containedby", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death"], ["location.location.containedby", "location.location.people_born_here"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.containedby <SEP> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.imports_and_exports.exported_to </PATH>"], "scores": [-0.006423805374652147, -0.03562053665518761, -0.04831552132964134, -0.06129308417439461, -0.07201652973890305, -0.07341038435697556, -0.08890154212713242, -0.09849373251199722, -0.1099475622177124, -0.11153000593185425], "norm_scores": [0.10657248646020889, 0.10350590944290161, 0.1022002100944519, 0.10088247060775757, 0.09980643540620804, 0.09966742247343063, 0.09813535213470459, 0.09719852358102798, 0.09609157592058182, 0.09593963623046875]}}
{"id": "WebQTest-645", "question": "what type of art did leonardo da vinci make", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"], ["visual_art.art_form.artists"], ["interests.hobby.people_with_this_hobby"], ["visual_art.visual_artist.visual_art_forms"], ["visual_art.visual_artists.art_forms"], ["visual_art.art_period_movement.artist"], ["film.film.subjects"], ["influence.influence_node.influenced_by"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did leonardo da vinci make [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.art_form.artists </PATH>", "<PATH> interests.hobby.people_with_this_hobby </PATH>", "<PATH> visual_art.visual_artist.visual_art_forms </PATH>", "<PATH> visual_art.visual_artists.art_forms </PATH>", "<PATH> visual_art.art_period_movement.artist </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.00020156170648988336, -0.09821739792823792, -0.10272727906703949, -0.10852287709712982, -0.11689499765634537, -0.1226850375533104, -0.12418057024478912, -0.12688879668712616, -0.1275680661201477, -0.13031916320323944], "norm_scores": [0.11106276512145996, 0.10069333761930466, 0.10024025291204453, 0.0996609777212143, 0.09883008152246475, 0.09825950860977173, 0.09811267256736755, 0.09784731268882751, 0.09778087586164474, 0.09751223772764206]}}
{"id": "WebQTest-646", "question": "where did mary bell live", "prediction": [["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.children", "people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death"], ["people.person.children", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mary bell live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> people.person.children <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.01602136716246605, -0.033400990068912506, -0.03502269089221954, -0.03635839745402336, -0.04915013536810875, -0.06031183525919914, -0.06458625197410583, -0.07592799514532089, -0.07754386961460114, -0.08633391559123993], "norm_scores": [0.10379055142402649, 0.10200229287147522, 0.10183700919151306, 0.10170107334852219, 0.10040843486785889, 0.09929393231868744, 0.0988704115152359, 0.09775538742542267, 0.09759755432605743, 0.09674342721700668]}}
{"id": "WebQTest-648", "question": "who did kimberly williams marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.children", "people.person.children"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.marriage.spouse"], ["film.film.produced_by", "film.music_contributor.film"], ["people.person.parents", "people.person.parents"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kimberly williams marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.019870132207870483, -0.021618368104100227, -0.04301381856203079, -0.04612346738576889, -0.04642046242952347, -0.04858458414673805, -0.05363639071583748, -0.053820159286260605, -0.05551934614777565, -0.057569943368434906], "norm_scores": [0.10249730199575424, 0.10231827199459076, 0.10015237331390381, 0.09984142333269119, 0.09981177002191544, 0.09959600865840912, 0.09909413009881973, 0.09907592833042145, 0.0989077165722847, 0.09870511293411255]}}
{"id": "WebQTest-649", "question": "what other movies has ryan gosling been in", "prediction": [["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"]], "ground_paths": [["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.performance.actor", "film.performance.film"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["people.person.nationality", "film.film.featured_film_locations"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.film.executive_produced_by"], ["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.performance.film"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "tv.tv_program.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has ryan gosling been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_winning_work.awards_won </PATH>"], "scores": [-0.0254933163523674, -0.028341839089989662, -0.032190900295972824, -0.03333667665719986, -0.03536269813776016, -0.04039347171783447, -0.04335641488432884, -0.04390069097280502, -0.044118110090494156, -0.04437922313809395], "norm_scores": [0.10116386413574219, 0.1008761003613472, 0.1004885733127594, 0.10037350654602051, 0.10017034411430359, 0.09966768324375153, 0.09937281161546707, 0.0993187353014946, 0.09929715096950531, 0.09927122294902802]}}
{"id": "WebQTest-650", "question": "what political party does julia gillard belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.party", "government.political_party.politicians_in_this_party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.government_position_held.office_holder", "government.political_party_tenure.party"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does julia gillard belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.politician.party <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> government.politician.election_campaigns <SEP> government.election_campaign.party </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.008215748704969883, -0.022006941959261894, -0.02251042053103447, -0.07487528026103973, -0.07637464255094528, -0.08653225004673004, -0.08736246824264526, -0.09105072170495987, -0.09336880594491959, -0.10087195038795471], "norm_scores": [0.10592442005872726, 0.10447362810373306, 0.10442104190587997, 0.09909374266862869, 0.0989452674984932, 0.09794531762599945, 0.09786403179168701, 0.09750374406576157, 0.09727798402309418, 0.0965508371591568]}}
{"id": "WebQTest-651", "question": "what is the currency of germany now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["finance.currency.countries_used", "location.country.currency_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["location.country.currency_currently_used"], ["time.event.locations", "location.location.events"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of germany now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_currently_used </PATH>", "<PATH> time.event.locations <SEP> location.location.events </PATH>"], "scores": [-0.009184877388179302, -0.02058127149939537, -0.05951566994190216, -0.0818951353430748, -0.16503813862800598, -0.17199373245239258, -0.17425459623336792, -0.1762317419052124, -0.18252681195735931, -0.19120725989341736], "norm_scores": [0.11181531101465225, 0.1105482429265976, 0.10632683336734772, 0.10397372394800186, 0.0956786572933197, 0.09501545876264572, 0.09480088949203491, 0.09461364150047302, 0.09401990473270416, 0.09320730715990067]}}
{"id": "WebQTest-652", "question": "what kind of hispanic is selena gomez", "prediction": [["people.ethnicity.included_in_group"], ["people.person.ethnicity"], ["people.ethnicity.people"], ["people.ethnicity.includes_people"], ["people.ethnicity.includes_groups"], ["people.ethnicity.languages_spoken"], ["people.ethnicity.people_with_this_ethnicity"], ["music.artist.genre"], ["people.ethnicity.contains"], ["people.ethnicity.people", "people.person.ethnicity"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of hispanic is selena gomez [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.ethnicity.includes_people </PATH>", "<PATH> people.ethnicity.includes_groups </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> people.ethnicity.contains </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.ethnicity </PATH>"], "scores": [-0.01173540111631155, -0.027074474841356277, -0.03295115381479263, -0.04154704511165619, -0.07056872546672821, -0.08909457176923752, -0.09703300148248672, -0.09919197857379913, -0.1115826889872551, -0.11454946547746658], "norm_scores": [0.10588057339191437, 0.10426885634660721, 0.10365790128707886, 0.10277068614959717, 0.0998309776186943, 0.09799854457378387, 0.0972236767411232, 0.09701399505138397, 0.09581934660673141, 0.09553548693656921]}}
{"id": "WebQTest-653", "question": "who did denise van outen married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.marriage.spouse"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.marriage.spouse", "people.marriage.spouse"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did denise van outen married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>"], "scores": [-0.016789259389042854, -0.03439244255423546, -0.038158781826496124, -0.040123701095581055, -0.04653777554631233, -0.047723907977342606, -0.05057116597890854, -0.05138740688562393, -0.05146201327443123, -0.05990903079509735], "norm_scores": [0.1027214452624321, 0.10092903673648834, 0.1005496233701706, 0.10035225003957748, 0.09971063584089279, 0.09959243983030319, 0.0993092805147171, 0.09922824800014496, 0.09922084957361221, 0.0983862578868866]}}
{"id": "WebQTest-654", "question": "what are egyptian money called", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["people.person.nationality", "people.person.gender", "finance.currency.countries_used"], ["finance.currency.countries_used", "location.country.currency_used"], ["location.country.currency_used", "location.country.currency_used", "finance.currency.countries_used"], ["location.country.currency_formerly_used", "location.country.currency_used", "finance.currency.countries_used"], ["finance.currency.countries_used", "common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are egyptian money called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> people.person.nationality <SEP> people.person.gender <SEP> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used <SEP> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used <SEP> location.country.currency_used <SEP> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_used <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.008535736240446568, -0.02844931371510029, -0.05768761783838272, -0.07050860673189163, -0.07204081118106842, -0.07344210892915726, -0.07834737747907639, -0.08591458946466446, -0.08642377704381943, -0.08802790939807892], "norm_scores": [0.10576845705509186, 0.10368306934833527, 0.10069543868303299, 0.09941265732049942, 0.09926045686006546, 0.09912146627902985, 0.0986364334821701, 0.09789284318685532, 0.09784301370382309, 0.09768618643283844]}}
{"id": "WebQTest-655", "question": "what is the zip code for wyoming michigan", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.city_town.postal_codes"], ["location.city_state.postal_codes"], ["location.location.containedby", "location.postal_code.country"], ["location.us_county.hud_county_place"], ["base.biblioness.bibs_location.state", "location.location.containedby"], ["location.Citytown.postal_codes"], ["location.postal_code.state_province_region"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for wyoming michigan [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.city_town.postal_codes </PATH>", "<PATH> location.city_state.postal_codes </PATH>", "<PATH> location.location.containedby <SEP> location.postal_code.country </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> location.location.containedby </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.postal_code.state_province_region </PATH>"], "scores": [-0.001455584540963173, -0.05214589461684227, -0.08318598568439484, -0.11950548738241196, -0.12589426338672638, -0.12646977603435516, -0.13543595373630524, -0.14219382405281067, -0.1541740447282791, -0.1542557328939438], "norm_scores": [0.11128151416778564, 0.1057811975479126, 0.10254818201065063, 0.0988905131816864, 0.09826073795557022, 0.09820419549942017, 0.09732761979103088, 0.09667211025953293, 0.09552086889743805, 0.09551306068897247]}}
{"id": "WebQTest-656", "question": "who is the president of costa rica in 2012", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of costa rica in 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.0034669155720621347, -0.03242842108011246, -0.041806481778621674, -0.04907604306936264, -0.052695132791996, -0.05533911660313606, -0.056087370961904526, -0.056883975863456726, -0.06109072268009186, -0.06382495909929276], "norm_scores": [0.10446235537528992, 0.10148035734891891, 0.10053312033414841, 0.09980493783950806, 0.09944438934326172, 0.09918180108070374, 0.09910761564970016, 0.09902869910001755, 0.09861299395561218, 0.09834372252225876]}}
{"id": "WebQTest-657", "question": "where did martin luther king got shot at", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["base.culturalevent.event.entity_involved"], ["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["location.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["base.activism.activist.area_of_activism"], ["location.location.people_born_here"], ["people.person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did martin luther king got shot at [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_death </PATH>"], "scores": [-0.0002155148540623486, -0.11328670382499695, -0.11491928994655609, -0.11985146254301071, -0.12283670157194138, -0.12376084923744202, -0.12388584762811661, -0.12561120092868805, -0.13092033565044403, -0.13657093048095703], "norm_scores": [0.11165487766265869, 0.09971753507852554, 0.0995548665523529, 0.09906505048274994, 0.09876976162195206, 0.09867852181196213, 0.09866619110107422, 0.09849610924720764, 0.09797456115484238, 0.0974225103855133]}}
{"id": "WebQTest-658", "question": "who wrote the gospel according to john", "prediction": [["book.book_subject.works"], ["book.author.works_written"], ["book.written_work.author"], ["book.written_work.subjects"], ["book.book.genre"], ["book.author.book_editions_published"], ["religion.religious_text.religious_text_of"], ["book.book_subject.works", "people.person.profession"], ["religion.religion.texts"], ["book.book.author"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the gospel according to john [/INST]", "raw_output": {"paths": ["<PATH> book.book_subject.works </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book.genre </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> book.book_subject.works <SEP> people.person.profession </PATH>", "<PATH> religion.religion.texts </PATH>", "<PATH> book.book.author </PATH>"], "scores": [-0.013505137525498867, -0.02378346025943756, -0.03395722433924675, -0.07568205893039703, -0.08770808577537537, -0.09954342991113663, -0.10238895565271378, -0.10430500656366348, -0.10441989451646805, -0.10619248449802399], "norm_scores": [0.10629234462976456, 0.10520543158054352, 0.1041405200958252, 0.09988468140363693, 0.09869065880775452, 0.09752951562404633, 0.09725238382816315, 0.09706621617078781, 0.09705507010221481, 0.09688318520784378]}}
{"id": "WebQTest-659", "question": "what form of government is the uk", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["form.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["base.locations.countries.form_of_government"], ["language.human_language.countries_spoken_in"], ["locations.country.form_of_government"], ["form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government is the uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> locations.country.form_of_government </PATH>", "<PATH> form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.00753673305734992, -0.018730128183960915, -0.13697361946105957, -0.15151706337928772, -0.16418786346912384, -0.16510999202728271, -0.16549073159694672, -0.1684240847826004, -0.1720656454563141, -0.17817063629627228], "norm_scores": [0.11313137412071228, 0.1118721067905426, 0.09939609467983246, 0.09796100109815598, 0.09672757983207703, 0.09663842618465424, 0.09660164266824722, 0.09631868451833725, 0.0959685891866684, 0.09538447856903076]}}
{"id": "WebQTest-661", "question": "who did nasri play for before arsenal", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did nasri play for before arsenal [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>"], "scores": [-0.025987913832068443, -0.028839726001024246, -0.03367133438587189, -0.035660047084093094, -0.038203850388526917, -0.03820634260773659, -0.038776323199272156, -0.039619650691747665, -0.04441871866583824, -0.044521450996398926], "norm_scores": [0.10108447819948196, 0.10079661756753922, 0.100310780107975, 0.10011149197816849, 0.0998571515083313, 0.099856898188591, 0.0997999981045723, 0.09971588104963303, 0.09923847019672394, 0.09922827780246735]}}
{"id": "WebQTest-662", "question": "what currency does the czech republic use now", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country"], ["location.country.currency_currently_used"], ["location.country.currency_formerly_used", "location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["common.topic.notable_types"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does the czech republic use now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> location.country.currency_currently_used </PATH>", "<PATH> location.country.currency_formerly_used <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.013364387676119804, -0.02213110961019993, -0.027991484850645065, -0.12009210139513016, -0.12066452205181122, -0.1523740291595459, -0.16108353435993195, -0.17485366761684418, -0.17745988070964813, -0.17773029208183289], "norm_scores": [0.11044062674045563, 0.10947665572166443, 0.10883696377277374, 0.09926076233386993, 0.09920396655797958, 0.0961076021194458, 0.0952741950750351, 0.09397124499082565, 0.09372665733098984, 0.0937013104557991]}}
{"id": "WebQTest-663", "question": "what age did william penn get married", "prediction": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what age did william penn get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.014063028618693352, -0.034913692623376846, -0.036945562809705734, -0.040412575006484985, -0.04487975314259529, -0.05277816578745842, -0.05374725162982941, -0.05965285748243332, -0.06446871161460876, -0.07541101425886154], "norm_scores": [0.10340961813926697, 0.10127578675746918, 0.10107021033763885, 0.10072040557861328, 0.10027147084474564, 0.09948260337114334, 0.09938624501228333, 0.09880103915929794, 0.09832637012004852, 0.09725632518529892]}}
{"id": "WebQTest-664", "question": "what was firefox programmed in", "prediction": [["computer.software_developer.software"], ["film.film.story_by"], ["computer.software.language"], ["film.film.story_by", "media_common.netflix_title.netflix_genres"], ["computer.software.languages_used"], ["common.topic.notable_types", "common.topic.notable_types"], ["computer.software.program"], ["computer.software.developer"], ["common.topic.notable_types"], ["base.schemastaging.phone_sandbox.service_location", "location.location.containedby"]], "ground_paths": [["computer.software.languages_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was firefox programmed in [/INST]", "raw_output": {"paths": ["<PATH> computer.software_developer.software </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> computer.software.language </PATH>", "<PATH> film.film.story_by <SEP> media_common.netflix_title.netflix_genres </PATH>", "<PATH> computer.software.languages_used </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> computer.software.program </PATH>", "<PATH> computer.software.developer </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.schemastaging.phone_sandbox.service_location <SEP> location.location.containedby </PATH>"], "scores": [-0.05283907428383827, -0.05339740216732025, -0.05782196298241615, -0.05819890275597572, -0.06430435925722122, -0.06443869322538376, -0.067356638610363, -0.07012920826673508, -0.07066438347101212, -0.07719862461090088], "norm_scores": [0.10108253359794617, 0.10102611035108566, 0.10058010369539261, 0.10054219514131546, 0.09993021935224533, 0.09991678595542908, 0.09962566196918488, 0.09934982657432556, 0.09929666668176651, 0.09864996373653412]}}
{"id": "WebQTest-665", "question": "what government does new zealand have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["base.onephylogeny.type_of_thing.things_of_this_type"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["base.biblioness.bibs_location.country", "location.country.form_of_government"], ["language.human_language.main_country", "language.human_language.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does new zealand have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> language.human_language.form_of_government </PATH>"], "scores": [-0.005884487647563219, -0.022821923717856407, -0.07790755480527878, -0.10686924308538437, -0.12546969950199127, -0.12711405754089355, -0.13628634810447693, -0.14026333391666412, -0.1468886137008667, -0.14778652787208557], "norm_scores": [0.11014460772275925, 0.10829474031925201, 0.10249059647321701, 0.09956486523151398, 0.09773003309965134, 0.09756945818662643, 0.09667862206697464, 0.09629489481449127, 0.09565902501344681, 0.09557316452264786]}}
{"id": "WebQTest-666", "question": "where is isthmus of panama located on the map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["geography.isthropical_cyclone.affected_areas"], ["base.schemastaging.disputed_location_claimant.disputed_territories"], ["geography.isthorough_connection.isthorough_connections"], ["geography.isthorough_body.isthorough_bodies"], ["common.topic.notable_types"], ["base.schemastaging.disputed_location.claimed_by"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is isthmus of panama located on the map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> geography.isthropical_cyclone.affected_areas </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories </PATH>", "<PATH> geography.isthorough_connection.isthorough_connections </PATH>", "<PATH> geography.isthorough_body.isthorough_bodies </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.schemastaging.disputed_location.claimed_by </PATH>"], "scores": [-0.001141041750088334, -0.05908169224858284, -0.08769720047712326, -0.09126333147287369, -0.10179764777421951, -0.10905686765909195, -0.11700715124607086, -0.11771360784769058, -0.12751805782318115, -0.12949238717556], "norm_scores": [0.1096738651394844, 0.1034998819231987, 0.10058015584945679, 0.10022211074829102, 0.09917187690734863, 0.09845457971096039, 0.09767494350671768, 0.09760595858097076, 0.09665365517139435, 0.09646302461624146]}}
{"id": "WebQTest-667", "question": "what is the capital of republic of ireland", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.location.contains"], ["location.country.capital"], ["base.aareas.schema.administrative_area.capital"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.subdivides_place"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.first_level_divisions"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of republic of ireland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.subdivides_place </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.country.first_level_divisions </PATH>"], "scores": [-0.007353767286986113, -0.028370168060064316, -0.05605186149477959, -0.05915800482034683, -0.0714150220155716, -0.07536667585372925, -0.07993016391992569, -0.10838381201028824, -0.1195339784026146, -0.128592386841774], "norm_scores": [0.10675789415836334, 0.10453763604164124, 0.10168354958295822, 0.10136818140745163, 0.10013329982757568, 0.09973839670419693, 0.09928426891565323, 0.09649909287691116, 0.09542907774448395, 0.09456854313611984]}}
{"id": "WebQTest-668", "question": "what has kristen stewart acted in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["people.person.languages", "film.film.language"], ["people.person.place_of_birth", "film.film.subjects"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has kristen stewart acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.027119483798742294, -0.02722354792058468, -0.03457625210285187, -0.03796921297907829, -0.03840931877493858, -0.038451652973890305, -0.04250916466116905, -0.04353956878185272, -0.043871358036994934, -0.045720864087343216], "norm_scores": [0.1010858416557312, 0.10107532143592834, 0.10033486783504486, 0.09999501705169678, 0.09995102137327194, 0.09994679689407349, 0.09954207390546799, 0.09943956136703491, 0.09940657019615173, 0.09922288358211517]}}
{"id": "WebQTest-669", "question": "what did dr josef mengele do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["base.activism.activist.area_of_activism"], ["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["base.qualia.deceased_person.place_of_death", "base.qualia.deceased_person.place_of_death"], ["base.crime.criminal_conviction.convicted_person", "people.person.profession"], ["base.crime.criminal_conviction.convicted_person", "base.crime.criminal_conviction.guilty_of"], ["base.crime.crime.victim_s", "base.crime.crime_type.people_with_this_crime"], ["base.crime.criminal_conviction.convicted_person", "base.fight.crime_type.people_convicted_of_this_crime"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr josef mengele do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> base.qualia.deceased_person.place_of_death <SEP> base.qualia.deceased_person.place_of_death </PATH>", "<PATH> base.crime.criminal_conviction.convicted_person <SEP> people.person.profession </PATH>", "<PATH> base.crime.criminal_conviction.convicted_person <SEP> base.crime.criminal_conviction.guilty_of </PATH>", "<PATH> base.crime.crime.victim_s <SEP> base.crime.crime_type.people_with_this_crime </PATH>", "<PATH> base.crime.criminal_conviction.convicted_person <SEP> base.fight.crime_type.people_convicted_of_this_crime </PATH>"], "scores": [-0.0022434762213379145, -0.05710172653198242, -0.07074633240699768, -0.07412513345479965, -0.08190075308084488, -0.0893123596906662, -0.0945160761475563, -0.10022521018981934, -0.10251113027334213, -0.10355446487665176], "norm_scores": [0.10778339207172394, 0.10202984511852264, 0.10064713656902313, 0.1003076434135437, 0.09953071177005768, 0.09879574924707413, 0.09828298538923264, 0.09772347658872604, 0.09750034660100937, 0.09739867597818375]}}
{"id": "WebQTest-670", "question": "what kind of language does egypt speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.culturalevent.event.entity_involved", "location.location.events", "language.human_language.countries_spoken_in"], ["base.culturalevent.event.entity_involved", "location.location.events", "language.human_language.main_country"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["base.militaryinfiction.location_in_fiction.languages"], ["base.culturalevent.event.entity_involved", "location.location.events", "location.country.languages_spoken"], ["base.culturalevent.event.entity_involved", "location.location.events", "location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does egypt speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.militaryinfiction.location_in_fiction.languages </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> location.country.languages_spoken </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> location.country.official_language </PATH>"], "scores": [-0.018924320116639137, -0.0244620218873024, -0.026258936151862144, -0.03607817739248276, -0.0937255248427391, -0.10226607322692871, -0.10323262959718704, -0.10499107092618942, -0.10579565912485123, -0.11398262530565262], "norm_scores": [0.10547496378421783, 0.10489248484373093, 0.10470417886972427, 0.10368108749389648, 0.09787316620349884, 0.09704083949327469, 0.096947081387043, 0.09677676111459732, 0.09669892489910126, 0.09591048955917358]}}
{"id": "WebQTest-671", "question": "who are the senators of hawaii 2012", "prediction": [["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.place_of_birth"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of hawaii 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.person.place_of_birth </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>"], "scores": [-0.02323892153799534, -0.025370420888066292, -0.030767524614930153, -0.031689081341028214, -0.0334111824631691, -0.033624641597270966, -0.03429434821009636, -0.03490624576807022, -0.03555284067988396, -0.03904079645872116], "norm_scores": [0.10089805722236633, 0.10068322718143463, 0.10014128684997559, 0.10004904121160507, 0.09987690299749374, 0.09985558688640594, 0.09978873282670975, 0.09972768276929855, 0.09966322034597397, 0.09931620955467224]}}
{"id": "WebQTest-672", "question": "what form of government does north korea have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["people.person.nationality", "government.form_of_government.countries"], ["people.person.nationality", "people.person.nationality", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["people.person.nationality", "people.person.nationality", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does north korea have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.010055938735604286, -0.013540409505367279, -0.13160207867622375, -0.14028969407081604, -0.14389418065547943, -0.14659081399440765, -0.1482502520084381, -0.148672416806221, -0.1490028202533722, -0.1491435021162033], "norm_scores": [0.1112472414970398, 0.11086028069257736, 0.09851501882076263, 0.09766285866498947, 0.09731146693229675, 0.09704941511154175, 0.09688850492238998, 0.09684760123491287, 0.09681560844182968, 0.0968019962310791]}}
{"id": "WebQTest-673", "question": "where did robert kardashian go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert kardashian go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.01289282739162445, -0.018225640058517456, -0.029510075226426125, -0.041992634534835815, -0.052917759865522385, -0.05933122709393501, -0.09777703136205673, -0.09931450337171555, -0.11512617766857147, -0.11672815680503845], "norm_scores": [0.10520857572555542, 0.10464900732040405, 0.10347474366426468, 0.10219114273786545, 0.1010807678103447, 0.1004345715045929, 0.09664656221866608, 0.09649808704853058, 0.09498428553342819, 0.09483224898576736]}}
{"id": "WebQTest-675", "question": "what county is st james city fl in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"], ["location.citytown.postal_codes"], ["location.hud_county_place.county", "location.hud_county_place.county"], ["location.us_county.county_seat", "location.us_county.county_seat"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.place_with_neighborhoods.neighborhoods"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st james city fl in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.hud_county_place.county <SEP> location.hud_county_place.county </PATH>", "<PATH> location.us_county.county_seat <SEP> location.us_county.county_seat </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.place_with_neighborhoods.neighborhoods </PATH>"], "scores": [-0.02020418457686901, -0.023452986031770706, -0.02921883948147297, -0.03177788108587265, -0.057781875133514404, -0.08967594057321548, -0.0943746492266655, -0.12580101191997528, -0.1262052357196808, -0.12724128365516663], "norm_scores": [0.10527980327606201, 0.1049383282661438, 0.1043350100517273, 0.10406836122274399, 0.10139704495668411, 0.09821409732103348, 0.09775371104478836, 0.09472943842411041, 0.09469114989042282, 0.0945931002497673]}}
{"id": "WebQTest-676", "question": "what did the akkadian empire include", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.location.containedby", "location.location.containedby"], ["location.location.partially_containedby"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.partially_contains"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the akkadian empire include [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.012046154588460922, -0.02982344478368759, -0.0345173105597496, -0.059333283454179764, -0.07197591662406921, -0.08124517649412155, -0.08345412462949753, -0.08668992668390274, -0.08778688311576843, -0.0932387188076973], "norm_scores": [0.10529448837041855, 0.10343917459249496, 0.10295478254556656, 0.10043130069971085, 0.09916958957910538, 0.09825459867715836, 0.09803779423236847, 0.09772108495235443, 0.09761394560337067, 0.09708321839570999]}}
{"id": "WebQTest-677", "question": "after how many years are the olympic games held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.current_frequency"], ["olympics.olympic_games.host_city", "olympics.olympic_games.host_city"], ["time.event.recurring_event"], ["time.recurring_event.instances", "time.event.instance_of_recurring_event"], ["time.event.instance_of_recurring_event", "time.event.instance_of_recurring_event"], ["time.event.instance_of_recurring_event", "time.recurring_event.current_frequency"], ["time.event.instance_of_recurring_event", "time.recurring_event.instances"], ["time.event.instance_of_recurrence"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: after how many years are the olympic games held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> olympics.olympic_games.host_city <SEP> olympics.olympic_games.host_city </PATH>", "<PATH> time.event.recurring_event </PATH>", "<PATH> time.recurring_event.instances <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> time.recurring_event.current_frequency </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurrence </PATH>"], "scores": [-0.010613840073347092, -0.016722286120057106, -0.047630615532398224, -0.08905602991580963, -0.09295628219842911, -0.09324876964092255, -0.09819076210260391, -0.10342302173376083, -0.10426847636699677, -0.1063070297241211], "norm_scores": [0.10671647638082504, 0.10606658458709717, 0.10283839702606201, 0.09866529703140259, 0.09828123450279236, 0.09825249016284943, 0.09776812046766281, 0.09725791960954666, 0.0971757248044014, 0.09697782248258591]}}
{"id": "WebQTest-678", "question": "what system of government does guyana have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["geography.river.basin_countries", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_parent", "location.country.form_of_government"], ["people.person.nationality", "people.person.nationality", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government does guyana have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> geography.river.basin_countries <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.country.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.007546888664364815, -0.017813803628087044, -0.13378334045410156, -0.13638265430927277, -0.14192306995391846, -0.14208021759986877, -0.1431800276041031, -0.1534353494644165, -0.15762044489383698, -0.15817317366600037], "norm_scores": [0.11164544522762299, 0.11050504446029663, 0.09840501099824905, 0.09814955294132233, 0.09760726988315582, 0.09759192913770676, 0.09748466312885284, 0.0964900329709053, 0.09608705341815948, 0.0960339605808258]}}
{"id": "WebQTest-680", "question": "what team did aguero play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did aguero play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.02408679760992527, -0.025812936946749687, -0.03033299744129181, -0.03564774990081787, -0.03646901622414589, -0.03827928751707077, -0.038674067705869675, -0.04208964854478836, -0.04667137935757637, -0.04921337962150574], "norm_scores": [0.10126902163028717, 0.10109437257051468, 0.10063844919204712, 0.10010500252246857, 0.1000228151679039, 0.09984190762042999, 0.09980250895023346, 0.09946220368146896, 0.09900753945112228, 0.09875617921352386]}}
{"id": "WebQTest-681", "question": "who is the president of the european union 2012", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["organization.organization_membership.organization", "organization.organization_membership.member"], ["organization.organization.geographic_scope", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.leadership.organization", "organization.leadership.organization", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of the european union 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_membership.member </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_membership.member </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.leadership.organization <SEP> organization.leadership.organization <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.010888866148889065, -0.017701447010040283, -0.029177585616707802, -0.041257888078689575, -0.046301890164613724, -0.047148361802101135, -0.04882167652249336, -0.053293462842702866, -0.05587052181363106, -0.058012496680021286], "norm_scores": [0.10302887856960297, 0.10232937335968018, 0.10116174072027206, 0.09994702786207199, 0.09944416582584381, 0.09936002641916275, 0.09919390082359314, 0.09875131398439407, 0.09849715977907181, 0.09828640520572662]}}
{"id": "WebQTest-683", "question": "what year was kenya moore crowned miss usa", "prediction": [["people.person.nationality", "location.location.events"], ["people.person.nationality", "time.event.locations"], ["time.event.instance_of_recurring_event"], ["award.competitor.competitions", "award.competitor.competitions"], ["award.competitor.competitions_won", "award.competitor.competitions_won"], ["people.person.nationality", "organization.organization.geographic_scope"], ["award.competitor.competitions_won"], ["award.competitor.competitions", "award.competition.competitors"], ["award.competitor.competitions_won", "time.event.instance_of_recurring_event"], ["award.competitor.competitions", "award.competition.winner"]], "ground_paths": [["time.recurring_event.instances"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was kenya moore crowned miss usa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.events </PATH>", "<PATH> people.person.nationality <SEP> time.event.locations </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> award.competitor.competitions <SEP> award.competitor.competitions </PATH>", "<PATH> award.competitor.competitions_won <SEP> award.competitor.competitions_won </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> award.competitor.competitions_won </PATH>", "<PATH> award.competitor.competitions <SEP> award.competition.competitors </PATH>", "<PATH> award.competitor.competitions_won <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> award.competitor.competitions <SEP> award.competition.winner </PATH>"], "scores": [-0.016405606642365456, -0.03165562078356743, -0.045625943690538406, -0.04671742394566536, -0.05811482295393944, -0.05965742468833923, -0.05970897153019905, -0.06133859232068062, -0.06331557035446167, -0.06365002691745758], "norm_scores": [0.10346890240907669, 0.10190296918153763, 0.10048925131559372, 0.10037963092327118, 0.09924205392599106, 0.09908907860517502, 0.09908398240804672, 0.09892264753580093, 0.0987272635102272, 0.09869424998760223]}}
{"id": "WebQTest-684", "question": "what sort of government does nigeria have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.locations.countries.continent"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["people.person.nationality", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sort of government does nigeria have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.00591666717082262, -0.021306458860635757, -0.13293157517910004, -0.1381259560585022, -0.14544224739074707, -0.15493077039718628, -0.15705560147762299, -0.16338419914245605, -0.16351723670959473, -0.17095573246479034], "norm_scores": [0.11249835044145584, 0.1107802763581276, 0.09907960891723633, 0.09856629371643066, 0.09784778207540512, 0.09692374616861343, 0.09671801328659058, 0.0961078554391861, 0.09609507769346237, 0.095382921397686]}}
{"id": "WebQTest-685", "question": "where does chef ina garten live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here"], ["people.person.parents", "people.person.place_of_birth"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["people.person.nationality", "location.country.administrative_divisions"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does chef ina garten live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.country.administrative_divisions </PATH>"], "scores": [-0.019725169986486435, -0.022296447306871414, -0.029268745332956314, -0.04530078545212746, -0.053716063499450684, -0.059266481548547745, -0.06718047708272934, -0.0684131607413292, -0.06927992403507233, -0.0725635290145874], "norm_scores": [0.10312668979167938, 0.10286186635494232, 0.1021471694111824, 0.10052260011434555, 0.09968023002147675, 0.09912849962711334, 0.09834709018468857, 0.09822593629360199, 0.09814082831144333, 0.09781909734010696]}}
{"id": "WebQTest-686", "question": "when the atlanta braves won the world series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.mascot.team"], ["time.event.instance_of_recurring_event"], ["sports.mascot.team", "sports.sports_team.championships"], ["sports.mascot.team", "sports.sports_championship_event.champion"], ["sports.professional_sports_team.owner_s", "sports.sports_championship_event.champion"], ["sports.professional_sports_team.owner_s", "sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when the atlanta braves won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.mascot.team </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.mascot.team <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.mascot.team <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.010088583454489708, -0.01544489897787571, -0.05204836651682854, -0.08819782733917236, -0.10030259937047958, -0.10090762376785278, -0.10272286087274551, -0.11152829229831696, -0.1181044653058052, -0.12017647922039032], "norm_scores": [0.10736779123544693, 0.10679423809051514, 0.1029558777809143, 0.09930054098367691, 0.0981057733297348, 0.0980464369058609, 0.09786862879991531, 0.09701063483953476, 0.09637477248907089, 0.09617528319358826]}}
{"id": "WebQTest-688", "question": "what kind of language do they speak in iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do they speak in iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken <PATH>"], "scores": [-0.013683661818504333, -0.01914563961327076, -0.03455130010843277, -0.048143986612558365, -0.17768104374408722, -0.18257620930671692, -0.18281354010105133, -0.19507931172847748, -0.1951950639486313, -0.19820533692836761], "norm_scores": [0.11139098554849625, 0.11078422516584396, 0.10909059643745422, 0.1076178029179573, 0.09454245865345001, 0.09408078342676163, 0.09405846148729324, 0.09291180968284607, 0.09290105104446411, 0.092621810734272]}}
{"id": "WebQTest-689", "question": "what was the first language spoken in spain", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["people.ethnicity.languages_spoken"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first language spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.018104251474142075, -0.02113538794219494, -0.021184096112847328, -0.06869371235370636, -0.14480994641780853, -0.14889559149742126, -0.15461456775665283, -0.15987364947795868, -0.1651640236377716, -0.17411284148693085], "norm_scores": [0.10914678126573563, 0.10881644487380981, 0.108811154961586, 0.10376244783401489, 0.09615755081176758, 0.09576547890901566, 0.09521935880184174, 0.09471991658210754, 0.09422013908624649, 0.09338073432445526]}}
{"id": "WebQTest-690", "question": "what did scott joplin play", "prediction": [["music.group_member.instruments_played"], ["people.person.profession"], ["book.written_work.author"], ["music.instrument.family"], ["music.musical_group.member", "music.group_membership.role"], ["book.author.works_written"], ["music.instrument.instrumentalists"], ["music.artist.instruments_played"], ["common.topic.notable_types"], ["music.artist.instrument"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did scott joplin play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.instrument.instrumentalists </PATH>", "<PATH> music.artist.instruments_played </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.instrument </PATH>"], "scores": [-0.002002349589020014, -0.07569944858551025, -0.08248546719551086, -0.08499222248792648, -0.08865363895893097, -0.09012974798679352, -0.092958003282547, -0.09826230257749557, -0.09998393803834915, -0.12318995594978333], "norm_scores": [0.10847816616296768, 0.10077112168073654, 0.10008960217237473, 0.09983901679515839, 0.09947413206100464, 0.09932740777730942, 0.09904688596725464, 0.09852289408445358, 0.09835342317819595, 0.09609731286764145]}}
{"id": "WebQTest-691", "question": "what school did miley cyrus", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["music.artist.genre", "music.artist.genre"], ["music.artist.origin", "location.location.containedby"], ["music.artist.education", "education.education.institution"], ["music.artist.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did miley cyrus [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.origin <SEP> location.location.containedby </PATH>", "<PATH> music.artist.education <SEP> education.education.institution </PATH>", "<PATH> music.artist.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.019913872703909874, -0.02457294426858425, -0.026051824912428856, -0.03288139030337334, -0.042404886335134506, -0.05492551624774933, -0.05844435095787048, -0.06261425465345383, -0.06375633925199509, -0.07706323266029358], "norm_scores": [0.10265190154314041, 0.10217475146055222, 0.10202375054359436, 0.10132934898138046, 0.1003689244389534, 0.09912008047103882, 0.09877189993858337, 0.09836088865995407, 0.09824861586093903, 0.09694989770650864]}}
{"id": "WebQTest-693", "question": "what language do most people speak in afghanistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "location.country.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most people speak in afghanistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>"], "scores": [-0.015744837000966072, -0.018511369824409485, -0.03248461335897446, -0.04365423321723938, -0.11955680698156357, -0.1202906146645546, -0.1328030526638031, -0.1340152472257614, -0.15358614921569824, -0.1579762101173401], "norm_scores": [0.10785289108753204, 0.10755492746829987, 0.10606248676776886, 0.10488440096378326, 0.09721803665161133, 0.09714671969413757, 0.09593874961137772, 0.0958225205540657, 0.09396541863679886, 0.0935538113117218]}}
{"id": "WebQTest-694", "question": "who is the prime minister of jamaica west indies", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of jamaica west indies [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.010955354198813438, -0.02731241285800934, -0.031092002987861633, -0.03565243259072304, -0.03920035436749458, -0.04023156315088272, -0.04156829044222832, -0.04289064183831215, -0.05052987113595009, -0.052969738841056824], "norm_scores": [0.10265655815601349, 0.10099105536937714, 0.10061007738113403, 0.10015229880809784, 0.09979759156703949, 0.09969472885131836, 0.09956155717372894, 0.09942998737096786, 0.09867331385612488, 0.09843286126852036]}}
{"id": "WebQTest-695", "question": "what role did paul mccartney play in the beatles", "prediction": [["music.musical_group.member", "music.group_membership.role"], ["music.musical_group.member", "music.performance_role.regular_performances"], ["music.producer.releases_produced", "music.group_membership.member", "music.group_membership.role"], ["music.artist.genre", "music.artist.genre"], ["music.producer.releases_produced", "music.group_membership.member", "music.performance_role.regular_performances"], ["music.group_member.instruments_played", "music.group_membership.role", "music.group_membership.role"], ["music.producer.releases_produced", "music.group_member.membership", "music.group_membership.role"], ["music.group_membership.member", "music.group_membership.role"], ["music.group_membership.group", "music.group_membership.role"], ["music.producer.releases_produced", "music.group_member.membership"]], "ground_paths": [["music.group_member.instruments_played", "music.instrument.variation"], ["music.group_member.membership", "music.performance_role.regular_performances"], ["music.group_member.membership", "music.group_membership.role"], ["music.group_membership.member", "music.group_membership.role"], ["music.group_member.instruments_played", "music.instrument.family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did paul mccartney play in the beatles [/INST]", "raw_output": {"paths": ["<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.musical_group.member <SEP> music.performance_role.regular_performances </PATH>", "<PATH> music.producer.releases_produced <SEP> music.group_membership.member <SEP> music.group_membership.role </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.producer.releases_produced <SEP> music.group_membership.member <SEP> music.performance_role.regular_performances </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_membership.role <SEP> music.group_membership.role </PATH>", "<PATH> music.producer.releases_produced <SEP> music.group_member.membership <SEP> music.group_membership.role </PATH>", "<PATH> music.group_membership.member <SEP> music.group_membership.role </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> music.producer.releases_produced <SEP> music.group_member.membership </PATH>"], "scores": [-0.013375352136790752, -0.021119240671396255, -0.025286488234996796, -0.049723297357559204, -0.049891673028469086, -0.051901575177907944, -0.06032266840338707, -0.06167842820286751, -0.06790491193532944, -0.06968772411346436], "norm_scores": [0.10340987890958786, 0.10261218249797821, 0.10218545794487, 0.09971863776445389, 0.09970185160636902, 0.09950166195631027, 0.09866726398468018, 0.09853358566761017, 0.09792197495698929, 0.0977475568652153]}}
{"id": "WebQTest-696", "question": "when did juventus win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.championship"], ["time.recurring_event.instances"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.season"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_team_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did juventus win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.season </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>"], "scores": [-0.005384049843996763, -0.023863304406404495, -0.0874461680650711, -0.09048954397439957, -0.09431963413953781, -0.1113770455121994, -0.12102571129798889, -0.12399140745401382, -0.1266530305147171, -0.13858243823051453], "norm_scores": [0.1089835911989212, 0.10698813945055008, 0.10039728134870529, 0.10009220987558365, 0.09970957040786743, 0.09802321344614029, 0.09708196669816971, 0.0967944785952568, 0.09653718769550323, 0.09539240598678589]}}
{"id": "WebQTest-698", "question": "who is robin roberts father", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"], ["fictional_universe.fictional_character.parents"], ["medicine.disease.notable_people_with_this_condition"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.celebrity.friend", "base.popstra.friend.friend"], ["people.person.parent"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is robin roberts father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.popstra.celebrity.friend <SEP> base.popstra.friend.friend </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.008354929275810719, -0.022556982934474945, -0.10118872672319412, -0.13790933787822723, -0.14691641926765442, -0.15122361481189728, -0.16235017776489258, -0.1648690402507782, -0.16490799188613892, -0.1678280234336853], "norm_scores": [0.11194092780351639, 0.11036238074302673, 0.10201679915189743, 0.09833863377571106, 0.09745686501264572, 0.09703800082206726, 0.09596428275108337, 0.09572286158800125, 0.09571913629770279, 0.09544004499912262]}}
{"id": "WebQTest-699", "question": "what is the capital of australia victoria state", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["base.biblioness.bibs_location.state"], ["base.aareas.schema.administrative_area.capital_city"], ["base.aareas.schema.administrative_area.capital"], ["location.administrative_division.country"], ["location.country.administrative_divisions"], ["location.us_state.capital"], ["location.country.capital"], ["location.australian_state.capital_city"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of australia victoria state [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.aareas.schema.administrative_area.capital_city </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.us_state.capital </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> location.australian_state.capital_city </PATH>"], "scores": [-0.015546261332929134, -0.03799064829945564, -0.038511134684085846, -0.04405418410897255, -0.04620113968849182, -0.04742230847477913, -0.05265728384256363, -0.05877341702580452, -0.07372866570949554, -0.0830935537815094], "norm_scores": [0.10346763581037521, 0.10117123275995255, 0.10111858695745468, 0.10055963695049286, 0.1003439649939537, 0.10022150725126266, 0.09969823062419891, 0.09909031540155411, 0.0976194217801094, 0.09670949727296829]}}
{"id": "WebQTest-700", "question": "where is french spoken most", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.local_name.locale"], ["country.official_language.countries_spoken_in"], ["location.country.form_of_government"], ["location.country.administrative_divisions"], ["country.languages_spoken.countries_spoken_in"], ["location.country.currency_used"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["base.rosetta.local_name.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is french spoken most [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> country.official_language.countries_spoken_in </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> country.languages_spoken.countries_spoken_in </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.016709886491298676, -0.023274224251508713, -0.023900004103779793, -0.09056830406188965, -0.13227884471416473, -0.1418570578098297, -0.14671717584133148, -0.1584753692150116, -0.16480842232704163, -0.16878271102905273], "norm_scores": [0.1092238649725914, 0.10850922763347626, 0.10844134539365768, 0.10144747048616409, 0.09730308502912521, 0.09637553989887238, 0.0959082767367363, 0.09478717297315598, 0.09418877959251404, 0.09381519258022308]}}
{"id": "WebQTest-701", "question": "what team is reggie bush on 2011", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "ground_paths": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is reggie bush on 2011 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.020374450832605362, -0.022737493738532066, -0.023981086909770966, -0.027078643441200256, -0.02874642051756382, -0.029614390805363655, -0.045609619468450546, -0.0465850904583931, -0.04952233284711838, -0.07403898984193802], "norm_scores": [0.10164608806371689, 0.10140618681907654, 0.10128015279769897, 0.1009669154882431, 0.10079865902662277, 0.10071121156215668, 0.09911313652992249, 0.09901649504899979, 0.09872607886791229, 0.09633508324623108]}}
{"id": "WebQTest-702", "question": "what highschool did emily osment go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.education", "education.educational_institution.students_graduates", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.school"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did emily osment go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.school </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.015064782463014126, -0.021513931453227997, -0.0224518571048975, -0.02626311406493187, -0.052808746695518494, -0.0752551257610321, -0.10517115890979767, -0.11774938553571701, -0.12147112935781479, -0.12703628838062286], "norm_scores": [0.10538524389266968, 0.10470778495073318, 0.10460962355136871, 0.10421168804168701, 0.10148172080516815, 0.09922919422388077, 0.09630461782217026, 0.09510086476802826, 0.09474758058786392, 0.09422176331281662]}}
{"id": "WebQTest-703", "question": "what currency should i take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "finance.currency.countries_used"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "finance.currency.countries_used"], ["travel.travel_destination.tour_operators"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> finance.currency.countries_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> finance.currency.countries_used </PATH>", "<PATH> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>"], "scores": [-0.0054479604586958885, -0.0330609567463398, -0.05847862362861633, -0.06809604912996292, -0.0714380145072937, -0.07214803993701935, -0.07733432203531265, -0.07826036214828491, -0.08656549453735352, -0.08772652596235275], "norm_scores": [0.10598251968622208, 0.10309606045484543, 0.10050862282514572, 0.09954661875963211, 0.09921449422836304, 0.09914407134056091, 0.09863121062517166, 0.09853992611169815, 0.09772492200136185, 0.09761153161525726]}}
{"id": "WebQTest-704", "question": "who is thor heyerdahl", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["influence.influence_node.influenced_by"], ["film.film_story_contributor.film_story_credits"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is thor heyerdahl [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.001299787894822657, -0.0576988123357296, -0.09904830157756805, -0.10300884395837784, -0.12511220574378967, -0.13056780397891998, -0.13617080450057983, -0.13700458407402039, -0.137704536318779, -0.14064258337020874], "norm_scores": [0.11102520674467087, 0.10493679344654083, 0.1006861999630928, 0.10028821229934692, 0.09809582680463791, 0.09756211936473846, 0.09701700508594513, 0.09693615138530731, 0.09686832129955292, 0.09658413380384445]}}
{"id": "WebQTest-706", "question": "where did troy aikman play football", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality", "location.location.containedby"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did troy aikman play football [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.011637682095170021, -0.014035754837095737, -0.05672772601246834, -0.058627404272556305, -0.05912256985902786, -0.06101025268435478, -0.06277512013912201, -0.06356346607208252, -0.06437858939170837, -0.06592926383018494], "norm_scores": [0.10407565534114838, 0.1038263812661171, 0.09948711097240448, 0.09929829835891724, 0.09924913942813873, 0.09906196594238281, 0.09888728708028793, 0.09880935400724411, 0.09872885793447495, 0.09857587516307831]}}
{"id": "WebQTest-707", "question": "who played cletus hogg", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played cletus hogg [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.02059147134423256, -0.027104148641228676, -0.031525030732154846, -0.03335916996002197, -0.036222778260707855, -0.03830068185925484, -0.051908448338508606, -0.0519544743001461, -0.05439194664359093, -0.0550542026758194], "norm_scores": [0.10195690393447876, 0.10129504650831223, 0.1008482277393341, 0.10066342353820801, 0.10037558525800705, 0.10016721487045288, 0.09881339967250824, 0.09880885481834412, 0.09856831282377243, 0.09850305318832397]}}
{"id": "WebQTest-708", "question": "who won fa cup 1976", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.runner_up"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_award.award", "sports.sports_award.award_winner"], ["sports.sports_award_type.winners", "sports.sports_award_winner.awards"], ["time.event.instance_of_recurring_event", "sports.sports_team.championships"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fa cup 1976 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_award.award <SEP> sports.sports_award_winner.awards </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_award.award <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_award_type.winners <SEP> sports.sports_award_winner.awards </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> sports.sports_team.championships </PATH>"], "scores": [-0.01903565786778927, -0.02349427528679371, -0.024033408612012863, -0.029843274503946304, -0.03364415839314461, -0.03512038663029671, -0.037727583199739456, -0.04269559681415558, -0.04946310073137283, -0.08007831126451492], "norm_scores": [0.10185091942548752, 0.10139781981706619, 0.10134316235780716, 0.10075608640909195, 0.10037384927272797, 0.1002257764339447, 0.09996481239795685, 0.09946941584348679, 0.09879853576421738, 0.0958196297287941]}}
{"id": "WebQTest-709", "question": "what did general robert e lee die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["base.cause_of_death.people"], ["people.person.cause_of_death"], ["base.activism.activist.area_of_activism", "people.deceased_person.cause_of_death"], ["base.culturalevent.event.entity_involved", "people.deceased_person.cause_of_death"], ["base.activism.activist.area_of_death", "people.deceased_person.cause_of_death"], ["people.person.deceased_person", "people.deceased_person.cause_of_death"], ["symbols.name_source.namesakes", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did general robert e lee die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> base.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> base.activism.activist.area_of_death <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_person <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> symbols.name_source.namesakes <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.001722640241496265, -0.040644802153110504, -0.10829762369394302, -0.11402147263288498, -0.1296643316745758, -0.1323300451040268, -0.13406682014465332, -0.13441583514213562, -0.1353805512189865, -0.13662491738796234], "norm_scores": [0.11095771938562393, 0.10672198235988617, 0.09974074363708496, 0.0991714745759964, 0.09763221442699432, 0.09737230837345123, 0.09720335155725479, 0.097169429063797, 0.09707573056221008, 0.09695500880479813]}}
{"id": "WebQTest-710", "question": "what style of art did henri matisse do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_period_movement.associated_artists"], ["visual_art.artist.associated_periods_or_movements"], ["visual_art.visual_artists.associated_periods_or_movements"], ["visual_art.visual_art_form.artists"], ["film.film_genre.films_in_this_genre"], ["visual_art.visual_art_form.associated_artists"], ["art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of art did henri matisse do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_period_movement.associated_artists </PATH>", "<PATH> visual_art.artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_artists.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> film.film_genre.films_in_this_genre </PATH>", "<PATH> visual_art.visual_art_form.associated_artists </PATH>", "<PATH> art.art_period_movement.associated_artists </PATH>"], "scores": [-0.0018668926786631346, -0.036907512694597244, -0.08214819431304932, -0.10077391564846039, -0.10934122651815414, -0.1132904589176178, -0.11918327957391739, -0.12460045516490936, -0.13100479543209076, -0.13280385732650757], "norm_scores": [0.10968714952468872, 0.10591020435094833, 0.10122551769018173, 0.09935756772756577, 0.09850998222827911, 0.09812171012163162, 0.09754519909620285, 0.09701819717884064, 0.09639885276556015, 0.09622558206319809]}}
{"id": "WebQTest-711", "question": "who plays chuck bass in gossip girl", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "ground_paths": [["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["base.gossipgirl.character.played_by"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays chuck bass in gossip girl [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.02165987156331539, -0.03118768520653248, -0.03398638218641281, -0.03479130566120148, -0.037766195833683014, -0.03915407881140709, -0.04005508869886398, -0.04217929393053055, -0.04406171664595604, -0.048803556710481644], "norm_scores": [0.10158025473356247, 0.10061701387166977, 0.10033581405878067, 0.1002550795674324, 0.09995727241039276, 0.0998186394572258, 0.09972874075174332, 0.09951712936162949, 0.09932997077703476, 0.09886007755994797]}}
{"id": "WebQTest-712", "question": "what countries have territories in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["book.written_work.subjects"], ["base.locations.countries.continent"], ["book.book_subject.works"], ["common.topic.notable_types"], ["book.book_subject.works", "book.written_work.subjects"], ["location.administrative_division.first_level_division_of"], ["location.administrative_division.country"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have territories in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.book_subject.works <SEP> book.written_work.subjects </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.008869275450706482, -0.03465424105525017, -0.04401019960641861, -0.0630752295255661, -0.06389296799898148, -0.06456782668828964, -0.07421264052391052, -0.08416566252708435, -0.10785778611898422, -0.10850349813699722], "norm_scores": [0.10576779395341873, 0.10307543724775314, 0.10211556404829025, 0.10018716752529144, 0.10010527819395065, 0.10003774613142014, 0.09907753765583038, 0.09809631109237671, 0.0957995131611824, 0.09573766589164734]}}
{"id": "WebQTest-713", "question": "what county is st. louis park in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.us_county.hud_county_place"], ["location.location.contains"], ["location.hud_county_place.county"], ["location.location.containedby", "location.location.containedby"], ["location.citytown.postal_codes"], ["location.location.containedby", "location.us_county.county_seat"], ["location.hud_county_place.county", "location.hud_county_place.county"], ["location.place_with_neighborhoods.neighborhoods"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st. louis park in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby <SEP> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county <SEP> location.hud_county_place.county </PATH>", "<PATH> location.place_with_neighborhoods.neighborhoods </PATH>"], "scores": [-0.006434126757085323, -0.03965459391474724, -0.046826716512441635, -0.05180521681904793, -0.05196350812911987, -0.08278500288724899, -0.0969676822423935, -0.10188093781471252, -0.10561220347881317, -0.10644495487213135], "norm_scores": [0.10640355944633484, 0.10292685031890869, 0.10219129174947739, 0.10168379545211792, 0.10166770219802856, 0.0985819473862648, 0.0971936583518982, 0.09671729058027267, 0.09635709226131439, 0.09627687931060791]}}
{"id": "WebQTest-714", "question": "what is the political system of south africa", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["base.locations.countries.continent"], ["people.person.nationality", "government.form_of_government.countries"], ["symbols.flag_use.flag_user", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system of south africa [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> symbols.flag_use.flag_user <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.011553299613296986, -0.012711015529930592, -0.07647979259490967, -0.09097196906805038, -0.09151474386453629, -0.11821792274713516, -0.12766045331954956, -0.1514209806919098, -0.15488159656524658, -0.1619633287191391], "norm_scores": [0.10907160490751266, 0.10894539952278137, 0.10221496224403381, 0.10074432939291, 0.10068965703248978, 0.0980365127325058, 0.09711515158414841, 0.09483484923839569, 0.09450723230838776, 0.09384031593799591]}}
{"id": "WebQTest-716", "question": "what language group does germany belong to", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.human_language.language_family"], ["language.language_family.geographic_distribution"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.region"], ["language.human_language.country"], ["language.language_family.languages"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language group does germany belong to [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> language.language_family.languages </PATH>"], "scores": [-0.012729897163808346, -0.02787572517991066, -0.03566282242536545, -0.03960458189249039, -0.06951752305030823, -0.09224620461463928, -0.13856029510498047, -0.14941538870334625, -0.17195473611354828, -0.17796163260936737], "norm_scores": [0.10800713300704956, 0.10638359934091568, 0.10555840283632278, 0.10514312982559204, 0.10204456746578217, 0.09975138306617737, 0.09523684531450272, 0.09420862793922424, 0.09210897237062454, 0.0915573462843895]}}
{"id": "WebQTest-717", "question": "what timezone is tucson arizona in right now", "prediction": [["location.location.time_zones"], ["location.location.time_zones", "location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"], ["location.location.time_zonessymbol"], ["locations.location.time_zones"], ["location.location.time_zonescommon"], ["location.location.time_zonespeople"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is tucson arizona in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.location.time_zonessymbol </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> location.location.time_zonescommon </PATH>", "<PATH> location.location.time_zonespeople </PATH>"], "scores": [-8.922633242036682e-06, -0.1742555797100067, -0.18192875385284424, -0.19505347311496735, -0.19706811010837555, -0.20452888309955597, -0.21871785819530487, -0.22451654076576233, -0.22693882882595062, -0.22814559936523438], "norm_scores": [0.12007405608892441, 0.10087297111749649, 0.10010191798210144, 0.09879669547080994, 0.09859785437583923, 0.09786497801542282, 0.0964861810207367, 0.09592830389738083, 0.09569622576236725, 0.0955808013677597]}}
{"id": "WebQTest-718", "question": "when was lucy lawless born", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["people.person.nationality", "location.location.containedby"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.place_of_birth", "location.location.containedby"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.seasons"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.seasons"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.seasons_of_birth"], ["film.actor.film", "film.performance.character"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was lucy lawless born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.seasons_of_birth </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.0007215573568828404, -0.07637301832437515, -0.08276374638080597, -0.09099090844392776, -0.1007550060749054, -0.11171984672546387, -0.1131768450140953, -0.11964607238769531, -0.12267647683620453, -0.1264597326517105], "norm_scores": [0.10976524651050568, 0.10176767408847809, 0.10111938416957855, 0.10029087215662003, 0.09931638836860657, 0.09823334962129593, 0.09809032082557678, 0.09745780378580093, 0.09716290980577469, 0.09679601341485977]}}
{"id": "WebQTest-719", "question": "which political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["base.politicalconventions.presidential_nominee.nominated_at", "base.politicalconventions.political_convention.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.politician.party", "government.political_party.politicians_in_this_party"], ["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["government.election_campaign.candidate", "government.election_campaign.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.party", "government.political_party_tenure.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.politician.election_campaigns <SEP> government.election_campaign.party </PATH>", "<PATH> base.politicalconventions.presidential_nominee.nominated_at <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.politician.party <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.01918364129960537, -0.02142876759171486, -0.022070715203881264, -0.034810036420822144, -0.03991980105638504, -0.049825116991996765, -0.05218742415308952, -0.06286012381315231, -0.06393121927976608, -0.07478015869855881], "norm_scores": [0.1025049239397049, 0.102275051176548, 0.1022094190120697, 0.10091559588909149, 0.10040125995874405, 0.09941165149211884, 0.09917709231376648, 0.09812423586845398, 0.09801919758319855, 0.09696153551340103]}}
{"id": "WebQTest-721", "question": "what kind of leukemia did jill clayburgh have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.deceased_person.cause_of_death"], ["people.person.medical_condition"], ["people.person.medical_condition", "medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition", "medical_condition.condition"], ["people.person.medical_condition", "medicine.notable_person_with_medical_condition.condition"], ["people.person.nationality", "medicine.disease.notable_people_with_this_condition"], ["medicine.medical_condition.notable_people_with_this_condition"], ["people.cause_of_death.people"]], "ground_paths": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of leukemia did jill clayburgh have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.medical_condition </PATH>", "<PATH> people.person.medical_condition <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition <SEP> medical_condition.condition </PATH>", "<PATH> people.person.medical_condition <SEP> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.nationality <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.medical_condition.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.011840435676276684, -0.01290363259613514, -0.04723794385790825, -0.05879443138837814, -0.060711562633514404, -0.06277082115411758, -0.07135633379220963, -0.09489380568265915, -0.10311771929264069, -0.10554304718971252], "norm_scores": [0.10518809407949448, 0.10507632046937943, 0.10152982920408249, 0.10036324709653854, 0.10017102211713791, 0.09996495395898819, 0.09911037981510162, 0.09680481255054474, 0.09601196646690369, 0.09577938914299011]}}
{"id": "WebQTest-723", "question": "what is there to see in sydney australia", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["film.film_festival.location"], ["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"], ["transportation.bridge.locale"], ["base.arthist.helynevek.intezmeny"], ["film.film.featured_film_locations"], ["geography.lake.cities"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to see in sydney australia [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> transportation.bridge.locale </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> geography.lake.cities </PATH>"], "scores": [-0.014345861971378326, -0.01608845964074135, -0.028463564813137054, -0.11115936934947968, -0.11638772487640381, -0.11647431552410126, -0.12223943322896957, -0.1310158222913742, -0.13136520981788635, -0.15731008350849152], "norm_scores": [0.10820372402667999, 0.10801532864570618, 0.10668686777353287, 0.09821925312280655, 0.09770706295967102, 0.09769860655069351, 0.09713698923587799, 0.09628821164369583, 0.09625457227230072, 0.09378937631845474]}}
{"id": "WebQTest-724", "question": "where was marilyn monroe found dead", "prediction": [["people.deceased_person.place_of_death"], ["people.cause_of_death.people"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.place_of_death.people_found_here"], ["deceased_person.place_of_death"], ["people.person.place_of_death"], ["people.place_of_death.people", "people.place_of_death.location"], ["people.place_of_death.people"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was marilyn monroe found dead [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_found_here </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> people.place_of_death.people </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.00030299738864414394, -0.08536519855260849, -0.10158272087574005, -0.11560630798339844, -0.11705472320318222, -0.11989499628543854, -0.12198811024427414, -0.12205122411251068, -0.13019558787345886, -0.1314689666032791], "norm_scores": [0.11090925335884094, 0.10186517983675003, 0.1002264991402626, 0.09883077442646027, 0.09868773818016052, 0.09840782731771469, 0.09820205718278885, 0.0981958657503128, 0.09739936888217926, 0.09727542102336884]}}
{"id": "WebQTest-725", "question": "what states does the sierra nevada run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["geography.mountain_range.mountains"], ["location.location.partial_containedby"], ["book.written_work.subjects"], ["location.location.partly_containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states does the sierra nevada run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> location.location.partial_containedby </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> location.location.partly_containedby </PATH>"], "scores": [-0.007088066544383764, -0.027231616899371147, -0.04350261017680168, -0.09235139191150665, -0.12790943682193756, -0.13051387667655945, -0.13620424270629883, -0.14442382752895355, -0.14469154179096222, -0.14500555396080017], "norm_scores": [0.10957928001880646, 0.10739404708147049, 0.10566078871488571, 0.10062342137098312, 0.09710831195116043, 0.09685572981834412, 0.09630615264177322, 0.09551779925823212, 0.09549223631620407, 0.09546224772930145]}}
{"id": "WebQTest-726", "question": "what language is spoken in greek", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["language.human_language.language_family"], ["language.human_language.region"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.writing_system"], ["language.human_language.country"], ["language.human_language.dialects"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in greek [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.writing_system </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> language.human_language.dialects </PATH>"], "scores": [-0.014315666630864143, -0.018232053145766258, -0.04292186722159386, -0.053426291793584824, -0.08939433842897415, -0.13946858048439026, -0.14474022388458252, -0.16530993580818176, -0.17347703874111176, -0.17680993676185608], "norm_scores": [0.10893221199512482, 0.10850642621517181, 0.10586021840572357, 0.10475404560565948, 0.10105319321155548, 0.09611763805150986, 0.09561227262020111, 0.09366564452648163, 0.09290377795696259, 0.09259466081857681]}}
{"id": "WebQTest-728", "question": "what is the name of the speaker of the house of representatives now 2011", "prediction": [["common.topic.image", "common.topic.image"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["common.topic.image"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["common.topic.image", "common.image.appears_in_topic_gallery"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the speaker of the house of representatives now 2011 [/INST]", "raw_output": {"paths": ["<PATH> common.topic.image <SEP> common.topic.image </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.010228220373392105, -0.016421332955360413, -0.0259590744972229, -0.030060602352023125, -0.04299347847700119, -0.051379766315221786, -0.05713342875242233, -0.05769769102334976, -0.06285270303487778, -0.06493302434682846], "norm_scores": [0.103206105530262, 0.10256890952587128, 0.10159529000520706, 0.10117944329977036, 0.09987933188676834, 0.09904521703720093, 0.09847697615623474, 0.09842143207788467, 0.09791537374258041, 0.09771188348531723]}}
{"id": "WebQTest-729", "question": "who plays stanley hudson", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["people.person.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stanley hudson [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.021965520456433296, -0.030552374199032784, -0.03254993259906769, -0.03714422881603241, -0.03951774537563324, -0.048096220940351486, -0.04971401393413544, -0.05104842782020569, -0.05235105752944946, -0.05682188272476196], "norm_scores": [0.10201530903577805, 0.10114306956529617, 0.1009412333369255, 0.10047853738069534, 0.10024033486843109, 0.09938410669565201, 0.09922344982624054, 0.09909113496541977, 0.09896214306354523, 0.0985206887125969]}}
{"id": "WebQTest-730", "question": "what record label is kelly clarkson signed with", "prediction": [["music.artist.label"], ["music.record_label.artist"], ["music.recording.artist"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nominee"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"], ["people.person.nationality", "music.artist.label"], ["music.album.artist"], ["award.award_nominee.award_nominations", "award.award_nomination.artist"]], "ground_paths": [["music.record_label.artist"], ["music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what record label is kelly clarkson signed with [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> people.person.nationality <SEP> music.artist.label </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.artist </PATH>"], "scores": [-0.0018547415966168046, -0.05834821239113808, -0.07305662333965302, -0.07400033622980118, -0.08623217791318893, -0.0930328518152237, -0.0968332514166832, -0.09875933825969696, -0.0988660678267479, -0.10269481688737869], "norm_scores": [0.10790583491325378, 0.1019788533449173, 0.10048988461494446, 0.1003950908780098, 0.09917455911636353, 0.09850239753723145, 0.09812875837087631, 0.09793993830680847, 0.09792947769165039, 0.0975552499294281]}}
{"id": "WebQTest-731", "question": "where does tim roth come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["music.artist.origin"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["music.recording.artist"], ["location.location.people_born_here", "location.location.containedby"], ["people.ethnicity.people"], ["symbols.namesake.named_after"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim roth come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.006862717680633068, -0.03256336227059364, -0.049585144966840744, -0.06653240323066711, -0.10366643965244293, -0.10965462028980255, -0.1225406676530838, -0.12511423230171204, -0.1372900754213333, -0.13774779438972473], "norm_scores": [0.10846925526857376, 0.10571704059839249, 0.10393278300762177, 0.10218624770641327, 0.09846124798059464, 0.09787340462207794, 0.09662029892206192, 0.09637196362018585, 0.09520566463470459, 0.09516210108995438]}}
{"id": "WebQTest-733", "question": "what currency should you take to vietnam", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["travel.travel_destination.tour_operators", "finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["base.monetarydebase.debased_currency.issuing_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to vietnam [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.011368771083652973, -0.01625196263194084, -0.07132072001695633, -0.07567736506462097, -0.08522595465183258, -0.08734876662492752, -0.09462880343198776, -0.10030817240476608, -0.10526838898658752, -0.1074611246585846], "norm_scores": [0.10656367242336273, 0.10604456812143326, 0.10036271065473557, 0.0999264195561409, 0.09897680580615997, 0.09876690804958344, 0.09805049747228622, 0.09749521315097809, 0.09701282531023026, 0.09680032730102539]}}
{"id": "WebQTest-734", "question": "who did roger federer married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did roger federer married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.024527261033654213, -0.0348198264837265, -0.04024740308523178, -0.04104727506637573, -0.04799988865852356, -0.04962778091430664, -0.05016528069972992, -0.050223853439092636, -0.05065450817346573, -0.05254436284303665], "norm_scores": [0.10198158025741577, 0.10093730688095093, 0.10039094090461731, 0.10031068325042725, 0.0996156707406044, 0.0994536429643631, 0.09940019994974136, 0.09939437359571457, 0.09935159236192703, 0.09916400164365768]}}
{"id": "WebQTest-735", "question": "what kind of money do i need in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"], ["government.form_of_government.countries", "location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["common.topic.notable_types"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i need in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0065006171353161335, -0.021518640220165253, -0.10484939068555832, -0.1303589940071106, -0.13467058539390564, -0.1346755176782608, -0.13498027622699738, -0.13825711607933044, -0.14569368958473206, -0.15448151528835297], "norm_scores": [0.11082998663187027, 0.10917797684669495, 0.100448839366436, 0.09791884571313858, 0.09749756753444672, 0.09749708324670792, 0.09746737778186798, 0.09714851528406143, 0.09642873704433441, 0.09558505564928055]}}
{"id": "WebQTest-737", "question": "what is the name of the capital of belgium", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"], ["location.location.containedby"], ["location.location.contains"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["book.newspaper.circulation_areas"], ["location.administrative_division.country"], ["location.country.first_level_divisions"], ["location.administrative_division.first_level_division_of"], ["book.written_work.subjects"]], "ground_paths": [["location.country.capital"], ["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the capital of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.011884222738444805, -0.022037001326680183, -0.030092084780335426, -0.08812765032052994, -0.10068240761756897, -0.10379832237958908, -0.1040068194270134, -0.11074063181877136, -0.11684903502464294, -0.12972205877304077], "norm_scores": [0.1071498766541481, 0.10606751590967178, 0.10521655529737473, 0.09928407520055771, 0.09804537892341614, 0.09774035215377808, 0.09771997481584549, 0.09706415981054306, 0.09647305309772491, 0.09523911774158478]}}
{"id": "WebQTest-738", "question": "what kind of government does japan have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["base.onephylogeny.type_of_thing.things_of_this_type"], ["language.human_language.main_country", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["form.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does japan have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.009330508299171925, -0.014852764084935188, -0.12179790437221527, -0.1318976730108261, -0.13262230157852173, -0.1474500447511673, -0.14888142049312592, -0.1584693044424057, -0.16589370369911194, -0.16674697399139404], "norm_scores": [0.11150272935628891, 0.11088867485523224, 0.09964179992675781, 0.09864049404859543, 0.09856903553009033, 0.09711827337741852, 0.09697935730218887, 0.09605397284030914, 0.09534347057342529, 0.09526215493679047]}}
{"id": "WebQTest-739", "question": "what pieces of music did claude debussy compose", "prediction": [["music.composition.composer"], ["music.artist.genre"], ["music.recording.artist"], ["music.composer.compositions"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["music.album.artist"], ["book.author.works_written"], ["music.composer.works_written"], ["film.film.music"]], "ground_paths": [["opera.opera.librettist"], ["book.author.works_written", "book.book.editions"], ["book.written_work.author"], ["music.lyricist.lyrics_written"], ["base.musiteca.music_teacher.composition"], ["music.recording.artist"], ["music.composition.lyricist"], ["music.composition.composer"], ["opera.librettist.libretti"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what pieces of music did claude debussy compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.composer.works_written </PATH>", "<PATH> film.film.music </PATH>"], "scores": [-0.007459812797605991, -0.034073300659656525, -0.053539808839559555, -0.05637165531516075, -0.07122307270765305, -0.07847291231155396, -0.0848187506198883, -0.08507231622934341, -0.10352743417024612, -0.10561727732419968], "norm_scores": [0.1061973124742508, 0.10340830683708191, 0.1014147698879242, 0.101127989590168, 0.09963719546794891, 0.09891745448112488, 0.09829172492027283, 0.09826680272817612, 0.09646991640329361, 0.09626851230859756]}}
{"id": "WebQTest-740", "question": "who was selena gomez in barney and friends", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.character"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["people.person.nationality", "tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["film.performance.actor", "film.performance.character"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_guest_role.character"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.character"], ["film.actor.film", "film.performance.character"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was selena gomez in barney and friends [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> people.person.nationality <SEP> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.024202074855566025, -0.026712574064731598, -0.033517807722091675, -0.04076400399208069, -0.041644386947155, -0.045986440032720566, -0.046752240508794785, -0.051301009953022, -0.05349436774849892, -0.05420536920428276], "norm_scores": [0.10177602618932724, 0.101520836353302, 0.10083231329917908, 0.10010429471731186, 0.10001620650291443, 0.09958287328481674, 0.09950664639472961, 0.09905503690242767, 0.09883800894021988, 0.09876776486635208]}}
{"id": "WebQTest-741", "question": "who plays caesar flickerman in the hunger games", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.film.starring", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_honor.honored_for", "award.award_honor.award_winner"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays caesar flickerman in the hunger games [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_honor.award_winner </PATH>"], "scores": [-0.026359299197793007, -0.030196046456694603, -0.03866438567638397, -0.042496245354413986, -0.04255124181509018, -0.042874351143836975, -0.04362589493393898, -0.0440848246216774, -0.04591445252299309, -0.047573722898960114], "norm_scores": [0.10141526162624359, 0.10102690756320953, 0.10017498582601547, 0.09979186207056046, 0.09978637844324112, 0.09975413233041763, 0.09967919439077377, 0.09963346272706985, 0.09945133328437805, 0.09928645193576813]}}
{"id": "WebQTest-742", "question": "where did the gettysburg address", "prediction": [["book.written_work.subjects"], ["book.author.book_editions_published"], ["book.book_subject.works"], ["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speech_topic"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["book.author.works_written"], ["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speaker_s"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gettysburg address [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speech_topic </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speaker_s </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.012133320793509483, -0.04407792538404465, -0.04843371734023094, -0.05868704989552498, -0.06049077585339546, -0.062102727591991425, -0.06814102828502655, -0.07066534459590912, -0.07097958773374557, -0.0711885541677475], "norm_scores": [0.10454059392213821, 0.10125386714935303, 0.10081377625465393, 0.09978538006544113, 0.09960555285215378, 0.09944512695074081, 0.09884645789861679, 0.09859725087881088, 0.09856627881526947, 0.09854568541049957]}}
{"id": "WebQTest-743", "question": "what was the name of jfk 's brothers", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.gender", "people.person.gender"], ["people.person.place_of_birth", "people.person.place_of_birth"]], "ground_paths": [["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["people.person.gender", "people.person.gender"], ["people.person.parents", "people.person.parents"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["film.film_subject.films", "film.film.subjects"], ["people.person.nationality", "people.person.nationality"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.person.ethnicity", "people.person.ethnicity"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.profession", "people.person.profession"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["people.family.members", "people.family_member.family"], ["people.person.parents", "people.person.children"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.religion", "people.person.religion"], ["people.family.members", "people.family.members"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of jfk 's brothers [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.sibling_relationship.sibling <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.03140072897076607, -0.034389007836580276, -0.03659434616565704, -0.03797595575451851, -0.03850248456001282, -0.03914029896259308, -0.04111850634217262, -0.04172134026885033, -0.04295564070343971, -0.050330087542533875], "norm_scores": [0.10080321133136749, 0.10050243884325027, 0.10028104484081268, 0.10014258325099945, 0.10008987039327621, 0.10002605617046356, 0.09982836991548538, 0.09976821392774582, 0.09964514523744583, 0.09891302138566971]}}
{"id": "WebQTest-744", "question": "who was tupac signed to", "prediction": [["music.artist.label"], ["music.record_label.artist"], ["music.recording.artist"], ["music.producer.releases_produced"], ["people.person.nationality", "music.record_label.artist"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.album.artist"], ["music.featured_artist.recordings", "music.record_label.artist"], ["music.record_label.artist", "music.record_label.artist"], ["music.featured_artist.recordings"]], "ground_paths": [["music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was tupac signed to [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> people.person.nationality <SEP> music.record_label.artist </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.featured_artist.recordings <SEP> music.record_label.artist </PATH>", "<PATH> music.record_label.artist <SEP> music.record_label.artist </PATH>", "<PATH> music.featured_artist.recordings </PATH>"], "scores": [-0.009314666502177715, -0.024345794692635536, -0.05730072781443596, -0.09282765537500381, -0.10327816009521484, -0.10377826541662216, -0.10423245280981064, -0.11413294076919556, -0.11425302177667618, -0.11702392995357513], "norm_scores": [0.10768331587314606, 0.10607682168483734, 0.10263804346323013, 0.09905564039945602, 0.098025843501091, 0.09797683358192444, 0.0979323461651802, 0.09696755558252335, 0.09695591032505035, 0.09668762236833572]}}
{"id": "WebQTest-745", "question": "what was augustus caesar famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_office_category.officeholders"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was augustus caesar famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_category.officeholders </PATH>"], "scores": [-0.004824616946280003, -0.04478972405195236, -0.052225541323423386, -0.057711951434612274, -0.06389283388853073, -0.07115913182497025, -0.07383961230516434, -0.07586757838726044, -0.09044865518808365, -0.09070143103599548], "norm_scores": [0.10591137409210205, 0.1017620861530304, 0.10100820660591125, 0.10045554488897324, 0.0998365581035614, 0.09911374747753143, 0.09884842485189438, 0.09864816814661026, 0.09722020477056503, 0.09719564020633698]}}
{"id": "WebQTest-747", "question": "what language does colombia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["film.film.featured_film_locations"], ["languages.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken"], ["country.languages_spoken"], ["location.country.form_of_government"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does colombia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> languages.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> country.languages_spoken </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.010449821129441261, -0.019479025155305862, -0.05973880738019943, -0.06457656621932983, -0.15384311974048615, -0.16892105340957642, -0.179063618183136, -0.1890379786491394, -0.19129569828510284, -0.19163773953914642], "norm_scores": [0.11160442233085632, 0.11060125380754471, 0.10623691231012344, 0.10572421550750732, 0.09669555723667145, 0.0952485129237175, 0.09428733587265015, 0.09335155785083771, 0.09314102679491043, 0.09310917556285858]}}
{"id": "WebQTest-748", "question": "what is the zip code for schenectady new york", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.us_county.hud_county_place"], ["location.postal_code.citytown"], ["location.mailing_address.citytown"], ["location.Citytown.postal_codes"], ["citytown.citytown.postal_codes"], ["location.us_county.county_seat"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for schenectady new york [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.postal_code.citytown </PATH>", "<PATH> location.mailing_address.citytown </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> citytown.citytown.postal_codes </PATH>", "<PATH> location.us_county.county_seat </PATH>"], "scores": [-0.0004714093520306051, -0.06573387235403061, -0.11044556647539139, -0.14605849981307983, -0.15236473083496094, -0.1533091813325882, -0.1543761044740677, -0.1651959866285324, -0.16817133128643036, -0.1716414839029312], "norm_scores": [0.11352916061878204, 0.10635656118392944, 0.10170592367649078, 0.09814761579036713, 0.09753062576055527, 0.09743855893611908, 0.09733465313911438, 0.09628718346357346, 0.09600111097097397, 0.09566855430603027]}}
{"id": "WebQTest-749", "question": "who was president in 1988 in the united states", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["people.person.nationality"]], "ground_paths": [["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president in 1988 in the united states [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> freebase.type_profile.equivalent_topic <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.02334004081785679, -0.02371559478342533, -0.026015423238277435, -0.03014064021408558, -0.03104877471923828, -0.033921174705028534, -0.038385145366191864, -0.03907269239425659, -0.039818163961172104, -0.04489286243915558], "norm_scores": [0.10097172111272812, 0.10093381255865097, 0.1007019504904747, 0.10028738528490067, 0.10019635409116745, 0.09990896284580231, 0.09946396946907043, 0.09939559549093246, 0.09932152926921844, 0.09881878644227982]}}
{"id": "WebQTest-751", "question": "who owns google 2012", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["people.person.gender", "people.person.gender"], ["organization.organization_board_membership.organization", "organization.organization_board_membership.member"], ["organization.organization.board_members", "organization.organization_board_membership.member"], ["business.business_operation.industry", "business.business_operation.industry"], ["business.employer.employees", "business.employment_tenure.person"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["organization.organization.parent", "organization.organization.child"], ["organization.organization.board_members", "business.board_member.organization_board_memberships"]], "ground_paths": [["organization.leadership.organization", "organization.leadership.person"], ["organization.leadership.organization", "business.board_member.leader_of"], ["conferences.conference.focus", "time.event.people_involved"], ["organization.organization_founder.organizations_founded"], ["organization.organization.leadership", "business.board_member.leader_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns google 2012 [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> organization.organization_board_membership.organization <SEP> organization.organization_board_membership.member </PATH>", "<PATH> organization.organization.board_members <SEP> organization.organization_board_membership.member </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> business.employer.employees <SEP> business.employment_tenure.person </PATH>", "<PATH> organization.organization.founders <SEP> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>", "<PATH> organization.organization.board_members <SEP> business.board_member.organization_board_memberships </PATH>"], "scores": [-0.020727718248963356, -0.02240787260234356, -0.05142669752240181, -0.05211574211716652, -0.05359095707535744, -0.053594447672367096, -0.0547383576631546, -0.05731678381562233, -0.05953427776694298, -0.06116105988621712], "norm_scores": [0.10282279551029205, 0.10265018045902252, 0.09971419721841812, 0.09964552521705627, 0.09949862211942673, 0.09949827939271927, 0.09938452392816544, 0.0991286039352417, 0.09890902787446976, 0.09874825924634933]}}
{"id": "WebQTest-752", "question": "what drink did john pemberton create", "prediction": [["food.beer_country_region.beers_from_here"], ["food.beer_country_region.beers_from_here", "food.beer_country_region.beers_from_here"], ["food.beer.from_region"], ["food.beer.from_region", "food.beer_country_region.beers_from_here"], ["food.beverage_creator.beverages_created"], ["food.beer_country_region.beers_from_here", "food.beverage_country_region.beers_from_here"], ["food.beer_country_region.beers_from_here", "location.location.containedby"], ["food.beer_country_region.beers_from_here", "food.beverage_country.beers_from_here"], ["food.beer_country_region.beers_from_here", "food.beer.from_region"], ["common.topic.notable_types"]], "ground_paths": [["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drink did john pemberton create [/INST]", "raw_output": {"paths": ["<PATH> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer_country_region.beers_from_here <SEP> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer.from_region </PATH>", "<PATH> food.beer.from_region <SEP> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beverage_creator.beverages_created </PATH>", "<PATH> food.beer_country_region.beers_from_here <SEP> food.beverage_country_region.beers_from_here </PATH>", "<PATH> food.beer_country_region.beers_from_here <SEP> location.location.containedby </PATH>", "<PATH> food.beer_country_region.beers_from_here <SEP> food.beverage_country.beers_from_here </PATH>", "<PATH> food.beer_country_region.beers_from_here <SEP> food.beer.from_region </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.00849117711186409, -0.027082310989499092, -0.045097220689058304, -0.04977233707904816, -0.05525078624486923, -0.06477254629135132, -0.07287294417619705, -0.07891304045915604, -0.08012766391038895, -0.08242359757423401], "norm_scores": [0.10488736629486084, 0.1029554083943367, 0.10111728310585022, 0.10064565390348434, 0.10009577125310898, 0.09914720803499222, 0.09834732860326767, 0.09775508940219879, 0.09763641655445099, 0.09741251170635223]}}
{"id": "WebQTest-753", "question": "what position did stanley matthews play", "prediction": [["sports.pro_athlete.teams", "sports.sports_position.teams"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_position.sport", "sports.sports_position.sport"], ["sports.pro_athlete.teams", "sports.sports_team_roster.position"], ["sports.sports_position.sport", "sports.sports_team_coach.sports_coached", "sports.sports_team_coach.sports_coached", "sports.sports_team.sport"], ["sports.sports_position.sport", "sports.sports_team_coach.sports_coached", "sports.sports_team.sport", "sports.sports_team.sport"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["soccer.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did stanley matthews play [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_position.teams </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_position.sport </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_coach.sports_coached <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team_coach.sports_coached <SEP> sports.sports_team.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.030756305903196335, -0.03447099030017853, -0.035692282021045685, -0.03729851171374321, -0.042837899178266525, -0.044439442455768585, -0.04602043330669403, -0.049531273543834686, -0.050174567848443985, -0.05276184156537056], "norm_scores": [0.10116845369338989, 0.10079333931207657, 0.10067032277584076, 0.10050874948501587, 0.0999535322189331, 0.09979358315467834, 0.09963592886924744, 0.09928673505783081, 0.09922288358211517, 0.09896649420261383]}}
{"id": "WebQTest-755", "question": "what was bonnie prince charlie full name", "prediction": [["people.person.nationality"], ["common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["people.person.children"], ["symbols.name_source.namesakes"], ["people.person.gender", "people.person.gender"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["people.person.place_of_birth"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was bonnie prince charlie full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.007693396415561438, -0.05173637717962265, -0.05184893682599068, -0.0553831085562706, -0.06196504086256027, -0.06929563730955124, -0.07636789977550507, -0.07665497809648514, -0.08100654929876328, -0.08597806096076965], "norm_scores": [0.10553440451622009, 0.10098722577095032, 0.10097585618495941, 0.10061962157487869, 0.09995952248573303, 0.09922944009304047, 0.09853014349937439, 0.09850186109542847, 0.09807415306568146, 0.09758778661489487]}}
{"id": "WebQTest-756", "question": "who makes the voice of stewie griffin", "prediction": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.tv_program_creator.programs_created"], ["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who makes the voice of stewie griffin [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>"], "scores": [-0.019446443766355515, -0.021859049797058105, -0.022562282159924507, -0.0504775196313858, -0.05106775090098381, -0.051785826683044434, -0.05346893519163132, -0.062060680240392685, -0.06219865381717682, -0.06375380605459213], "norm_scores": [0.10266291350126266, 0.1024155244231224, 0.10234352946281433, 0.09952609241008759, 0.09946736693382263, 0.09939596801996231, 0.09922882169485092, 0.09837992489337921, 0.09836634248495102, 0.09821348637342453]}}
{"id": "WebQTest-757", "question": "who is angelina jolie mom and dad", "prediction": [["people.person.parents"], ["people.person.children"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nominee"], ["award.award_nomination.award_nominee", "award.award_nominee.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["film.film_crewmember.films_crewed", "film.film_crew_gig.film_crewmember"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.celebrity.infidelity_victim", "base.popstra.infidelity.victim"], ["award.award_winner.awards_won", "award.award_honor.award_winner"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie mom and dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> film.film_crewmember.films_crewed <SEP> film.film_crew_gig.film_crewmember </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.popstra.celebrity.infidelity_victim <SEP> base.popstra.infidelity.victim </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award_winner </PATH>"], "scores": [-0.010418022982776165, -0.015820035710930824, -0.09592162817716599, -0.09985725581645966, -0.10117190331220627, -0.10230506211519241, -0.1093120351433754, -0.11392175406217575, -0.11778724193572998, -0.11991414427757263], "norm_scores": [0.108054980635643, 0.10747283697128296, 0.09919986128807068, 0.09881021082401276, 0.09868039190769196, 0.0985686406493187, 0.09788037836551666, 0.09743022918701172, 0.09705433249473572, 0.09684812277555466]}}
{"id": "WebQTest-758", "question": "who is the governor of indiana 2009", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["location.location.containedby", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"]], "ground_paths": [["common.topic.image"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of indiana 2009 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.appointed_by </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.appointed_by </PATH>"], "scores": [-0.019072866067290306, -0.019184555858373642, -0.023726319894194603, -0.028087489306926727, -0.02813008241355419, -0.042981114238500595, -0.043406467884778976, -0.046058814972639084, -0.050289664417505264, -0.055497195571660995], "norm_scores": [0.10166256874799728, 0.10165121406316757, 0.10119058936834335, 0.10075023770332336, 0.10074593871831894, 0.09926082193851471, 0.09921861439943314, 0.09895579516887665, 0.09853801876306534, 0.09802620857954025]}}
{"id": "WebQTest-759", "question": "who brad pitt has dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["common.topic.notable_types", "common.topic.notable_types"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.insult_victim", "base.popstra.public_insult.perpetrator"], ["base.popstra.lived_with.participant", "base.popstra.celebrity.lived_with"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.place_of_birth"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who brad pitt has dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.celebrity.sexual_relationships </PATH>", "<PATH> base.popstra.dated.participant <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.01640831120312214, -0.030525121837854385, -0.03663363307714462, -0.050203267484903336, -0.05132637545466423, -0.053096525371074677, -0.053196124732494354, -0.05970264598727226, -0.05987594276666641, -0.06356700509786606], "norm_scores": [0.10314274579286575, 0.1016969308257103, 0.1010776087641716, 0.09971528500318527, 0.09960335493087769, 0.09942720085382462, 0.0994172990322113, 0.09877254068851471, 0.09875542670488358, 0.0983915850520134]}}
{"id": "WebQTest-760", "question": "what party does barack obama belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["base.politicalconventions.presidential_nominee.nominated_at", "base.politicalconventions.political_convention.party"], ["government.politician.party", "government.political_party.politicians_in_this_party"], ["organization.organization.founders"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"]], "ground_paths": [["government.election_campaign.candidate", "government.election_campaign.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.party", "government.political_party_tenure.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party does barack obama belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.politician.election_campaigns <SEP> government.election_campaign.party </PATH>", "<PATH> base.politicalconventions.presidential_nominee.nominated_at <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.politician.party <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>"], "scores": [-0.017368897795677185, -0.020057793706655502, -0.027710095047950745, -0.03523893281817436, -0.04441658779978752, -0.04852429777383804, -0.05025118961930275, -0.05933568254113197, -0.06045341491699219, -0.06742190569639206], "norm_scores": [0.10259021073579788, 0.10231472551822662, 0.10153476893901825, 0.10077320039272308, 0.09985257685184479, 0.09944324940443039, 0.09927167743444443, 0.09837392717599869, 0.09826403111219406, 0.0975816547870636]}}
{"id": "WebQTest-761", "question": "what is san francisco zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"], ["location.mailing_address.citytown"], ["location.postal_code.citytown"], ["location.country.administrative_divisions"], ["location.citytown.postal_ codes"]], "ground_paths": [["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is san francisco zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.mailing_address.citytown </PATH>", "<PATH> location.postal_code.citytown </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "</PATH> location.citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_ codes </PATH>"], "scores": [-0.0002325000532437116, -0.08640413731336594, -0.1617346554994583, -0.17257602512836456, -0.17673711478710175, -0.18584492802619934, -0.18826726078987122, -0.18874838948249817, -0.18926696479320526, -0.189758762717247], "norm_scores": [0.1164054349064827, 0.10679463297128677, 0.0990452766418457, 0.09797728806734085, 0.09757044911384583, 0.09668583422899246, 0.09645190834999084, 0.0964055135846138, 0.09635552763938904, 0.0963081493973732]}}
{"id": "WebQTest-762", "question": "who has stephen ireland played for", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has stephen ireland played for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.03182661160826683, -0.03207710012793541, -0.033192966133356094, -0.03473082184791565, -0.03730229660868645, -0.037310365587472916, -0.038189906626939774, -0.04301358759403229, -0.04611827805638313, -0.04839520901441574], "norm_scores": [0.10063941776752472, 0.10061421245336533, 0.10050200670957565, 0.10034756362438202, 0.10008985549211502, 0.10008904337882996, 0.10000105947256088, 0.09951984882354736, 0.09921134263277054, 0.0989857017993927]}}
{"id": "WebQTest-764", "question": "where did john mayer go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.parent_institution"], ["music.artist.education", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john mayer go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>", "<PATH> music.artist.education <SEP> education.education.institution </PATH>"], "scores": [-0.015168563462793827, -0.01916445977985859, -0.02106182649731636, -0.042098164558410645, -0.07768134772777557, -0.10505972802639008, -0.11028726398944855, -0.11091023683547974, -0.1139799952507019, -0.12048625946044922], "norm_scores": [0.10592194646596909, 0.10549953579902649, 0.10529955476522446, 0.10310757160186768, 0.09950318187475204, 0.09681591391563416, 0.09631112217903137, 0.09625114500522614, 0.09595612436532974, 0.09533383697271347]}}
{"id": "WebQTest-765", "question": "what celebrities has brody jenner dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["base.popstra.canoodled.participant", "base.popstra.canoodled.participant"], ["base.popstra.dated.participant", "base.popstra.dated.participant"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.breakup.participant", "base.popstra.breakup.participant"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["people.person.nationality", "people.person.nationality"], ["people.person.gender", "people.person.gender"], ["base.popstra.breakup.participant", "base.popstra.celebrity.breakup"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["base.popstra.canoodled.participant", "base.popstra.celebrity.canoodled"], ["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what celebrities has brody jenner dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.canoodled.participant </PATH>", "<PATH> base.popstra.celebrity.breakup <SEP> base.popstra.celebrity.breakup </PATH>", "<PATH> base.popstra.dated.participant <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> celebrities.romantic_relationship.celebrity <SEP> celebrities.celebrity.sexual_relationships </PATH>", "<PATH> base.popstra.canoodled.participant <SEP> base.popstra.canoodled.participant </PATH>", "<PATH> base.popstra.dated.participant <SEP> base.popstra.dated.participant </PATH>"], "scores": [-0.01673721708357334, -0.027846815064549446, -0.033326130360364914, -0.03601652756333351, -0.03835710510611534, -0.040582478046417236, -0.04147281497716904, -0.04965541511774063, -0.05265995115041733, -0.05542204901576042], "norm_scores": [0.10226613283157349, 0.10113628208637238, 0.10058364272117615, 0.10031340271234512, 0.10007888078689575, 0.0998564213514328, 0.09976755827665329, 0.09895452111959457, 0.09865766018629074, 0.0983855351805687]}}
{"id": "WebQTest-766", "question": "what is malaysian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["finance.currency.countries_used", "location.country.currency_used"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.currency_used", "location.location.time_zones"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["location.country.currency_used", "location.country.currency_used"], ["language.human_language.countries_spoken_in", "location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is malaysian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.country.currency_used <SEP> location.location.time_zones </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.currency_used </PATH>"], "scores": [-0.0069463117979466915, -0.023781146854162216, -0.12776519358158112, -0.13320469856262207, -0.14772982895374298, -0.15377236902713776, -0.15670457482337952, -0.1582796573638916, -0.16168135404586792, -0.16255956888198853], "norm_scores": [0.11215768754482269, 0.11028533428907394, 0.09939352422952652, 0.09885433316230774, 0.09742884337902069, 0.09684189409017563, 0.0965583547949791, 0.09640638530254364, 0.09607899934053421, 0.09599465876817703]}}
{"id": "WebQTest-768", "question": "who owns volkswagen car company", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization_board_membership.organization", "organization.organization_board_membership.member"], ["organization.organization.board_members", "business.board_member.organization_board_memberships"], ["business.employer.employees", "business.employment_tenure.person"], ["organization.organization.board_members", "organization.organization_board_membership.member"], ["organization.organization_board_membership.organization", "business.board_member.organization_board_memberships"], ["business.employer.employees", "people.person.employment_history"], ["business.business_operation.industry", "business.business_operation.industry"], ["business.employment_tenure.company", "business.employment_tenure.person"]], "ground_paths": [["organization.organization.parent", "organization.organization_relationship.parent"], ["business.business_operation.industry", "business.business_operation.industry"], ["base.schemastaging.corresponding_entity.entities", "base.schemastaging.entity_hint.corresponds_to"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns volkswagen car company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_board_membership.organization <SEP> organization.organization_board_membership.member </PATH>", "<PATH> organization.organization.board_members <SEP> business.board_member.organization_board_memberships </PATH>", "<PATH> business.employer.employees <SEP> business.employment_tenure.person </PATH>", "<PATH> organization.organization.board_members <SEP> organization.organization_board_membership.member </PATH>", "<PATH> organization.organization_board_membership.organization <SEP> business.board_member.organization_board_memberships </PATH>", "<PATH> business.employer.employees <SEP> people.person.employment_history </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> business.employment_tenure.company <SEP> business.employment_tenure.person </PATH>"], "scores": [-0.025539163500070572, -0.030533574521541595, -0.03231243044137955, -0.039192572236061096, -0.039375633001327515, -0.04221916198730469, -0.04726801812648773, -0.04800233244895935, -0.055927202105522156, -0.05926566943526268], "norm_scores": [0.10165058821439743, 0.10114416480064392, 0.10096440464258194, 0.10027213394641876, 0.10025378316640854, 0.09996911883354187, 0.09946566075086594, 0.09939264506101608, 0.09860808402299881, 0.0982794389128685]}}
{"id": "WebQTest-769", "question": "which is the main train station in rome", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["trains.travel_destination.tourist_attractions"], ["travel.transportation.mode_of_transportation"], ["travel.transport_terminus.travel_destinations_served"], ["trainsdaily.travel_destination.travel_destinations"], ["base.biblioness.bibs_location.city"], ["location.location.nearby_trains"], ["travel.travel_destination.transport_terminus"], ["travel.tourist_station.serves"]], "ground_paths": [["travel.travel_destination.how_to_get_here", "travel.transportation.transport_terminus"], ["location.location.containedby", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which is the main train station in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> trains.travel_destination.tourist_attractions </PATH>", "<PATH> travel.transportation.mode_of_transportation </PATH>", "<PATH> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> trainsdaily.travel_destination.travel_destinations </PATH>", "<PATH> base.biblioness.bibs_location.city </PATH>", "<PATH> location.location.nearby_trains </PATH>", "<PATH> travel.travel_destination.transport_terminus </PATH>", "<PATH> travel.tourist_station.serves </PATH>"], "scores": [-0.002901635132730007, -0.06388771533966064, -0.07254904508590698, -0.08638065308332443, -0.08935307711362839, -0.09177060425281525, -0.09777581691741943, -0.10009226948022842, -0.10402066260576248, -0.10437329113483429], "norm_scores": [0.10811023414134979, 0.10171404480934143, 0.1008368656039238, 0.09945172816514969, 0.09915655851364136, 0.09891713410615921, 0.09832489490509033, 0.09809739142656326, 0.09771278500556946, 0.09767833352088928]}}
{"id": "WebQTest-770", "question": "what did steve mcnair died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["deceased_person.cause_of_death"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.person.deceased_cause_of_death"], ["people.person.deceased_people", "people.deceased_person.cause_of_death"], ["people.person.deceased_place_of_death", "people.deceased_person.cause_of_death"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["people.person.deceased_person", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did steve mcnair died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_cause_of_death </PATH>", "<PATH> people.person.deceased_people <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_place_of_death <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_person <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0009999078465625644, -0.05038977786898613, -0.11037259548902512, -0.11105630546808243, -0.11683866381645203, -0.11976613104343414, -0.1284225732088089, -0.13021796941757202, -0.13647311925888062, -0.13691233098506927], "norm_scores": [0.11076663434505463, 0.1054287925362587, 0.09929079562425613, 0.09922294318675995, 0.09865085035562515, 0.09836247563362122, 0.09751468151807785, 0.09733976423740387, 0.09673279523849487, 0.09669031202793121]}}
{"id": "WebQTest-771", "question": "who is the newly elected governor of california", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the newly elected governor of california [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.person.place_of_birth </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.020871112123131752, -0.023149393498897552, -0.027525870129466057, -0.029521500691771507, -0.030474504455924034, -0.036814723163843155, -0.03710132837295532, -0.044746220111846924, -0.04877764359116554, -0.05426725000143051], "norm_scores": [0.10145027190446854, 0.10121940821409225, 0.10077738761901855, 0.10057646781206131, 0.10048067569732666, 0.09984561800956726, 0.09981699287891388, 0.09905681759119034, 0.09865827858448029, 0.09811817854642868]}}
{"id": "WebQTest-772", "question": "who plays roxanne in ghost rider", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.performance.film", "film.actor.film"], ["film.film.starring", "film.performance.actor"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["film.film.starring", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays roxanne in ghost rider [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.film.language <SEP> people.person.languages </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>"], "scores": [-0.021083476021885872, -0.02415742725133896, -0.029163071885704994, -0.03264807537198067, -0.04431017115712166, -0.04929365590214729, -0.054333627223968506, -0.058892957866191864, -0.05952621251344681, -0.06363915652036667], "norm_scores": [0.10227639228105545, 0.10196247696876526, 0.10145336389541626, 0.10110041499137878, 0.09992823004722595, 0.09943146258592606, 0.09893160313367844, 0.09848156571388245, 0.09841921925544739, 0.09801525622606277]}}
{"id": "WebQTest-773", "question": "where is the sony ericsson company", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded"], ["organization.organization_founder.organizations_founded", "location.mailing_address.citytown"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization_headquarters.mailing_address", "location.mailing_address.citytown"], ["organization.organization.place_founded"], ["organization.headquarters.mailing_address", "location.mailing_address.citytown"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the sony ericsson company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_headquarters.mailing_address <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.headquarters.mailing_address <SEP> location.mailing_address.citytown </PATH>"], "scores": [-0.005183143075555563, -0.03361358493566513, -0.04593609273433685, -0.0689738318324089, -0.0705646350979805, -0.07514943182468414, -0.07819446921348572, -0.08114645630121231, -0.0843079462647438, -0.08562073856592178], "norm_scores": [0.105904720723629, 0.10293620079755783, 0.10167554765939713, 0.09935995191335678, 0.09920202195644379, 0.09874823689460754, 0.0984480008482933, 0.09815781563520432, 0.09784797579050064, 0.09771960973739624]}}
{"id": "WebQTest-774", "question": "what kind of currency does the dominican republic have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"], ["location.country. currency_used"], ["location.country.currency_formerly_used"], ["location.location.containedby"], ["location.country.languages_spoken"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does the dominican republic have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country. currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.currency_used <PATH>", "</PATH> location.country.currency_used </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-6.410557398339733e-05, -0.1085883229970932, -0.13422656059265137, -0.17714786529541016, -0.17796526849269867, -0.17901955544948578, -0.17953477799892426, -0.18926724791526794, -0.1912487894296646, -0.20132538676261902], "norm_scores": [0.11642131209373474, 0.10444821417331696, 0.10180438309907913, 0.09752725064754486, 0.09744756668806076, 0.09734488278627396, 0.09729473292827606, 0.09635241329669952, 0.09616167843341827, 0.09519756585359573]}}
{"id": "WebQTest-775", "question": "what to do this weekend in manchester with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["organization.organization.geographic_scope"], ["film.film_festival.location"], ["base.arthist.helynevek.intezmeny"], ["organization.organization_scope.organizations_with_this_scope"], ["film.film.featured_film_locations"], ["metropolitan_transit.transit_system.area_served"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do this weekend in manchester with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>"], "scores": [-0.012516412883996964, -0.016411710530519485, -0.028912503272294998, -0.111299067735672, -0.11458433419466019, -0.12504048645496368, -0.12738221883773804, -0.1346731185913086, -0.13527366518974304, -0.1391063928604126], "norm_scores": [0.10840810835361481, 0.10798664391040802, 0.1066451296210289, 0.09821119159460068, 0.09788907319307327, 0.09687086939811707, 0.09664428234100342, 0.09594222158193588, 0.09588462859392166, 0.0955178290605545]}}
{"id": "WebQTest-776", "question": "what sport is popular in italy", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["cricket.cricket_team.country"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.sports"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.sport"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"]], "ground_paths": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sport is popular in italy [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> cricket.cricket_team.country </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.sports </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.sport </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>"], "scores": [-0.010908502154052258, -0.01478859968483448, -0.11849352717399597, -0.12890899181365967, -0.13128156960010529, -0.1319984793663025, -0.14363408088684082, -0.14444544911384583, -0.14597772061824799, -0.14999772608280182], "norm_scores": [0.11049878597259521, 0.11007086932659149, 0.09922792762517929, 0.09819979220628738, 0.09796707332134247, 0.09789685904979706, 0.09676438570022583, 0.09668589383363724, 0.09653787314891815, 0.09615056961774826]}}
{"id": "WebQTest-777", "question": "where is the main headquarters of google", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["location.location.contains"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["symbols.namesake.named_after", "location.location.containedby"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.phone_number", "common.phone_number.service_location"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the main headquarters of google [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.0158541277050972, -0.025743529200553894, -0.03523193299770355, -0.0506417453289032, -0.05658254399895668, -0.06638220697641373, -0.07447031140327454, -0.08399400860071182, -0.086034394800663, -0.09396015107631683], "norm_scores": [0.1043635830283165, 0.10333658009767532, 0.10236071795225143, 0.1007954478263855, 0.1001984179019928, 0.09922130405902863, 0.09842202812433243, 0.097489133477211, 0.09729041904211044, 0.09652236849069595]}}
{"id": "WebQTest-778", "question": "what was jesse owens education", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["sports.education.student", "education.education.institution"], ["people.education.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse owens education [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> sports.education.student <SEP> education.education.institution </PATH>", "<PATH> people.education.education <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.013005891814827919, -0.019359637051820755, -0.032750487327575684, -0.03360453620553017, -0.06905202567577362, -0.11124199628829956, -0.11616121232509613, -0.11620445549488068, -0.12681123614311218, -0.13572192192077637], "norm_scores": [0.10653553158044815, 0.10586077719926834, 0.10445266962051392, 0.10436349362134933, 0.10072887688875198, 0.09656751900911331, 0.09609364718198776, 0.09608949720859528, 0.09507568925619125, 0.09423225373029709]}}
{"id": "WebQTest-779", "question": "where does carlos santana live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.children", "people.person.place_of_birth"], ["symbols.namesake.named_after", "music.artist.origin"], ["people.person.children", "people.deceased_person.place_of_death"], ["people.person.nationality", "location.administrative_division.second_level_division_of"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.administrative_division.country"], ["people.person.nationality", "location.location.containedby"], ["music.artist.origin", "location.location.containedby"], ["people.person.nationality", "location.country.first_level_divisions"], ["people.person.children", "people.person.place_of_birth"], ["symbols.namesake.named_after", "music.artist.origin"], ["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does carlos santana live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after <SEP> music.artist.origin </PATH>", "<PATH> people.person.children <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.second_level_division_of </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.019826054573059082, -0.022470001131296158, -0.028559334576129913, -0.0462137907743454, -0.0545792318880558, -0.06171690300107002, -0.06433654576539993, -0.06730823963880539, -0.06892939656972885, -0.07756724208593369], "norm_scores": [0.10316180437803268, 0.10288941115140915, 0.10226479172706604, 0.10047519952058792, 0.0996381863951683, 0.09892953187227249, 0.09867072105407715, 0.09837792813777924, 0.09821856766939163, 0.097373828291893]}}
{"id": "WebQTest-780", "question": "what is the new movie john carter about", "prediction": [["film.film_story_contributor.film_story_credits"], ["film.film.subjects"], ["film.film.story_by"], ["film.film_subject.films"], ["book.book_subject.works"], ["book.written_work.subjects"], ["film.film.film_casting_director"], ["film.film.film_production_design_by"], ["film.film.produced_by"], ["film.performance.film", "film.performance.character"]], "ground_paths": [["film.film.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the new movie john carter about [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> film.film_subject.films </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> film.film.film_casting_director </PATH>", "<PATH> film.film.film_production_design_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>"], "scores": [-0.02153918892145157, -0.021725350990891457, -0.03276349604129791, -0.05673021450638771, -0.06947959959506989, -0.06979010254144669, -0.07880868017673492, -0.0856349766254425, -0.08705362677574158, -0.09194173663854599], "norm_scores": [0.1040472611784935, 0.1040278971195221, 0.10288593173027039, 0.10044941306114197, 0.09917686879634857, 0.09914607554674149, 0.0982559472322464, 0.09758750349283218, 0.0974491611123085, 0.0969739779829979]}}
{"id": "WebQTest-782", "question": "what countries does nafta include", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["common.topic.notable_types"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization.founders"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.district_represented"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "ground_paths": [["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.membership_organization.members", "organization.organization_member.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does nafta include [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_membership.member </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.district_represented </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.continents.countries_within </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.010820742696523666, -0.026042528450489044, -0.03793768584728241, -0.054123226553201675, -0.054664865136146545, -0.06661267578601837, -0.06759867072105408, -0.06842858344316483, -0.06989186257123947, -0.07199711352586746], "norm_scores": [0.1042674258351326, 0.10269230604171753, 0.10147800296545029, 0.09984874725341797, 0.09979467839002609, 0.09860944747924805, 0.09851226955652237, 0.09843054413795471, 0.09828662127256393, 0.09807991981506348]}}
{"id": "WebQTest-783", "question": "what county is houston tx in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.citytown.postal_codes"], ["location.location.contains"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.county.county_seat"], ["base.aareas.schema.administrative_area.county"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is houston tx in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.county.county_seat </PATH>", "<PATH> base.aareas.schema.administrative_area.county </PATH>"], "scores": [-0.005502514541149139, -0.03465452417731285, -0.044232264161109924, -0.07841382920742035, -0.10157514363527298, -0.11728950589895248, -0.11762364208698273, -0.13036438822746277, -0.13234227895736694, -0.13992610573768616], "norm_scores": [0.10872821509838104, 0.10560432076454163, 0.10459769517183304, 0.10108280181884766, 0.09876849502325058, 0.09722854942083359, 0.097196064889431, 0.09596556425094604, 0.09577594697475433, 0.09505234658718109]}}
{"id": "WebQTest-784", "question": "where do the seattle seahawks play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["sports.sports_championship_event.champion"], ["sports.sports_facility.teams", "sports.sports_facility.teams"], ["sports.sports_facility.teams", "sports.sports_team.arena_stadium"], ["location.location.containedby"], ["sports.sports_location.teams"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the seattle seahawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> sports.sports_location.teams </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.00910942442715168, -0.01581963337957859, -0.08461864292621613, -0.10321266204118729, -0.11423517018556595, -0.11948991566896439, -0.12206050753593445, -0.12264307588338852, -0.127828449010849, -0.13027779757976532], "norm_scores": [0.1088579073548317, 0.10812988877296448, 0.10094080120325089, 0.09908124804496765, 0.09799512475728989, 0.09748154133558273, 0.09723127633333206, 0.09717464447021484, 0.09667205810546875, 0.0964355617761612]}}
{"id": "WebQTest-785", "question": "who does michael keaton play in cars", "prediction": [["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.performance.character"], ["film.producer.films_executive_produced", "fictional_universe.fictional_character.character_created_by"]], "ground_paths": [["film.performance.film", "film.film_character.portrayed_in_films"], ["fictional_universe.fictional_universe.works_set_here", "fictional_universe.fictional_universe.characters"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael keaton play in cars [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.performance.character </PATH>", "<PATH> film.producer.films_executive_produced <SEP> fictional_universe.fictional_character.character_created_by </PATH>"], "scores": [-0.02820516936480999, -0.03157643973827362, -0.032987408339977264, -0.03730123117566109, -0.03775739669799805, -0.03875483572483063, -0.04234408959746361, -0.05084291100502014, -0.05423733592033386, -0.06054598093032837], "norm_scores": [0.1013287752866745, 0.1009877473115921, 0.1008453518152237, 0.10041125863790512, 0.10036546736955643, 0.10026541352272034, 0.09990617632865906, 0.09906068444252014, 0.09872500598430634, 0.09810414910316467]}}
{"id": "WebQTest-787", "question": "where is the riviera casino", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"], ["government.governmental_jurisdiction.government"], ["government.government_agency.jurisdiction", "location.location.containedby"], ["business.asset_ownership.owner", "business.asset_ownership.owned_asset"], ["government.government_agency.jurisdiction"], ["government.governmental_jurisdiction.government", "location.location.containedby"], ["business.asset_owner.assets_owned", "business.asset_ownership.owned_asset"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the riviera casino [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>", "<PATH> government.government_agency.jurisdiction <SEP> location.location.containedby </PATH>", "<PATH> business.asset_ownership.owner <SEP> business.asset_ownership.owned_asset </PATH>", "<PATH> government.government_agency.jurisdiction </PATH>", "<PATH> government.governmental_jurisdiction.government <SEP> location.location.containedby </PATH>", "<PATH> business.asset_owner.assets_owned <SEP> business.asset_ownership.owned_asset </PATH>"], "scores": [-0.008924432098865509, -0.036720797419548035, -0.04449349269270897, -0.04655440151691437, -0.0655699223279953, -0.07880354672670364, -0.08222196996212006, -0.08467214554548264, -0.08900562673807144, -0.09955292195081711], "norm_scores": [0.10558635741472244, 0.10269185155630112, 0.10189676284790039, 0.1016869768500328, 0.09977161884307861, 0.09845997393131256, 0.09812396764755249, 0.09788384288549423, 0.09746058285236359, 0.09643803536891937]}}
{"id": "WebQTest-788", "question": "what is the holy book of the buddhist religion", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"], ["book.written_work.subjects"], ["religion.religion.texts", "religion.religious_text.religious_text_of"], ["common.topic.notable_types"], ["location.statistical_region.religions", "location.religion_text.religious_text_of"], ["religion.religion.texts", "religion.religion.texts"], ["book.book.genre"], [], ["book.book_subject.works"]], "ground_paths": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the holy book of the buddhist religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> religion.religion.texts <SEP> religion.religious_text.religious_text_of </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_text.religious_text_of </PATH>", "<PATH> religion.religion.texts <SEP> religion.religion.texts </PATH>", "<PATH> book.book.genre </PATH>", "<PATH> </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.00470422487705946, -0.027187127619981766, -0.062200937420129776, -0.08115831017494202, -0.08298858255147934, -0.08764191716909409, -0.08825668692588806, -0.09775944799184799, -0.10710817575454712, -0.11413688957691193], "norm_scores": [0.10725710541009903, 0.10487255454063416, 0.1012641191482544, 0.09936249256134033, 0.09918079525232315, 0.09872034937143326, 0.09865967929363251, 0.09772658348083496, 0.09681721776723862, 0.0961391031742096]}}
{"id": "WebQTest-789", "question": "who became president when henry harrison died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["fictional_universe.character_occupation.characters_with_this_occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["people.person.nationality", "people.person.nationality"], ["people.person.gender", "people.person.gender"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation", "people.person.profession"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who became president when henry harrison died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.02314992994070053, -0.025318656116724014, -0.031843606382608414, -0.04198765009641647, -0.0432451032102108, -0.04928836598992348, -0.05135904997587204, -0.058039817959070206, -0.05864841863512993, -0.06078203022480011], "norm_scores": [0.1021355465054512, 0.10191427916288376, 0.10125146061182022, 0.10022955387830734, 0.10010360181331635, 0.09950046986341476, 0.09929464757442474, 0.09863349795341492, 0.09857349097728729, 0.09836339205503464]}}
{"id": "WebQTest-790", "question": "where did richard arkwright grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["people.person.nationality"], ["influence.influence_node.influenced_by", "location.location.people_born_here"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard arkwright grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0016582810785621405, -0.04484199360013008, -0.11385312676429749, -0.1272708922624588, -0.12734344601631165, -0.13142241537570953, -0.13882969319820404, -0.13934965431690216, -0.14075122773647308, -0.14877846837043762], "norm_scores": [0.11147920042276382, 0.10676757991313934, 0.09964792430400848, 0.09831979125738144, 0.09831266105175018, 0.09791246801614761, 0.09718987345695496, 0.09713936597108841, 0.09700331091880798, 0.0962277501821518]}}
{"id": "WebQTest-791", "question": "what other books did louis sachar write", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"], ["book.author.book_editions_published"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.writer.film"], ["book.writer.works_written"], ["book.book_edition.author_editor", "book.book.editions"]], "ground_paths": [["book.author.works_written"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did louis sachar write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> film.writer.film </PATH>", "<PATH> book.writer.works_written </PATH>", "<PATH> book.book_edition.author_editor <SEP> book.book.editions </PATH>"], "scores": [-0.012119186110794544, -0.026367466896772385, -0.028975892812013626, -0.057155508548021317, -0.07618243247270584, -0.09351050853729248, -0.11411052942276001, -0.1290731430053711, -0.1338515728712082, -0.14286965131759644], "norm_scores": [0.1070622056722641, 0.10554756224155426, 0.10527260601520538, 0.1023474782705307, 0.10041853040456772, 0.09869346022605896, 0.09668117016553879, 0.09524533897638321, 0.09479130059480667, 0.09394030272960663]}}
{"id": "WebQTest-792", "question": "when did roth ira originate", "prediction": [["finance.currency.countries_used"], ["common.topic.notable_types"], ["finance.currency.countries_formerly_used"], ["organization.organization.founders"], ["symbols.namesake.named_after"], ["law.inventor.inventions"], ["location.location.events"], ["time.event.included_in_event"], ["business.business_operation.industry"], ["government.government_program.sponsored_by"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did roth ira originate [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> business.business_operation.industry </PATH>", "<PATH> government.government_program.sponsored_by </PATH>"], "scores": [-0.013902293518185616, -0.02997681125998497, -0.0391230434179306, -0.06158517301082611, -0.08307357877492905, -0.08743050694465637, -0.09286212921142578, -0.09618695825338364, -0.09849867224693298, -0.10390301793813705], "norm_scores": [0.1057891771197319, 0.10410226881504059, 0.10315445810556412, 0.10086322575807571, 0.09871895611286163, 0.09828978031873703, 0.09775735437870026, 0.09743285924196243, 0.09720788896083832, 0.09668396413326263]}}
{"id": "WebQTest-794", "question": "who does the voice for darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["tv.tv_guest_role.special_performance_type", "tv.tv_actor.guest_roles"], ["film.performance.special_performance_type", "film.actor.film"], ["tv.tv_guest_role.special_performance_type", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_guest_role.special_performance_type <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> tv.tv_guest_role.special_performance_type <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> tv.tv_guest_role.character <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.023189058527350426, -0.02427496388554573, -0.028271472081542015, -0.0324723906815052, -0.04691697284579277, -0.04705354571342468, -0.04842473566532135, -0.05154003947973251, -0.054314445704221725, -0.06983903050422668], "norm_scores": [0.10195261985063553, 0.10184197872877121, 0.10143576562404633, 0.10101053863763809, 0.09956198185682297, 0.09954837709665298, 0.0994119644165039, 0.09910275042057037, 0.09882818907499313, 0.0973057672381401]}}
{"id": "WebQTest-796", "question": "what did thomas malthus do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["book.written_work.author"], ["influence.influence_node.influenced_by"], ["book.author.works_written"], ["influence.influence_node.influenced"], ["base.argumentmaps.innovator.original_ideas"], ["book.written_work.subjects"], ["business.employment_tenure.person", "people.person.profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas malthus do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> business.employment_tenure.person <SEP> people.person.profession </PATH>"], "scores": [-0.0018221616046503186, -0.056015558540821075, -0.0700671374797821, -0.09671751409769058, -0.11038312315940857, -0.11268141865730286, -0.1175033450126648, -0.12418463826179504, -0.12968745827674866, -0.13031713664531708], "norm_scores": [0.10967420786619186, 0.10388876497745514, 0.10243917256593704, 0.09974519163370132, 0.09839139133691788, 0.09816551208496094, 0.0976933091878891, 0.0970427617430687, 0.09651022404432297, 0.09644946455955505]}}
{"id": "WebQTest-797", "question": "what is the zip code for newark california", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.us_county.hud_county_place"], ["location.Citytown.postal_codes"], ["citytown.citytown.postal_codes"], ["location.mailing_address.citytown"], ["location.citytown.postal_ codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for newark california [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> citytown.citytown.postal_codes </PATH>", "<PATH> location.mailing_address.citytown </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.citytown.postal_ codes </PATH>", "</PATH> location.citytown.postal_codes </PATH>"], "scores": [-0.00013045637751929462, -0.09316131472587585, -0.15150928497314453, -0.15710966289043427, -0.16069583594799042, -0.16988421976566315, -0.16990822553634644, -0.1700180619955063, -0.17419841885566711, -0.17934313416481018], "norm_scores": [0.11514472216367722, 0.10491587966680527, 0.09896942228078842, 0.09841670840978622, 0.09806440025568008, 0.0971674695611, 0.09716514497995377, 0.09715446084737778, 0.09674917906522751, 0.09625270962715149]}}
{"id": "WebQTest-798", "question": "what states share a border with tennessee", "prediction": [["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["base.locations.countries.states_provinces_within", "location.country.first_level_divisions"], ["time.event.locations", "location.location.events"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["base.locations.countries.states_provinces_within", "base.locations.countries.states_provinces_within"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["location.location.events", "time.event.locations"], ["location.location.partially_contains", "geography.river.origin"], ["time.event.locations", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.containedby", "location.location.contains"], ["base.locations.countries.states_provinces_within", "location.administrative_division.first_level_division_of"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_contains", "location.location.partially_containedby"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["base.locations.countries.states_provinces_within", "location.administrative_division.country"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_children"], ["book.written_work.subjects", "book.written_work.subjects"], ["location.location.containedby", "base.usnationalparks.us_national_park.state"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.containedby", "location.location.containedby"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.partially_contains", "location.location.partially_contains"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["location.location.time_zones", "location.location.time_zones"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states share a border with tennessee [/INST]", "raw_output": {"paths": ["<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>"], "scores": [-0.03342760354280472, -0.03345669433474541, -0.03523922339081764, -0.040746841579675674, -0.04119078814983368, -0.04119563475251198, -0.042881932109594345, -0.04313598573207855, -0.0449199378490448, -0.045391909778118134], "norm_scores": [0.10067445784807205, 0.10067152976989746, 0.10049223899841309, 0.09994028508663177, 0.09989592432975769, 0.09989544004201889, 0.09972713887691498, 0.09970179945230484, 0.09952409565448761, 0.0994771271944046]}}
{"id": "WebQTest-800", "question": "what war did the us lose the most soldiers", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.military_conflict"], ["military.military_command.military_combatant", "military.military_command.military_conflict"], ["military.casualties.combatant", "military.casualties.military_conflict"], ["military.military_conflict.casualties", "military.military_conflict.casualties"], ["military.casualties.combatant", "military.military_conflict.casualties"], ["military.military_combatant.casualties", "military.casualties.military_conflict"], ["military.military_combatant.military_commanders", "military.military_command.military_conflict"], ["military.military_conflict.combatants", "military.military_conflict.military_personnel_involved"], ["military.military_conflict.combatants", "military.military_conflict.casualties"]], "ground_paths": [["military.military_command.military_combatant", "military.military_conflict.commanders"], ["military.military_combatant.military_commanders", "military.military_conflict.commanders"], ["people.person.nationality", "military.military_person.participated_in_conflicts"], ["military.military_combatant.military_commanders", "military.military_command.military_conflict"], ["location.location.partially_containedby", "location.location.events"], ["military.military_combatant.casualties", "military.casualties.military_conflict"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_command.military_combatant", "military.military_command.military_conflict"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what war did the us lose the most soldiers [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.military_conflict </PATH>", "<PATH> military.military_command.military_combatant <SEP> military.military_command.military_conflict </PATH>", "<PATH> military.casualties.combatant <SEP> military.casualties.military_conflict </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>", "<PATH> military.casualties.combatant <SEP> military.military_conflict.casualties </PATH>", "<PATH> military.military_combatant.casualties <SEP> military.casualties.military_conflict </PATH>", "<PATH> military.military_combatant.military_commanders <SEP> military.military_command.military_conflict </PATH>", "<PATH> military.military_conflict.combatants <SEP> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_conflict.combatants <SEP> military.military_conflict.casualties </PATH>"], "scores": [-0.01509565394371748, -0.030289774760603905, -0.0359027273952961, -0.03765023127198219, -0.03819909319281578, -0.045681681483983994, -0.04839210957288742, -0.048819929361343384, -0.04932340234518051, -0.05635848268866539], "norm_scores": [0.10257375240325928, 0.10102701187133789, 0.10046154260635376, 0.10028614848852158, 0.1002311110496521, 0.099483922123909, 0.09921464323997498, 0.0991722121834755, 0.0991222932934761, 0.098427414894104]}}
{"id": "WebQTest-801", "question": "what kind of government is egypt", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent", "location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["language.human_language.main_country", "government.form_of_government.countries"], ["people.person.nationality", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is egypt [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.0062071592546999454, -0.021887311711907387, -0.1220293715596199, -0.12432299554347992, -0.1401512622833252, -0.14541247487068176, -0.14598537981510162, -0.15182499587535858, -0.15235039591789246, -0.15434813499450684], "norm_scores": [0.1114983856678009, 0.10976371169090271, 0.0993042066693306, 0.09907670319080353, 0.09752083569765091, 0.09700910747051239, 0.09695354104042053, 0.09638901799917221, 0.09633839875459671, 0.0961461290717125]}}
{"id": "WebQTest-802", "question": "what musical instruments does justin bieber play", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.recording_contribution.contributor", "music.track_contribution.role"], ["music.musical_group.member", "music.group_membership.role"], ["music.instrument.family"], ["people.person.profession", "music.group_member.instruments_played"], ["music.producer.releases_produced"], ["people.person.profession"], ["people.person.profession", "music.music_profession.specializations"]], "ground_paths": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.membership", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what musical instruments does justin bieber play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.recording_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> people.person.profession <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.profession <SEP> music.music_profession.specializations </PATH>"], "scores": [-0.0036354658659547567, -0.04522683098912239, -0.055012982338666916, -0.0630568191409111, -0.07272183895111084, -0.07758786529302597, -0.081693135201931, -0.0907680094242096, -0.09348219633102417, -0.09900084882974625], "norm_scores": [0.10663217306137085, 0.10228816419839859, 0.10129203647375107, 0.10048051923513412, 0.09951405227184296, 0.0990309938788414, 0.09862527996301651, 0.09773430973291397, 0.09746940433979034, 0.09693299233913422]}}
{"id": "WebQTest-805", "question": "where do john lennon die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["location.location.people_born_here"], ["deceased_person.place_of_death"], ["people.person.place_of_death"], ["music.recording.artist", "music.recording.artist"], ["location.location.events"], ["time.event.locations"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do john lennon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_death </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0006589837139472365, -0.07855049520730972, -0.09202057123184204, -0.10924401879310608, -0.11226761341094971, -0.11520307511091232, -0.11823111027479172, -0.12114287167787552, -0.12287894636392593, -0.12346446514129639], "norm_scores": [0.11030248552560806, 0.10203694552183151, 0.10067171603441238, 0.09895265102386475, 0.09865390509366989, 0.09836473315954208, 0.09806733578443527, 0.097782202064991, 0.09761258959770203, 0.09755545854568481]}}
{"id": "WebQTest-809", "question": "what is the current time in bangalore india", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["location.location.time_zones", "location.location.time_zones"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.location.time_Zones"], ["language.human_language.countries_spoken_in", "location.country.official_language"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in bangalore india [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.official_language </PATH>"], "scores": [-0.00017791577556636184, -0.07158602029085159, -0.17215923964977264, -0.17549049854278564, -0.1765073984861374, -0.17807427048683167, -0.17959439754486084, -0.18292537331581116, -0.193160280585289, -0.19334588944911957], "norm_scores": [0.11620854586362839, 0.10819967091083527, 0.09784700721502304, 0.0975215956568718, 0.09742248058319092, 0.0972699448466301, 0.0971221923828125, 0.09679922461509705, 0.09581354260444641, 0.09579576551914215]}}
{"id": "WebQTest-810", "question": "what language do people from greece speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"], ["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.country.form_of_government"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from greece speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.013974976725876331, -0.01438175980001688, -0.04937049746513367, -0.05959268659353256, -0.15361370146274567, -0.1615029275417328, -0.17525465786457062, -0.18012158572673798, -0.18295365571975708, -0.1852232962846756], "norm_scores": [0.11064577847719193, 0.1106007844209671, 0.10679791867733002, 0.10571176558732986, 0.09622558206319809, 0.09546942263841629, 0.09416553378105164, 0.09370835870504379, 0.09344334155321121, 0.09323149919509888]}}
{"id": "WebQTest-811", "question": "where is the phillies spring training stadium", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["location.location.containedby"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.location"], ["location.location.contains"], ["travel.travel_destination.tourist_attractions"], ["baseball.baseball_team.arena_stadium"], ["time.event.locations"], ["travel.tourist_attraction.near_travel_destination"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the phillies spring training stadium [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.location </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> baseball.baseball_team.arena_stadium </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.0034805217292159796, -0.04638274386525154, -0.0497899129986763, -0.07442929595708847, -0.07903394103050232, -0.08300545066595078, -0.08859576284885406, -0.09433499723672867, -0.09930620342493057, -0.10433333367109299], "norm_scores": [0.10707449913024902, 0.10257791727781296, 0.10222901403903961, 0.09974092245101929, 0.09928271919488907, 0.09888919442892075, 0.09833791106939316, 0.09777513891458511, 0.0972902923822403, 0.09680243581533432]}}
{"id": "WebQTest-812", "question": "what movies does matt dallas play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"]], "ground_paths": [["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does matt dallas play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>"], "scores": [-0.012451324611902237, -0.034387700259685516, -0.034581318497657776, -0.03687964379787445, -0.037217333912849426, -0.06865614652633667, -0.06911010295152664, -0.07465700805187225, -0.0760592371225357, -0.08186963945627213], "norm_scores": [0.10406821966171265, 0.10181019455194473, 0.10179048776626587, 0.10155680775642395, 0.1015225201845169, 0.0983804240822792, 0.09833578020334244, 0.09779182821512222, 0.0976547971367836, 0.097089022397995]}}
{"id": "WebQTest-813", "question": "who was john f kennedy 's family", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.parents", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.religion", "people.person.religion"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.parents", "people.person.parents"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["religion.religion.practices", "people.person.religion"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["people.person.profession", "common.topic.notable_types"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["people.person.gender", "people.person.gender"], ["people.sibling_relationship.sibling", "people.person.sibling_s"], ["people.person.parents"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["film.film_subject.films", "film.film.subjects"], ["people.person.nationality", "people.person.nationality"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.person.ethnicity", "people.person.ethnicity"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.profession", "people.person.profession"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["people.family.members", "people.family_member.family"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["symbols.name_source.namesakes"], ["people.person.parents", "people.person.children"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.religion", "people.person.religion"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.family.members", "people.family.members"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was john f kennedy 's family [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.sibling_relationship.sibling <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> religion.religion.practices <SEP> people.person.religion </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.032704468816518784, -0.03276178240776062, -0.034388426691293716, -0.040090035647153854, -0.04021779075264931, -0.04271531105041504, -0.044582903385162354, -0.04662386700510979, -0.049741536378860474, -0.0513695552945137], "norm_scores": [0.10088333487510681, 0.1008775532245636, 0.10071359574794769, 0.1001409962773323, 0.10012820363044739, 0.0998784527182579, 0.09969209134578705, 0.09948881715536118, 0.09917913377285004, 0.09901779890060425]}}
{"id": "WebQTest-814", "question": "where is harley davidson corporate headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["symbols.namesake.named_after", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.street_address"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.place_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harley davidson corporate headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.street_address </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.0051111201755702496, -0.04105977341532707, -0.043310537934303284, -0.04628489539027214, -0.05186055228114128, -0.07983788847923279, -0.08794428408145905, -0.09252555668354034, -0.09349075704813004, -0.09652294963598251], "norm_scores": [0.10599897801876068, 0.10225613415241241, 0.10202623903751373, 0.10172322392463684, 0.10115763545036316, 0.09836672991514206, 0.09757255762815475, 0.0971265658736229, 0.09703286737203598, 0.09673909097909927]}}
{"id": "WebQTest-818", "question": "where was the ottoman empire based", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.country.capital"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.events", "time.event.locations"], ["location.location.events"], ["base.locations.continents.countries_within"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the ottoman empire based [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> location.location.events <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.events <SEP> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.0069669084623456, -0.041263192892074585, -0.06023909151554108, -0.07309093326330185, -0.07773738354444504, -0.08043789118528366, -0.08309019356966019, -0.08323340117931366, -0.08690566569566727, -0.08730027079582214], "norm_scores": [0.1062641367316246, 0.10268145799636841, 0.1007513552904129, 0.0994647890329361, 0.09900370985269547, 0.0987367108464241, 0.09847518056631088, 0.0984610766172409, 0.09810017049312592, 0.0980614647269249]}}
{"id": "WebQTest-819", "question": "where was bin laden found and killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_conflict.military_personnel_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved", "location.location.events"], ["people.place_lived.person", "people.place_lived.location"], ["base.culturalevent.event.entity_involved"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["book.written_work.subjects", "book.written_work.subjects"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was bin laden found and killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> military.military_conflict.military_personnel_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved <SEP> location.location.events </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.subjects </PATH>"], "scores": [-0.02058205008506775, -0.03141942620277405, -0.034362565726041794, -0.03907397761940956, -0.04012595862150192, -0.045045156031847, -0.04512622207403183, -0.04773006588220596, -0.052747417241334915, -0.05315985903143883], "norm_scores": [0.10205166786909103, 0.1009516716003418, 0.10065498948097229, 0.10018187761306763, 0.10007654130458832, 0.09958545118570328, 0.09957738220691681, 0.09931843727827072, 0.09882137179374695, 0.09878061711788177]}}
{"id": "WebQTest-820", "question": "what are the names of harry potter movies in order", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["fictional_universe.fictional_universe.works_set_here"], ["film.film_series.films_in_series"], ["book.book_character.appears_in_book"], ["book.written_work.subjects"], ["book.book.characters"], ["film.film.film_series"], ["book.written_work.previous_in_series"], ["book.book.genre"], ["film.film_story_contributor.film_story_credits"]], "ground_paths": [["film.film.film_series"], ["fictional_universe.fictional_universe.works_set_here"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["film.film_series.films_in_series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of harry potter movies in order [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>", "<PATH> film.film_series.films_in_series </PATH>", "<PATH> book.book_character.appears_in_book </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book.characters </PATH>", "<PATH> film.film.film_series </PATH>", "<PATH> book.written_work.previous_in_series </PATH>", "<PATH> book.book.genre </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>"], "scores": [-0.0066049122251570225, -0.03299442678689957, -0.037765033543109894, -0.041320160031318665, -0.058996040374040604, -0.06139317899942398, -0.07421553134918213, -0.07817475497722626, -0.08082935959100723, -0.08437512069940567], "norm_scores": [0.10499794781208038, 0.10226333886384964, 0.10177665203809738, 0.1014154702425003, 0.09963860362768173, 0.09940004348754883, 0.0981336385011673, 0.0977458730340004, 0.09748673439025879, 0.09714169055223465]}}
{"id": "WebQTest-821", "question": "what did pamela courson die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["deceased_person.cause_of_death"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.people.deceased_person", "people.deceased_person.cause_of_death"], ["influence.influence_node.influenced", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did pamela courson die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.people.deceased_person <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.002355811884626746, -0.037144776433706284, -0.11456605046987534, -0.11783522367477417, -0.12293952703475952, -0.12876126170158386, -0.13243995606899261, -0.13255317509174347, -0.135646790266037, -0.1408461481332779], "norm_scores": [0.11086364090442657, 0.10707312822341919, 0.09909616410732269, 0.09877272695302963, 0.09826984256505966, 0.09769940376281738, 0.0973406583070755, 0.09732964634895325, 0.09702900797128677, 0.09652583301067352]}}
{"id": "WebQTest-822", "question": "what did dr jack kevorkian do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["base.activism.activist.area_of_activism"], ["medicine.disease.notable_people_with_this_condition"], ["fictional_universe.fictional_character.occupation"], ["base.activism.activism_issue.activists"], ["film.film.subjects"], ["people.person.profession", "people.profession.specializations"], ["film.film_subject.films"]], "ground_paths": [["base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr jack kevorkian do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> film.film_subject.films </PATH>"], "scores": [-0.0017947455635294318, -0.04893812537193298, -0.07178555428981781, -0.13109581172466278, -0.1443307101726532, -0.14952655136585236, -0.15902255475521088, -0.16035670042037964, -0.16083340346813202, -0.16119305789470673], "norm_scores": [0.11225385218858719, 0.10708463191986084, 0.10466576367616653, 0.09863851219415665, 0.09734164923429489, 0.09683717787265778, 0.09592196345329285, 0.09579407423734665, 0.0957484245300293, 0.09571399539709091]}}
{"id": "WebQTest-823", "question": "what does bolivia border", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["time.event.locations", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["location.location.events", "time.event.locations"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["time.event.locations", "time.event.locations"], ["location.country.languages_spoken", "location.country.official_language"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.location.contains", "location.location.containedby"], ["symbols.name_source.namesakes", "people.person.nationality"], ["location.country.official_language", "location.country.languages_spoken"], ["location.country.official_language", "language.human_language.main_country"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["location.country.form_of_government", "government.form_of_government.countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["people.deceased_person.place_of_death", "people.person.nationality"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.containedby", "location.location.containedby"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["language.language_family.geographic_distribution", "location.country.official_language"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.contains", "location.location.contains"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["language.language_family.geographic_distribution", "language.human_language.main_country"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.events", "location.location.events"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does bolivia border [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.03483113646507263, -0.036272551864385605, -0.03850242868065834, -0.041918497532606125, -0.04315761849284172, -0.04481731355190277, -0.046061862260103226, -0.04724470153450966, -0.0502568818628788, -0.05160703510046005], "norm_scores": [0.10086587071418762, 0.10072058439254761, 0.10049624741077423, 0.10015352815389633, 0.10002950578927994, 0.09986362606287003, 0.09973940998315811, 0.09962150454521179, 0.0993218794465065, 0.09918786585330963]}}
{"id": "WebQTest-825", "question": "who does nolan ryan play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["batting.batting_statistics.player", "batting.batting_statistics.team"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does nolan ryan play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> batting.batting_statistics.player <SEP> batting.batting_statistics.team </PATH>"], "scores": [-0.00801326334476471, -0.017922671511769295, -0.040757618844509125, -0.04877090081572533, -0.07928010076284409, -0.08404462784528732, -0.08749224990606308, -0.09109354019165039, -0.10696213692426682, -0.11359789222478867], "norm_scores": [0.10609572380781174, 0.10504957288503647, 0.10267795622348785, 0.1018584594130516, 0.0987977609038353, 0.09832815825939178, 0.09798974543809891, 0.09763748198747635, 0.0961003452539444, 0.09546475857496262]}}
{"id": "WebQTest-827", "question": "what country does justin bieber come from", "prediction": [["people.person.nationality"], ["music.artist.origin"], ["people.person.place_of_birth"], ["people.person.ethnicity"], ["music.recording.artist", "music.artist.origin"], ["location.location.people_born_here"], ["music.artist.genre", "music.artist.genre"], ["music.recording_contribution.contributor", "music.artist.origin"], ["people.person.children", "people.person.nationality"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does justin bieber come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> music.recording.artist <SEP> music.artist.origin </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.recording_contribution.contributor <SEP> music.artist.origin </PATH>", "<PATH> people.person.children <SEP> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.004106571432203054, -0.04251525178551674, -0.050423573702573776, -0.08516324311494827, -0.09617782384157181, -0.09687123447656631, -0.10764146596193314, -0.1115172728896141, -0.11415968090295792, -0.11457506567239761], "norm_scores": [0.10806545615196228, 0.10399351269006729, 0.10317433625459671, 0.09965162724256516, 0.09856004267930984, 0.09849172085523605, 0.09743662923574448, 0.0970597192645073, 0.09680359065532684, 0.09676338732242584]}}
{"id": "WebQTest-828", "question": "what is the head judge of the supreme court called", "prediction": [["law.judicial_tenure.title", "law.judge.courts"], ["law.court.jurisdiction", "people.person.nationality"], ["government.governmental_jurisdiction.government_bodies", "people.person.nationality"], ["law.judicial_tenure.court", "law.judge.courts"], ["law.court.jurisdiction", "government.government_office_or_title.office_holders"], ["law.court.jurisdiction", "government.government_position_held.office_holder"], ["people.appointed_role.appointment", "people.appointee.position"], ["law.court.superior_courts", "law.judicial_tenure.judge"], ["law.judicial_tenure.title", "law.judicial_tenure.judge"], ["people.appointment.appointed_role", "people.appointee.position"]], "ground_paths": [["government.governmental_body.offices_positions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the head judge of the supreme court called [/INST]", "raw_output": {"paths": ["<PATH> law.judicial_tenure.title <SEP> law.judge.courts </PATH>", "<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.government_bodies <SEP> people.person.nationality </PATH>", "<PATH> law.judicial_tenure.court <SEP> law.judge.courts </PATH>", "<PATH> law.court.jurisdiction <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> law.court.jurisdiction <SEP> government.government_position_held.office_holder </PATH>", "<PATH> people.appointed_role.appointment <SEP> people.appointee.position </PATH>", "<PATH> law.court.superior_courts <SEP> law.judicial_tenure.judge </PATH>", "<PATH> law.judicial_tenure.title <SEP> law.judicial_tenure.judge </PATH>", "<PATH> people.appointment.appointed_role <SEP> people.appointee.position </PATH>"], "scores": [-0.02838803455233574, -0.034243304282426834, -0.03563327342271805, -0.03910958394408226, -0.043229203671216965, -0.04438846558332443, -0.04929424822330475, -0.05305400863289833, -0.05356302857398987, -0.056330759078264236], "norm_scores": [0.10154136270284653, 0.10094854980707169, 0.10080832242965698, 0.10045849531888962, 0.10004549473524094, 0.099929578602314, 0.0994405597448349, 0.09906737506389618, 0.09901696443557739, 0.09874328970909119]}}
{"id": "WebQTest-829", "question": "who was frida kahlo influenced by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["symbols.namesake.named_after"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was frida kahlo influenced by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.011475849896669388, -0.012281891889870167, -0.07342010736465454, -0.07425088435411453, -0.08462738990783691, -0.08635984361171722, -0.10588076710700989, -0.1254497468471527, -0.13400843739509583, -0.13744689524173737], "norm_scores": [0.10747940093278885, 0.1073928028345108, 0.10102368146181107, 0.1009397953748703, 0.09989780187606812, 0.09972487390041351, 0.09779703617095947, 0.09590184688568115, 0.09508455544710159, 0.09475817531347275]}}
{"id": "WebQTest-830", "question": "where is taylor swift from", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["music.album.artist"], ["music.recording.artist"], ["music.recording_contribution.contributor", "music.artist.origin"], ["music.artist.origin", "location.location.people_born_here"], ["symbols.namesake.named_after"], ["music.composition.composer"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is taylor swift from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.recording_contribution.contributor <SEP> music.artist.origin </PATH>", "<PATH> music.artist.origin <SEP> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.015305852517485619, -0.021693291142582893, -0.03169885277748108, -0.10492761433124542, -0.10537684708833694, -0.11523498594760895, -0.1393733024597168, -0.14979293942451477, -0.15857283771038055, -0.16616936028003693], "norm_scores": [0.10876195877790451, 0.10806946456432343, 0.10699355602264404, 0.09943855553865433, 0.09939388930797577, 0.09841886907815933, 0.09607163816690445, 0.0950758159160614, 0.09424471110105515, 0.09353148937225342]}}
{"id": "WebQTest-831", "question": "what type of religion does argentina have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["religion.religion.countries"], ["royalty.kingdom.rulers", "people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["book.book_subject.works", "base.argumentmaps.thing_of_disputed_value.disparagement"], ["book.newspaper.circulation_areas", "book.periodical.subjects"], ["symbols.namesake.named_after"], ["people.person.quotations", "people.person.religion"], ["book.book_subject.works", "base.argumentmaps.expression_of_disvalue.thing_of_disputed_value"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion does argentina have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> religion.religion.countries </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> book.book_subject.works <SEP> base.argumentmaps.thing_of_disputed_value.disparagement </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.subjects </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>", "<PATH> book.book_subject.works <SEP> base.argumentmaps.expression_of_disvalue.thing_of_disputed_value </PATH>"], "scores": [-0.009161559864878654, -0.014043533243238926, -0.07489169389009476, -0.0862380862236023, -0.09065613895654678, -0.10765967518091202, -0.11027099937200546, -0.11807388812303543, -0.11946442723274231, -0.12000598013401031], "norm_scores": [0.10779867321252823, 0.10727368295192719, 0.10094089806079865, 0.09980206191539764, 0.09936210513114929, 0.09768687933683395, 0.09743212163448334, 0.0966748297214508, 0.09654048830270767, 0.09648822247982025]}}
{"id": "WebQTest-832", "question": "when is the last time the giants won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["time.participant.event"], ["sports.professional_sports_team.owner_s", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["time.recurring_event.instances"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.professional_sports_team.owner_s", "sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the giants won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.participant.event </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.championships </PATH>"], "scores": [-0.007361537776887417, -0.017219053581357002, -0.08134733885526657, -0.08855661004781723, -0.10321465879678726, -0.11888910084962845, -0.1289384365081787, -0.13833218812942505, -0.14499875903129578, -0.15073157846927643], "norm_scores": [0.10935453325510025, 0.10828186571598053, 0.10155589878559113, 0.10082639008760452, 0.09935925155878067, 0.09781400114297867, 0.09683594107627869, 0.0959305539727211, 0.09529315680265427, 0.09474841505289078]}}
{"id": "WebQTest-833", "question": "what is the main language used in china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.administrative_divisions"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["base.rosetta.languoid.languoid_class"], ["location.country.main_language"], ["language.language_family.geographic_distribution"], ["language.language_family.languages"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language used in china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.main_language </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>", "<PATH> language.language_family.languages </PATH>"], "scores": [-0.006637117359787226, -0.030708089470863342, -0.04169419780373573, -0.07239319384098053, -0.13870443403720856, -0.14058062434196472, -0.158233180642128, -0.16405244171619415, -0.16958408057689667, -0.1714843362569809], "norm_scores": [0.11061467230319977, 0.10798386484384537, 0.10680403560400009, 0.10357506573200226, 0.0969296544790268, 0.09674796462059021, 0.09505509585142136, 0.09450354427099228, 0.09398223459720612, 0.09380381554365158]}}
{"id": "WebQTest-834", "question": "where did the battle of waterloo occur", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["military.military_conflict.commanders", "military.military_command.military_combatant"], ["military.military_conflict.military_personnel_involved"], ["military.military_conflict.casualties"], ["book.written_work.subjects"], ["military.military_command.military_conflict"], ["geography.river.basin_countries"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of waterloo occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> military.military_conflict.commanders <SEP> military.military_command.military_combatant </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_conflict.casualties </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> military.military_command.military_conflict </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.0030859699472784996, -0.0380769819021225, -0.11486899107694626, -0.12887461483478546, -0.13093255460262299, -0.1329067349433899, -0.14041867852210999, -0.14225035905838013, -0.14393754303455353, -0.14493770897388458], "norm_scores": [0.11138302832841873, 0.10755302011966705, 0.09960296750068665, 0.0982176885008812, 0.09801576286554337, 0.09782246500253677, 0.09709037840366364, 0.09691270440816879, 0.09674932807683945, 0.09665261954069138]}}
{"id": "WebQTest-836", "question": "who did vladimir lenin marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.person.spouse_s"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did vladimir lenin marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>"], "scores": [-0.025227317586541176, -0.043570395559072495, -0.04428768530488014, -0.044492028653621674, -0.04648759216070175, -0.047948963940143585, -0.04923522099852562, -0.051264166831970215, -0.05274071544408798, -0.054776426404714584], "norm_scores": [0.10209618508815765, 0.10024049133062363, 0.1001686230301857, 0.10014815628528595, 0.09994850307703018, 0.09980254620313644, 0.09967425465583801, 0.09947223216295242, 0.09932546317577362, 0.09912346303462982]}}
{"id": "WebQTest-837", "question": "what year did the baltimore ravens win superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.runner_up"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.championship"], ["sports.sports_championship"], ["sports.sports_winning_work.champion"], ["sports.sports_championship_event.season"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the baltimore ravens win superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship </PATH>", "<PATH> sports.sports_winning_work.champion </PATH>", "<PATH> sports.sports_championship_event.season </PATH>"], "scores": [-0.013352155685424805, -0.014688047580420971, -0.032298628240823746, -0.0657036304473877, -0.09087994694709778, -0.11176346987485886, -0.11374553292989731, -0.1275356411933899, -0.14723742008209229, -0.1480454057455063], "norm_scores": [0.10745962709188461, 0.10731617361307144, 0.10544281452894211, 0.10197868198156357, 0.09944327920675278, 0.09738808870315552, 0.09719525277614594, 0.09586411714553833, 0.0939939096570015, 0.09391799569129944]}}
{"id": "WebQTest-838", "question": "what team does cristiano ronaldo play for 2011", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does cristiano ronaldo play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.020055031403899193, -0.02210257761180401, -0.02450641058385372, -0.03194141015410423, -0.036192554980516434, -0.037513792514801025, -0.03761003538966179, -0.03897363319993019, -0.04027470573782921, -0.046301405876874924], "norm_scores": [0.1013549342751503, 0.1011476218700409, 0.10090477019548416, 0.10015732049942017, 0.09973244369029999, 0.09960075467824936, 0.09959117323160172, 0.09945546835660934, 0.09932614862918854, 0.09872934222221375]}}
{"id": "WebQTest-841", "question": "what is money called in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country"], ["common.topic.notable_types"], ["finance.currency.countries_used", "location.country.currency_used"], ["language.human_language.main_country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is money called in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.009860516525804996, -0.01778462529182434, -0.1025865450501442, -0.11951053142547607, -0.1395195573568344, -0.1607598215341568, -0.1832846701145172, -0.1928912103176117, -0.1980499029159546, -0.19919903576374054], "norm_scores": [0.1127711609005928, 0.1118810772895813, 0.10278450697660446, 0.10105961561203003, 0.09905760735273361, 0.09697578847408295, 0.09481582790613174, 0.09390934556722641, 0.09342614561319351, 0.09331884980201721]}}
{"id": "WebQTest-843", "question": "where did theodor schwann study", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.place_of_birth", "location.location.containedby"], ["location.location.people_born_here", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["business.employment_tenure.person", "business.employer.employees"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann study [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>"], "scores": [-0.02113577350974083, -0.0217369943857193, -0.023989424109458923, -0.02823060005903244, -0.06005348637700081, -0.06134108826518059, -0.07456459850072861, -0.07515528053045273, -0.0814150795340538, -0.09361705929040909], "norm_scores": [0.10331781208515167, 0.10325571894645691, 0.10302340239286423, 0.10258738696575165, 0.09937416017055511, 0.09924628585577011, 0.0979425385594368, 0.09788470715284348, 0.09727387875318527, 0.09609415382146835]}}
{"id": "WebQTest-845", "question": "who are the virginia senators 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.election.district", "government.election.winner"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.election.district", "government.election.winner"], ["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the virginia senators 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.election.district <SEP> government.election.winner </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.01611730083823204, -0.020341947674751282, -0.027912864461541176, -0.033237893134355545, -0.03432750701904297, -0.03477724641561508, -0.03532293066382408, -0.036701079457998276, -0.04405884072184563, -0.061974477022886276], "norm_scores": [0.10184565931558609, 0.10141629725694656, 0.10065138339996338, 0.10011684149503708, 0.1000078096985817, 0.09996283799409866, 0.09990830719470978, 0.09977071732282639, 0.09903932362794876, 0.09728077054023743]}}
{"id": "WebQTest-846", "question": "what was dr seuss 's real name", "prediction": [["book.book.interior_illustrations_by", "people.person.nationality"], ["people.person.profession"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["book.book.interior_illustrations_by", "people.person.interred_names"], ["book.book.interior_illustrations_by"], ["book.book_edition.author_editor", "people.person.profession"], ["book.book.interior_illustrations_by", "people.person.profession"], ["book.written_work.author", "people.person.profession"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss 's real name [/INST]", "raw_output": {"paths": ["<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.interred_names </PATH>", "<PATH> book.book.interior_illustrations_by </PATH>", "<PATH> book.book_edition.author_editor <SEP> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.profession </PATH>", "<PATH> book.written_work.author <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.022312460467219353, -0.03191162273287773, -0.036561019718647, -0.0430435948073864, -0.04929383844137192, -0.04971558600664139, -0.052885398268699646, -0.0538959763944149, -0.060253437608480453, -0.06099088117480278], "norm_scores": [0.1023985892534256, 0.10142035782337189, 0.10094990581274033, 0.10029760003089905, 0.09967267513275146, 0.09963064640760422, 0.0993153378367424, 0.09921503067016602, 0.09858626127243042, 0.09851359575986862]}}
{"id": "WebQTest-847", "question": "what countries are members of the security council", "prediction": [["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_body_membership.member"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_body_membership.member"], ["government.governmental_body.members", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.governmental_body", "government.government_position_held.district_represented"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_membership.member"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_members_state.member_of"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_body_member_state.member_of"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_body.members"]], "ground_paths": [["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_body_membership.member"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are members of the security council [/INST]", "raw_output": {"paths": ["<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body.members <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_body_membership.member </PATH>", "<PATH> base.unitednations.united_nations_body.members <SEP> base.unitednations.united_nations_body_membership.member </PATH>", "<PATH> government.governmental_body.members <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.district_represented </PATH>", "<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_membership.member </PATH>", "<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_members_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_body_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_body.members </PATH>"], "scores": [-0.010376671329140663, -0.012841678224503994, -0.026093563064932823, -0.03258781507611275, -0.06876564770936966, -0.069304920732975, -0.07473353296518326, -0.0774729773402214, -0.07910919934511185, -0.08091700822114944], "norm_scores": [0.10433755815029144, 0.1040806919336319, 0.10271052271127701, 0.10204565525054932, 0.09841984510421753, 0.09836678206920624, 0.09783423691987991, 0.09756658971309662, 0.09740708023309708, 0.09723114222288132]}}
{"id": "WebQTest-848", "question": "where are the nordic countries", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["geography.river.basin_countries"], ["language.human_language.countries_spoken_in"], ["book.written_work.subjects"], ["book.book_subject.works"], ["language.human_language.main_country"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nordic countries [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.008704980835318565, -0.026168441399931908, -0.05323399230837822, -0.11519607901573181, -0.14924754202365875, -0.14968754351139069, -0.15028738975524902, -0.16764400899410248, -0.17135298252105713, -0.1789805293083191], "norm_scores": [0.11123637109994888, 0.10931065678596497, 0.1063917949795723, 0.09999961405992508, 0.09665180742740631, 0.09660928696393967, 0.09655135869979858, 0.09488999843597412, 0.09453871846199036, 0.09382035583257675]}}
{"id": "WebQTest-849", "question": "what country sponsored ferdinand magellan", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved"], ["people.person.sponsorship", "people.person.nationality"], ["people.person.nationality", "people.person.nationality"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["organization.organization_founder.organizations_founded"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.nationality", "royalty.monarch.kingdom"], ["base.culturalevent.event.entity_involved", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country sponsored ferdinand magellan [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.person.sponsorship <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> royalty.monarch.kingdom </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.person.nationality </PATH>"], "scores": [-0.0035869411658495665, -0.05741859972476959, -0.06778638809919357, -0.06895733624696732, -0.07936987280845642, -0.07987694442272186, -0.08253160864114761, -0.08361255377531052, -0.08555248379707336, -0.09022817760705948], "norm_scores": [0.10682404041290283, 0.10122556239366531, 0.10018149763345718, 0.10006426274776459, 0.09902773797512054, 0.09897755086421967, 0.09871513396501541, 0.09860849380493164, 0.09841737896203995, 0.0979582890868187]}}
{"id": "WebQTest-850", "question": "what did vera ellen die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["people.people.deceased_person.cause_of_death"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["people.marriage.spouse", "people.marriage.cause_of_death"], ["people.people.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did vera ellen die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.cause_of_death </PATH>", "<PATH> people.people.cause_of_death </PATH>", "</PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.002410391578450799, -0.037255894392728806, -0.130707785487175, -0.13078497350215912, -0.13794702291488647, -0.1475009173154831, -0.1646190732717514, -0.16971302032470703, -0.17124822735786438, -0.17409195005893707], "norm_scores": [0.11304336041212082, 0.10917215049266815, 0.09943201392889023, 0.099424347281456, 0.09871479868888855, 0.09777618199586868, 0.09611666947603226, 0.09562830626964569, 0.09548161178827286, 0.0952104777097702]}}
{"id": "WebQTest-851", "question": "what is the capital of modern day egypt", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.capital"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.country.administrative_divisions"], ["location.administrative_division.country"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern day egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010282715782523155, -0.024882052093744278, -0.038910623639822006, -0.06737715005874634, -0.07474299520254135, -0.09049612283706665, -0.0963490903377533, -0.11417815834283829, -0.11742640286684036, -0.11929156631231308], "norm_scores": [0.1066523864865303, 0.1051066517829895, 0.10364244133234024, 0.1007336974143982, 0.09999443590641022, 0.0984315574169159, 0.09785712510347366, 0.09612788259983063, 0.09581613540649414, 0.09563759714365005]}}
{"id": "WebQTest-852", "question": "what province is toronto in ( 7 letters )", "prediction": [["location.province.capital"], ["base.biblioness.bibs_location.state"], ["base.biblioness.bibs_location.province"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.city"], ["base.biblioness.bibs_location.country"], ["base.biblioness.bibs_location.province_of"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"]], "ground_paths": [["location.province.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is toronto in ( 7 letters ) [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.biblioness.bibs_location.province </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.city </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.biblioness.bibs_location.province_of </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.0028382064774632454, -0.04265986755490303, -0.0593409389257431, -0.07635371387004852, -0.07926937937736511, -0.09089978784322739, -0.09117872267961502, -0.09428679198026657, -0.09732048958539963, -0.10941687226295471], "norm_scores": [0.10736440867185593, 0.10317298769950867, 0.10146622359752655, 0.09975460171699524, 0.09946417808532715, 0.098314069211483, 0.09828665107488632, 0.09798163920640945, 0.097684845328331, 0.09651032835245132]}}
{"id": "WebQTest-855", "question": "what is the currency of brazil now", "prediction": [["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_used"], ["location.country.currency_used", "location.country.currency_used"], ["location.country.currency_used", "finance.currency.countries_used"], ["location.country.currency_currently_used"], ["location.country.currency_formerly_used", "finance.currency.countries_formerly_used"], ["government.form_of_government.countries"], ["location.country.currency_formerly_used", "location.country.currency_used"], ["common.topic.notable_types"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <SEP> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_currently_used </PATH>", "<PATH> location.country.currency_formerly_used <SEP> finance.currency.countries_formerly_used </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.currency_formerly_used <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0012761896941810846, -0.0597342923283577, -0.06764018535614014, -0.1591629534959793, -0.16244880855083466, -0.17878499627113342, -0.18340744078159332, -0.18530090153217316, -0.18697497248649597, -0.18762563169002533], "norm_scores": [0.11432011425495148, 0.1078287735581398, 0.10697964578866959, 0.09762325882911682, 0.09730301052331924, 0.0957263633608818, 0.09528489410877228, 0.0951046422123909, 0.09494557231664658, 0.09488381445407867]}}
{"id": "WebQTest-856", "question": "what types of art did andy warhol do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.art_form.artists"], ["visual_art.visual_art_form.artists"], ["visual_art.visual_artists.associated_periods_or_movements"], ["visual_art.artist.associated_periods_or_movements"], ["visual_art.visual_artist.associated_period_or_movements"], ["film.film.subjects"], ["film.film_subject.films"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what types of art did andy warhol do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.art_form.artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.visual_artists.associated_periods_or_movements </PATH>", "<PATH> visual_art.artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_artist.associated_period_or_movements </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> film.film_subject.films </PATH>"], "scores": [-0.006766539998352528, -0.020267268642783165, -0.059281639754772186, -0.07913728058338165, -0.09729211777448654, -0.10827507823705673, -0.11407281458377838, -0.11432348936796188, -0.12068866193294525, -0.1259477734565735], "norm_scores": [0.10800546407699585, 0.10655710846185684, 0.10247989743947983, 0.10046515613794327, 0.09865769743919373, 0.09758007526397705, 0.09701596200466156, 0.0969916507601738, 0.09637624025344849, 0.0958707183599472]}}
{"id": "WebQTest-857", "question": "what other states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.contains", "location.location.containedby"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.containedby", "location.location.contains"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.contains", "location.location.contains"], ["location.location.events", "time.event.locations"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.time_zones", "location.location.time_zones"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.adjoining_relationship.adjoins </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.contains <SEP> location.location.containedby </PATH>"], "scores": [-0.025724073871970177, -0.036830417811870575, -0.03876184672117233, -0.04039090499281883, -0.04043102264404297, -0.04125189036130905, -0.045946236699819565, -0.04797813668847084, -0.04806727170944214, -0.04890161380171776], "norm_scores": [0.10158058255910873, 0.10045863687992096, 0.1002647876739502, 0.10010159015655518, 0.10009757429361343, 0.10001543909311295, 0.09954703599214554, 0.09934496879577637, 0.09933611005544662, 0.09925326704978943]}}
{"id": "WebQTest-859", "question": "what are the predominant religions in russia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["book.newspaper.circulation_areas", "book.periodical.subjects"], ["language.human_language.main_country", "location.country.languages_spoken"], ["language.human_language.main_country", "religion.religion.notable_figures"], ["royalty.kingdom.rulers", "people.person.religion"], ["people.person.nationality", "people.deceased_person.religion"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "religion.religion.sacred_sites"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the predominant religions in russia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.subjects </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country <SEP> religion.religion.notable_figures </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>", "<PATH> people.person.nationality <SEP> people.deceased_person.religion </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> religion.religion.sacred_sites </PATH>"], "scores": [-0.008259591646492481, -0.012835451401770115, -0.10690215975046158, -0.10855483263731003, -0.10943413525819778, -0.11942480504512787, -0.12159055471420288, -0.12494176626205444, -0.1274469494819641, -0.13153041899204254], "norm_scores": [0.10918191075325012, 0.10868345201015472, 0.09892607480287552, 0.09876271337270737, 0.0986759141087532, 0.09769498556852341, 0.09748362749814987, 0.09715748578310013, 0.0969143956899643, 0.09651945531368256]}}
{"id": "WebQTest-861", "question": "who will coach carolina panthers in 2013", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team.sport", "sports.sport.team_coaches"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["american_football.football_team.current_head_coach"], ["american_football.football_historical_coach_position.team", "american_football.football_coach.coaching_history"], ["american_football.football_team.historical_coaching_staff", "american_football.football_historical_coach_position.coach"], ["american_football.football_coach.current_team_head_coached"], ["american_football.football_team.historical_coaching_staff", "american_football.football_coach.coaching_history"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will coach carolina panthers in 2013 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.team_coaches </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>", "<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_coach.coaching_history </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_coach.current_team_head_coached </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_coach.coaching_history </PATH>"], "scores": [-0.004165312275290489, -0.034858472645282745, -0.03588147461414337, -0.03667892888188362, -0.040612027049064636, -0.050697121769189835, -0.05426935851573944, -0.0637638121843338, -0.07081569731235504, -0.07096580415964127], "norm_scores": [0.10428088158369064, 0.10112880170345306, 0.10102539509534836, 0.10094486176967621, 0.10054861754179001, 0.09953966736793518, 0.09918472170829773, 0.0982474833726883, 0.09755708277225494, 0.09754244238138199]}}
{"id": "WebQTest-862", "question": "where did louis sachar grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here", "location.location.containedby"], ["book.author.book_editions_published", "book.book_edition.place_of_publication"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.place_of_birth", "location.location.containedby"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did louis sachar grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> book.author.book_editions_published <SEP> book.book_edition.place_of_publication </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.00441918708384037, -0.027841076254844666, -0.09640669077634811, -0.10104301571846008, -0.10586658120155334, -0.1110076829791069, -0.12074121087789536, -0.12185206264257431, -0.12330743670463562, -0.1272708624601364], "norm_scores": [0.10927792638540268, 0.10674817115068436, 0.09967420250177383, 0.09921316057443619, 0.0987357497215271, 0.09822943806648254, 0.09727795422077179, 0.09716995060443878, 0.09702863544225693, 0.09664483368396759]}}
{"id": "WebQTest-865", "question": "what to do for fun in houston", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["metropolitan_transit.transit_system.area_served"], ["base.usnationalparks.us_national_park.state"], ["organization.organization.geographic_scope"], ["olympics.olympic_participating_country.olympics_participated_in"], ["film.film.featured_film_locations"], ["film.film_festival.location"], ["olympics.olympic_games.host_city"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do for fun in houston [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> olympics.olympic_games.host_city </PATH>"], "scores": [-0.01337297260761261, -0.018197879195213318, -0.027726642787456512, -0.12033862620592117, -0.13122232258319855, -0.1466616541147232, -0.14723031222820282, -0.15256977081298828, -0.15430551767349243, -0.1592998206615448], "norm_scores": [0.10963620245456696, 0.10910850018262863, 0.10807376354932785, 0.09851433336734772, 0.0974479392170906, 0.09595496952533722, 0.09590041637420654, 0.095389723777771, 0.0952242910861969, 0.09474989771842957]}}
{"id": "WebQTest-866", "question": "where did otto frank go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["book.written_work.subjects", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did otto frank go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.0142679987475276, -0.01843384839594364, -0.02672559954226017, -0.04432736337184906, -0.06941573321819305, -0.07649954408407211, -0.08172710984945297, -0.0849541500210762, -0.09141580760478973, -0.10023235529661179], "norm_scores": [0.10471466183662415, 0.10427934676408768, 0.1034182608127594, 0.1016138419508934, 0.09909623116254807, 0.09839674085378647, 0.09788370132446289, 0.0975683405995369, 0.09693991392850876, 0.09608899801969528]}}
{"id": "WebQTest-867", "question": "what else has jennifer lawrence played in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else has jennifer lawrence played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.honored_for </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.02311798371374607, -0.03148985654115677, -0.03244450315833092, -0.033520977944135666, -0.03528929874300957, -0.03740999847650528, -0.037546444684267044, -0.04011095315217972, -0.04083283990621567, -0.04540390148758888], "norm_scores": [0.10126612335443497, 0.10042188316583633, 0.1003260537981987, 0.10021811723709106, 0.10004105418920517, 0.09982912242412567, 0.0998155027627945, 0.09955984354019165, 0.09948800504207611, 0.09903427958488464]}}
{"id": "WebQTest-868", "question": "who is the governor of pennsylvania state now", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["location.location.containedby", "people.person.place_of_birth"], ["location.administrative_division.country", "people.person.nationality"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"]], "ground_paths": [["location.location.containedby", "people.person.nationality"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of pennsylvania state now [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.appointed_by </PATH>"], "scores": [-0.024056050926446915, -0.02449388988316059, -0.025246286764740944, -0.027365418151021004, -0.03113148733973503, -0.046744950115680695, -0.04700280353426933, -0.047079265117645264, -0.04767555370926857, -0.051050275564193726], "norm_scores": [0.101315438747406, 0.10127109289169312, 0.10119492560625076, 0.10098070651292801, 0.10060111433267593, 0.09904258698225021, 0.09901704639196396, 0.09900947660207748, 0.09895046055316925, 0.09861709177494049]}}
{"id": "WebQTest-869", "question": "where is ancient phoenician", "prediction": [["location.location.containedby"], ["language.human_language.countries_spoken_in"], ["location.location.contains"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["language.human_language.region"], ["location.location.people_born_here"], ["people.ethnicity.geographic_distribution"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ancient phoenician [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.023122282698750496, -0.02686046063899994, -0.030448351055383682, -0.0524337962269783, -0.05500215291976929, -0.05951258912682533, -0.07533206045627594, -0.08165883272886276, -0.08538128435611725, -0.0900382399559021], "norm_scores": [0.10351798683404922, 0.1031317412853241, 0.102762371301651, 0.10052775591611862, 0.10026989132165909, 0.099818654358387, 0.09825199097394943, 0.09763234853744507, 0.09726958721876144, 0.09681765735149384]}}
{"id": "WebQTest-870", "question": "where does the celtics practice", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.professional_sports_team.owner_s", "sports.sports_team.arena_stadium"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.location"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.arena_stadium"], ["sports.professional_sports_team.owner_s", "sports.sports_facility.teams"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["sports.sports_facility.teams", "sports.sports_team.arena_stadium"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the celtics practice [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.location </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.arena_stadium </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.002138763200491667, -0.03998677805066109, -0.11360856890678406, -0.1208271011710167, -0.12557531893253326, -0.12598955631256104, -0.14047950506210327, -0.14329028129577637, -0.15091988444328308, -0.15456143021583557], "norm_scores": [0.11145297437906265, 0.10731352865695953, 0.09969674050807953, 0.0989796593785286, 0.09851080179214478, 0.09846999496221542, 0.09705346822738647, 0.09678105264902115, 0.09604545682668686, 0.0956963524222374]}}
{"id": "WebQTest-872", "question": "where bin laden got killed", "prediction": [["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_conflict.military_personnel_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["military.military_conflict.military_personnel_involved", "location.location.events"], ["book.written_work.subjects", "book.written_work.subjects"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where bin laden got killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> military.military_conflict.military_personnel_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> military.military_conflict.military_personnel_involved <SEP> location.location.events </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.subjects </PATH>"], "scores": [-0.0134050901979208, -0.027707701548933983, -0.04407973960042, -0.04601017013192177, -0.04837339371442795, -0.05224029719829559, -0.05872659385204315, -0.059332720935344696, -0.059559278190135956, -0.07591097801923752], "norm_scores": [0.10356083512306213, 0.10209018737077713, 0.1004323661327362, 0.10023866593837738, 0.10000207275152206, 0.09961611777544022, 0.09897207468748093, 0.09891209751367569, 0.09888969361782074, 0.09728582948446274]}}
{"id": "WebQTest-873", "question": "where does toronto get its water from", "prediction": [["location.location.containedby"], ["base.infrastructure.sewage_treatment_plant.serves_location_s"], ["base.infrastructure.sewage_treatment_plant.serves_location_s", "base.infrastructure.sewage_treatment_plant.serves_location_s"], ["location.location.containedby", "base.infrastructure.sewage_treatment_plant.serves_location_s"], ["base.infrastructure.sewage_treatment_plant.serves_location"], ["base.infrastructure.sewage_treatment_plant.serves_location_s", "base.infrastructure.sewage_treatment_plant.serves_location"], ["base.infrastructure.sewage_treatment_plant.serves_location_s", "base.infrastructure.sewage_treatment_plant.serves_location_s"], ["base.infrastructure.sewage_treatment_plant.serves_location_s"], ["organization.organization.place_founded", "base.infrastructure.sewage_treatment_plant.serves_location_s"], ["base.infrastructure.sewage_treatment_plant.serves"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does toronto get its water from [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s <SEP> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> location.location.containedby <SEP> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s <SEP> base.infrastructure.sewage_treatment_plant.serves_location </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s <SEP> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> organization.organization.place_founded <SEP> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves </PATH>"], "scores": [-0.014742156490683556, -0.024916894733905792, -0.04060455784201622, -0.05002385005354881, -0.05696636438369751, -0.06708838045597076, -0.06898237764835358, -0.07039918005466461, -0.07282982766628265, -0.0777658224105835], "norm_scores": [0.10402696579694748, 0.10297388583421707, 0.10137107223272324, 0.10042071342468262, 0.09972595423460007, 0.09872161597013474, 0.09853481501340866, 0.09839531034231186, 0.09815643727779388, 0.09767313301563263]}}
{"id": "WebQTest-874", "question": "what language do people from bosnia speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"], ["base.rosetta.languoid.languoid_class"], ["people.person.nationality", "people.ethnicity.people", "people.ethnicity.languages_spoken"], ["language.human_language.region"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from bosnia speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.person.nationality <SEP> people.ethnicity.people <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.014592226594686508, -0.02127060852944851, -0.02845275029540062, -0.0639953687787056, -0.15588314831256866, -0.1619935929775238, -0.16600193083286285, -0.18305030465126038, -0.18544723093509674, -0.19118167459964752], "norm_scores": [0.11052056401968002, 0.10978492349386215, 0.10899925976991653, 0.10519318282604218, 0.09595800191164017, 0.09537345170974731, 0.09499192982912064, 0.0933861956000328, 0.09316261857748032, 0.09262991696596146]}}
{"id": "WebQTest-875", "question": "where did hugo chavez go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.contains"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["influence.influence_node.influenced", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"], ["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hugo chavez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced <SEP> education.education.institution </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>"], "scores": [-0.015381469391286373, -0.02085700072348118, -0.028328539803624153, -0.028431475162506104, -0.0488404706120491, -0.05564077943563461, -0.06621582806110382, -0.07393532246351242, -0.12965305149555206, -0.1307433396577835], "norm_scores": [0.10446080565452576, 0.10389038920402527, 0.10311706364154816, 0.10310644656419754, 0.10102347284555435, 0.10033881664276123, 0.09928331524133682, 0.09851985424757004, 0.09318067878484726, 0.09307914227247238]}}
{"id": "WebQTest-876", "question": "who does japan import from", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["fictional_universe.fictional_universe.locations", "fictional_universe.fictional_universe.locations"], ["location.imports_and_exports.exported_to", "location.statistical_region.places_exported_to"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"], ["travel.travel_destination.tour_operators", "organization.organization.geographic_scope"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["location.country.form_of_government", "government.form_of_government.countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["location.country.form_of_government", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization.founders"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["cvg.computer_game_region.versions_released_in_this_region", "cvg.computer_game_region.versions_released_in_this_region"], ["film.film.featured_film_locations", "film.film.country"], ["location.statistical_region.places_imported_from", "location.imports_and_exports.imported_from"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_parent", "base.locations.countries.planet"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does japan import from [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.imports_and_exports.exported_to </PATH>"], "scores": [-0.026761040091514587, -0.033429164439439774, -0.03446776419878006, -0.03589626029133797, -0.037355341017246246, -0.03792969509959221, -0.04041176289319992, -0.041301701217889786, -0.041560299694538116, -0.04532545804977417], "norm_scores": [0.1010727658867836, 0.10040104389190674, 0.10029681771993637, 0.10015364736318588, 0.10000762343406677, 0.09995020180940628, 0.09970242530107498, 0.0996137410402298, 0.09958798438310623, 0.09921371191740036]}}
{"id": "WebQTest-877", "question": "what is the name of sonia gandhi daughter", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"], ["people.person.parent"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["influence.influence_node.influenced_by", "people.person.children"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["influence.influence_node.influenced_by", "people.person.parents"], ["fictional_universe.fictional_character.parents"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of sonia gandhi daughter [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.children </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.parents </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>"], "scores": [-0.008655338548123837, -0.018912959843873978, -0.11010847240686417, -0.14106567203998566, -0.14270472526550293, -0.14301304519176483, -0.14447812736034393, -0.1451863944530487, -0.14553891122341156, -0.1473010629415512], "norm_scores": [0.11103574186563492, 0.10990259796380997, 0.1003234013915062, 0.09726525843143463, 0.0971059575676918, 0.09707603603601456, 0.09693390876054764, 0.09686527401208878, 0.09683113545179367, 0.09666065126657486]}}
{"id": "WebQTest-878", "question": "who do islamic people pray to", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["base.argumentmaps.intellectual_dispute.positions"], ["base.argumentmaps.intellectual_dispute.positions", "book.book_subject.works"], ["religion.belief.belief_of"], ["location.statistical_region.religions", "location.religion_deity.deity_of"], ["common.topic.image"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["religion.belief.belief_of", "religion.religion.deities"]], "ground_paths": [["religion.religion.deities"], ["religion.deity.deity_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people pray to [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions <SEP> book.book_subject.works </PATH>", "<PATH> religion.belief.belief_of </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_deity.deity_of </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> religion.belief.belief_of <SEP> religion.religion.deities </PATH>"], "scores": [-0.013118801638484001, -0.013718116097152233, -0.06434079259634018, -0.07124070823192596, -0.0988348126411438, -0.10370062291622162, -0.11073195189237595, -0.11087192595005035, -0.1293574422597885, -0.13163802027702332], "norm_scores": [0.10733496397733688, 0.10727066546678543, 0.10197549313306808, 0.10127429664134979, 0.09851791709661484, 0.09803970903158188, 0.0973527804017067, 0.09733916074037552, 0.09555632621049881, 0.09533864259719849]}}
{"id": "WebQTest-879", "question": "where was mary todd lincoln from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "location.location.people_born_here"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mary todd lincoln from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0035959710367023945, -0.03279351070523262, -0.09394857287406921, -0.09523879736661911, -0.09703000634908676, -0.10545562952756882, -0.10817712545394897, -0.10956354439258575, -0.11412950605154037, -0.11992228031158447], "norm_scores": [0.10873211920261383, 0.10560330003499985, 0.09933863580226898, 0.09921056032180786, 0.09903300553560257, 0.09820209443569183, 0.09793519973754883, 0.09779951721429825, 0.0973539873957634, 0.09679166972637177]}}
{"id": "WebQTest-880", "question": "where did charles darwin die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["influence.influence_node.influenced_by"], ["location.location.people_deceased_here"], ["deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> location.location.people_deceased_here </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0008669394301250577, -0.07094317674636841, -0.07472921162843704, -0.08657500147819519, -0.09774671494960785, -0.10478320717811584, -0.11241164803504944, -0.11256494373083115, -0.11364154517650604, -0.11747664213180542], "norm_scores": [0.10917029529809952, 0.1017819494009018, 0.1013973280787468, 0.10020327568054199, 0.09909006953239441, 0.09839527308940887, 0.097647525370121, 0.09763256460428238, 0.09752750396728516, 0.09715419262647629]}}
{"id": "WebQTest-881", "question": "what movies does nick cannon play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["music.artist.track", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.actor.film", "film.film.starring"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does nick cannon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>"], "scores": [-0.016339680179953575, -0.03535657376050949, -0.03565231338143349, -0.03837478533387184, -0.04274061694741249, -0.043286655098199844, -0.04824031889438629, -0.048648104071617126, -0.06048634648323059, -0.06689123809337616], "norm_scores": [0.10275456309318542, 0.1008189469575882, 0.1007891371846199, 0.10051511973142624, 0.10007724165916443, 0.10002261400222778, 0.09952836483716965, 0.0994877889752388, 0.0983169674873352, 0.09768927097320557]}}
{"id": "WebQTest-882", "question": "when did the big earthquake hit san francisco", "prediction": [["time.event.locations"], ["location.location.events"], ["event.disaster.areas_affected"], ["meteorology.tropical_cyclone.affected_areas"], ["time.recurring_event.current_frequency"], ["time.recurring_event.locations"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["time.recurring_event.instances"], ["time.event.included_in_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the big earthquake hit san francisco [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.recurring_event.locations </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.included_in_event </PATH>"], "scores": [-0.00705300085246563, -0.029265474528074265, -0.04903097450733185, -0.07143539935350418, -0.0738600492477417, -0.08471384644508362, -0.09111476689577103, -0.0921000987291336, -0.09305356442928314, -0.11754630506038666], "norm_scores": [0.10654105246067047, 0.10420060157775879, 0.10216125100851059, 0.0998978391289711, 0.09965590387582779, 0.09858011454343796, 0.09795112907886505, 0.09785466641187668, 0.09776140004396439, 0.09539604187011719]}}
{"id": "WebQTest-883", "question": "who was king george vi married to", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.marriage.spouse", "people.marriage.spouse"], ["royalty.monarch.kingdom", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.spouse_s", "people.person.spouse_s"], ["film.film.subjects", "film.film_subject.films"], ["people.person.religion", "people.person.religion"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.family.members", "people.family.members"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king george vi married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.03450396656990051, -0.04004383087158203, -0.04096274450421333, -0.04106653109192848, -0.043932076543569565, -0.04805179685354233, -0.04915989562869072, -0.05038435384631157, -0.050566595047712326, -0.05204949900507927], "norm_scores": [0.1010608822107315, 0.10050256550312042, 0.10041025280952454, 0.10039982944726944, 0.10011254996061325, 0.09970096498727798, 0.09959054738283157, 0.0994686707854271, 0.09945055097341537, 0.09930317848920822]}}
{"id": "WebQTest-884", "question": "where was jesus after he died on the cross", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["time.event.locations"], ["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["people.place_of_interment.interred_here"], ["symbols.name_source.namesakes", "location.location.containedby"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_burial"], ["location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was jesus after he died on the cross [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> symbols.name_source.namesakes <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.00891293864697218, -0.028431423008441925, -0.06658853590488434, -0.07021664083003998, -0.07087153941392899, -0.07244899868965149, -0.07262890040874481, -0.07321048527956009, -0.07425643503665924, -0.0842607393860817], "norm_scores": [0.10544418543577194, 0.10340602695941925, 0.09953468292951584, 0.09917421638965607, 0.09910929203033447, 0.09895306825637817, 0.09893526881933212, 0.09887775033712387, 0.09877437353134155, 0.09779113531112671]}}
{"id": "WebQTest-885", "question": "who is sir james dyson", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced_by"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "people.person.profession"], ["symbols.name_source.namesakes"], ["influence.influence_node.influenced", "people.person.profession"], ["business.employment_tenure.person", "people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir james dyson [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> business.employment_tenure.person <SEP> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0006784267025068402, -0.068021260201931, -0.10299590229988098, -0.1198517382144928, -0.1261652112007141, -0.13086329400539398, -0.1392275094985962, -0.1441461741924286, -0.15075375139713287, -0.16333790123462677], "norm_scores": [0.11194741725921631, 0.10465680807828903, 0.10105974227190018, 0.0993705689907074, 0.09874516725540161, 0.0982823446393013, 0.09746372699737549, 0.09698551148176193, 0.09634678065776825, 0.0951419398188591]}}
{"id": "WebQTest-886", "question": "who voiced darth vader in the original star wars movies", "prediction": [["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.special_performance_type", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["tv.tv_guest_role.special_performance_type", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film.country", "people.person.nationality"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader in the original star wars movies [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_guest_role.special_performance_type <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.022288255393505096, -0.02254566177725792, -0.025503957644104958, -0.02948589436709881, -0.036158621311187744, -0.04953327029943466, -0.0519084632396698, -0.05671730265021324, -0.06213410571217537, -0.06395446509122849], "norm_scores": [0.10198038816452026, 0.1019541472196579, 0.10165297985076904, 0.10124899446964264, 0.10057564824819565, 0.09923943877220154, 0.09900400787591934, 0.09852905571460724, 0.097996786236763, 0.09781855344772339]}}
{"id": "WebQTest-887", "question": "what high school did maya angelou go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["book.written_work.author", "education.educational_institution.students_graduates"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here", "location.location.containedby"], ["book.written_work.author", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what high school did maya angelou go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.author <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.author <SEP> location.location.containedby </PATH>"], "scores": [-0.016323411837220192, -0.02041962742805481, -0.022249924018979073, -0.03184683993458748, -0.03644203022122383, -0.062368761748075485, -0.11231580376625061, -0.11349456757307053, -0.11785474419593811, -0.12407127767801285], "norm_scores": [0.10496638715267181, 0.10453730821609497, 0.1043461412191391, 0.10334952920675278, 0.1028757095336914, 0.10024276375770569, 0.09535890817642212, 0.0952465757727623, 0.09483218193054199, 0.09424448758363724]}}
{"id": "WebQTest-888", "question": "what 5 countries border switzerland", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.events", "time.event.locations"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["time.event.locations", "location.location.events"], ["location.country.official_language", "location.country.official_language"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.events", "time.event.locations"], ["film.film.country", "film.film.country"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["geography.lake.basin_countries", "location.location.partially_containedby"], ["time.event.locations", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.location.contains", "location.location.containedby"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["base.mystery.cryptid_classification.area_of_occurrence", "base.mystery.cryptid_observation_location.cryptid_s_occurring_here"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["location.location.containedby", "base.nations.supranational_body.member"], ["location.country.form_of_government", "government.form_of_government.countries"], ["film.film_location.featured_in_films", "film.film.featured_film_locations"], ["people.person.nationality", "people.person.nationality"], ["location.country.languages_spoken", "language.human_language.main_country"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.partially_contains", "location.location.partially_containedby"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["geography.river.origin", "location.location.partially_contains"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["geography.lake.basin_countries", "location.location.partially_contains"], ["location.location.contains", "location.location.partially_containedby"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["book.written_work.subjects", "book.written_work.subjects"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["location.location.containedby", "location.location.containedby"], ["base.aareas.schema.administrative_area.subdivides_place"], ["location.location.containedby", "base.locations.countries.continent"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["fictional_universe.work_of_fiction.setting", "fictional_universe.fictional_setting.works_set_here"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.contains", "location.location.contains"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.is_reviewed"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.events", "location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 5 countries border switzerland [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.events <SEP> time.event.locations </PATH>"], "scores": [-0.03593514487147331, -0.04003658890724182, -0.04228554666042328, -0.043019186705350876, -0.04403560981154442, -0.046389296650886536, -0.04664874076843262, -0.04675603657960892, -0.04766736552119255, -0.04816145822405815], "norm_scores": [0.10081848502159119, 0.10040583461523056, 0.10018027573823929, 0.10010680556297302, 0.10000510513782501, 0.09977000951766968, 0.09974413365125656, 0.09973341971635818, 0.09964257478713989, 0.09959335625171661]}}
{"id": "WebQTest-889", "question": "what country did benito mussolini govern", "prediction": [["people.person.nationality"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.governmental_jurisdiction.governing_officials"], ["government.political_appointer.appointees", "government.governmental_jurisdiction.governing_officials"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.appointed_by", "government.government_position_held.jurisdiction_of_office"], ["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did benito mussolini govern [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.016141291707754135, -0.027112731710076332, -0.03045695461332798, -0.0356314480304718, -0.039700139313936234, -0.0498131699860096, -0.052614662796258926, -0.055039651691913605, -0.05543846637010574, -0.05975699424743652], "norm_scores": [0.10262728482484818, 0.10150747001171112, 0.10116857290267944, 0.10064642876386642, 0.10023776441812515, 0.09922916442155838, 0.0989515632390976, 0.09871190041303635, 0.0986725315451622, 0.09824733436107635]}}
{"id": "WebQTest-891", "question": "who was the leader of the first communist party of china", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization.founders", "influence.influence_node.influenced"], ["government.political_party_tenure.party", "government.political_party_tenure.politician"], ["organization.organization.founders", "influence.influence_node.influenced_by"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["people.person.profession"], ["government.political_party_tenure.party", "government.politician.party"], ["people.person.nationality"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the first communist party of china [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.founders <SEP> influence.influence_node.influenced </PATH>", "<PATH> government.political_party_tenure.party <SEP> government.political_party_tenure.politician </PATH>", "<PATH> organization.organization.founders <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> government.political_party_tenure.party <SEP> government.politician.party </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.003865029662847519, -0.029095811769366264, -0.08371713012456894, -0.08398599922657013, -0.0852578729391098, -0.08592735975980759, -0.08644168823957443, -0.09016706794500351, -0.09033992141485214, -0.09363400191068649], "norm_scores": [0.10713809728622437, 0.10446873307228088, 0.09891555458307266, 0.09888897091150284, 0.09876327216625214, 0.09869717061519623, 0.09864642471075058, 0.09827961772680283, 0.09826263040304184, 0.09793946892023087]}}
{"id": "WebQTest-892", "question": "what did miles davis died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.people.deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by"], ["music.artist.genre", "music.artist.genre"], ["people.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did miles davis died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.0021725662518292665, -0.03898417577147484, -0.12597791850566864, -0.14761190116405487, -0.15167436003684998, -0.16223815083503723, -0.16260327398777008, -0.16352055966854095, -0.16423045098781586, -0.16473141312599182], "norm_scores": [0.11327071487903595, 0.10917685180902481, 0.10008054226636887, 0.0979386568069458, 0.09754159301519394, 0.09651660174131393, 0.09648136794567108, 0.0963929072022438, 0.09632451087236404, 0.09627626836299896]}}
{"id": "WebQTest-893", "question": "what is the zip code for azle texas", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"], ["location.citytown.postal_ codes"], ["location.citytown.postal_codes"], ["location.town.postal_codes"], ["location.us_county.county_seat"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for azle texas [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_ codes </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.town.postal_codes </PATH>", "<PATH> location.us_county.county_seat </PATH>", "</PATH> location.citytown.postal_codes </PATH>"], "scores": [-0.00013193751510698348, -0.09751643240451813, -0.13071320950984955, -0.17019331455230713, -0.1733916997909546, -0.17591355741024017, -0.17724201083183289, -0.17798157036304474, -0.1795397847890854, -0.1819995939731598], "norm_scores": [0.11557543277740479, 0.10485085099935532, 0.10142728686332703, 0.09750093519687653, 0.09718959778547287, 0.09694480150938034, 0.09681610763072968, 0.09674452245235443, 0.09659390151500702, 0.0963565781712532]}}
{"id": "WebQTest-894", "question": "what date did sally pearson won gold", "prediction": [["people.person.nationality", "olympics.olympic_participating_country.olympics_participated_in"], ["people.person.profession", "common.topic.notable_types"], ["people.person.nationality", "olympics.olympic_games.participating_countries"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.olympics"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["people.person.nationality", "olympics.olympic_games.host_city"], ["people.person.profession", "people.person.profession"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_medal_honor.olympics"], ["people.person.nationality", "olympics.olympic_medal_honor.olympic_medalists"], ["people.person.nationality", "olympics.olympic_medal_honor.olympics"]], "ground_paths": [["olympics.olympic_athlete_affiliation.athlete", "olympics.olympic_athlete_affiliation.olympics"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.olympics"], ["people.person.nationality", "olympics.olympic_games.participating_countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what date did sally pearson won gold [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.olympics </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.host_city </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_medal_honor.olympics </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_medal_honor.olympic_medalists </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_medal_honor.olympics </PATH>"], "scores": [-0.020235707983374596, -0.02757745049893856, -0.03307000920176506, -0.03704467788338661, -0.039304353296756744, -0.04988747835159302, -0.05194098874926567, -0.05528029426932335, -0.05615215003490448, -0.05852536857128143], "norm_scores": [0.10228431969881058, 0.10153611749410629, 0.10097996145486832, 0.1005793958902359, 0.10035237669944763, 0.09929593652486801, 0.09909223765134811, 0.0987618938088417, 0.09867583215236664, 0.09844192862510681]}}
{"id": "WebQTest-895", "question": "when did the detroit pistons last win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_teams.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["sports.sports_championship"], ["sports.sports_league_season.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the detroit pistons last win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_teams.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship </PATH>", "<PATH> sports.sports_league_season.champion </PATH>", "<PATH> sports.sports_championship_event.champion <PATH>"], "scores": [-0.011059620417654514, -0.01157063152641058, -0.11440572887659073, -0.11926373094320297, -0.14497889578342438, -0.15163083374500275, -0.15450726449489594, -0.1632331907749176, -0.16618196666240692, -0.16868282854557037], "norm_scores": [0.11138389259576797, 0.11132698506116867, 0.1004476472735405, 0.09996085613965988, 0.09742312133312225, 0.09677720814943314, 0.0964992344379425, 0.09566085785627365, 0.09537918865680695, 0.0951409563422203]}}
{"id": "WebQTest-896", "question": "when did the philadelphia flyers win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_teams.champion"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["sports.sports_club.championships"], ["sports.sports_league_season.champion"], ["sports.sports_winning_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the philadelphia flyers win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_teams.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_club.championships </PATH>", "<PATH> sports.sports_league_season.champion </PATH>", "<PATH> sports.sports_winning_team.championships </PATH>"], "scores": [-0.009012648835778236, -0.014437543228268623, -0.10620885342359543, -0.12693598866462708, -0.147760808467865, -0.15271206200122833, -0.15824072062969208, -0.16738857328891754, -0.1676451712846756, -0.16853097081184387], "norm_scores": [0.11175564676523209, 0.11115103214979172, 0.10140462219715118, 0.09932442754507065, 0.09727739542722702, 0.09679693728685379, 0.09626325219869614, 0.09538666903972626, 0.09536220133304596, 0.09527777135372162]}}
{"id": "WebQTest-898", "question": "what county is st paul nc in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.citytown.postal_codes"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.place_with_neighborhoods.neighborhoods"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul nc in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.place_with_neighborhoods.neighborhoods </PATH>"], "scores": [-0.010282509960234165, -0.028302907943725586, -0.03586192429065704, -0.04874666780233383, -0.07505425065755844, -0.09191861748695374, -0.09266214072704315, -0.09887297451496124, -0.105504110455513, -0.10829239338636398], "norm_scores": [0.10604459792375565, 0.10415074229240417, 0.1033664345741272, 0.10204314440488815, 0.09939362853765488, 0.09773147106170654, 0.09765883535146713, 0.09705417603254318, 0.09641272574663162, 0.09614426642656326]}}
{"id": "WebQTest-899", "question": "where is olympic national park wa", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"], ["location.hud_county_place.county"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["geography.lake.basin_countries"], ["location.location.containedby", "location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park wa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.008817878551781178, -0.03094925731420517, -0.031207744032144547, -0.07771807909011841, -0.09630028903484344, -0.0979820191860199, -0.10044872760772705, -0.10369677096605301, -0.12850762903690338, -0.1312611699104309], "norm_scores": [0.10736297816038132, 0.10501298308372498, 0.10498584806919098, 0.10021474212408066, 0.09836971759796143, 0.09820442646741867, 0.09796249121427536, 0.09764481335878372, 0.09525196999311447, 0.09499004483222961]}}
{"id": "WebQTest-900", "question": "where did nat turner 1831 rebellion occur", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["military.military_person.participated_in_conflicts"], ["event.disaster.areas_attack_process"], ["event.disaster.areas_attack_event"], ["event.disaster.areas_attack"], ["military.military_conflict.military_personnel_involved"], ["base.culturalevent.event.entity_involved", "time.event.locations"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did nat turner 1831 rebellion occur [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> event.disaster.areas_attack_process </PATH>", "<PATH> event.disaster.areas_attack_event </PATH>", "<PATH> event.disaster.areas_attack </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>"], "scores": [-0.01209018100053072, -0.014057577587664127, -0.058276306837797165, -0.08657216280698776, -0.09852388501167297, -0.10142196714878082, -0.10228560864925385, -0.11089737713336945, -0.11153827607631683, -0.11329567432403564], "norm_scores": [0.10704786330461502, 0.10683746635913849, 0.10221617668867111, 0.09936441481113434, 0.09818391501903534, 0.09789977222681046, 0.09781526774168015, 0.09697651863098145, 0.0969143882393837, 0.09674421697854996]}}
{"id": "WebQTest-901", "question": "who would play luke skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films_dubbed", "film.actor.dubbing_performances"], ["film.film_character.portrayed_in_films_dubbed", "film.dubbing_performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who would play luke skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films_dubbed <SEP> film.actor.dubbing_performances </PATH>", "<PATH> film.film_character.portrayed_in_films_dubbed <SEP> film.dubbing_performance.actor </PATH>"], "scores": [-0.02074948325753212, -0.021670810878276825, -0.02299751155078411, -0.03609924763441086, -0.0388772152364254, -0.054280322045087814, -0.06245430186390877, -0.08157455921173096, -0.08676230907440186, -0.09452863037586212], "norm_scores": [0.10313746333122253, 0.10304249078035355, 0.10290586948394775, 0.10156641155481339, 0.10128466039896011, 0.0997365191578865, 0.09892459958791733, 0.09705109894275665, 0.09654892235994339, 0.09580199420452118]}}
{"id": "WebQTest-903", "question": "where turkish people originate", "prediction": [["people.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "people.ethnicity.geographic_distribution"], ["people.ethnicity.included_in_group"], ["people.ethnicity.geographic_distribution", "base.locations.continents.countries_within"], ["people.ethnicity.geographic_distribution", "base.locations.countries.continent"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["people.person.ethnicity"], ["base.popstra.ethnicity.geographic_distribution"], ["location.location.containedby"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where turkish people originate [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.continents.countries_within </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.countries.continent </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> base.popstra.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-7.357802678598091e-05, -0.12638713419437408, -0.13429388403892517, -0.140268474817276, -0.141449972987175, -0.1443910002708435, -0.1494327336549759, -0.15312816202640533, -0.15590602159500122, -0.15651629865169525], "norm_scores": [0.11377954483032227, 0.10027828067541122, 0.09948853403329849, 0.09889590740203857, 0.09877913445234299, 0.09848905354738235, 0.09799373894929886, 0.0976322814822197, 0.09736144542694092, 0.09730204939842224]}}
{"id": "WebQTest-904", "question": "what state is barack obama from", "prediction": [["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_parent"], ["people.person.place_of_birth"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["government.government_position_held.office_holder", "government.political_district.representatives"]], "ground_paths": [["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["base.popstra.celebrity.vacations_in", "base.popstra.location.vacationers"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is barack obama from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.state </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.political_district.representatives </PATH>"], "scores": [-0.01792788691818714, -0.023041723296046257, -0.04022899270057678, -0.0445338636636734, -0.04474993050098419, -0.04566249996423721, -0.047159723937511444, -0.05427400767803192, -0.05624452233314514, -0.05671689286828041], "norm_scores": [0.10253643989562988, 0.10201342403888702, 0.10027506947517395, 0.09984433650970459, 0.0998227596282959, 0.09973170608282089, 0.0995825007557869, 0.09887655824422836, 0.09868191182613373, 0.09863530099391937]}}
{"id": "WebQTest-905", "question": "what years did joe montana win super bowl", "prediction": [["sports.sports_championship_event.champion"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_championship_event.champion"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team.championships"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what years did joe montana win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.006060664542019367, -0.02908727526664734, -0.0488661490380764, -0.04915230721235275, -0.050608281046152115, -0.051321838051080704, -0.06102579087018967, -0.06487000733613968, -0.08174750953912735, -0.08253991603851318], "norm_scores": [0.10473156720399857, 0.10234750062227249, 0.10034307837486267, 0.10031437128782272, 0.10016841441392899, 0.10009697079658508, 0.09913033246994019, 0.09874998033046722, 0.09709731489419937, 0.09702040255069733]}}
{"id": "WebQTest-908", "question": "what awards gary paulsen won", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_winner.awards_won", "award.award_category.winners"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nominee.award_nominations", "award.award_category.nominees"], ["award.award_nomination.award_nominee", "award.award_category.nominees"], ["award.award_nominee.award_nominations", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_honor.award"]], "ground_paths": [["award.award_winner.awards_won", "award.award_category.winners"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_winner.awards_won", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards gary paulsen won [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_category.winners </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_category.winners </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_category.nominees </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_category.nominees </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_honor.award </PATH>"], "scores": [-0.02045481838285923, -0.02453303337097168, -0.024812212213873863, -0.03035379759967327, -0.03054855205118656, -0.03190764784812927, -0.04237177595496178, -0.0493580587208271, -0.09628000110387802, -0.11096835881471634], "norm_scores": [0.10255806148052216, 0.10214065760374069, 0.10211215913295746, 0.10154785215854645, 0.10152807831764221, 0.10139018297195435, 0.10033474862575531, 0.09963623434305191, 0.09506909549236298, 0.09368288516998291]}}
{"id": "WebQTest-910", "question": "where did alexander graham bell die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people", "people.place_of_death.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.nationality", "location.location.containedby"], ["people.place_of_death.people_deceased_here"], ["location.location.people_born_here"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alexander graham bell die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0003902257012668997, -0.09048815071582794, -0.09304311126470566, -0.09444381296634674, -0.10413346439599991, -0.10753291845321655, -0.11219985038042068, -0.11416349560022354, -0.11462868750095367, -0.11710280925035477], "norm_scores": [0.10984182357788086, 0.10037803649902344, 0.10012190788984299, 0.09998176246881485, 0.09901764988899231, 0.09868161380290985, 0.09822215884923935, 0.0980294719338417, 0.09798388183116913, 0.0977417528629303]}}
{"id": "WebQTest-911", "question": "what year did the golden state warriors win their first nba championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_championship.events"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["sports.sports_team_championship_event.champion"], ["sports.sports_league_season.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the golden state warriors win their first nba championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_team_championship_event.champion </PATH>", "<PATH> sports.sports_league_season.champion </PATH>"], "scores": [-0.010641677305102348, -0.01132627110928297, -0.10160156339406967, -0.10808061063289642, -0.12678299844264984, -0.1388860046863556, -0.14556945860385895, -0.14856261014938354, -0.15900228917598724, -0.15948140621185303], "norm_scores": [0.1103961169719696, 0.1103205680847168, 0.10079765319824219, 0.10014669597148895, 0.09829112142324448, 0.09710866957902908, 0.09646181017160416, 0.09617352485656738, 0.09517472237348557, 0.09512913227081299]}}
{"id": "WebQTest-912", "question": "what currency should i take to turkey", "prediction": [["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.016908809542655945, -0.016980264335870743, -0.041262783110141754, -0.041842006146907806, -0.11965789645910263, -0.12209701538085938, -0.13509106636047363, -0.13702981173992157, -0.14233502745628357, -0.1488451361656189], "norm_scores": [0.10767024755477905, 0.10766255110502243, 0.10507971793413162, 0.10501887649297714, 0.09715660661458969, 0.09691992402076721, 0.09566868841648102, 0.09548338502645493, 0.09497816860675812, 0.09436186403036118]}}
{"id": "WebQTest-914", "question": "what countries are in the uk yahoo", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.country.first_level_divisions"], ["base.biblioness.bibs_location.country"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["location.location.contains"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the uk yahoo [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.020513176918029785, -0.026127280667424202, -0.027738861739635468, -0.03947218507528305, -0.04458551108837128, -0.048490528017282486, -0.056092310696840286, -0.07600945234298706, -0.08905322849750519, -0.10061326622962952], "norm_scores": [0.10325364023447037, 0.10267558693885803, 0.1025102511048317, 0.1013144925236702, 0.10079775750637054, 0.10040491074323654, 0.09964454919099808, 0.09767954796552658, 0.09641370922327042, 0.09530557692050934]}}
{"id": "WebQTest-915", "question": "what superbowl did the baltimore ravens win", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship"], ["sports.championship_event.champion"], ["sports.sports_title.champion"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what superbowl did the baltimore ravens win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship </PATH>", "<PATH> sports.championship_event.champion </PATH>", "<PATH> sports.sports_title.champion </PATH>"], "scores": [-0.011437477543950081, -0.01369277574121952, -0.04605631157755852, -0.08713553100824356, -0.09744977205991745, -0.12249228358268738, -0.12955574691295624, -0.13936595618724823, -0.14743924140930176, -0.153041809797287], "norm_scores": [0.10854695737361908, 0.10830243676900864, 0.10485349595546722, 0.10063346475362778, 0.09960083663463593, 0.09713755548000336, 0.09645384550094604, 0.09551224112510681, 0.0947442501783371, 0.09421492367982864]}}
{"id": "WebQTest-916", "question": "what is the language of russia s spoken", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.language_family"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.country"], ["language.human_language.region"], ["country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the language of russia s spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.0192762091755867, -0.01982332579791546, -0.02031373605132103, -0.07885004580020905, -0.1475442498922348, -0.16010844707489014, -0.16254571080207825, -0.16947375237941742, -0.19075678288936615, -0.19156591594219208], "norm_scores": [0.10988956689834595, 0.10982945561408997, 0.109775610268116, 0.10353420674800873, 0.0966607928276062, 0.09545391798019409, 0.09522156417369843, 0.09456414729356766, 0.09257280081510544, 0.09249792248010635]}}
{"id": "WebQTest-918", "question": "who is the king of cambodia now", "prediction": [["people.person.nationality"], ["royalty.monarch.kingdom"], ["royalty.kingdom.rulers"], ["royalty.monarch.kingdom", "people.person.nationality"], ["people.person.place_of_birth"], ["royalty.noble_title_tenure.noble_person", "royalty.noble_title_tenure.noble_person"], ["royalty.royal_line.monarchs_from_this_line", "people.person.nationality"], ["royalty.noble_title_tenure.noble_person", "royalty.noble_title.holders"], ["people.family.members"], ["people.family_member.family"]], "ground_paths": [["royalty.kingdom.rulers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the king of cambodia now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> royalty.noble_title_tenure.noble_person <SEP> royalty.noble_title_tenure.noble_person </PATH>", "<PATH> royalty.royal_line.monarchs_from_this_line <SEP> people.person.nationality </PATH>", "<PATH> royalty.noble_title_tenure.noble_person <SEP> royalty.noble_title.holders </PATH>", "<PATH> people.family.members </PATH>", "<PATH> people.family_member.family </PATH>"], "scores": [-0.0162083450704813, -0.01693788729608059, -0.036888040602207184, -0.08766471594572067, -0.08955606073141098, -0.09553661197423935, -0.10116210579872131, -0.10630329698324203, -0.10770057886838913, -0.11436101794242859], "norm_scores": [0.10622101277112961, 0.10614355653524399, 0.10404694825410843, 0.09889567643404007, 0.09870880842208862, 0.09812023490667343, 0.09756980836391449, 0.09706947952508926, 0.09693393856287003, 0.0962904617190361]}}
{"id": "WebQTest-919", "question": "who are serena williams parents", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.profession"], ["influence.influence_node.influenced_by"], ["people.person.parent"], ["influence.influence_node.influenced"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are serena williams parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.celebrity.canoodled </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.008727136999368668, -0.02134324051439762, -0.15842165052890778, -0.16828273236751556, -0.1691819280385971, -0.1696823090314865, -0.17412756383419037, -0.18006111681461334, -0.18080179393291473, -0.18314024806022644], "norm_scores": [0.1139470785856247, 0.11251853406429291, 0.09810511767864227, 0.09714245796203613, 0.09705513715744019, 0.09700659662485123, 0.09657633304595947, 0.09600498527288437, 0.09593390673398972, 0.09570983052253723]}}
{"id": "WebQTest-920", "question": "what schools did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"], ["influence.influence_node.influenced_by", "education.education.student", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["influence.influence_node.influenced_by", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.010734726674854755, -0.023037130013108253, -0.02862645499408245, -0.02961265854537487, -0.060123711824417114, -0.1034146398305893, -0.10576874762773514, -0.12333627790212631, -0.12394385039806366, -0.1247573047876358], "norm_scores": [0.10635259002447128, 0.10505221784114838, 0.10446666926145554, 0.10436370223760605, 0.1012275367975235, 0.09693881869316101, 0.09671088308095932, 0.09502674639225006, 0.09496902674436569, 0.09489179402589798]}}
{"id": "WebQTest-921", "question": "what was the cause of death for huell howser", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.cause_of_death"], ["deceased_person.cause_of_death"], ["film.film.subjects", "people.deceased_person.cause_of_death"], ["people.people.deceased_person.cause_of_death"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["influence.influence_node.influenced", "people.deceased_person.cause_of_death"], ["people.person.deceased_people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for huell howser [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> film.film.subjects <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_people </PATH>"], "scores": [-0.0010955168399959803, -0.04911136254668236, -0.09249565750360489, -0.10459210723638535, -0.10953152924776077, -0.12062240391969681, -0.1220880076289177, -0.12500841915607452, -0.12521085143089294, -0.1266518086194992], "norm_scores": [0.11004964262247086, 0.1048903688788414, 0.10043708235025406, 0.09922946244478226, 0.09874053299427032, 0.09765146672725677, 0.09750845283269882, 0.09722410887479782, 0.09720442444086075, 0.09706445783376694]}}
{"id": "WebQTest-922", "question": "what books are written by suzanne collins", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.author.book_editions_published"], ["book.written_work.author"], ["film.film_story_contributor.film_story_credits"], ["film.producer.films_executive_produced"], ["film.film.story_by"], ["book.writer.works_written"], ["film.writer.film"], ["book.author.work_written"]], "ground_paths": [["book.author.book_editions_published"], ["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books are written by suzanne collins [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.producer.films_executive_produced </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> book.writer.works_written </PATH>", "<PATH> film.writer.film </PATH>", "<PATH> book.author.work_written </PATH>"], "scores": [-0.014006117358803749, -0.021128877997398376, -0.0393386036157608, -0.043436769396066666, -0.0778229758143425, -0.08267273753881454, -0.10499854385852814, -0.12336883693933487, -0.12775911390781403, -0.1420787125825882], "norm_scores": [0.10646907240152359, 0.10571341961622238, 0.10380582511425018, 0.10338128358125687, 0.09988681226968765, 0.09940356016159058, 0.0972088873386383, 0.09543943405151367, 0.09502135217189789, 0.09367036819458008]}}
{"id": "WebQTest-923", "question": "what countries does england border", "prediction": [["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["location.location.events", "time.event.locations"], ["location.location.events", "location.location.events"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["time.event.locations", "location.location.events"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["time.event.locations", "time.event.locations"]], "ground_paths": [["time.event.locations", "location.location.events"], ["base.biblioness.bibs_location.country", "base.aareas.schema.administrative_area.administrative_children"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["sports.sports_team_location.teams", "cricket.cricket_team.country"], ["cricket.cricket_tournament_event.host", "location.location.events"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["time.event.locations", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["finance.currency.countries_used", "location.country.currency_used"], ["location.country.languages_spoken", "location.country.official_language"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["cricket.cricket_tournament_event.host", "time.event.locations"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["government.governmental_jurisdiction.government_bodies", "government.governmental_jurisdiction.government_bodies"], ["people.person.nationality", "people.person.nationality"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.partially_contains", "location.location.partially_containedby"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["law.court_jurisdiction_area.courts", "law.court_jurisdiction_area.courts"], ["location.location.events", "location.location.events"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["location.country.form_of_government", "location.country.form_of_government"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["government.government_agency.jurisdiction", "government.government_agency.jurisdiction"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["organization.organization.geographic_scope", "common.topic.subject_of"], ["periodicals.newspaper_circulation_area.newspapers", "periodicals.newspaper_circulation_area.newspapers"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["law.court_jurisdiction_area.courts", "law.court.jurisdiction"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.time_zones", "location.location.time_zones"], ["location.country.internet_tld", "location.country.internet_tld"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["government.government_agency.jurisdiction", "government.governmental_jurisdiction.agencies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does england border [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> location.location.events <SEP> time.event.locations </PATH>", "<PATH> location.location.events <SEP> location.location.events </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> biology.animal_breed.place_of_origin <SEP> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> time.event.locations <SEP> location.location.events </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.adjoining_relationship.adjoins </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>"], "scores": [-0.042453519999980927, -0.042686525732278824, -0.042897824198007584, -0.045039236545562744, -0.04532422125339508, -0.04559975117444992, -0.048352163285017014, -0.048830293118953705, -0.05157219618558884, -0.057165298610925674], "norm_scores": [0.1004539206624031, 0.10043051838874817, 0.10040929913520813, 0.10019451379776001, 0.1001659631729126, 0.100138358771801, 0.09986311942338943, 0.09981539100408554, 0.09954208135604858, 0.09898688644170761]}}
{"id": "WebQTest-924", "question": "what team did messi play for in 2010", "prediction": [["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did messi play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.02466162107884884, -0.024916844442486763, -0.02492799423635006, -0.03322378173470497, -0.03392334282398224, -0.034999534487724304, -0.037755392491817474, -0.03838082775473595, -0.03944866359233856, -0.040663257241249084], "norm_scores": [0.10086479038000107, 0.1008390486240387, 0.10083792358636856, 0.10000485181808472, 0.09993492066860199, 0.09982742369174957, 0.0995526984333992, 0.0994904488325119, 0.09938427060842514, 0.09926363080739975]}}
{"id": "WebQTest-925", "question": "what time zone is greensburg indiana", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.location.contains", "location.location.time_zones"], ["location.location.contains"], ["location.location.containedby", "location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.nearby_airports", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is greensburg indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains <SEP> location.location.time_zones </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.nearby_airports <SEP> location.location.time_zones </PATH>"], "scores": [-2.2522090148413554e-05, -0.15071333944797516, -0.15096759796142578, -0.16115087270736694, -0.16588498651981354, -0.17045585811138153, -0.18194131553173065, -0.18564173579216003, -0.1880386918783188, -0.19527818262577057], "norm_scores": [0.1165890097618103, 0.1002797931432724, 0.10025429725646973, 0.0992385596036911, 0.09876985847949982, 0.09831943362951279, 0.09719664603471756, 0.09683763980865479, 0.0966058075428009, 0.09590895473957062]}}
{"id": "WebQTest-927", "question": "who plays voldemort", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["cvg.game_character.games", "cvg.game_voice_actor.computer_game_voice_performances"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays voldemort [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> cvg.game_character.games <SEP> cvg.game_voice_actor.computer_game_voice_performances </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.021735360845923424, -0.02294539287686348, -0.02383691631257534, -0.04011811316013336, -0.043650828301906586, -0.06461428850889206, -0.07127731293439865, -0.08198419213294983, -0.08230944722890854, -0.08398810029029846], "norm_scores": [0.10321097075939178, 0.10308616608381271, 0.10299430042505264, 0.10133100301027298, 0.10097365826368332, 0.09887893497943878, 0.0982222929596901, 0.09717625379562378, 0.09714465588331223, 0.0969817116856575]}}
{"id": "WebQTest-928", "question": "what colleges did harper lee attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["common.topic.notable_types"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.educational_institution.parent_institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did harper lee attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>"], "scores": [-0.014063344337046146, -0.023396695032715797, -0.028328631073236465, -0.029166605323553085, -0.05060850828886032, -0.06141326576471329, -0.0779699981212616, -0.1010534018278122, -0.10900654643774033, -0.12449362874031067], "norm_scores": [0.10483195632696152, 0.10385807603597641, 0.10334711521863937, 0.10326054692268372, 0.10107002407312393, 0.09998385608196259, 0.09834207594394684, 0.09609800577163696, 0.09533676505088806, 0.09387164562940598]}}
{"id": "WebQTest-929", "question": "who founded the kkk", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["base.activism.activism_issue.activists"], ["base.activism.activist_organization.founders"], ["base.activism.activist_issue.activists"], ["base.activism.activist.organizations_founded"], ["organization.organization.spun_off_from", "organization.organization_founder.organizations_founded"], ["symbols.organization_founder.organizations_founded"], ["base.activism.activist_organization_founder.organizations_founded"], ["base.activism.activism_issue.activists", "base.activism.activist.area_of_activism"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the kkk [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> base.activism.activist_organization.founders </PATH>", "<PATH> base.activism.activist_issue.activists </PATH>", "<PATH> base.activism.activist.organizations_founded </PATH>", "<PATH> organization.organization.spun_off_from <SEP> organization.organization_founder.organizations_founded </PATH>", "<PATH> symbols.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activist_organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activism_issue.activists <SEP> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.010038294829428196, -0.018074093386530876, -0.08808799088001251, -0.0973639041185379, -0.11385547369718552, -0.1225351020693779, -0.12376610934734344, -0.12992724776268005, -0.13017046451568604, -0.1308227926492691], "norm_scores": [0.10892259329557419, 0.10805081576108932, 0.10074450820684433, 0.09981433302164078, 0.09818173944950104, 0.09733325988054276, 0.0972135066986084, 0.09661640226840973, 0.09659290313720703, 0.09652991592884064]}}
{"id": "WebQTest-931", "question": "what jobs did john adams have before he was president", "prediction": [["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["people.person.nationality", "government.governmental_jurisdiction.government_positions"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["common.topic.notable_types", "type.type.expected_by"]], "ground_paths": [["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["common.topic.image", "common.topic.image"], ["tv.tv_program.country_of_origin", "government.government_office_or_title.jurisdiction"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did john adams have before he was president [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> people.person.nationality <SEP> government.government_office_or_title.jurisdiction </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.nationality <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>"], "scores": [-0.022276293486356735, -0.023362470790743828, -0.024077914655208588, -0.03083833120763302, -0.03262472525238991, -0.035881441086530685, -0.048460692167282104, -0.04970979318022728, -0.04985595494508743, -0.050011079758405685], "norm_scores": [0.1014474630355835, 0.10133734345436096, 0.10126485675573349, 0.10058257728815079, 0.10040305554866791, 0.1000766009092331, 0.0988255962729454, 0.09870222955942154, 0.0986878052353859, 0.09867250174283981]}}
{"id": "WebQTest-932", "question": "what year did super mario bros 2 come out", "prediction": [["film.film.sequel"], ["film.film.country"], ["common.topic.notable_types"], [], ["computer.software_genre.games"], ["computer.computer_game_region.versions_released_in_this_region"], ["film.film.release_date"], ["computer.software_genre.games_in_this_genre"], ["computer.computer_game_version.developer"], ["computer.computer_game_region.releases"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did super mario bros 2 come out [/INST]", "raw_output": {"paths": ["<PATH> film.film.sequel </PATH>", "<PATH> film.film.country </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> </PATH>", "<PATH> computer.software_genre.games </PATH>", "<PATH> computer.computer_game_region.versions_released_in_this_region </PATH>", "<PATH> film.film.release_date </PATH>", "<PATH> computer.software_genre.games_in_this_genre </PATH>", "<PATH> computer.computer_game_version.developer </PATH>", "<PATH> computer.computer_game_region.releases </PATH>"], "scores": [-0.02268178202211857, -0.03608083352446556, -0.0462692528963089, -0.054102592170238495, -0.0669366642832756, -0.06710836291313171, -0.07268303632736206, -0.07773745059967041, -0.07777035236358643, -0.08051126450300217], "norm_scores": [0.1038036197423935, 0.10242202877998352, 0.10138380527496338, 0.1005927324295044, 0.09930997341871262, 0.09929291903972626, 0.09874092787504196, 0.0982431173324585, 0.09823988378047943, 0.09797099232673645]}}
{"id": "WebQTest-933", "question": "what political party did andrew johnson belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.us_president.vice_president", "organization.organization.founders"], ["government.politician.party", "government.political_party_tenure.party"], ["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["base.politicalconventions.presidential_nominee.nominated_at", "base.politicalconventions.political_convention.party"], ["base.culturalevent.event.entity_involved", "base.politicalconventions.political_convention.party"]], "ground_paths": [["government.politician.party", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.us_president.vice_president", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party did andrew johnson belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.us_president.vice_president <SEP> organization.organization.founders </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> base.politicalconventions.presidential_nominee.nominated_at <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.politicalconventions.political_convention.party </PATH>"], "scores": [-0.016185035929083824, -0.025491351261734962, -0.026619721204042435, -0.029339564964175224, -0.04352976754307747, -0.046590495854616165, -0.05629628896713257, -0.0567975714802742, -0.05984722077846527, -0.06366279721260071], "norm_scores": [0.10264665633440018, 0.10169582068920135, 0.1015811339020729, 0.10130522400140762, 0.09987783432006836, 0.09957260638475418, 0.09861084818840027, 0.09856143593788147, 0.09826131165027618, 0.09788709878921509]}}
{"id": "WebQTest-935", "question": "where was osama bin laden killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved", "time.event.locations"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["military.military_conflict.military_personnel_involved", "location.location.events"], ["book.written_work.subjects", "book.written_work.subjects"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was osama bin laden killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved <SEP> time.event.locations </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> military.military_conflict.military_personnel_involved <SEP> location.location.events </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.subjects </PATH>"], "scores": [-0.016740139573812485, -0.024830596521496773, -0.03575202450156212, -0.04262135922908783, -0.04473854973912239, -0.04567256197333336, -0.05427248403429985, -0.054684579372406006, -0.05629235506057739, -0.06202516704797745], "norm_scores": [0.10272945463657379, 0.10190168023109436, 0.10079482197761536, 0.10010480135679245, 0.09989308565855026, 0.09979982674121857, 0.0989452451467514, 0.09890446811914444, 0.09874558448791504, 0.0981811136007309]}}
{"id": "WebQTest-936", "question": "where were greek temples built", "prediction": [["location.location.containedby"], ["location.location.contains"], ["architecture.architect.structures_designed"], ["religion.religion.types_of_places_of_worship"], ["architecture.building.building_function"], ["architecture.structure.architect"], ["religion.place_of_worship.religion"], ["location.location.people_born_here"], ["architecture.building.building_complex"], ["architecture.building.country"]], "ground_paths": [["architecture.building_function.buildings", "location.location.contains"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"], ["architecture.building_function.buildings", "location.location.containedby"], ["architecture.building.building_function", "travel.tourist_attraction.near_travel_destination"], ["architecture.building.building_function", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were greek temples built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> religion.religion.types_of_places_of_worship </PATH>", "<PATH> architecture.building.building_function </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> religion.place_of_worship.religion </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> architecture.building.country </PATH>"], "scores": [-0.0063360328786075115, -0.04707023873925209, -0.06723914295434952, -0.07659584283828735, -0.07702483981847763, -0.08694080263376236, -0.0940248891711235, -0.09493099898099899, -0.10217145085334778, -0.10366465896368027], "norm_scores": [0.10712838917970657, 0.10285227745771408, 0.10079865157604218, 0.09985989332199097, 0.09981706738471985, 0.09883217513561249, 0.09813451766967773, 0.09804563224315643, 0.09733831137418747, 0.09719306975603104]}}
{"id": "WebQTest-937", "question": "where are people who speak arabic from", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["people.ethnicity.languages_spoken"], ["people.ethnicity.included_in_group"], ["language.human_language.main_country"], ["people.ethnicity.geographic_distribution"], ["people.person.ethnicity"], ["people.ethnicity.people"], ["people.ethnicity.people_with_this_ethnicity"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are people who speak arabic from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>"], "scores": [-0.01622404344379902, -0.023584265261888504, -0.035590384155511856, -0.039421018213033676, -0.06967852264642715, -0.07181227952241898, -0.08895798027515411, -0.09483154863119125, -0.11553628742694855, -0.11921556293964386], "norm_scores": [0.1051936224102974, 0.10442221164703369, 0.10317599773406982, 0.10278152674436569, 0.09971819818019867, 0.09950564801692963, 0.09781409800052643, 0.09724126756191254, 0.09524860978126526, 0.09489880502223969]}}
{"id": "WebQTest-938", "question": "who was queen elizabeth ii mom", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["royalty.monarch.kingdom"], ["film.film.country", "people.person.nationality"], ["people.person.parent"], ["people.person.mother"], ["symbols.name_source.namesakes"], ["people.person.gender"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen elizabeth ii mom [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> people.person.mother </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.0052626910619437695, -0.029431624338030815, -0.12733477354049683, -0.13561168313026428, -0.14870110154151917, -0.15179473161697388, -0.15413637459278107, -0.1542448103427887, -0.16029368340969086, -0.16476179659366608], "norm_scores": [0.11234328150749207, 0.10966061055660248, 0.0994333028793335, 0.09861370176076889, 0.09733131527900696, 0.09703067690134048, 0.09680372476577759, 0.09679323434829712, 0.09620950371026993, 0.09578058868646622]}}
{"id": "WebQTest-939", "question": "what type of art did pablo picasso paint", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"], ["visual_art.visual_artists.associated_periods_or_movements"], ["film.film_genre.films_in_this_genre"], ["visual_art.artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artist"], ["visual_art.visual_art_period_movement.associated_artists"], ["influence.influence_node.influenced_by"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did pablo picasso paint [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.visual_artists.associated_periods_or_movements </PATH>", "<PATH> film.film_genre.films_in_this_genre </PATH>", "<PATH> visual_art.artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artist </PATH>", "<PATH> visual_art.visual_art_period_movement.associated_artists </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.0017240878660231829, -0.04025736451148987, -0.07158573716878891, -0.1061861664056778, -0.10711658000946045, -0.10745284706354141, -0.1142868623137474, -0.11788680404424667, -0.12520858645439148, -0.1286727786064148], "norm_scores": [0.10936542600393295, 0.105231374502182, 0.10198575258255005, 0.09851735085248947, 0.0984257310628891, 0.09839264303445816, 0.09772252291440964, 0.09737136214971542, 0.0966610237956047, 0.09632675349712372]}}
{"id": "WebQTest-941", "question": "what is ella fitzgerald parents name", "prediction": [["people.person.children"], ["people.person.parents"], ["music.artist.genre", "music.artist.genre"], ["people.person.parent"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.child"], ["music.artist.origin"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ella fitzgerald parents name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.child </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.010722123086452484, -0.016825828701257706, -0.1323058307170868, -0.13298773765563965, -0.1356136053800583, -0.14197823405265808, -0.15505217015743256, -0.15548546612262726, -0.16198314726352692, -0.16990156471729279], "norm_scores": [0.11151720583438873, 0.11083860695362091, 0.09875037521123886, 0.09868305921554565, 0.0984242707490921, 0.09779983013868332, 0.096529521048069, 0.09648770093917847, 0.09586278349161148, 0.09510669857263565]}}
{"id": "WebQTest-942", "question": "who is anne hathaway parents", "prediction": [["people.person.children"], ["people.person.parents"], ["film.actor.film", "film.performance.character"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["film.performance.actor", "film.performance.parents"], ["people.person.parent"], ["common.topic.notable_types"], ["people.person.offspring"], ["people.person.profession"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is anne hathaway parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> film.performance.actor <SEP> film.performance.parents </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.offspring </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.014085917733609676, -0.014116230420768261, -0.13942298293113708, -0.14474739134311676, -0.14732269942760468, -0.14955511689186096, -0.1523503214120865, -0.17092648148536682, -0.17320430278778076, -0.1748298853635788], "norm_scores": [0.11187732219696045, 0.11187393218278885, 0.09869810938835144, 0.09817399829626083, 0.09792150557041168, 0.09770314395427704, 0.09743043035268784, 0.095637246966362, 0.09541965276002884, 0.09526466578245163]}}
{"id": "WebQTest-943", "question": "who does don draper gets engaged to", "prediction": [["fictional_universe.fictional_character.character_created_by", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character_creator.fictional_characters_created", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.marriage_of_fictional_characters.spouses", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.character_created_by", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.parents"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"], ["fictional_universe.fictional_character_creator.fictional_characters_created", "fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does don draper gets engaged to [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.marriage_of_fictional_characters.spouses <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.children <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.children <SEP> fictional_universe.fictional_character.parents </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.022422609850764275, -0.025869624689221382, -0.033610887825489044, -0.040114689618349075, -0.04132816568017006, -0.04276266694068909, -0.04298347234725952, -0.044727955013513565, -0.04496492072939873, -0.046167001128196716], "norm_scores": [0.10161703079938889, 0.10126736015081406, 0.10048644989728928, 0.09983503073453903, 0.09971395134925842, 0.09957101196050644, 0.09954903274774551, 0.09937552362680435, 0.09935197234153748, 0.09923261404037476]}}
{"id": "WebQTest-944", "question": "what was van halen first album", "prediction": [["music.artist.album"], ["music.album.artist"], ["music.recording.artist"], ["music.composition.composer"], ["music.producer.releases_produced"], ["music.composer.compositions"], ["music.producer.tracks_produced"], ["music.group_membership.group"], ["music.artist.genre"], ["music.recording.producer"]], "ground_paths": [["music.album.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was van halen first album [/INST]", "raw_output": {"paths": ["<PATH> music.artist.album </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.producer.tracks_produced </PATH>", "<PATH> music.group_membership.group </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> music.recording.producer </PATH>"], "scores": [-0.021661240607500076, -0.025104328989982605, -0.030949445441365242, -0.059294331818819046, -0.06996038556098938, -0.07192021608352661, -0.08646444231271744, -0.08959271758794785, -0.09719846397638321, -0.09958042204380035], "norm_scores": [0.10440509766340256, 0.10404624044895172, 0.10343985259532928, 0.10054901987314224, 0.09948226809501648, 0.0992874801158905, 0.09785386919975281, 0.0975482389330864, 0.09680913388729095, 0.09657880663871765]}}
{"id": "WebQTest-945", "question": "what political system did spain have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["base.locations.countries.continent"], ["people.person.nationality", "people.person.nationality", "location.country.form_of_government"], ["people.person.nationality", "people.person.nationality", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political system did spain have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality <SEP> location.country.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government <PATH>"], "scores": [-0.004360605496913195, -0.02676290087401867, -0.14466431736946106, -0.15632200241088867, -0.16571156680583954, -0.16735999286174774, -0.16747939586639404, -0.17187955975532532, -0.1729123443365097, -0.17999979853630066], "norm_scores": [0.11382253468036652, 0.1113010048866272, 0.09892251342535019, 0.09777599573135376, 0.09686222672462463, 0.0967026874423027, 0.09669113904237747, 0.09626661241054535, 0.09616723656654358, 0.09548807889223099]}}
{"id": "WebQTest-948", "question": "what are the supreme court cases", "prediction": [["law.court.jurisdiction", "government.governmental_jurisdiction.government_bodies"], ["law.court.jurisdiction", "book.written_work.subjects"], ["law.court.jurisdiction", "government.governmental_body.jurisdiction"], ["book.written_work.subjects"], ["law.court.jurisdiction", "base.aareas.schema.administrative_area.administrative_parent"], ["law.court.jurisdiction", "base.aareas.schema.administrative_area.cases"], ["law.court.jurisdiction", "government.governmental_body.cases"], ["law.court.jurisdiction", "law.court.jurisdiction"], ["law.court.jurisdiction", "law.court_jurisdiction.cases"], ["law.court.jurisdiction", "government.governmental_case.government_bodies"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the supreme court cases [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> law.court.jurisdiction <SEP> book.written_work.subjects </PATH>", "<PATH> law.court.jurisdiction <SEP> government.governmental_body.jurisdiction </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> law.court.jurisdiction <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> law.court.jurisdiction <SEP> base.aareas.schema.administrative_area.cases </PATH>", "<PATH> law.court.jurisdiction <SEP> government.governmental_body.cases </PATH>", "<PATH> law.court.jurisdiction <SEP> law.court.jurisdiction </PATH>", "<PATH> law.court.jurisdiction <SEP> law.court_jurisdiction.cases </PATH>", "<PATH> law.court.jurisdiction <SEP> government.governmental_case.government_bodies </PATH>"], "scores": [-0.021663298830389977, -0.0330323651432991, -0.03891170024871826, -0.041034404188394547, -0.04197026416659355, -0.04773557931184769, -0.04934300482273102, -0.05269902944564819, -0.05428619682788849, -0.06359954923391342], "norm_scores": [0.10229605436325073, 0.10113963484764099, 0.10054673999547958, 0.10033354163169861, 0.10023968666791916, 0.09966343641281128, 0.09950335323810577, 0.09916998445987701, 0.09901271015405655, 0.09809485822916031]}}
{"id": "WebQTest-949", "question": "what is penn state 's main campus", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.parent_institution"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools"], ["organization.organization.headquarters", "location.mailing_address.citytown"], ["education.educational_institution.parent_institution", "location.location.contains"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is penn state 's main campus [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.contains </PATH>"], "scores": [-0.0031448625959455967, -0.048670701682567596, -0.05155659466981888, -0.07215743511915207, -0.07515008747577667, -0.08231407403945923, -0.08386232703924179, -0.08964844793081284, -0.10466445982456207, -0.11178692430257797], "norm_scores": [0.10711152851581573, 0.10234452039003372, 0.10204958915710449, 0.0999687910079956, 0.09967005997896194, 0.0989585816860199, 0.0988054871559143, 0.09823544323444366, 0.09677135199308395, 0.09608454257249832]}}
{"id": "WebQTest-952", "question": "what country does alaska belong to", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby", "location.location.containedby"], ["base.locations.countries.states_provinces_within"], ["base.locations.continents.countries_within"], ["base.aareas.schema.administrative_area.administrative_parent", "location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.containedby", "base.locations.continents.countries_within"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does alaska belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.containedby <SEP> base.locations.continents.countries_within </PATH>"], "scores": [-0.01606401801109314, -0.03363949805498123, -0.03833434730768204, -0.04645382985472679, -0.05832806974649429, -0.06088262423872948, -0.06292592734098434, -0.06678447127342224, -0.07196825742721558, -0.07328252494335175], "norm_scores": [0.1037324070930481, 0.10192518681287766, 0.10144778341054916, 0.10062741488218307, 0.09943961352109909, 0.09918590635061264, 0.09898344427347183, 0.09860225021839142, 0.09809243679046631, 0.0979636088013649]}}
{"id": "WebQTest-953", "question": "where is kansas city", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.biblioness.bibs_location.state"], ["base.biblioness.bibs_location.country"], ["location.us_county.county_seat"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.us_county.hud_county_place"], ["base.aareas.schema.administrative_area.capital"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.biblioness.bibs_location.city"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is kansas city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.biblioness.bibs_location.city </PATH>"], "scores": [-0.005949817597866058, -0.050412725657224655, -0.05600729584693909, -0.06425420939922333, -0.07378699630498886, -0.07788248360157013, -0.08120162039995193, -0.09286117553710938, -0.09326568990945816, -0.09407740831375122], "norm_scores": [0.10646957159042358, 0.1018393337726593, 0.10127116739749908, 0.10043942928314209, 0.09948650747537613, 0.09907989203929901, 0.09875158220529556, 0.09760686755180359, 0.09756738692522049, 0.09748823195695877]}}
{"id": "WebQTest-954", "question": "what was pink floyd 's first album", "prediction": [["music.recording.artist"], ["music.artist.album"], ["music.artist.genre", "music.album.genre"], ["music.group_membership.group", "music.group_membership.album"], ["music.artist.label", "music.artist.label"], ["music.album.artist"], ["music.artist.label"], ["music.recording.artist", "music.recording.releases"], ["music.producer.tracks_produced"], ["music.recording.artist", "music.producer.tracks_produced"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was pink floyd 's first album [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.artist.genre <SEP> music.album.genre </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.album </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.artist.label </PATH>", "<PATH> music.recording.artist <SEP> music.recording.releases </PATH>", "<PATH> music.producer.tracks_produced </PATH>", "<PATH> music.recording.artist <SEP> music.producer.tracks_produced </PATH>"], "scores": [-0.027537958696484566, -0.03503826633095741, -0.04236651584506035, -0.05189095437526703, -0.05270315334200859, -0.053566668182611465, -0.05895157903432846, -0.061035316437482834, -0.061984799802303314, -0.0624876394867897], "norm_scores": [0.10234229266643524, 0.10157756507396698, 0.1008359044790268, 0.09988005459308624, 0.09979895502328873, 0.09971282631158829, 0.09917732328176498, 0.09897087514400482, 0.098876953125, 0.09882725030183792]}}
{"id": "WebQTest-955", "question": "who did granville t. woods married", "prediction": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did granville t. woods married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.031262416392564774, -0.033728983253240585, -0.037343837320804596, -0.03871691972017288, -0.039801597595214844, -0.041551005095243454, -0.04359426721930504, -0.04757830873131752, -0.04859074205160141, -0.0486217737197876], "norm_scores": [0.10098481178283691, 0.10073603689670563, 0.1003725454211235, 0.10023482143878937, 0.10012615472078323, 0.09995114803314209, 0.09974712878465652, 0.09935052692890167, 0.09924998879432678, 0.09924690425395966]}}
{"id": "WebQTest-957", "question": "what language did the sioux indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["language.human_language.language_family"], ["people.person.ethnicity", "people.person.nationality", "location.country.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"], ["people.ethnicity.included_in_group"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["people.person.ethnicity", "people.person.nationality", "location.country.official_language"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the sioux indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.official_language </PATH>"], "scores": [-0.0007384275086224079, -0.07957486063241959, -0.07967163622379303, -0.08047747611999512, -0.09870507568120956, -0.0989600419998169, -0.09931629151105881, -0.10357854515314102, -0.10671845078468323, -0.11153606325387955], "norm_scores": [0.10884073376655579, 0.10058963298797607, 0.10057990252971649, 0.10049888491630554, 0.09868361800909042, 0.09865846484899521, 0.09862332791090012, 0.0982038676738739, 0.09789599478244781, 0.09742550551891327]}}
{"id": "WebQTest-958", "question": "what are some famous people from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["location.location.containedby", "people.person.nationality"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death"], ["location.location.containedby", "people.person.place_of_birth"], ["influence.influence_node.influenced_by", "people.person.profession"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some famous people from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.002737719565629959, -0.04360898584127426, -0.07512187957763672, -0.07899457961320877, -0.088166743516922, -0.08893737196922302, -0.08945322781801224, -0.10206139087677002, -0.10641098767518997, -0.11267948150634766], "norm_scores": [0.10785127431154251, 0.10353211313486099, 0.10032039880752563, 0.09993264079093933, 0.09902022778987885, 0.09894394874572754, 0.09889291971921921, 0.09765388816595078, 0.09723006188869476, 0.09662247449159622]}}
{"id": "WebQTest-959", "question": "who did brett gardner play for in 2008", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["freebase.domain_profile.featured_topic", "sports.sports_team.sport"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did brett gardner play for in 2008 [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> freebase.domain_profile.featured_topic <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.009802261367440224, -0.016339678317308426, -0.031644050031900406, -0.036488890647888184, -0.05261022225022316, -0.06037519499659538, -0.060626670718193054, -0.06811418384313583, -0.08266214281320572, -0.09201943874359131], "norm_scores": [0.1041782945394516, 0.10349945724010468, 0.10192751884460449, 0.10143489390611649, 0.09981273859739304, 0.09904070198535919, 0.09901578724384308, 0.09827718883752823, 0.09685780107975006, 0.09595569968223572]}}
{"id": "WebQTest-960", "question": "who plays dwight in the office", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program_writer_relationship.tv_program", "tv.tv_writer.tv_programs"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_series_season.episodes", "tv.tv_director.episodes_directed"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["tv.tv_series_season.regular_cast", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays dwight in the office [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> tv.tv_program_writer_relationship.tv_program <SEP> tv.tv_writer.tv_programs </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.015768641605973244, -0.037311192601919174, -0.04027353227138519, -0.04323720559477806, -0.046517446637153625, -0.04691775143146515, -0.050728537142276764, -0.05099064111709595, -0.0515112541615963, -0.053889695554971695], "norm_scores": [0.10282822698354721, 0.10063672810792923, 0.10033905506134033, 0.10004211962223053, 0.099714495241642, 0.09967458993196487, 0.099295474588871, 0.09926944971084595, 0.09921777993440628, 0.0989820808172226]}}
{"id": "WebQTest-962", "question": "what county is san diego ca", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.location.containedby"], ["location.us_county.county_seat"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.county"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.contains"], ["location.citytown.postal_codes"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego ca [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.county </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes </PATH>"], "scores": [-0.0162559375166893, -0.024174107238650322, -0.030368464067578316, -0.039714910089969635, -0.08213045448064804, -0.09165342897176743, -0.10284970700740814, -0.1030655950307846, -0.1100516989827156, -0.11746153980493546], "norm_scores": [0.105633944272995, 0.10480082035064697, 0.10415365546941757, 0.10318472981452942, 0.09889961034059525, 0.09796226024627686, 0.09687156230211258, 0.09685064852237701, 0.0961764007806778, 0.09546638280153275]}}
{"id": "WebQTest-963", "question": "who is the miami dolphins quarterback", "prediction": [["american_football.football_player.position_s"], ["american_football.game_rushing_statistics.team", "american_football.game_rushing_statistics.player"], ["american_football.game_passing_statistics.team", "american_football.game_passing_statistics.player"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.player"], ["american_football.football_team.current_head_coach"], ["sports.sports_team.sport", "type.domain.types", "people.person.profession"], ["american_football.football_player.position_s", "people.person.gender", "people.person.gender"], ["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_team.sport", "sports.sport.positions", "american_football.football_player.position_s"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the miami dolphins quarterback [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.game_rushing_statistics.team <SEP> american_football.game_rushing_statistics.player </PATH>", "<PATH> american_football.game_passing_statistics.team <SEP> american_football.game_passing_statistics.player </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>", "<PATH> sports.sports_team.sport <SEP> type.domain.types <SEP> people.person.profession </PATH>", "<PATH> american_football.football_player.position_s <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.positions <SEP> american_football.football_player.position_s </PATH>"], "scores": [-0.0077442992478609085, -0.03717498853802681, -0.03840939328074455, -0.03996368870139122, -0.04203646257519722, -0.05048864707350731, -0.056219931691884995, -0.060644302517175674, -0.06114475801587105, -0.06296045333147049], "norm_scores": [0.10385321825742722, 0.10084128379821777, 0.10071688145399094, 0.10056045651435852, 0.10035223513841629, 0.09950761497020721, 0.0989389419555664, 0.09850215911865234, 0.09845288097858429, 0.09827428311109543]}}
{"id": "WebQTest-965", "question": "where did anne frank go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["book.book_edition.author_editor", "people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["book.book_edition.author_editor", "people.person.education", "education.educational_institution.students_graduates"], ["book.author.book_editions_published", "people.person.education", "education.education.institution"]], "ground_paths": [["book.book_subject.works", "book.book_subject.works"], ["education.education.student", "education.education.institution"], ["book.book_subject.works", "book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did anne frank go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> book.book_edition.author_editor <SEP> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> book.book_edition.author_editor <SEP> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> book.author.book_editions_published <SEP> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.021144846454262733, -0.02394588105380535, -0.028828324750065804, -0.03006632626056671, -0.04037366434931755, -0.042440615594387054, -0.06419851630926132, -0.06616928428411484, -0.07024227827787399, -0.07154795527458191], "norm_scores": [0.10248716175556183, 0.10220049321651459, 0.1017027273774147, 0.10157689452171326, 0.10053528100252151, 0.10032770037651062, 0.09816835075616837, 0.09797508269548416, 0.09757683426141739, 0.09744951128959656]}}
{"id": "WebQTest-966", "question": "what basketball player is married to monica", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.gender", "people.person.gender"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball player is married to monica [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> base.popstra.celebrity.breakup <SEP> base.popstra.celebrity.breakup </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.03096870891749859, -0.03314519301056862, -0.04058453068137169, -0.04452062025666237, -0.048510074615478516, -0.04894645884633064, -0.04965320602059364, -0.05288563668727875, -0.054787587374448776, -0.05574549362063408], "norm_scores": [0.10150851309299469, 0.10128781944513321, 0.10053709894418716, 0.10014215856790543, 0.09974344074726105, 0.09969991445541382, 0.0996294766664505, 0.09930796176195145, 0.09911926090717316, 0.09902435541152954]}}
{"id": "WebQTest-967", "question": "what did hitler use to kill himself", "prediction": [["people.deceased_person.cause_of_death"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["base.activism.activist.area_of_activism", "base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism", "people.deceased_person.cause_of_death"], ["base.activism.activist.area_of_activism", "base.activism.activist_issue.activists_organizations"], ["people.cause_of_death.people"], ["base.activism.activist.area_of_activism", "base.activism.activist_issue.activists"], ["base.activism.activism_issue.activists", "people.deceased_person.cause_of_death"], ["base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism", "base.activism.activist.activism_issue"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hitler use to kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activism_issue.activists </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist_issue.activists_organizations </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist_issue.activists </PATH>", "<PATH> base.activism.activism_issue.activists <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.activism_issue </PATH>"], "scores": [-0.008512427099049091, -0.029720474034547806, -0.04676276445388794, -0.055994827300310135, -0.058984532952308655, -0.06275632232427597, -0.06592735648155212, -0.06606230139732361, -0.06955219805240631, -0.07130929827690125], "norm_scores": [0.10458836704492569, 0.10239361226558685, 0.10066337138414383, 0.09973831474781036, 0.0994405746459961, 0.09906620532274246, 0.09875257313251495, 0.09873924404382706, 0.09839525818824768, 0.098222516477108]}}
{"id": "WebQTest-968", "question": "what do people speak in germany", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["base.rosetta.languoid.languoid_class"], ["film.film.featured_film_locations"], ["location.country.currency_used"], ["location.country.language_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in germany [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> location.country.language_spoken </PATH>"], "scores": [-0.013177793473005295, -0.020010383799672127, -0.03662131354212761, -0.06517434865236282, -0.16999605298042297, -0.17405778169631958, -0.17807883024215698, -0.1868252158164978, -0.18991847336292267, -0.19306357204914093], "norm_scores": [0.1112651601433754, 0.11050751805305481, 0.10868705064058304, 0.10562758892774582, 0.09511606395244598, 0.09473052620887756, 0.09435036778450012, 0.09352873265743256, 0.09323988109827042, 0.0929470956325531]}}
{"id": "WebQTest-969", "question": "when is the last time the denver broncos won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.runner_up"], ["common.topic.notable_types"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the denver broncos won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>"], "scores": [-0.015591970644891262, -0.01571163535118103, -0.02386736311018467, -0.061347052454948425, -0.07367055118083954, -0.07608699798583984, -0.08185922354459763, -0.09644285589456558, -0.10139764100313187, -0.1216990277171135], "norm_scores": [0.10518451035022736, 0.10517193377017975, 0.10431765764951706, 0.10048022866249084, 0.09924956411123276, 0.09901002049446106, 0.09844015538692474, 0.09701496362686157, 0.09653546661138535, 0.09459542483091354]}}
{"id": "WebQTest-970", "question": "where was caesar when he was stabbed", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.place_of_interment.interred_here"], ["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.person.nationality"], ["fictional_universe.fictional_character.place_of_birth"], ["location.location.events"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was caesar when he was stabbed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> location.location.events </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.001617681817151606, -0.0691051334142685, -0.07012321054935455, -0.0756777673959732, -0.08576444536447525, -0.08923715353012085, -0.09252799302339554, -0.0945996567606926, -0.09951256960630417, -0.10074509680271149], "norm_scores": [0.10788381844758987, 0.10084325820207596, 0.10074064880609512, 0.10018263012170792, 0.09917720407247543, 0.09883337467908859, 0.09850867837667465, 0.09830480813980103, 0.09782302379608154, 0.09770254045724869]}}
{"id": "WebQTest-971", "question": "what style of music did bessie smith perform", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.written_work.author"], ["book.written_work.subjects"], ["book.author.works_written"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nominee"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["book.book_subject.works"], ["people.person.profession"]], "ground_paths": [["music.artist.genre"], ["book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music did bessie smith perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.000559270556550473, -0.0723494365811348, -0.10584121942520142, -0.1075514629483223, -0.1157868355512619, -0.1208401620388031, -0.1247052326798439, -0.12658391892910004, -0.1278022676706314, -0.12885725498199463], "norm_scores": [0.11071597784757614, 0.10304626077413559, 0.09965221583843231, 0.09948193281888962, 0.09866602718830109, 0.09816869348287582, 0.09778998792171478, 0.09760644286870956, 0.09748759865760803, 0.09738480299711227]}}
{"id": "WebQTest-972", "question": "what team did jackie robinson", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["baseball.baseball_player.lifetime_batting_statistics", "baseball.lifetime_batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["baseball.baseball_player.lifetime_batting_statistics", "baseball.lifetime_lifetime_batting_statistics.team"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did jackie robinson [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> baseball.baseball_player.lifetime_batting_statistics <SEP> baseball.lifetime_batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> baseball.baseball_player.lifetime_batting_statistics <SEP> baseball.lifetime_lifetime_batting_statistics.team </PATH>"], "scores": [-0.014876045286655426, -0.017325589433312416, -0.028957996517419815, -0.03306609392166138, -0.049366213381290436, -0.05952593684196472, -0.061152514070272446, -0.08728612959384918, -0.0886259600520134, -0.09367990493774414], "norm_scores": [0.10388501733541489, 0.10363086313009262, 0.10243237018585205, 0.10201242566108704, 0.1003630980849266, 0.09934859722852707, 0.09918712824583054, 0.0966285839676857, 0.09649920463562012, 0.09601273387670517]}}
{"id": "WebQTest-973", "question": "what time zone is chicago in right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"], ["location.location.time_zones", "location.location.time_zones"], ["locations.location.time_zones"], ["location.location.containedby"], ["location.location.time_zonessymbol"], ["location.location.time_zoneslanguage"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is chicago in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zonessymbol </PATH>", "<PATH> location.location.time_zoneslanguage </PATH>"], "scores": [-8.803071978036314e-06, -0.1644667088985443, -0.1851940006017685, -0.18895596265792847, -0.20197637379169464, -0.21138684451580048, -0.21208372712135315, -0.23139864206314087, -0.2331055849790573, -0.23777593672275543], "norm_scores": [0.12024258822202682, 0.10200822353363037, 0.09991562366485596, 0.09954045712947845, 0.0982527956366539, 0.09733253717422485, 0.09726472944021225, 0.09540408849716187, 0.09524138271808624, 0.09479761123657227]}}
{"id": "WebQTest-975", "question": "who was the vice president under ronald reagan", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["people.person.profession", "people.person.profession"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["people.person.nationality", "people.person.nationality"], ["people.person.gender", "people.person.gender"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the vice president under ronald reagan [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> base.americancomedy.impersonated_celebrity.impersonated_by <SEP> base.americancomedy.impersonated_celebrity.impersonated_by </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.006383269559592009, -0.020068075507879257, -0.07415413111448288, -0.07717015594244003, -0.0805605873465538, -0.09157011657953262, -0.09849681705236435, -0.09985635429620743, -0.10537908971309662, -0.10557640343904495], "norm_scores": [0.10714129358530045, 0.10568507015705109, 0.10012082010507584, 0.09981929510831833, 0.09948144853115082, 0.09839221090078354, 0.09771303087472916, 0.09758028388023376, 0.09704285115003586, 0.09702370315790176]}}
{"id": "WebQTest-976", "question": "where do the chicago white sox play home games", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_championship_event.champion"], ["sports.sports_team_location.teams"], ["sports.sports_venue_relationship.team", "sports.sports_facility.teams"], ["sports.sports_team_location.team"], ["sports.sports_venue_relationship.team", "sports.sports_team.arena_stadium"], ["sports.sports_venue_relationship.team", "sports.sports_facility.home_venue_for"], ["sports.sports_team.arena_stadium", "sports.sports_facility.teams"], ["sports.sports_facility.teams", "sports.sports_team.arena_stadium"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the chicago white sox play home games [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_venue_relationship.team <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.team </PATH>", "<PATH> sports.sports_venue_relationship.team <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_venue_relationship.team <SEP> sports.sports_facility.home_venue_for </PATH>", "<PATH> sports.sports_team.arena_stadium <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_team.arena_stadium </PATH>"], "scores": [-0.0005607783677987754, -0.05963773652911186, -0.10925871133804321, -0.13423529267311096, -0.13908328115940094, -0.14363999664783478, -0.1493472456932068, -0.15277805924415588, -0.1530655026435852, -0.15348567068576813], "norm_scores": [0.11249678581953049, 0.10604331642389297, 0.1009097620844841, 0.09842060506343842, 0.09794461727142334, 0.09749932587146759, 0.09694445133209229, 0.09661242365837097, 0.09658466279506683, 0.09654409438371658]}}
{"id": "WebQTest-977", "question": "what part did jeff conaway play in grease", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["film.actor.film", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.performance.character"], ["theater.play.characters"]], "ground_paths": [["theater.theater_character.plays_appears_in"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did jeff conaway play in grease [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.performance.character </PATH>", "<PATH> theater.play.characters </PATH>"], "scores": [-0.026531830430030823, -0.031908415257930756, -0.03242349997162819, -0.03539041057229042, -0.04211147502064705, -0.04278804734349251, -0.0428767129778862, -0.04672950878739357, -0.05312638729810715, -0.05721119046211243], "norm_scores": [0.1014641746878624, 0.10092010349035263, 0.10086813569068909, 0.10056930780410767, 0.0998956486582756, 0.0998280867934227, 0.09981923550367355, 0.09943538904190063, 0.09880134463310242, 0.09839858114719391]}}
{"id": "WebQTest-978", "question": "what book did benjamin franklin published", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.book_editions_published"], ["book.author.works_written"], ["people.person.profession", "common.topic.subject_of"], ["people.person.nationality", "book.written_work.subjects"], ["influence.influence_node.influenced_by", "book.book_edition.author_editor"], ["influence.influence_node.influenced_by", "book.written_work.author"], ["people.person.profession", "common.topic.subjects"], ["law.inventor.inventions"]], "ground_paths": [["book.book_edition.author_editor", "book.book.genre"], ["book.author.book_editions_published"], ["book.book_edition.author_editor"], ["people.person.profession", "common.topic.subject_of"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did benjamin franklin published [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> people.person.profession <SEP> common.topic.subject_of </PATH>", "<PATH> people.person.nationality <SEP> book.written_work.subjects </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.book_edition.author_editor </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.written_work.author </PATH>", "<PATH> people.person.profession <SEP> common.topic.subjects </PATH>", "<PATH> law.inventor.inventions </PATH>"], "scores": [-0.011776281520724297, -0.029131095856428146, -0.033243682235479355, -0.048425864428281784, -0.07893183827400208, -0.09255113452672958, -0.10437314957380295, -0.10592534393072128, -0.10635524988174438, -0.10720177739858627], "norm_scores": [0.10611800849437714, 0.10429223626852036, 0.10386420786380768, 0.10229922831058502, 0.09922561794519424, 0.09788338840007782, 0.09673303365707397, 0.09658300131559372, 0.09654148668050766, 0.09645979106426239]}}
{"id": "WebQTest-979", "question": "what currency to take to barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["location.country.form_of_government"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.country.currency_formerly_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tourist_attraction.near_travel_destination"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency to take to barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.010767072439193726, -0.015813954174518585, -0.12502127885818481, -0.1253070831298828, -0.13742710649967194, -0.13799306750297546, -0.13830579817295074, -0.149224653840065, -0.16335968673229218, -0.1668853759765625], "norm_scores": [0.11104648560285568, 0.1104874536395073, 0.09905692934989929, 0.09902862459421158, 0.09783563017845154, 0.09778027981519699, 0.09774971008300781, 0.09668819606304169, 0.0953311175107956, 0.09499560296535492]}}
{"id": "WebQTest-980", "question": "where did they get the name stanley cup", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["award.award_honor.award", "award.award_honor.award_winner"], ["sports.sports_championship_event.championship"], ["symbols.namesake.named_after", "symbols.namesake.named_after"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship", "sports.sports_team.previously_known_as"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion", "sports.sports_team.previously_known_as"], ["sports.sports_team.championships", "sports.sports_team.previously_known_as"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they get the name stanley cup [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> award.award_honor.award <SEP> award.award_honor.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> symbols.namesake.named_after <SEP> symbols.namesake.named_after </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.previously_known_as </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.previously_known_as </PATH>", "<PATH> sports.sports_team.championships <SEP> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.01330843847244978, -0.022596854716539383, -0.04910976067185402, -0.058005716651678085, -0.06840994954109192, -0.07134923338890076, -0.07249950617551804, -0.07266473025083542, -0.07384736090898514, -0.07592057436704636], "norm_scores": [0.10452229529619217, 0.10355593264102936, 0.10084645450115204, 0.0999533012509346, 0.09891875833272934, 0.09862843155860901, 0.09851504862308502, 0.09849877655506134, 0.09838235378265381, 0.09817860275506973]}}
{"id": "WebQTest-982", "question": "what is the zip code of syracuse ny", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"], ["location.mailing_address.citytown"], ["location.us_county.hud_county_place"], ["location.postal_code.citytown"], ["location.citytown.postal_ codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code of syracuse ny [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.mailing_address.citytown </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.postal_code.citytown </PATH>", "<PATH> location.citytown.postal_ codes </PATH>", "</PATH> location.citytown.postal_codes </PATH>"], "scores": [-0.0002700084005482495, -0.08018240332603455, -0.1358487606048584, -0.1697911024093628, -0.1705811470746994, -0.17577223479747772, -0.17736119031906128, -0.18089887499809265, -0.18107402324676514, -0.18199729919433594], "norm_scores": [0.11542291939258575, 0.10655811429023743, 0.10078849643468857, 0.09742490202188492, 0.09734796732664108, 0.09684392809867859, 0.09669017046689987, 0.09634871780872345, 0.09633184224367142, 0.09624294191598892]}}
{"id": "WebQTest-983", "question": "where did tennessee williams go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["book.written_work.author", "education.education.institution"], ["book.written_work.author", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tennessee williams go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.author <SEP> education.education.institution </PATH>", "<PATH> book.written_work.author <SEP> location.location.containedby </PATH>"], "scores": [-0.013779344968497753, -0.020916985347867012, -0.021260520443320274, -0.045590080320835114, -0.0689699649810791, -0.0805211290717125, -0.1012268215417862, -0.10582546144723892, -0.10917922109365463, -0.11132114380598068], "norm_scores": [0.10548223555088043, 0.10473202168941498, 0.10469605028629303, 0.10217957943677902, 0.09981834143400192, 0.09867195785045624, 0.0966498926281929, 0.09620645642280579, 0.09588434547185898, 0.09567917883396149]}}
{"id": "WebQTest-984", "question": "what is the name of airport in new york", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["location.location.containedby", "location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"], ["base.aareas.schema.administrative_area.administrative_children", "location.location.containedby"], ["common.topic.notable_types"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["location.location.contains"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of airport in new york [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> film.film.featured_film_locations <SEP> film.film.featured_film_locations </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.009647623635828495, -0.02681032568216324, -0.03162448853254318, -0.07592480629682541, -0.08744358271360397, -0.0906527191400528, -0.10988287627696991, -0.11480266600847244, -0.11657517403364182, -0.11694415658712387], "norm_scores": [0.10699628293514252, 0.10517560690641403, 0.10467048734426498, 0.10013476759195328, 0.09898795187473297, 0.09867079555988312, 0.09679146856069565, 0.09631644189357758, 0.0961458757519722, 0.09611039608716965]}}
{"id": "WebQTest-985", "question": "who was queen victoria 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.parents", "people.person.children"], ["base.crime.crime_victim.crime", "base.crime.crime_victim.crime"], ["royalty.kingdom.rulers", "people.person.nationality"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen victoria 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.03666520491242409, -0.038551174104213715, -0.03915514051914215, -0.040597859770059586, -0.04248010739684105, -0.04337954521179199, -0.04590959474444389, -0.04873840883374214, -0.052969254553318024, -0.053589195013046265], "norm_scores": [0.10075507313013077, 0.10056523233652115, 0.10050451755523682, 0.10035961866378784, 0.10017089545726776, 0.10008083283901215, 0.09982794523239136, 0.09954594820737839, 0.09912567585706711, 0.09906423836946487]}}
{"id": "WebQTest-986", "question": "where did benjamin franklin work", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employer.employees"], ["people.person.children", "people.person.place_of_birth"], ["people.person.profession"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.profession", "people.person.employment_history"], ["people.person.employment_history"]], "ground_paths": [["people.person.nationality", "government.government_agency.jurisdiction"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.children"], ["people.person.employment_history", "business.employer.employees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin work [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>", "<PATH> people.person.profession <SEP> people.person.employment_history </PATH>", "<PATH> people.person.employment_history </PATH>"], "scores": [-0.017076734453439713, -0.02776707336306572, -0.038092806935310364, -0.04318041354417801, -0.05536791309714317, -0.06407560408115387, -0.06577621400356293, -0.07357697933912277, -0.07659367471933365, -0.0812133327126503], "norm_scores": [0.10376721620559692, 0.10266381502151489, 0.10160919278860092, 0.10109356045722961, 0.09986896067857742, 0.09900310635566711, 0.09883487969636917, 0.09806688874959946, 0.09777150303125381, 0.09732087701559067]}}
{"id": "WebQTest-987", "question": "what does barbara bush do for work", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.person.children"], ["base.activism.activist.area_of_activism"], ["people.person.parents"], ["government.political_appointer.appointees", "government.government_position_held.office_position_or_title"], ["government.political_appointer.appointees", "government.government_office_or_title.office_holders"], ["government.political_appointer.appointees", "government.government_position_held.office_holder"], ["government.political_appointer.appointees", "government.politician.government_positions_held"], ["people.profession.people_with_this_profession"]], "ground_paths": [["business.employment_tenure.person", "business.job_title.people_with_this_title"], ["government.politician.government_positions_held", "government.government_office_category.officeholders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does barbara bush do for work [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.children </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.politician.government_positions_held </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.002015745034441352, -0.06666098535060883, -0.07576575875282288, -0.08067489415407181, -0.09001979976892471, -0.09049083292484283, -0.09051429480314255, -0.09328742325305939, -0.09570575505495071, -0.09816808998584747], "norm_scores": [0.10788992047309875, 0.10113601386547089, 0.10021936893463135, 0.09972858428955078, 0.09880097210407257, 0.09875443577766418, 0.09875212609767914, 0.09847865253686905, 0.09824078530073166, 0.09799918532371521]}}
{"id": "WebQTest-988", "question": "who is jacob latimore", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["common.topic.notable_types", "common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"], ["influence.influence_node.influenced", "people.person.profession"], ["common.topic.notable_types", "rdf-schema#domain"], ["film.performance.actor", "film.performance.character"], ["freebase.valuenotation.is_reviewed", "rdf-schema#domain"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jacob latimore [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> common.topic.notable_types <SEP> rdf-schema#domain </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> rdf-schema#domain </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.00038860528729856014, -0.07768402993679047, -0.14477385580539703, -0.15032167732715607, -0.1532786637544632, -0.1570669561624527, -0.16813315451145172, -0.1702747493982315, -0.17115017771720886, -0.1724577695131302], "norm_scores": [0.11442415416240692, 0.10591287165880203, 0.09904032200574875, 0.09849237650632858, 0.09820156544446945, 0.09783025830984116, 0.09675361216068268, 0.09654662758111954, 0.09646214544773102, 0.09633609652519226]}}
{"id": "WebQTest-989", "question": "what happened to dunkirk during ww2", "prediction": [["time.event.locations"], ["military.military_conflict.casualties", "military.casualties.event"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.military_conflict.casualties"], ["time.event.included_in_event"], ["location.location.events"], ["military.military_conflict.casualties", "military.casualties.military_conflict"], ["military.military_command.military_conflict", "military.military_command.military_conflict"], ["military.military_conflict.combatants", "military.military_conflict.casualties"], ["military.military_conflict.casualties", "military.military_person.participated_in_conflicts"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to dunkirk during ww2 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> location.location.events </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.military_conflict </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_command.military_conflict </PATH>", "<PATH> military.military_conflict.combatants <SEP> military.military_conflict.casualties </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_person.participated_in_conflicts </PATH>"], "scores": [-0.03032909892499447, -0.03221940994262695, -0.03865661099553108, -0.039802324026823044, -0.04075181856751442, -0.045246101915836334, -0.04691942781209946, -0.04936881735920906, -0.05122637376189232, -0.055746473371982574], "norm_scores": [0.10127480328083038, 0.10108353942632675, 0.10043493658304214, 0.10031992942094803, 0.10022472590208054, 0.0997752919793129, 0.09960847347974777, 0.09936479479074478, 0.09918039292097092, 0.09873310476541519]}}
{"id": "WebQTest-990", "question": "what was scottie pippen good at", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position_s"], ["sports.sports_position.sport", "sports.sports_position.sport"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["sports.sports_position.sport", "sports.sports_position.sport", "basketball.basketball_player.position_s"], ["sports.pro_athlete.teams", "sports.sports_position.teams"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"], ["people.person.profession"]], "ground_paths": [["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"], ["basketball.basketball_player.position_s", "sports.sport.positions"], ["common.topic.notable_types", "type.type.domain"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen good at [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_position.sport </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_position.sport <SEP> basketball.basketball_player.position_s </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_position.teams </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.01685257814824581, -0.029116190969944, -0.032213471829891205, -0.0482691265642643, -0.04862816631793976, -0.06029054895043373, -0.061117611825466156, -0.06277348846197128, -0.06391982734203339, -0.06509438157081604], "norm_scores": [0.1032354012131691, 0.10197709500789642, 0.10166172683238983, 0.10004251450300217, 0.100006602704525, 0.09884706884622574, 0.09876534342765808, 0.09860193729400635, 0.09848897159099579, 0.09837336093187332]}}
{"id": "WebQTest-991", "question": "who played donna on west wing", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.languages", "people.person.languages"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played donna on west wing [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_program.languages <SEP> people.person.languages </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>"], "scores": [-0.022196196019649506, -0.03574496507644653, -0.03885896876454353, -0.04281437024474144, -0.04296121001243591, -0.04604419320821762, -0.04612495377659798, -0.04664705693721771, -0.049485690891742706, -0.050274334847927094], "norm_scores": [0.10200867801904678, 0.10063591599464417, 0.10032300651073456, 0.09992697834968567, 0.09991230815649033, 0.09960475564002991, 0.09959670901298523, 0.0995447188615799, 0.09926255792379379, 0.0991842970252037]}}
{"id": "WebQTest-993", "question": "who played meg in season 1 of family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played meg in season 1 of family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.022495202720165253, -0.030442552641034126, -0.032233331352472305, -0.03671441972255707, -0.04196221008896828, -0.04203923046588898, -0.042262785136699677, -0.04440152272582054, -0.04910177364945412, -0.04955306649208069], "norm_scores": [0.10167303681373596, 0.10086820274591446, 0.10068773478269577, 0.10023755580186844, 0.09971290826797485, 0.09970522671937943, 0.09968294203281403, 0.09946997463703156, 0.09900353848934174, 0.09895887225866318]}}
{"id": "WebQTest-994", "question": "when was the last time the dallas cowboys won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["common.topic.notable_types"], ["common.image.appears_in_topic_gallery", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dallas cowboys won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.image.appears_in_topic_gallery <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.006028955802321434, -0.025712160393595695, -0.03862721100449562, -0.058524783700704575, -0.06785760819911957, -0.07339873909950256, -0.07927681505680084, -0.09610236436128616, -0.09649349749088287, -0.10262039303779602], "norm_scores": [0.10596749186515808, 0.10390210896730423, 0.10256883502006531, 0.1005481407046318, 0.09961410611867905, 0.09906364977359772, 0.09848304837942123, 0.09683988988399506, 0.09680202603340149, 0.09621074050664902]}}
{"id": "WebQTest-996", "question": "where do ireland play rugby union", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_team_location.teams"], ["sports.sports_facility.teams"], ["sports.sports_team.location"], ["location.location.contains"], ["location.location.containedby"], ["location.location.events"], ["sports.sports_championship_event.champion"], ["sports.sports_team.locations"], ["time.event.locations"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do ireland play rugby union [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.events </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.locations </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.005133174359798431, -0.04307801276445389, -0.04803409054875374, -0.06596872210502625, -0.06637249141931534, -0.09776640683412552, -0.11382243037223816, -0.12480767071247101, -0.1252080202102661, -0.13087797164916992], "norm_scores": [0.10791224986314774, 0.10389425605535507, 0.10338062047958374, 0.10154306143522263, 0.10150206089019775, 0.09836501628160477, 0.09679827094078064, 0.09574073553085327, 0.09570241719484329, 0.0951613262295723]}}
{"id": "WebQTest-997", "question": "who killed lee harvey oswald shot", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.gender", "people.person.gender"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.crime.crime_victim.crime_type", "people.deceased_person.cause_of_death"], ["people.person.nationality", "people.person.nationality"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who killed lee harvey oswald shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.deceased_person.cause_of_death <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.crime.crime_victim.crime_type <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.01054341159760952, -0.026507863774895668, -0.04391471669077873, -0.04749809205532074, -0.048942096531391144, -0.05081367865204811, -0.05280633270740509, -0.06017147749662399, -0.06078946962952614, -0.0662911981344223], "norm_scores": [0.10368179529905319, 0.10203971713781357, 0.10027889162302017, 0.09992020577192307, 0.0997760221362114, 0.09958945959806442, 0.09939120709896088, 0.0986618623137474, 0.09860090911388397, 0.09805992990732193]}}
{"id": "WebQTest-998", "question": "when was the last time knicks won the championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.runner_up"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.draft"], ["sports.professional_sports_team.owner_s", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_teams.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["sports.sports_championship"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time knicks won the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_teams.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship </PATH>"], "scores": [-0.008724050596356392, -0.015260746702551842, -0.11437521874904633, -0.13191595673561096, -0.13411659002304077, -0.14096792042255402, -0.15295162796974182, -0.15781694650650024, -0.16264699399471283, -0.16639916598796844], "norm_scores": [0.11143004149198532, 0.11070402711629868, 0.10025790333747864, 0.098514623939991, 0.098298080265522, 0.09762690961360931, 0.09646395593881607, 0.09599576145410538, 0.09553322196006775, 0.09517543017864227]}}
{"id": "WebQTest-999", "question": "when did celtics win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_teams.champion"], ["sports.sports_league_season.champion"], ["sports.sports_title.champion"], ["sports.sports_club.championships"], ["sports.sports_Championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did celtics win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_teams.champion </PATH>", "<PATH> sports.sports_league_season.champion </PATH>", "<PATH> sports.sports_title.champion </PATH>", "<PATH> sports.sports_championship_event.champion <PATH>", "<PATH> sports.sports_club.championships </PATH>", "<PATH> sports.sports_Championship_event.champion </PATH>"], "scores": [-0.011915097013115883, -0.012265997938811779, -0.14921516180038452, -0.14980529248714447, -0.16913199424743652, -0.17185141146183014, -0.18073931336402893, -0.1816057562828064, -0.18414677679538727, -0.18445397913455963], "norm_scores": [0.11336375027894974, 0.11332397162914276, 0.09882016479969025, 0.09876186400651932, 0.09687144309282303, 0.09660836309194565, 0.09575352072715759, 0.09567059576511383, 0.09542780369520187, 0.09539849311113358]}}
{"id": "WebQTest-1000", "question": "what countries included in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["book.book_subject.works"], ["book.written_work.subjects"], ["base.locations.countries.continent"], ["common.topic.notable_types"], ["geography.river.basin_countries"], ["location.administrative_division.first_level_division_of"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.006357965525239706, -0.03786904364824295, -0.05659040808677673, -0.064825639128685, -0.07435642927885056, -0.07467075437307358, -0.10761500149965286, -0.14782992005348206, -0.1509467214345932, -0.15553031861782074], "norm_scores": [0.10834227502346039, 0.10498151928186417, 0.10303441435098648, 0.10218938440084457, 0.10122006386518478, 0.10118824988603592, 0.09790899604558945, 0.09404970705509186, 0.0937570333480835, 0.09332827478647232]}}
{"id": "WebQTest-1001", "question": "what country is nicki minaj from", "prediction": [["people.person.nationality"], ["music.artist.origin"], ["people.person.place_of_birth"], ["music.recording.artist"], ["music.album.artist"], ["people.person.ethnicity"], ["music.recording.featured_artists"], ["location.location.people_born_here"], ["people.ethnicity.people"], ["music.artist.album"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nicki minaj from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> music.recording.featured_artists </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> music.artist.album </PATH>"], "scores": [-0.006277468521147966, -0.03980744630098343, -0.05310237407684326, -0.05850406736135483, -0.07543659955263138, -0.08530460298061371, -0.09418091922998428, -0.1009330153465271, -0.10513408482074738, -0.11204186826944351], "norm_scores": [0.10685323178768158, 0.1033298447728157, 0.10196517407894135, 0.10141588002443314, 0.09971310943365097, 0.09873396903276443, 0.09786145389080048, 0.09720291197299957, 0.09679541736841202, 0.09612908214330673]}}
{"id": "WebQTest-1002", "question": "who played harry potter dumbledore actor", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["people.person.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films_dubbed", "film.actor.dubbing_performances"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played harry potter dumbledore actor [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films_dubbed <SEP> film.actor.dubbing_performances </PATH>"], "scores": [-0.016470640897750854, -0.023868437856435776, -0.0239190012216568, -0.04297299310564995, -0.04307993873953819, -0.04991389065980911, -0.07116387784481049, -0.07580642402172089, -0.08293887227773666, -0.08822453767061234], "norm_scores": [0.10356765240430832, 0.10280430316925049, 0.10279911011457443, 0.10085892677307129, 0.10084813833236694, 0.10016129165887833, 0.09805531799793243, 0.09760115295648575, 0.09690749645233154, 0.09639662504196167]}}
{"id": "WebQTest-1003", "question": "who was the prime minister of pakistan in 1999", "prediction": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.political_appointer.appointees", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the prime minister of pakistan in 1999 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.00994781218469143, -0.01604456827044487, -0.020790373906493187, -0.03487895801663399, -0.048229604959487915, -0.050036683678627014, -0.05521226301789284, -0.07966091483831406, -0.08339446038007736, -0.09055513888597488], "norm_scores": [0.10393056273460388, 0.1032988503575325, 0.10280977934598923, 0.10137148946523666, 0.10002710670232773, 0.09984651207923889, 0.0993310809135437, 0.09693202376365662, 0.09657079726457596, 0.09588175266981125]}}
{"id": "WebQTest-1004", "question": "what movies did christopher plummer play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "ground_paths": [["music.artist.album", "film.film.soundtrack"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["music.recording.artist", "music.recording.releases"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["film.actor.dubbing_performances", "film.film.dubbing_performances"], ["film.performance.actor"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.producer.films_executive_produced"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did christopher plummer play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.015597010962665081, -0.034069955348968506, -0.03587957099080086, -0.03626664727926254, -0.03721824288368225, -0.045237407088279724, -0.05364520475268364, -0.05719636008143425, -0.057273153215646744, -0.05799679085612297], "norm_scores": [0.10277329385280609, 0.10089219361543655, 0.10070978850126266, 0.10067081451416016, 0.10057505965232849, 0.09977176040410995, 0.09893641620874405, 0.09858569502830505, 0.09857813268899918, 0.09850682318210602]}}
{"id": "WebQTest-1005", "question": "what is capital city of morocco", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["location.country.administrative_divisions"], ["location.country.first_level_divisions"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is capital city of morocco [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.006156031042337418, -0.034492045640945435, -0.042537204921245575, -0.09403492510318756, -0.11479737609624863, -0.1190589964389801, -0.11932247132062912, -0.12452995032072067, -0.12720157206058502, -0.13349182903766632], "norm_scores": [0.10881045460700989, 0.10577047616243362, 0.10492295026779175, 0.09965641796588898, 0.09760864824056625, 0.09719356149435043, 0.0971679612994194, 0.09666327387094498, 0.09640536457300186, 0.09580085426568985]}}
{"id": "WebQTest-1006", "question": "what movies is omar epps in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies is omar epps in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.person_or_entity_appearing_in_film.films <SEP> film.film.personal_appearances </PATH>"], "scores": [-0.014521649107336998, -0.03045097179710865, -0.03665448725223541, -0.039585813879966736, -0.03966069966554642, -0.040830206125974655, -0.04412897303700447, -0.04507138580083847, -0.05158922076225281, -0.09327605366706848], "norm_scores": [0.10292958468198776, 0.10130298137664795, 0.10067649185657501, 0.10038179904222488, 0.10037428885698318, 0.10025696456432343, 0.09992678463459015, 0.09983266144990921, 0.0991840809583664, 0.09513440728187561]}}
{"id": "WebQTest-1007", "question": "what district does nancy pelosi represent", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["government.government_position_held.office_holder", "government.political_district.representatives"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["government.politician.government_positions_held", "government.political_district.representatives"], ["people.person.nationality", "base.locations.countries.states_provinces_within"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["government.politician.government_positions_held", "government.political_district.representatives"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["base.government2.elected_government_positions_held.officeholder", "base.government2.elected_government_positions_held.district_represented"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what district does nancy pelosi represent [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.political_district.representatives </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.political_district.representatives </PATH>", "<PATH> people.person.nationality <SEP> base.locations.countries.states_provinces_within </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.state </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.022322330623865128, -0.025054996833205223, -0.03024252876639366, -0.034935738891363144, -0.04102736711502075, -0.041716158390045166, -0.04174289479851723, -0.04236213117837906, -0.04571454972028732, -0.05128190293908119], "norm_scores": [0.10153962671756744, 0.10126253217458725, 0.10073859244585037, 0.10026691854000092, 0.09965798258781433, 0.09958936274051666, 0.09958669543266296, 0.09952504932880402, 0.09919195622205734, 0.09864125400781631]}}
{"id": "WebQTest-1008", "question": "where is walton indiana", "prediction": [["location.location.containedby"], ["location.hud_county_place.county"], ["location.location.contains"], ["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.citytown.postal_codes"], ["location.place_with_neighborhoods.neighborhoods"], ["base.biblioness.bibs_location.state"], ["geography.lake.basin_countries"], ["location.town.postal_codes"]], "ground_paths": [["location.location.containedby"], ["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.place_with_neighborhoods.neighborhoods </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.town.postal_codes </PATH>"], "scores": [-0.005545706953853369, -0.04628773033618927, -0.05069835111498833, -0.051283154636621475, -0.0815587118268013, -0.10091708600521088, -0.12076299637556076, -0.15226149559020996, -0.16418008506298065, -0.1689242124557495], "norm_scores": [0.10911848396062851, 0.10476211458444595, 0.1043010726571083, 0.10424009710550308, 0.10113146156072617, 0.0991925448179245, 0.09724339097738266, 0.0942281037569046, 0.09311170130968094, 0.09267101436853409]}}
{"id": "WebQTest-1009", "question": "where did george herbert walker bush go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.educational_institution.parent_institution"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["base.yalebase.person.residential_college", "education.educational_institution.parent_institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george herbert walker bush go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.015711791813373566, -0.017577890306711197, -0.01905794069170952, -0.050411101430654526, -0.06159554421901703, -0.0667378306388855, -0.07029857486486435, -0.08854144811630249, -0.09199061244726181, -0.100635826587677], "norm_scores": [0.1042984127998352, 0.10410396754741669, 0.10395000129938126, 0.10074140131473541, 0.09962094575166702, 0.09910997748374939, 0.09875769913196564, 0.09697240591049194, 0.0966385155916214, 0.09580665826797485]}}
{"id": "WebQTest-1010", "question": "what did bruce jenner win gold medal for", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_event_competition.medalists"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_medal_honor.event"], ["olympics.olympic_athlete_affiliation.athlete", "olympics.olympic_event_competition.medalists"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_athlete_affiliation.event"], ["olympics.olympic_athlete_affiliation.athlete", "olympics.olympic_athlete_affiliation.sport"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_athlete_affiliation.sport"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_event_competition.medalists"]], "ground_paths": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bruce jenner win gold medal for [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> olympics.olympic_athlete_affiliation.athlete <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_athlete_affiliation.event </PATH>", "<PATH> olympics.olympic_athlete_affiliation.athlete <SEP> olympics.olympic_athlete_affiliation.sport </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_athlete_affiliation.sport </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_event_competition.medalists </PATH>"], "scores": [-0.007286984473466873, -0.019844822585582733, -0.04456159472465515, -0.04593244567513466, -0.05315012484788895, -0.05354681983590126, -0.05689249560236931, -0.06646226346492767, -0.06727328896522522, -0.06757214665412903], "norm_scores": [0.10416221618652344, 0.10286233574151993, 0.10035108029842377, 0.10021360963582993, 0.09949290752410889, 0.09945344179868698, 0.09912125766277313, 0.0981772169470787, 0.09809762239456177, 0.09806831181049347]}}
{"id": "WebQTest-1011", "question": "where do they speak tibetan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["people.ethnicity.languages_spoken"], ["language.language_family.languages"], ["language.language_family.geographic_distribution"], ["language.human_language.writing_system"], ["language.human_language.region"], ["people.ethnicity.geographic_distribution"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak tibetan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>", "<PATH> language.human_language.writing_system </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> people.ethnicity.geographic_distribution </PATH>"], "scores": [-0.013334019109606743, -0.033902931958436966, -0.03556505963206291, -0.03945374861359596, -0.052668944001197815, -0.09386847913265228, -0.0938786193728447, -0.09813036769628525, -0.10681506991386414, -0.10802961885929108], "norm_scores": [0.10551145672798157, 0.10336336493492126, 0.10319170355796814, 0.1027911975979805, 0.10144174098968506, 0.09734730422496796, 0.09734632074832916, 0.09693330526351929, 0.09609511494636536, 0.09597846865653992]}}
{"id": "WebQTest-1012", "question": "what to do in hollywood ca this weekend", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.neighborhood.neighborhood_of"], ["location.location.contains"], ["film.film.featured_film_locations"], ["film.film_location.featured_in_films"], ["film.film_festival.location"], ["organization.organization.geographic_scope"], ["metropolitan_transit.transit_system.area_served"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in hollywood ca this weekend [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.neighborhood.neighborhood_of </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> film.film_location.featured_in_films </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>"], "scores": [-0.015126170590519905, -0.015382304787635803, -0.0279648769646883, -0.11249712854623795, -0.11648336797952652, -0.1269274801015854, -0.13015052676200867, -0.13359296321868896, -0.1370900273323059, -0.13786709308624268], "norm_scores": [0.10820943862199783, 0.10818172991275787, 0.10682905465364456, 0.0981697142124176, 0.09777915477752686, 0.0967632606625557, 0.09645188599824905, 0.0961204320192337, 0.09578487277030945, 0.09571046382188797]}}
{"id": "WebQTest-1014", "question": "where did kim kardashian come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["symbols.namesake.named_after"], ["place.person.place_of_birth"], ["people.ethnicity.people"], ["base.popstra.celebrity.dated", "location.location.containedby"], ["base.popstra.celebrity.breakup"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim kardashian come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> place.person.place_of_birth </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> location.location.containedby </PATH>", "<PATH> base.popstra.celebrity.breakup </PATH>"], "scores": [-0.0039672465063631535, -0.03926648572087288, -0.04527849331498146, -0.10570395737886429, -0.13482701778411865, -0.13537494838237762, -0.15273471176624298, -0.15298710763454437, -0.15339957177639008, -0.15707607567310333], "norm_scores": [0.11080492287874222, 0.10696182399988174, 0.10632070153951645, 0.10008646547794342, 0.09721367806196213, 0.09716042876243591, 0.0954883024096489, 0.09546419978141785, 0.0954248309135437, 0.09507465362548828]}}
{"id": "WebQTest-1015", "question": "what town did justin bieber grew up in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "location.administrative_division.second_level_division_of"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["influence.influence_node.influenced", "people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"], ["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town did justin bieber grew up in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.second_level_division_of </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.013223539106547832, -0.025985661894083023, -0.033218491822481155, -0.03810397535562515, -0.08048443496227264, -0.0894550010561943, -0.0930178314447403, -0.10194386541843414, -0.1077750101685524, -0.11481225490570068], "norm_scores": [0.10575193166732788, 0.10441089421510696, 0.10365843027830124, 0.10315324366092682, 0.09887290000915527, 0.09798992425203323, 0.09764141589403152, 0.09677375108003616, 0.09621109068393707, 0.09553640335798264]}}
{"id": "WebQTest-1017", "question": "who plays charlie in the santa clause movies", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.performance.film", "film.performance.actor"], ["film.film.starring", "film.actor.film"], ["film.performance.film", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays charlie in the santa clause movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.02721535786986351, -0.027623599395155907, -0.028728308156132698, -0.03457816690206528, -0.037852950394153595, -0.050590384751558304, -0.05267084389925003, -0.05273669213056564, -0.05535811558365822, -0.05726173147559166], "norm_scores": [0.10152927041053772, 0.10148782283067703, 0.10137578099966049, 0.10078448057174683, 0.10045496374368668, 0.09918353706598282, 0.09897740185260773, 0.09897089004516602, 0.0987117812037468, 0.0985240563750267]}}
{"id": "WebQTest-1020", "question": "what highschool did r. kelly attend", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["music.artist.origin", "location.location.containedby"], ["music.artist.genre", "music.artist.genre"], ["music.artist.education", "education.educational_institution.students_graduates"], ["music.artist.label", "music.artist.label"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did r. kelly attend [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> music.artist.origin <SEP> location.location.containedby </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>"], "scores": [-0.017080849036574364, -0.018568556755781174, -0.019865041598677635, -0.040492769330739975, -0.04972367361187935, -0.06755213439464569, -0.09683840721845627, -0.10390643030405045, -0.10429218411445618, -0.10690028220415115], "norm_scores": [0.10458046197891235, 0.10442499071359634, 0.1042896956205368, 0.10216047614812851, 0.10122177749872208, 0.09943313896656036, 0.09656334668397903, 0.09588323533535004, 0.0958462506532669, 0.09559660404920578]}}
{"id": "WebQTest-1021", "question": "what happened at benghazi", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["military.military_combatant_group.conflict", "military.military_combatant_group.conflict"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["time.event.includes_event"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["time.event.included_in_event"], ["book.written_work.subjects"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened at benghazi [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.conflict </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.00505610229447484, -0.044140931218862534, -0.05109771713614464, -0.07681862264871597, -0.09365297108888626, -0.09560047090053558, -0.09779618680477142, -0.0990188792347908, -0.10224169492721558, -0.10323726385831833], "norm_scores": [0.10739188641309738, 0.10327546298503876, 0.10255949199199677, 0.09995520114898682, 0.09828660637140274, 0.0980953797698021, 0.09788022935390472, 0.0977606251835823, 0.09744606912136078, 0.09734909236431122]}}
{"id": "WebQTest-1022", "question": "what country is nike based in", "prediction": [["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.organization.place_founded"], ["organization.organization.phone_number", "common.phone_number.service_location"], ["organization.organization.place_founded", "location.country.capital"], ["organization.organization.founders", "people.person.nationality"], ["organization.organization.place_founded", "location.country.administrative_divisions"]], "ground_paths": [["organization.organization.founders", "people.person.nationality"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["organization.organization.headquarters", "location.mailing_address.country"], ["film.film.production_companies", "film.film.country"], ["organization.organization.phone_number", "common.phone_number.service_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nike based in [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> people.person.nationality </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.phone_number <SEP> common.phone_number.service_location </PATH>", "<PATH> organization.organization.place_founded <SEP> location.country.capital </PATH>", "<PATH> organization.organization.founders <SEP> people.person.nationality </PATH>", "<PATH> organization.organization.place_founded <SEP> location.country.administrative_divisions </PATH>"], "scores": [-0.004892728757113218, -0.039841242134571075, -0.04423147812485695, -0.05587310716509819, -0.0607830248773098, -0.06370101124048233, -0.07056885212659836, -0.0763578936457634, -0.0802636668086052, -0.08343382179737091], "norm_scores": [0.10542701929807663, 0.1018061488866806, 0.10136017203330994, 0.10018701106309891, 0.0996963158249855, 0.09940582513809204, 0.09872546046972275, 0.09815558791160583, 0.09777295589447021, 0.09746349602937698]}}
{"id": "WebQTest-1024", "question": "what kind of money do i bring to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "finance.currency.countries_used"], ["travel.travel_destination.tour_operators"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i bring to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> finance.currency.countries_used </PATH>", "<PATH> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.007210189010947943, -0.02639804221689701, -0.056601632386446, -0.06067028269171715, -0.06329511106014252, -0.06473653018474579, -0.07014960050582886, -0.07602080702781677, -0.08419428765773773, -0.09278056025505066], "norm_scores": [0.10541073232889175, 0.10340739786624908, 0.10033082962036133, 0.09992345422506332, 0.09966151416301727, 0.09951796382665634, 0.09898071736097336, 0.09840128570795059, 0.0976002886891365, 0.09676584601402283]}}
{"id": "WebQTest-1025", "question": "who did steve spurrier play pro football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["common.topic.notable_types"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["common.topic.notable_types", "common.topic.notable_types"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did steve spurrier play pro football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.008290351368486881, -0.022170810028910637, -0.03163955733180046, -0.05902920663356781, -0.06416420638561249, -0.07687672972679138, -0.08149419724941254, -0.082234226167202, -0.08474987000226974, -0.0873042419552803], "norm_scores": [0.10524557530879974, 0.10379481315612793, 0.102816641330719, 0.10003875195980072, 0.09952636808156967, 0.09826914221048355, 0.09781643748283386, 0.09774407744407654, 0.09749849140644073, 0.09724976867437363]}}
{"id": "WebQTest-1027", "question": "where do the blackhawks play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"], ["sports.sports_championship_event.champion"], ["sports.sports_arena_stadium.team"], ["sports.sports_facility.teams", "sports.sports_facility.teams"], ["sports.sports_facility.teams", "sports.sports_team.arena_stadium"], ["sports.sports_venue_relationship.team", "sports.sports_facility.teams"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the blackhawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_arena_stadium.team </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_venue_relationship.team <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.0025159590877592564, -0.03588831424713135, -0.1012643501162529, -0.10687604546546936, -0.13451950252056122, -0.14273184537887573, -0.14444375038146973, -0.153911292552948, -0.1576811522245407, -0.15853136777877808], "norm_scores": [0.11162494868040085, 0.1079612448811531, 0.10112892836332321, 0.10056301206350327, 0.0978211760520935, 0.09702112525701523, 0.09685517847537994, 0.09594252705574036, 0.09558150917291641, 0.09550029039382935]}}
{"id": "WebQTest-1028", "question": "who plays john connor", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["people.person.gender", "people.person.gender"], ["film.film_character.portrayed_in_films_dubbed", "film.actor.dubbing_performances"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays john connor [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films_dubbed <SEP> film.actor.dubbing_performances </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.02170960046350956, -0.021871929988265038, -0.026776457205414772, -0.03653609752655029, -0.03829404339194298, -0.07910307496786118, -0.08783894777297974, -0.09353526681661606, -0.09372919797897339, -0.09524448215961456], "norm_scores": [0.1037973091006279, 0.10378046333789825, 0.10327272117137909, 0.10226970911026001, 0.10209008306264877, 0.09800775349140167, 0.09715529531240463, 0.09660344570875168, 0.09658471494913101, 0.096438467502594]}}
{"id": "WebQTest-1029", "question": "what is the main language spoken in mexico", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["film.film.featured_film_locations"], ["film.film_location.featured_in_films"], ["film.film_location.featured_in_films", "film.film.language"], ["film.film.country", "film.film.language"], ["film.film.featured_film_locations", "film.film.language"], ["film.film.language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language spoken in mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> film.film_location.featured_in_films </PATH>", "<PATH> film.film_location.featured_in_films <SEP> film.film.language </PATH>", "<PATH> film.film.country <SEP> film.film.language </PATH>", "<PATH> film.film.featured_film_locations <SEP> film.film.language </PATH>", "<PATH> film.film.language </PATH>"], "scores": [-0.017565591260790825, -0.025993354618549347, -0.028084207326173782, -0.04518328607082367, -0.0486598014831543, -0.062362343072891235, -0.09690946340560913, -0.10664962232112885, -0.11261220276355743, -0.1221361979842186], "norm_scores": [0.10495367646217346, 0.10407286137342453, 0.10385549068450928, 0.10209475457668304, 0.10174043476581573, 0.1003558412194252, 0.09694804251194, 0.09600834548473358, 0.09543757885694504, 0.09453294426202774]}}
{"id": "WebQTest-1030", "question": "what are italians classified as", "prediction": [["people.ethnicity.included_in_group"], ["people.ethnicity.includes_groups"], ["people.person.ethnicity"], ["people.ethnicity.languages_spoken"], ["people.ethnicity.people"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["people.ethnicity.languages_spoken", "people.ethnicity.languages_spoken"], ["people.ethnicity.people_included_in_group"], ["people.ethnicity.people", "people.person.nationality"], ["people.person.nationality"]], "ground_paths": [["people.ethnicity.includes_groups"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are italians classified as [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.ethnicity.includes_groups </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.people_included_in_group </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.009829845279455185, -0.020823607221245766, -0.06405748426914215, -0.07470820844173431, -0.07960009574890137, -0.09115663170814514, -0.09239787608385086, -0.09283330291509628, -0.1016307920217514, -0.10497423261404037], "norm_scores": [0.10648958384990692, 0.10532527416944504, 0.10086868703365326, 0.09980006515979767, 0.09931304305791855, 0.09817194193601608, 0.09805016219615936, 0.09800747036933899, 0.09714903682470322, 0.0968247652053833]}}
{"id": "WebQTest-1031", "question": "what has angelina jolie accomplished", "prediction": [["people.person.profession"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_category.winners"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has angelina jolie accomplished [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_category.winners </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_category.winners </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.014837468974292278, -0.028466850519180298, -0.030602462589740753, -0.043293263763189316, -0.045364636927843094, -0.06031271442770958, -0.06250064820051193, -0.06378965824842453, -0.06424035131931305, -0.06575772166252136], "norm_scores": [0.10334765911102295, 0.10194864869117737, 0.10173116624355316, 0.10044826567173004, 0.10024041682481766, 0.09875316172838211, 0.09853732585906982, 0.09841039776802063, 0.09836605191230774, 0.09821690618991852]}}
{"id": "WebQTest-1032", "question": "who played captain kirk in star trek movie", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["media_common.quotation.spoken_by_character", "media_common.quotation.author"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played captain kirk in star trek movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> media_common.quotation.spoken_by_character <SEP> media_common.quotation.author </PATH>", "<PATH> fictional_universe.fictional_character.quotations <SEP> media_common.quotation.author </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.024190133437514305, -0.02662591263651848, -0.027561478316783905, -0.02969578094780445, -0.03546111658215523, -0.03809238597750664, -0.048612892627716064, -0.05025074630975723, -0.07108896225690842, -0.07387936860322952], "norm_scores": [0.10183760523796082, 0.10158985108137131, 0.10149484872817993, 0.10127846151590347, 0.10069624334573746, 0.10043162852525711, 0.09938058257102966, 0.09921793639659882, 0.09717180579900742, 0.09690103679895401]}}
{"id": "WebQTest-1033", "question": "who played dana scully in x files", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_honor.honored_for", "award.award_honor.award_winner"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.quotations", "people.person.quotations"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["people.person.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played dana scully in x files [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_honor.award_winner </PATH>"], "scores": [-0.024088039994239807, -0.030841616913676262, -0.032028909772634506, -0.03404548764228821, -0.0345458909869194, -0.037542473524808884, -0.04147909954190254, -0.047179531306028366, -0.04719490557909012, -0.05113859474658966], "norm_scores": [0.10139842331409454, 0.10071592777967453, 0.10059642046689987, 0.10039376467466354, 0.10034354031085968, 0.10004329681396484, 0.09965024143457413, 0.099083811044693, 0.09908229112625122, 0.09869230538606644]}}
{"id": "WebQTest-1035", "question": "what was f. scott fitzgerald", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["influence.influence_node.influenced_by", "people.person.profession"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["film.film_story_contributor.film_story_credits"], ["influence.influence_node.influenced_by"], ["freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was f. scott fitzgerald [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.0005446966970339417, -0.06900031864643097, -0.09711021929979324, -0.11124850064516068, -0.12150933593511581, -0.12942180037498474, -0.13877563178539276, -0.15946365892887115, -0.1605825424194336, -0.1611129194498062], "norm_scores": [0.11198370903730392, 0.1045742928981781, 0.10167565196752548, 0.10024824738502502, 0.09922487288713455, 0.09844285994768143, 0.09752634167671204, 0.09552942961454391, 0.09542260318994522, 0.09537201374769211]}}
{"id": "WebQTest-1036", "question": "what is the dominant language spoken in jamaica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.region"], ["base.rosetta.languoid.languoid_class"], ["book.periodical.language"], ["language.human_language.language_family"], ["language.human_language.dialects"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the dominant language spoken in jamaica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> book.periodical.language </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.dialects </PATH>"], "scores": [-0.015538879670202732, -0.021892141550779343, -0.02317376621067524, -0.061570022255182266, -0.14805330336093903, -0.1536833643913269, -0.1817842721939087, -0.18292167782783508, -0.18587461113929749, -0.190632626414299], "norm_scores": [0.11033374071121216, 0.10963497310876846, 0.10949456691741943, 0.10537007451057434, 0.09664025157690048, 0.09609769284725189, 0.09343484789133072, 0.09332863241434097, 0.09305344521999359, 0.09261175245046616]}}
{"id": "WebQTest-1037", "question": "who is the coach for the new york knicks", "prediction": [["basketball.basketball_team.head_coach"], ["basketball.basketball_coach.team"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"], ["base.schemastaging.athlete_salary.team", "base.schemastaging.athlete_salary.athlete"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["base.schemastaging.athlete_salary.team", "base.schemastaging.athlete_salary.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.sports_coached"]], "ground_paths": [["basketball.basketball_team.head_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach for the new york knicks [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_team.head_coach </PATH>", "<PATH> basketball.basketball_coach.team </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> base.schemastaging.athlete_salary.team <SEP> base.schemastaging.athlete_salary.athlete </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> base.schemastaging.athlete_salary.team <SEP> base.schemastaging.athlete_salary.coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.sports_coached </PATH>"], "scores": [-0.004335065372288227, -0.029282569885253906, -0.06129945442080498, -0.06171170994639397, -0.06535439193248749, -0.07544253766536713, -0.08001647889614105, -0.08102995902299881, -0.08242135494947433, -0.08856070041656494], "norm_scores": [0.10600197315216064, 0.10339020192623138, 0.10013240575790405, 0.1000911295413971, 0.09972719103097916, 0.09872619062662125, 0.09827566146850586, 0.09817610681056976, 0.09803960472345352, 0.09743954241275787]}}
{"id": "WebQTest-1038", "question": "who was wesley snipes", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["influence.influence_node.influenced_by", "people.person.profession"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.special_performance_type"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was wesley snipes [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.special_performance_type </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>"], "scores": [-0.00037922640331089497, -0.08439373970031738, -0.09930896013975143, -0.1335073858499527, -0.1378372609615326, -0.1399148404598236, -0.14956778287887573, -0.14984281361103058, -0.15083225071430206, -0.15185803174972534], "norm_scores": [0.11255885660648346, 0.10348862409591675, 0.10195652395486832, 0.09852872043848038, 0.09810302406549454, 0.0978994220495224, 0.0969589427113533, 0.09693228453397751, 0.09683643281459808, 0.09673713892698288]}}
{"id": "WebQTest-1039", "question": "where does sherrod brown live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.nationality", "location.location.containedby"], ["location.location.people_born_here"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.children", "people.person.place_of_birth"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.nationality"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sherrod brown live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.015113345347344875, -0.01796242594718933, -0.03633752465248108, -0.05282673239707947, -0.06018424406647682, -0.06554052978754044, -0.08595366775989532, -0.09730878472328186, -0.1006704792380333, -0.1068701222538948], "norm_scores": [0.10494361072778702, 0.10464505106210709, 0.10273973643779755, 0.10105954110622406, 0.10031872242689133, 0.09978281706571579, 0.09776658564805984, 0.09666271507740021, 0.09633830934762955, 0.09574289619922638]}}
{"id": "WebQTest-1040", "question": "what kind of language does colombia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["film.film.featured_film_locations"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does colombia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken <PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.006391856819391251, -0.02244526892900467, -0.07265743613243103, -0.08289946615695953, -0.16981123387813568, -0.183991476893425, -0.1882340908050537, -0.1920570731163025, -0.192826509475708, -0.1931285411119461], "norm_scores": [0.11290919035673141, 0.11111108213663101, 0.10566970705986023, 0.10459296405315399, 0.09588642418384552, 0.09453633427619934, 0.09413609653711319, 0.09377691149711609, 0.09370478242635727, 0.09367647767066956]}}
{"id": "WebQTest-1041", "question": "what currency does england use now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["measurement_unit.dated_money_value.currency"], ["language.human_language.main_country", "location.country.currency_used"], ["language.human_language.main_country", "finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"], ["location.country.form_of_government"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does england use now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> measurement_unit.dated_money_value.currency </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.currency_used </PATH>", "<PATH> language.human_language.main_country <SEP> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.0031653279438614845, -0.04373801499605179, -0.06040608510375023, -0.08809726685285568, -0.1355065256357193, -0.16342097520828247, -0.1712789684534073, -0.17448927462100983, -0.1754537671804428, -0.1759202480316162], "norm_scores": [0.11208204925060272, 0.10762560367584229, 0.10584654659032822, 0.10295575112104416, 0.09818859398365021, 0.09548560529947281, 0.09473822265863419, 0.09443457424640656, 0.09434353560209274, 0.09429953992366791]}}
{"id": "WebQTest-1042", "question": "who was the original voice of meg griffin on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_program_creator.programs_created"], ["tv.tv_program_writer_relationship.tv_program", "tv.tv_writer.tv_programs"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the original voice of meg griffin on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program_writer_relationship.tv_program <SEP> tv.tv_writer.tv_programs </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.014362973161041737, -0.028416620567440987, -0.032746247947216034, -0.03337491676211357, -0.03844965249300003, -0.039518292993307114, -0.04454483091831207, -0.048919856548309326, -0.05483345687389374, -0.05669667571783066], "norm_scores": [0.10250578820705414, 0.10107528418302536, 0.10063861310482025, 0.10057535767555237, 0.10006626695394516, 0.09995938837528229, 0.09945819526910782, 0.09902401268482208, 0.09844015538692474, 0.098256915807724]}}
{"id": "WebQTest-1043", "question": "what kind of government is mali", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.locations.countries.continent"], ["base.aareas.schema.administrative_area.administrative_parent", "location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is mali [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.00810949131846428, -0.0179282296448946, -0.1301494538784027, -0.13748736679553986, -0.14024318754673004, -0.15019957721233368, -0.1515311896800995, -0.15189319849014282, -0.15624161064624786, -0.1594037562608719], "norm_scores": [0.11170530319213867, 0.1106138601899147, 0.09887181222438812, 0.09814894944429398, 0.0978788509964943, 0.09690917283296585, 0.0967802032828331, 0.09674517065286636, 0.09632540494203568, 0.09602128714323044]}}
{"id": "WebQTest-1044", "question": "who did cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_transfer.player", "soccer.football_player_transfer.purchasing_team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_transfer.player <SEP> soccer.football_player_transfer.purchasing_team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>"], "scores": [-0.019647924229502678, -0.022282443940639496, -0.02547472156584263, -0.032744377851486206, -0.034580379724502563, -0.03538572043180466, -0.03549633547663689, -0.040741514414548874, -0.044684529304504395, -0.047368213534355164], "norm_scores": [0.10142553597688675, 0.10115867853164673, 0.10083626955747604, 0.100105881690979, 0.09992224723100662, 0.09984181821346283, 0.09983076900243759, 0.09930851310491562, 0.09891770780086517, 0.09865259379148483]}}
{"id": "WebQTest-1045", "question": "what is the zip code for morgantown wv", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.citytown.postal_ codes"], ["location.Citytown.postal_codes"], ["location.citytown.postal_codes"], ["location.citytown.postal_code"], ["location.mailing_address.citytown"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for morgantown wv [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.citytown.postal_ codes </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "</PATH> location.citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_code </PATH>", "<PATH> location.mailing_address.citytown </PATH>"], "scores": [-6.234789179870859e-05, -0.10605676472187042, -0.15338772535324097, -0.1666841208934784, -0.173284649848938, -0.17489628493785858, -0.17598648369312286, -0.17797349393367767, -0.19806934893131256, -0.1987110674381256], "norm_scores": [0.11627572774887085, 0.10458184033632278, 0.0997471958398819, 0.09842969477176666, 0.09778214991092682, 0.09762468934059143, 0.09751830995082855, 0.09732473641633987, 0.09538843482732773, 0.09532724320888519]}}
{"id": "WebQTest-1046", "question": "what is jacksonville fl timezone", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.time_Zones"], ["location.city.time_zones"], ["location.location.time_zonessymbol"], ["locations.location.time_zones"], ["location.location.time_zonescommon"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jacksonville fl timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.city.time_zones </PATH>", "<PATH> location.location.time_zonessymbol </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> location.location.time_zonescommon </PATH>"], "scores": [-9.499107363808434e-06, -0.1902889460325241, -0.19354432821273804, -0.19472135603427887, -0.1995183229446411, -0.20824266970157623, -0.22878707945346832, -0.23324531316757202, -0.2442772537469864, -0.24490953981876373], "norm_scores": [0.12108701467514038, 0.10010603070259094, 0.09978067874908447, 0.09966330230236053, 0.09918636828660965, 0.09832480549812317, 0.09632538259029388, 0.0958968922495842, 0.09484478086233139, 0.09478483349084854]}}
{"id": "WebQTest-1047", "question": "who does amy stiller play in dodgeball", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["film.actor.film", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film.starring", "film.performance.character"]], "ground_paths": [["film.film.starring", "film.performance.character"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does amy stiller play in dodgeball [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film.starring <SEP> film.performance.character </PATH>"], "scores": [-0.022979117929935455, -0.0247341375797987, -0.02657298371195793, -0.03451250493526459, -0.03929351642727852, -0.04119250550866127, -0.055611517280340195, -0.05712755024433136, -0.058714255690574646, -0.07075106352567673], "norm_scores": [0.1020248755812645, 0.10184598714113235, 0.10165887326002121, 0.10085494071245193, 0.10037390887737274, 0.10018347948789597, 0.0987493023276329, 0.09859970211982727, 0.09844338148832321, 0.09726554155349731]}}
{"id": "WebQTest-1048", "question": "who were king henry vii children", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"], ["fictional_universe.fictional_character.parents"], ["people.deceased_person.place_of_burial"], ["fictional_universe.fictional_character.children"], ["people.person.offspring"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.children.parents"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were king henry vii children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>", "<PATH> people.person.offspring </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.children.parents </PATH>"], "scores": [-0.012192842550575733, -0.015561849810183048, -0.14121390879154205, -0.14353704452514648, -0.16505132615566254, -0.16662536561489105, -0.16860325634479523, -0.1738995462656021, -0.1762324869632721, -0.18341052532196045], "norm_scores": [0.1128043606877327, 0.11242496222257614, 0.09915001690387726, 0.09891994297504425, 0.09681447595357895, 0.09666220843791962, 0.09647120535373688, 0.09596161544322968, 0.0957380086183548, 0.09505325555801392]}}
{"id": "WebQTest-1050", "question": "who does mila kunis play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.fictional_character.character_created_by", "fictional_universe.fictional_character.character_created_by"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does mila kunis play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_universe.characters </PATH>", "<PATH> tv.tv_program_creator.programs_created <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.020237542688846588, -0.023047884926199913, -0.02347601391375065, -0.033357832580804825, -0.034460920840501785, -0.036488160490989685, -0.038970544934272766, -0.05074356868863106, -0.05539657548069954, -0.055495087057352066], "norm_scores": [0.10169952362775803, 0.10141411423683167, 0.10137070715427399, 0.10037390887737274, 0.10026325285434723, 0.10006019473075867, 0.0998121127486229, 0.09864392131567001, 0.09818600118160248, 0.09817633032798767]}}
{"id": "WebQTest-1051", "question": "when did the san francisco earthquake occur", "prediction": [["time.event.locations"], ["event.disaster.areas_affected"], ["location.location.events"], ["time.recurring_event.instances"], ["time.recurring_event.locations"], ["time.recurring_event.current_frequency"], ["time.event.included_in_event"], ["meteorology.tropical_cyclone.affected_areas"], ["time.recurring_event.included_in_event"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the san francisco earthquake occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.recurring_event.locations </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> time.recurring_event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.0028787325136363506, -0.05016526207327843, -0.062078237533569336, -0.07048619538545609, -0.08448473364114761, -0.08830013871192932, -0.09805615246295929, -0.09950461238622665, -0.101203553378582, -0.11476049572229385], "norm_scores": [0.10766195505857468, 0.10268948972225189, 0.10147340595722198, 0.10062380135059357, 0.09922502189874649, 0.0988471731543541, 0.09788750112056732, 0.09774582087993622, 0.09757989645004272, 0.09626594185829163]}}
{"id": "WebQTest-1052", "question": "who played damien in the omen 2006", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.film", "film.actor.film"], ["film.film.starring", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film.starring", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played damien in the omen 2006 [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.016972653567790985, -0.02376565709710121, -0.03719639778137207, -0.03728848695755005, -0.03953399136662483, -0.04898420348763466, -0.04964388161897659, -0.05216892808675766, -0.06260471791028976, -0.0653957948088646], "norm_scores": [0.10266222059726715, 0.10196719318628311, 0.10060685873031616, 0.10059759020805359, 0.10037195682525635, 0.09942788630723953, 0.0993623211979866, 0.09911173582077026, 0.09808281064033508, 0.09780943393707275]}}
{"id": "WebQTest-1054", "question": "who played denver in four christmases", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["people.person.place_of_birth", "people.person.gender", "people.person.gender"], ["film.performance.film", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film.starring", "film.actor.film"], ["people.person.gender", "people.person.gender"], ["film.film.starring", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.film.featured_film_locations", "people.person.place_of_birth"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.film.language", "people.person.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played denver in four christmases [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film.featured_film_locations <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.0197160467505455, -0.031132731586694717, -0.04653313755989075, -0.04728517681360245, -0.051722146570682526, -0.05227562040090561, -0.05749715119600296, -0.05853959172964096, -0.058807212859392166, -0.06488028913736343], "norm_scores": [0.10294623672962189, 0.10177762061357498, 0.10022220760583878, 0.10014686733484268, 0.09970349818468094, 0.09964833408594131, 0.09912937134504318, 0.09902609139680862, 0.09899959713220596, 0.09840019047260284]}}
{"id": "WebQTest-1055", "question": "what to do with kids in essex", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["organization.organization.geographic_scope"], ["base.aareas.schema.administrative_area.administrative_children"], ["film.film_location.featured_in_films"], ["base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in"], ["transportation.bridge.locale"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in essex [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> film.film_location.featured_in_films </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> transportation.bridge.locale </PATH>"], "scores": [-0.012176761403679848, -0.017527297139167786, -0.0354963056743145, -0.06753510236740112, -0.15126599371433258, -0.15441831946372986, -0.15632329881191254, -0.1565818190574646, -0.16829688847064972, -0.16905945539474487], "norm_scores": [0.10992777347564697, 0.10934117436408997, 0.10739397257566452, 0.10400773584842682, 0.0956537052989006, 0.0953526422381401, 0.09517117589712143, 0.09514656662940979, 0.09403841942548752, 0.09396673738956451]}}
{"id": "WebQTest-1056", "question": "who did the cleveland cavaliers draft", "prediction": [["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.player"], ["sports.sports_team.roster", "sports.pro_athlete.teams"], ["sports.sports_team.roster", "sports.sports_team_roster.player"], ["sports.sports_team_roster.team", "sports.pro_athlete.teams"], ["sports.sports_league_draft.picks", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft.picks", "sports.drafted_athlete.drafted"]], "ground_paths": [["sports.sports_team.roster", "sports.pro_athlete.teams"], ["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.player"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["sports.sports_team_roster.team", "sports.pro_athlete.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the cleveland cavaliers draft [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.team <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.player </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.sports_team.roster <SEP> sports.pro_athlete.teams </PATH>", "<PATH> sports.sports_team.roster <SEP> sports.sports_team_roster.player </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.pro_athlete.teams </PATH>", "<PATH> sports.sports_league_draft.picks <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.sports_league_draft.picks <SEP> sports.drafted_athlete.drafted </PATH>"], "scores": [-0.021154159680008888, -0.023356884717941284, -0.02448219433426857, -0.028931476175785065, -0.029843661934137344, -0.030493659898638725, -0.035077352076768875, -0.057594481855630875, -0.06767147779464722, -0.07038731127977371], "norm_scores": [0.10177414864301682, 0.10155021399259567, 0.10143599659204483, 0.10098568350076675, 0.10089361667633057, 0.10082805156707764, 0.10036694258451462, 0.09813223034143448, 0.09714831411838531, 0.0968848317861557]}}
{"id": "WebQTest-1057", "question": "what concentration camp did anne frank died in", "prediction": [["base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["film.film.story_by", "base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["book.written_work.subjects"], ["symbols.name_source.namesakes", "base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["book.written_work.subjects", "base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["people.deceased_person.place_of_death"], ["base.popstra.celebrity.arrest", "base.popstra.location.arrestee"], ["base.fictionaluniverse.deceased_fictional_character.place_of_death", "base.fictional_universe.deceased_fictional_character.place_of_death"], ["base.fictionaluniverse.fictional_character.occupation", "base.fictional_universe.deceased_fictional_character.place_of_death"], ["film.film.story_by"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what concentration camp did anne frank died in [/INST]", "raw_output": {"paths": ["<PATH> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> film.film.story_by <SEP> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> symbols.name_source.namesakes <SEP> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> book.written_work.subjects <SEP> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.popstra.celebrity.arrest <SEP> base.popstra.location.arrestee </PATH>", "<PATH> base.fictionaluniverse.deceased_fictional_character.place_of_death <SEP> base.fictional_universe.deceased_fictional_character.place_of_death </PATH>", "<PATH> base.fictionaluniverse.fictional_character.occupation <SEP> base.fictional_universe.deceased_fictional_character.place_of_death </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.00673291552811861, -0.04055332764983177, -0.04536345973610878, -0.05655757710337639, -0.057230204343795776, -0.06220440939068794, -0.0684838816523552, -0.06995002180337906, -0.07849748432636261, -0.07878032326698303], "norm_scores": [0.10507362335920334, 0.10157940536737442, 0.10109197348356247, 0.09996664524078369, 0.09989942610263824, 0.0994037389755249, 0.09878149628639221, 0.09863678365945816, 0.09779726713895798, 0.0977696105837822]}}
{"id": "WebQTest-1059", "question": "what language does east timor speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["book.book_subject.works"], ["book.written_work.subjects"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does east timor speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.form_of_government </PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> location.country.languages_spoken <PATH>"], "scores": [-0.007763299625366926, -0.02920791320502758, -0.0472755990922451, -0.05263764411211014, -0.1442084163427353, -0.16544610261917114, -0.19785800576210022, -0.2014884054660797, -0.2036561816930771, -0.20366542041301727], "norm_scores": [0.11213663220405579, 0.10975751280784607, 0.10779225826263428, 0.10721581429243088, 0.09783408045768738, 0.0957782119512558, 0.09272363036870956, 0.0923876240849495, 0.0921875610947609, 0.09218670427799225]}}
{"id": "WebQTest-1060", "question": "what is the currency in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["base.coinsdaily.coin_type.country"], ["government.form_of_government.countries"], ["common.topic.notable_types"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005395091604441404, -0.02701214887201786, -0.14369474351406097, -0.1459270566701889, -0.15168151259422302, -0.1558055728673935, -0.15852472186088562, -0.17044833302497864, -0.17784278094768524, -0.18413256108760834], "norm_scores": [0.11329636722803116, 0.1108735129237175, 0.09866274893283844, 0.09844274818897247, 0.09787788987159729, 0.097475066781044, 0.09721037745475769, 0.09605816751718521, 0.0953504890203476, 0.09475263953208923]}}
{"id": "WebQTest-1061", "question": "where does greek language come from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"], ["language.language_family.languages"], ["common.topic.notable_types"], ["location.country.languages_spoken"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.language_family", "language.human_language.language_family"], ["location.country.official_language"], ["base.rosetta.languoid.children"]], "ground_paths": [["base.rosetta.languoid.child"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does greek language come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.language_family <SEP> language.human_language.language_family </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.children </PATH>"], "scores": [-0.00856891367584467, -0.025239309296011925, -0.05542740225791931, -0.055595919489860535, -0.11585061252117157, -0.12357670068740845, -0.13077862560749054, -0.13291919231414795, -0.1336008906364441, -0.14338672161102295], "norm_scores": [0.10862763226032257, 0.10683177411556244, 0.10365492105484009, 0.1036374568939209, 0.09757722914218903, 0.09682624042034149, 0.09613140672445297, 0.09592584520578384, 0.09586047381162643, 0.09492698311805725]}}
{"id": "WebQTest-1062", "question": "what is the name of the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country"], ["common.topic.notable_types"], ["government.form_of_government.countries"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.currency_used <PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.005133545026183128, -0.02469995804131031, -0.14039300382137299, -0.14574210345745087, -0.14576254785060883, -0.17045558989048004, -0.1766718178987503, -0.17917026579380035, -0.18165479600429535, -0.18234211206436157], "norm_scores": [0.11366485059261322, 0.11146245896816254, 0.09928502887487411, 0.0987553521990776, 0.09875334054231644, 0.09634468704462051, 0.09574764221906662, 0.09550871700048447, 0.09527172148227692, 0.09520626068115234]}}
{"id": "WebQTest-1063", "question": "where is audrey hepburn from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["place.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth", "location.location.containedby"], ["symbols.namesake.named_after"], ["location.location.people_born_here", "location.location.containedby"], ["symbols.name_source.namesakes"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is audrey hepburn from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> place.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.001143945730291307, -0.05320613086223602, -0.09932297468185425, -0.14174486696720123, -0.14191187918186188, -0.14560310542583466, -0.15500345826148987, -0.15924997627735138, -0.17236532270908356, -0.17373870313167572], "norm_scores": [0.11294281482696533, 0.10721320658922195, 0.1023811474442482, 0.0981287807226181, 0.09811239689588547, 0.09775090962648392, 0.09683632105588913, 0.09642597287893295, 0.09516957402229309, 0.09503895789384842]}}
{"id": "WebQTest-1065", "question": "who does darth vaders voice", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.special_performance_type", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.021609367802739143, -0.022343764081597328, -0.03721171244978905, -0.037341177463531494, -0.04157424345612526, -0.04772587865591049, -0.054554328322410583, -0.058995891362428665, -0.07020802050828934, -0.07051720470190048], "norm_scores": [0.10247625410556793, 0.10240103304386139, 0.10088979452848434, 0.10087673366069794, 0.10045062005519867, 0.09983457624912262, 0.09915518760681152, 0.09871575236320496, 0.09761512279510498, 0.09758494794368744]}}
{"id": "WebQTest-1067", "question": "what school did douglas macarthur go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.educational_institution.students_graduates"], ["military.military_person.education", "education.educational_institution.students_graduates"], ["military.military_person.education", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did douglas macarthur go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> military.military_person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> military.military_person.education <SEP> education.education.institution </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.015229683369398117, -0.017166294157505035, -0.030790194869041443, -0.03494206815958023, -0.03511657938361168, -0.06916199624538422, -0.07732082903385162, -0.07758232951164246, -0.09015311300754547, -0.11202533543109894], "norm_scores": [0.10410359501838684, 0.1039021834731102, 0.10249622911214828, 0.10207156091928482, 0.10205374658107758, 0.09863776713609695, 0.09783627092838287, 0.09781068563461304, 0.09658883512020111, 0.09449915587902069]}}
{"id": "WebQTest-1068", "question": "what famous people are from barbados", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["location.location.containedby", "people.person.nationality"], ["location.location.containedby", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "location.location.people_born_here"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["location.statistical_region.places_imported_from", "location.imports_and_exports.imported_from"]], "ground_paths": [["people.person.nationality"], ["location.location.contains", "location.location.people_born_here"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from barbados [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.containedby <SEP> location.location.people_born_here </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.imports_and_exports.exported_to </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.statistical_region.places_imported_from </PATH>", "<PATH> location.statistical_region.places_imported_from <SEP> location.imports_and_exports.imported_from </PATH>"], "scores": [-0.013180709443986416, -0.025534233078360558, -0.0413508415222168, -0.05155795067548752, -0.06596928834915161, -0.06599394232034683, -0.07949285954236984, -0.0806342214345932, -0.08221843093633652, -0.0869765505194664], "norm_scores": [0.10468797385692596, 0.10340265929698944, 0.10178004950284958, 0.10074646025896072, 0.09930497407913208, 0.09930252283811569, 0.09797105938196182, 0.09785930067300797, 0.09770438820123672, 0.09724061191082001]}}
{"id": "WebQTest-1069", "question": "what is the name of the currency used in china", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["location.country.form_of_government"], ["finance.currency.countries_formerly_used"], ["location.country.currency_formerly_used"], ["language.human_language.main_country", "location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in china [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.006621329579502344, -0.020915336906909943, -0.12907026708126068, -0.1458638310432434, -0.1461755335330963, -0.14852167665958405, -0.15109531581401825, -0.15227650105953217, -0.1542789191007614, -0.1543274223804474], "norm_scores": [0.11193978041410446, 0.1103510931134224, 0.09903884679079056, 0.09738951921463013, 0.09735916554927826, 0.09713101387023926, 0.09688135981559753, 0.09676699340343475, 0.09657341986894608, 0.09656873345375061]}}
{"id": "WebQTest-1070", "question": "what are the names of michael jackson 's 3 children", "prediction": [["people.person.parents"], ["people.person.children"], ["music.composition.composer", "people.person.parents"], ["music.recording.artist", "music.recording.artist"], ["music.artist.genre", "music.artist.genre"], ["music.artist.origin", "people.person.place_of_birth"], ["people.person.nationality", "people.person.nationality"], ["music.composition.composer", "people.person.children"], ["music.composition.composer"], ["people.person.profession"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's 3 children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.composition.composer <SEP> people.person.parents </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.origin <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> music.composition.composer <SEP> people.person.children </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.008979160338640213, -0.01723060943186283, -0.09454560279846191, -0.09967362880706787, -0.10622576624155045, -0.11040927469730377, -0.1112952008843422, -0.11164719611406326, -0.11348994821310043, -0.11524495482444763], "norm_scores": [0.10823601484298706, 0.10734657943248749, 0.09935981035232544, 0.09885159879922867, 0.09820602834224701, 0.09779603779315948, 0.09770943224430084, 0.09767504781484604, 0.09749522060155869, 0.09732426702976227]}}
{"id": "WebQTest-1072", "question": "who is the current governor of arizona 2010", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"]], "ground_paths": [["location.country.first_level_divisions", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current governor of arizona 2010 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.appointed_by </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.01962282508611679, -0.020620277151465416, -0.02363540418446064, -0.024947084486484528, -0.028604626655578613, -0.04099568352103233, -0.042403340339660645, -0.04316708818078041, -0.04583374783396721, -0.04890899360179901], "norm_scores": [0.10142938047647476, 0.10132826119661331, 0.10102319717407227, 0.10089077055454254, 0.10052243620157242, 0.09928454458713531, 0.09914488345384598, 0.09906918555498123, 0.09880536049604416, 0.09850198030471802]}}
{"id": "WebQTest-1074", "question": "who is lamar odom playing for this year", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is lamar odom playing for this year [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>"], "scores": [-0.01704476587474346, -0.019521333277225494, -0.03154768422245979, -0.03224547952413559, -0.033754076808691025, -0.03655335679650307, -0.039405614137649536, -0.07588997483253479, -0.08483144640922546, -0.09055694937705994], "norm_scores": [0.10291796922683716, 0.10266340523958206, 0.10143612325191498, 0.10136537253856659, 0.10121256113052368, 0.1009296402335167, 0.10064216703176498, 0.09703648835420609, 0.09617270529270172, 0.09562363475561142]}}
{"id": "WebQTest-1075", "question": "where were the great pyramids of giza built", "prediction": [["architecture.building.building_complex"], ["location.location.containedby"], ["architecture.architect.structures_designed"], ["architecture.structure.architect"], ["architecture.building.building_sites"], ["architecture.building.building_location"], ["architecture.building_complex.building_sites"], ["architecture.building.location"], ["architecture.building.building_place"], ["architecture.building.structure"]], "ground_paths": [["architecture.building.building_complex"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the great pyramids of giza built [/INST]", "raw_output": {"paths": ["<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.building.building_sites </PATH>", "<PATH> architecture.building.building_location </PATH>", "<PATH> architecture.building_complex.building_sites </PATH>", "<PATH> architecture.building.location </PATH>", "<PATH> architecture.building.building_place </PATH>", "<PATH> architecture.building.structure </PATH>"], "scores": [-0.007183150853961706, -0.023340675979852676, -0.08695339411497116, -0.09400252997875214, -0.09582214057445526, -0.10349222272634506, -0.11171921342611313, -0.11347981542348862, -0.12250664085149765, -0.12309397757053375], "norm_scores": [0.10835368931293488, 0.1066170260310173, 0.10004603862762451, 0.09934327751398087, 0.09916268289089203, 0.09840500354766846, 0.09759874641895294, 0.09742707014083862, 0.0965515673160553, 0.0964948758482933]}}
{"id": "WebQTest-1077", "question": "what language do the russian speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.language_family"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.region"], ["language.human_language.country"], ["language.human_language.writing_system"], ["language.human_language.dialects"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the russian speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> language.human_language.writing_system </PATH>", "<PATH> language.human_language.dialects </PATH>"], "scores": [-0.01692117750644684, -0.020261548459529877, -0.024699976667761803, -0.1104741096496582, -0.12223070114850998, -0.1457982361316681, -0.14774195849895477, -0.15448208153247833, -0.1765141785144806, -0.18624921143054962], "norm_scores": [0.10959728807210922, 0.10923180729150772, 0.10874805599451065, 0.09980913251638412, 0.09864259511232376, 0.09634500741958618, 0.09615792334079742, 0.09551198780536652, 0.09343066811561584, 0.09252553433179855]}}
{"id": "WebQTest-1078", "question": "what language do australia people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["book.newspaper.circulation_areas"], ["country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do australia people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> country.languages_spoken </PATH>"], "scores": [-0.00784268043935299, -0.01988060586154461, -0.07827910035848618, -0.12488202005624771, -0.16707417368888855, -0.1910172700881958, -0.19427649676799774, -0.19442830979824066, -0.19659709930419922, -0.20011095702648163], "norm_scores": [0.11353613436222076, 0.11217758059501648, 0.10581419616937637, 0.10099609196186066, 0.09682349115610123, 0.09453276544809341, 0.09422516822814941, 0.09421087056398392, 0.09400676190853119, 0.09367701411247253]}}
{"id": "WebQTest-1079", "question": "what country was king tut 's tomb found", "prediction": [["people.person.nationality"], ["people.deceased_person.place_of_death"], ["location.location.containedby"], ["people.deceased_person.place_of_burial"], ["people.person.place_of_birth"], ["people.place_of_interment.interred_here"], ["location.location.contains"], ["people.family.members", "people.family.country"], ["people.person.nationality", "location.location.containedby"], ["location.location.people_born_here"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was king tut 's tomb found [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> people.family.members <SEP> people.family.country </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.01658887229859829, -0.03019263595342636, -0.03095327876508236, -0.030984988436102867, -0.06578215211629868, -0.07610150426626205, -0.09527740627527237, -0.10459839552640915, -0.10909633338451385, -0.12125644832849503], "norm_scores": [0.10521331429481506, 0.10379170626401901, 0.1037127897143364, 0.10370949655771255, 0.10016277432441711, 0.09913447499275208, 0.09725159406661987, 0.0963493213057518, 0.09591691941022873, 0.09475763142108917]}}
{"id": "WebQTest-1080", "question": "where did abraham and his descendants live", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.ethnicity.geographic_distribution"], ["people.deceased_person.place_of_burial"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.deceased_person.place_of_death"], ["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did abraham and his descendants live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.019412348046898842, -0.033750955015420914, -0.03572309762239456, -0.04276121035218239, -0.04875994473695755, -0.051176149398088455, -0.05463220179080963, -0.057450056076049805, -0.06325973570346832, -0.06365584582090378], "norm_scores": [0.10279380530118942, 0.10133039951324463, 0.10113076865673065, 0.1004214957356453, 0.09982088953256607, 0.09957999736070633, 0.09923644363880157, 0.09895720332860947, 0.09838395565748215, 0.09834499657154083]}}
{"id": "WebQTest-1081", "question": "where did george washington carver go to collage", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["common.topic.notable_types"], ["people.deceased_person.place_of_death", "location.location.contains"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george washington carver go to collage [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>"], "scores": [-0.01715898886322975, -0.017802996560931206, -0.018394755199551582, -0.03385819122195244, -0.0597466416656971, -0.0971149280667305, -0.09805180877447128, -0.11375950276851654, -0.11441386491060257, -0.11685609817504883], "norm_scores": [0.10520035028457642, 0.10513262450695038, 0.10507042706012726, 0.10345818102359772, 0.10081417858600616, 0.09711644798517227, 0.09702549129724503, 0.09551336616277695, 0.09545088559389114, 0.09521805495023727]}}
{"id": "WebQTest-1082", "question": "what are the sports in spain", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["sports.sports_facility.teams"], ["sports.sports_teamlocation.teams"], ["sports.sports_team.country"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sports in spain [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_teamlocation.teams </PATH>", "<PATH> sports.sports_team_location.teams <PATH>", "<PATH> sports.sports_team.country </PATH>", "<PATH> sports.sports_team.location <PATH>"], "scores": [-0.010585447773337364, -0.01518003549426794, -0.1395309865474701, -0.17468079924583435, -0.1822115182876587, -0.1827247589826584, -0.18350079655647278, -0.19075201451778412, -0.19366352260112762, -0.19985431432724], "norm_scores": [0.11436593532562256, 0.11384167522192001, 0.10053016245365143, 0.09705791622400284, 0.09632974863052368, 0.09628032892942429, 0.09620563685894012, 0.09551054239273071, 0.09523288160562515, 0.09464512765407562]}}
{"id": "WebQTest-1083", "question": "where president obama was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "location.location.people_born_here"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["people.person.nationality"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where president obama was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.003377349814400077, -0.03998265042901039, -0.08031286299228668, -0.0815693587064743, -0.10568323731422424, -0.10572795569896698, -0.10796713829040527, -0.11087056994438171, -0.1162082701921463, -0.11992762237787247], "norm_scores": [0.10866820067167282, 0.10476230084896088, 0.1006212830543518, 0.10049492120742798, 0.09810058772563934, 0.09809619933366776, 0.09787679463624954, 0.0975930243730545, 0.097073495388031, 0.0967131108045578]}}
{"id": "WebQTest-1084", "question": "what sports do people play in poland", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["cricket.cricket_team.country"], ["book.book_subject.works", "book.book_subject.works"], ["book.book_subject.works", "book.written_work.subjects"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.sports_teamlocation.teams"], ["people.person.nationality", "sports.sports_team.location"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports do people play in poland [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> cricket.cricket_team.country </PATH>", "<PATH> book.book_subject.works <SEP> book.book_subject.works </PATH>", "<PATH> book.book_subject.works <SEP> book.written_work.subjects </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_teamlocation.teams </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams <PATH>"], "scores": [-0.010257863439619541, -0.015063238330185413, -0.15208126604557037, -0.15882034599781036, -0.16008318960666656, -0.16620837152004242, -0.17946721613407135, -0.17948076128959656, -0.1823294311761856, -0.1889256238937378], "norm_scores": [0.11352874338626862, 0.11298450082540512, 0.09851735085248947, 0.09785567224025726, 0.09773216396570206, 0.09713537245988846, 0.0958559662103653, 0.09585466980934143, 0.09558200091123581, 0.09495358914136887]}}
{"id": "WebQTest-1085", "question": "who was demi lovato dating", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.artist.genre", "music.artist.genre"], ["people.person.profession", "people.person.profession"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.friendship", "base.popstra.friendship.participant"], ["people.person.nationality", "people.person.nationality"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["music.composition.composer", "music.composer.compositions"], ["music.album.artist", "music.composer.compositions"], ["music.artist.album", "music.album.artist"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["music.composer.compositions", "music.composition.composer"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.person.nationality", "people.person.nationality"], ["people.person.ethnicity", "people.person.ethnicity"], ["music.recording.artist", "music.composer.compositions"], ["people.person.profession", "people.person.profession"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["music.artist.genre", "music.artist.genre"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["music.composer.compositions", "music.composer.compositions"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.recording.featured_artists", "music.album.featured_artists"], ["music.artist.album", "music.composer.compositions"], ["music.album.artist", "music.album.artist"], ["people.person.languages", "people.person.languages"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["music.recording.artist", "music.composition.composer"], ["music.artist.label", "music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was demi lovato dating [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> base.popstra.celebrity.friendship <SEP> base.popstra.friendship.participant </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.celebrity.sexual_relationships </PATH>", "<PATH> music.composition.composer <SEP> music.composer.compositions </PATH>", "<PATH> music.album.artist <SEP> music.composer.compositions </PATH>", "<PATH> music.artist.album <SEP> music.album.artist </PATH>"], "scores": [-0.02884952537715435, -0.0305353794246912, -0.04003762826323509, -0.0427396297454834, -0.04637124389410019, -0.047811370342969894, -0.04820685461163521, -0.04985718056559563, -0.05072272941470146, -0.054135024547576904], "norm_scores": [0.10151583701372147, 0.10134484618902206, 0.10038640350103378, 0.10011552274227142, 0.09975259751081467, 0.09960904717445374, 0.0995696559548378, 0.09940548241138458, 0.0993194729089737, 0.09898114949464798]}}
{"id": "WebQTest-1087", "question": "what films has scarlett johansson been in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["people.person.languages", "film.film.language"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["film.performance.actor", "film.performance.character"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has scarlett johansson been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.023682011291384697, -0.024073846638202667, -0.033103156834840775, -0.033942729234695435, -0.03857550024986267, -0.04154575616121292, -0.0416736975312233, -0.04662285000085831, -0.04932316765189171, -0.04988870397210121], "norm_scores": [0.10146266222000122, 0.10142291337251663, 0.10051125288009644, 0.10042689740657806, 0.09996271878480911, 0.09966624528169632, 0.09965349733829498, 0.09916152060031891, 0.09889411181211472, 0.0988381952047348]}}
{"id": "WebQTest-1088", "question": "who originally voiced meg on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced meg on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02434156835079193, -0.025770332664251328, -0.026135003194212914, -0.03857843577861786, -0.043580055236816406, -0.047940514981746674, -0.04869465157389641, -0.049500904977321625, -0.051455654203891754, -0.05158192664384842], "norm_scores": [0.10164936631917953, 0.10150423645973206, 0.10146722197532654, 0.1002124473452568, 0.09971247613430023, 0.09927862882614136, 0.09920378774404526, 0.09912382811307907, 0.09893026202917099, 0.09891776740550995]}}
{"id": "WebQTest-1089", "question": "who played amy squirrel in bad teacher", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.film.starring", "film.actor.film"], ["film.performance.film", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.starring", "film.performance.actor"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["film.film.starring", "film.actor.film"], ["film.film.language", "people.person.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played amy squirrel in bad teacher [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.028049057349562645, -0.03229205310344696, -0.03586053475737572, -0.03593061491847038, -0.03997622802853584, -0.041895151138305664, -0.04679352417588234, -0.04982801526784897, -0.05317341163754463, -0.05716460570693016], "norm_scores": [0.10141055285930634, 0.10098118335008621, 0.10062147676944733, 0.10061442852020264, 0.10020820051431656, 0.10001609474420547, 0.09952736645936966, 0.09922581166028976, 0.09889442473649979, 0.09850049763917923]}}
{"id": "WebQTest-1090", "question": "where are boeing headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization.place_founded"], ["organization.organization.headquarters", "location.mailing_address.country"], ["aviation.aircraft_manufacturer.aircraft_models_made"], ["symbols.namesake.named_after", "location.location.containedby"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are boeing headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> aviation.aircraft_manufacturer.aircraft_models_made </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown <SEP> location.location.containedby </PATH>"], "scores": [-0.005033185705542564, -0.04404154792428017, -0.05341355875134468, -0.055585186928510666, -0.05597614124417305, -0.08624812960624695, -0.0919448658823967, -0.09397232532501221, -0.0952867716550827, -0.10369691997766495], "norm_scores": [0.10650815814733505, 0.10243344306945801, 0.10147792100906372, 0.10125777870416641, 0.10121820122003555, 0.09820003807544708, 0.0976422131061554, 0.09744444489479065, 0.0973164513707161, 0.096501424908638]}}
{"id": "WebQTest-1091", "question": "what type of religions are in the united states", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical.subjects"], ["royalty.monarch.kingdom", "people.person.religion"], ["people.person.nationality", "base.argumentmaps.thing_of_disputed_value.disparagement"], ["religion.religion.countries"], ["book.book_subject.works", "base.argumentmaps.thing_of_disputed_value.disparagement"], ["people.person.nationality", "religion.religion.notable_figures"], ["book.newspaper.circulation_areas", "book.periodical_subject.periodicals"], ["symbols.namesake.named_after"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religions are in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.subjects </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> people.person.nationality <SEP> base.argumentmaps.thing_of_disputed_value.disparagement </PATH>", "<PATH> religion.religion.countries </PATH>", "<PATH> book.book_subject.works <SEP> base.argumentmaps.thing_of_disputed_value.disparagement </PATH>", "<PATH> people.person.nationality <SEP> religion.religion.notable_figures </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical_subject.periodicals </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.007858856581151485, -0.01589992083609104, -0.06528476625680923, -0.07443919032812119, -0.07863803207874298, -0.07992809265851974, -0.08109030872583389, -0.08408620953559875, -0.09706026315689087, -0.09872137755155563], "norm_scores": [0.10618302971124649, 0.1053326278924942, 0.10025715082883835, 0.09934354573488235, 0.09892728924751282, 0.09879975020885468, 0.09868498891592026, 0.09838978201150894, 0.09712151437997818, 0.09696032106876373]}}
{"id": "WebQTest-1092", "question": "who did viggo mortensen play in lord of the rings", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.film_series.films_in_series", "book.book_character.appears_in_book"], ["film.performance.film", "film.performance.character"], ["award.award_nomination.award_nominee", "award.award_nomination.character"]], "ground_paths": [["media_common.adaptation.adapted_from", "book.book_character.appears_in_book"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.film_series.films_in_series", "book.book.characters"], ["film.film.story_by", "fictional_universe.fictional_character.character_created_by"], ["music.featured_artist.recordings", "book.book_character.appears_in_book"], ["film.performance.film", "film.performance.character"], ["media_common.adapted_work.adaptations", "book.book.characters"], ["film.actor.film", "film.performance.character"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did viggo mortensen play in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film_series.films_in_series <SEP> book.book_character.appears_in_book </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.character </PATH>"], "scores": [-0.02197333239018917, -0.022768309339880943, -0.02784980833530426, -0.03243766352534294, -0.040582846850156784, -0.051427196711301804, -0.054760947823524475, -0.05638068541884422, -0.06171640381217003, -0.06266727298498154], "norm_scores": [0.10213924199342728, 0.10205808281898499, 0.10154078155755997, 0.10107599943876266, 0.1002560630440712, 0.09917471557855606, 0.09884464740753174, 0.09868468344211578, 0.09815952181816101, 0.09806623309850693]}}
{"id": "WebQTest-1094", "question": "who was isaac newton and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["symbols.namesake.named_after"], ["law.invention.inventor"], ["film.film_subject.films"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was isaac newton and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> film.film_subject.films </PATH>"], "scores": [-0.0006585912778973579, -0.06503710150718689, -0.08150342851877213, -0.09659537672996521, -0.11432501673698425, -0.1314956545829773, -0.1422685831785202, -0.14592967927455902, -0.16155000030994415, -0.16588056087493896], "norm_scores": [0.11147946119308472, 0.10452871769666672, 0.10282160341739655, 0.10128147155046463, 0.09950162470340729, 0.09780769795179367, 0.09675968438386917, 0.09640607982873917, 0.09491188079118729, 0.09450175613164902]}}
{"id": "WebQTest-1095", "question": "who did william shakespeare marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.nationality", "people.person.nationality"], ["book.book_subject.works", "book.book_subject.works"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.parents"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did william shakespeare marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.has_value <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.018222765997052193, -0.03572285547852516, -0.04390227049589157, -0.045089058578014374, -0.04621678963303566, -0.047109585255384445, -0.04823580011725426, -0.04839939624071121, -0.05319256708025932, -0.05456465110182762], "norm_scores": [0.10261290520429611, 0.10083278268575668, 0.10001140087842941, 0.09989277273416519, 0.09978017956018448, 0.09969114512205124, 0.09957893192768097, 0.0995626449584961, 0.09908656775951385, 0.09895070642232895]}}
{"id": "WebQTest-1096", "question": "what movies queen latifah is in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.performance.actor", "film.performance.film"], ["film.producer.films_executive_produced"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.producer.film"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["music.recording.artist"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.film.executive_produced_by"], ["film.performance.actor", "film.film.starring"], ["film.film.produced_by"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.performance.film"], ["film.producer.film", "film.film.sequel"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies queen latifah is in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.024806000292301178, -0.027132892981171608, -0.039542458951473236, -0.0418751984834671, -0.04222407191991806, -0.04244581237435341, -0.04496346414089203, -0.04669823870062828, -0.04715951159596443, -0.048392053693532944], "norm_scores": [0.1015811637043953, 0.10134506225585938, 0.10009519010782242, 0.09986197203397751, 0.09982713311910629, 0.09980499744415283, 0.09955403953790665, 0.09938148409128189, 0.09933565557003021, 0.09921329468488693]}}
{"id": "WebQTest-1097", "question": "where did michelle obama graduated from", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.parent_institution"], ["book.written_work.subjects", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did michelle obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>"], "scores": [-0.01597837172448635, -0.019625935703516006, -0.02102222852408886, -0.05161888524889946, -0.054465401917696, -0.06962534785270691, -0.11541314423084259, -0.11648578196763992, -0.11980009078979492, -0.1219605877995491], "norm_scores": [0.10551995784044266, 0.10513576865196228, 0.10498907417058945, 0.10182540863752365, 0.10153596848249435, 0.1000082939863205, 0.09553239494562149, 0.09542997926473618, 0.09511420875787735, 0.0949089378118515]}}
{"id": "WebQTest-1098", "question": "what kind of music did franz liszt compose", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.composition.composer"], ["music.composer.genre"], ["music.composer.compositions"], ["book.author.works_written"], ["music.album.genre"], ["music.music_genre.artists"], ["music.genre.artist"], ["music.composition.form"]], "ground_paths": [["music.artist.genre"], ["music.genre.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.composer.genre </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.album.genre </PATH>", "<PATH> music.music_genre.artists </PATH>", "<PATH> music.genre.artist </PATH>", "<PATH> music.composition.form </PATH>"], "scores": [-0.005039851646870375, -0.032437559217214584, -0.06315890699625015, -0.08600186556577682, -0.09796532988548279, -0.11177324503660202, -0.11564862728118896, -0.12011120468378067, -0.12069183588027954, -0.12449952960014343], "norm_scores": [0.10853587090969086, 0.10560259968042374, 0.10240767151117325, 0.10009489208459854, 0.098904550075531, 0.09754827618598938, 0.09717096388339996, 0.096738301217556, 0.09668214619159698, 0.0963146984577179]}}
{"id": "WebQTest-1099", "question": "what country does david luiz play for", "prediction": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does david luiz play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.031007111072540283, -0.03137657791376114, -0.03171798959374428, -0.03367755189538002, -0.035309165716171265, -0.03752809390425682, -0.038376279175281525, -0.03852320834994316, -0.04274848476052284, -0.04514412209391594], "norm_scores": [0.10055384784936905, 0.10051669925451279, 0.10048239678144455, 0.10028568655252457, 0.10012218356132507, 0.0999002605676651, 0.09981557726860046, 0.09980089962482452, 0.09938010573387146, 0.09914231300354004]}}
{"id": "WebQTest-1100", "question": "what document did thomas jefferson wrote", "prediction": [["book.written_work.author"], ["book.author.book_editions_published"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["book.written_work.subjects"], ["book.book_subject.works"], ["symbols.namesake.named_after"], ["common.topic.notable_types"], ["architecture.architect.structures_designed", "book.book_edition.author_editor"], ["book.author.book_editions_published", "book.book_edition.author_editor"]], "ground_paths": [["book.book_subject.works"], ["book.author.book_editions_published"], ["book.written_work.subjects"], ["book.book_edition.author_editor"], [], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did thomas jefferson wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> architecture.architect.structures_designed <SEP> book.book_edition.author_editor </PATH>", "<PATH> book.author.book_editions_published <SEP> book.book_edition.author_editor </PATH>"], "scores": [-0.01085883378982544, -0.027438007295131683, -0.03679182380437851, -0.07036405056715012, -0.07467467337846756, -0.07652594894170761, -0.09481221437454224, -0.10714659839868546, -0.10865607112646103, -0.1093461737036705], "norm_scores": [0.10620756447315216, 0.10446123778820038, 0.10348868370056152, 0.10007202625274658, 0.0996415764093399, 0.0994572788476944, 0.09765510261058807, 0.09645798802375793, 0.0963124930858612, 0.09624604880809784]}}
{"id": "WebQTest-1101", "question": "who is martin luther king jr facts", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["base.activism.activist.area_of_activism"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced_by"], ["base.activism.activism_issue.activists"], ["fictional_universe.fictional_character.occupation"], ["influence.influence_node.influenced"]], "ground_paths": [["people.profession.people_with_this_profession"], ["base.activism.activist.area_of_activism"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is martin luther king jr facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.0017137826653197408, -0.06610238552093506, -0.06734368950128555, -0.07082192599773407, -0.10071250796318054, -0.10440872609615326, -0.11121014505624771, -0.11315634846687317, -0.11392104625701904, -0.11721556633710861], "norm_scores": [0.1088009625673294, 0.10201619565486908, 0.10188963264226913, 0.1015358567237854, 0.09854579716920853, 0.09818223118782043, 0.09751672297716141, 0.09732711315155029, 0.09725271910429001, 0.09693284332752228]}}
{"id": "WebQTest-1102", "question": "where is victoria british columbia", "prediction": [["location.location.containedby"], ["location.province.capital"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.state"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.first_level_division_of"], ["location.location.contains"], ["base.aareas.schema.administrative_area.capital"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["geography.lake.basin_countries"]], "ground_paths": [["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is victoria british columbia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.province.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> geography.lake.basin_countries </PATH>"], "scores": [-0.00973054300993681, -0.019904691725969315, -0.06875821202993393, -0.07155219465494156, -0.08583071082830429, -0.0998774990439415, -0.10539301484823227, -0.10693268477916718, -0.11756179481744766, -0.1218767836689949], "norm_scores": [0.1072850301861763, 0.10619902610778809, 0.10113552957773209, 0.10085336118936539, 0.09942354261875153, 0.09803672879934311, 0.09749749302864075, 0.09734749794006348, 0.09631825983524323, 0.09590353816747665]}}
{"id": "WebQTest-1103", "question": "how many mary mary sisters", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["music.artist.genre", "music.artist.genre"], ["people.person.profession", "people.person.profession"], ["music.recording.artist", "music.recording.artist"], ["people.person.sibling_s", "people.person.sibling_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.children"], ["people.person.parents", "people.person.parents"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.children", "people.person.children"]], "ground_paths": [["music.album.artist", "music.composition.composer"], ["music.album.artist", "music.composer.compositions"], ["music.musical_group.member", "music.group_member.membership"], ["music.group_membership.group", "music.group_membership.member"], ["music.artist.origin", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many mary mary sisters [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>", "<PATH> people.person.sibling_s <SEP> people.person.sibling_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.sibling_relationship.sibling <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.0388590432703495, -0.03990314528346062, -0.04590829089283943, -0.04600406810641289, -0.04673440381884575, -0.047255922108888626, -0.048463787883520126, -0.04924550652503967, -0.050568126142024994, -0.05489653721451759], "norm_scores": [0.10079462081193924, 0.10068944096565247, 0.10008659213781357, 0.10007700324058533, 0.10000395029783249, 0.09995179623365402, 0.09983114153146744, 0.09975314140319824, 0.09962128847837448, 0.09919102489948273]}}
{"id": "WebQTest-1104", "question": "what do we call the currency of france", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country"], ["location.country.currency_formerly_used"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["finance.currency.countries_used", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do we call the currency of france [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>"], "scores": [-0.008551424369215965, -0.018859410658478737, -0.09831862896680832, -0.10590530931949615, -0.13629409670829773, -0.13810648024082184, -0.1398966908454895, -0.14008694887161255, -0.15511250495910645, -0.17051923274993896], "norm_scores": [0.11065013706684113, 0.10951541364192963, 0.10115015506744385, 0.10038566589355469, 0.09738095104694366, 0.09720461815595627, 0.09703075885772705, 0.09701230376958847, 0.09556553512811661, 0.09410446137189865]}}
{"id": "WebQTest-1106", "question": "what team does david beckham play for in 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["book.written_work.subjects", "book.written_work.subjects"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does david beckham play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>"], "scores": [-0.020562835037708282, -0.02530056983232498, -0.02915177121758461, -0.02948012948036194, -0.030759820714592934, -0.035484977066516876, -0.03649337962269783, -0.03717385604977608, -0.042956966906785965, -0.0430053249001503], "norm_scores": [0.10125278681516647, 0.10077421367168427, 0.1003868579864502, 0.10035390406847, 0.10022556781768799, 0.09975309669971466, 0.09965255111455917, 0.09958476573228836, 0.09901051968336105, 0.09900572896003723]}}
{"id": "WebQTest-1107", "question": "what movies did nick cannon star in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["music.artist.track", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.actor.film", "film.film.starring"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did nick cannon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>"], "scores": [-0.016987372189760208, -0.035103507339954376, -0.03567317873239517, -0.03901776298880577, -0.039040472358465195, -0.04419049993157387, -0.044746603816747665, -0.044795382767915726, -0.057240862399339676, -0.06688497960567474], "norm_scores": [0.10256233811378479, 0.10072103887796402, 0.10066366195678711, 0.10032755136489868, 0.10032527893781662, 0.099809929728508, 0.0997544378042221, 0.09974957257509232, 0.0985158383846283, 0.09757030755281448]}}
{"id": "WebQTest-1108", "question": "what radio station is npr on in nyc", "prediction": [["broadcast.radio_station.owner"], ["broadcast.radio_station_owner.radio_stations"], ["broadcast.content.location"], ["broadcast.content.location", "broadcast.content.location"], ["media_common.netflix_title.netflix_genres"], ["broadcast.broadcast.content", "broadcast.broadcast.area_served"], ["location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["book.newspaper.circulation_areas"]], "ground_paths": [["broadcast.radio_network.affiliates", "broadcast.radio_affiliation_duration.station"], ["broadcast.producer.location", "broadcast.broadcast.area_served"], ["broadcast.producer.produces", "broadcast.producer.produces"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what radio station is npr on in nyc [/INST]", "raw_output": {"paths": ["<PATH> broadcast.radio_station.owner </PATH>", "<PATH> broadcast.radio_station_owner.radio_stations </PATH>", "<PATH> broadcast.content.location </PATH>", "<PATH> broadcast.content.location <SEP> broadcast.content.location </PATH>", "<PATH> media_common.netflix_title.netflix_genres </PATH>", "<PATH> broadcast.broadcast.content <SEP> broadcast.broadcast.area_served </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>"], "scores": [-0.018980026245117188, -0.0355442613363266, -0.044281598180532455, -0.05038360506296158, -0.053994905203580856, -0.05569988116621971, -0.06274890899658203, -0.06785620003938675, -0.07073523849248886, -0.07123545557260513], "norm_scores": [0.10346256196498871, 0.10176289826631546, 0.10087763518095016, 0.10026395320892334, 0.09990253299474716, 0.09973234683275223, 0.09903179854154587, 0.09852729737758636, 0.09824404865503311, 0.0981949120759964]}}
{"id": "WebQTest-1109", "question": "who did tom landry play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.professional_sports_team.owner_s", "sports.sports_team.previously_known_as"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom landry play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.011386407539248466, -0.012305968441069126, -0.05595039203763008, -0.0571623258292675, -0.05751587450504303, -0.061622414737939835, -0.06730443984270096, -0.070381760597229, -0.07169820368289948, -0.0934419259428978], "norm_scores": [0.10451839864253998, 0.10442233085632324, 0.09996289759874344, 0.09984182566404343, 0.0998065397143364, 0.09939751029014587, 0.098834328353405, 0.09853065758943558, 0.0984010323882103, 0.09628451615571976]}}
{"id": "WebQTest-1110", "question": "where was john jay born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["people.person.nationality"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["location.location.people_born_here", "location.location.containedby"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john jay born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> location.location.people_born_here </PATH>"], "scores": [-0.010450405068695545, -0.01615784503519535, -0.12304447591304779, -0.12340853363275528, -0.12345966696739197, -0.12882304191589355, -0.13504880666732788, -0.14093270897865295, -0.14389795064926147, -0.15039391815662384], "norm_scores": [0.11028371006250381, 0.10965606570243835, 0.09853996336460114, 0.09850410372018814, 0.09849906712770462, 0.09797218441963196, 0.09736413508653641, 0.09679292887449265, 0.09650634229183197, 0.09588146954774857]}}
{"id": "WebQTest-1111", "question": "where is mount fuji located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_range"], ["geography.mountain.mountain_type"], ["geography.mountain_range.mountains"], ["base.schemastaging.disputed_location_claimant.disputed_territories"], ["geography.lake.basin_countries"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["common.image.appears_in_topic_gallery"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount fuji located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> geography.mountain.mountain_type </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> common.image.appears_in_topic_gallery </PATH>"], "scores": [-0.011906147934496403, -0.023138927295804024, -0.050535980612039566, -0.08937650918960571, -0.09545101970434189, -0.10245469212532043, -0.10404318571090698, -0.11614181101322174, -0.12161716818809509, -0.12204639613628387], "norm_scores": [0.107359878718853, 0.10616067051887512, 0.10329167544841766, 0.09935668110847473, 0.09875496476888657, 0.09806574136018753, 0.0979100838303566, 0.09673264622688293, 0.09620444476604462, 0.09616315364837646]}}
{"id": "WebQTest-1113", "question": "where was toussaint l'ouverture born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"], ["film.film_subject.films"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by"], ["symbols.name_source.namesakes"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was toussaint l'ouverture born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> film.film_subject.films </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.0040942528285086155, -0.030270112678408623, -0.08552680909633636, -0.09468189626932144, -0.10727936029434204, -0.1088143065571785, -0.10951586067676544, -0.11646062880754471, -0.1198478564620018, -0.12124213576316833], "norm_scores": [0.10886487364768982, 0.1060522049665451, 0.10035108029842377, 0.09943655878305435, 0.0981917604804039, 0.09804115444421768, 0.09797240048646927, 0.09729436039924622, 0.09696536511182785, 0.09683025628328323]}}
{"id": "WebQTest-1114", "question": "what countries makes up the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.country.first_level_divisions"], ["location.administrative_division.first_level_division_of"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries makes up the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.01913052424788475, -0.030992582440376282, -0.03111402504146099, -0.03465018793940544, -0.045574408024549484, -0.05088486149907112, -0.05520236864686012, -0.10248465836048126, -0.11161365360021591, -0.11759689450263977], "norm_scores": [0.1041012704372406, 0.10287372022867203, 0.10286121815443039, 0.1024981364607811, 0.10138451308012009, 0.10084753483533859, 0.10041306912899017, 0.0957757979631424, 0.09490544348955154, 0.0943392962217331]}}
{"id": "WebQTest-1115", "question": "what is lawton ok zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.city.postal_codes"], ["location.location.contains"], ["location.us_county.county_seat"], ["location.Citytown.postal_codes"], ["location.us_county.hud_county_place"], ["location.citytown.postal_ codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is lawton ok zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.city.postal_codes </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "</PATH> location.citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_ codes </PATH>"], "scores": [-0.00018202113278675824, -0.09792473167181015, -0.13600967824459076, -0.14050433039665222, -0.17957471311092377, -0.18033826351165771, -0.1819235384464264, -0.18625707924365997, -0.19062088429927826, -0.1932838410139084], "norm_scores": [0.11580918729305267, 0.1050252914428711, 0.1011006236076355, 0.1006472259759903, 0.09679073095321655, 0.09671685099601746, 0.09656365215778351, 0.0961460992693901, 0.09572745114564896, 0.09547286480665207]}}
{"id": "WebQTest-1116", "question": "what is the capital city of spain wiki answers", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.capital"], ["location.administrative_division.country"], ["location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.contains"], ["location.country.first_level_divisions"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital city of spain wiki answers [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.first_level_divisions </PATH>"], "scores": [-0.006932823918759823, -0.03259918838739395, -0.040478434413671494, -0.05854814872145653, -0.08730079233646393, -0.08844440430402756, -0.08852248638868332, -0.09086088836193085, -0.09400442987680435, -0.10127998888492584], "norm_scores": [0.1063423752784729, 0.10364767909049988, 0.10283422470092773, 0.10099273175001144, 0.09813026338815689, 0.0980181023478508, 0.09801045060157776, 0.09778153151273727, 0.09747464209794998, 0.09676802903413773]}}
{"id": "WebQTest-1117", "question": "where do they speak german", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["book.newspaper.circulation_areas"], ["location.country.form_of_government"], ["film.film.featured_film_locations"], ["location.country.administrative_divisions"], ["location.country.currency_used"], ["base.rosetta.local_name.locale"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak german [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>"], "scores": [-0.017067601904273033, -0.02283741533756256, -0.02329903654754162, -0.10882455855607986, -0.15907743573188782, -0.16668164730072021, -0.1701306700706482, -0.17896625399589539, -0.179884135723114, -0.1826828271150589], "norm_scores": [0.11068391054868698, 0.1100471243262291, 0.10999634116888046, 0.10097990930080414, 0.09603077918291092, 0.09530330449342728, 0.09497515857219696, 0.0941396951675415, 0.09405333548784256, 0.09379047155380249]}}
{"id": "WebQTest-1118", "question": "when did mark mcgwire retired", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_league_draft_pick.draft"], ["sports.pro_athlete.teams", "sports.sports_league_season.teams"], ["baseball.baseball_player.lifetime_batting_statistics", "baseball.lifetime_batting_statistics.starting_season"], ["sports.sports_league_draft_pick.player", "sports.drafted_athlete.drafted"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.ending_season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did mark mcgwire retired [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_league_draft_pick.draft </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_season.teams </PATH>", "<PATH> baseball.baseball_player.lifetime_batting_statistics <SEP> baseball.lifetime_batting_statistics.starting_season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.013241853564977646, -0.015026960521936417, -0.026296410709619522, -0.04653911292552948, -0.054868828505277634, -0.07116765528917313, -0.07403545826673508, -0.0823764055967331, -0.09672248363494873, -0.09846529364585876], "norm_scores": [0.10451636463403702, 0.10432995855808258, 0.1031608134508133, 0.10109356045722961, 0.10025497525930405, 0.09863418340682983, 0.09835172444581985, 0.09753478318452835, 0.09614553302526474, 0.09597811847925186]}}
{"id": "WebQTest-1119", "question": "what is colorado technical university", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["common.topic.notable_types"], ["education.educational_institution.colors"], ["education.educational_institution.parent_institution"], ["education.educational_institution.colors", "education.educational_institution.colors"], ["education.educational_institution.subsidiary_or_constituent_schools", "education.educational_institution.subsidiary_or_constituent_schools"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], []], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is colorado technical university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> education.educational_institution.colors </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>", "<PATH> education.educational_institution.colors <SEP> education.educational_institution.colors </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> </PATH>"], "scores": [-0.025317968800663948, -0.03326015546917915, -0.03932690620422363, -0.043430145829916, -0.04620778560638428, -0.05193634331226349, -0.055112987756729126, -0.05829384922981262, -0.06279565393924713, -0.06678873300552368], "norm_scores": [0.10231133550405502, 0.10150197893381119, 0.10088805854320526, 0.10047493875026703, 0.1001962423324585, 0.099623903632164, 0.09930792450904846, 0.09899254888296127, 0.09854791313409805, 0.0981551855802536]}}
{"id": "WebQTest-1120", "question": "what was the currency used in france before the euro", "prediction": [["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"], ["location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_formerly_used"], ["location.country.currency_formerly_used", "location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used", "location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country", "government.form_of_government.countries"], ["location.country.currency_historical_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the currency used in france before the euro [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used <SEP> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used <SEP> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> government.form_of_government.countries </PATH>", "<PATH> location.country.currency_historical_used </PATH>"], "scores": [-0.005864219274371862, -0.025859666988253593, -0.048597753047943115, -0.08098464459180832, -0.11885126680135727, -0.1278669834136963, -0.13370811939239502, -0.13800591230392456, -0.1476752758026123, -0.15701241791248322], "norm_scores": [0.10955274850130081, 0.10738394409418106, 0.10496979206800461, 0.10162460803985596, 0.09784838557243347, 0.09697017073631287, 0.09640540182590485, 0.09599196165800095, 0.09506826102733612, 0.09418471902608871]}}
{"id": "WebQTest-1121", "question": "what is kentucky state slogan", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "government.government_position_held.jurisdiction_of_office", "location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is kentucky state slogan [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.01369092334061861, -0.020114243030548096, -0.023386258631944656, -0.028043916448950768, -0.033936358988285065, -0.04039788991212845, -0.0432659350335598, -0.047025371342897415, -0.047278597950935364, -0.04786963015794754], "norm_scores": [0.10209556668996811, 0.1014418676495552, 0.10111049562692642, 0.1006406620144844, 0.10004938393831253, 0.09940499812364578, 0.09912030398845673, 0.09874836355447769, 0.09872336685657501, 0.0986650288105011]}}
{"id": "WebQTest-1124", "question": "who played samantha stephens in bewitched", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program.languages", "people.person.languages"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played samantha stephens in bewitched [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> tv.tv_program.languages <SEP> people.person.languages </PATH>"], "scores": [-0.0160044115036726, -0.032517965883016586, -0.03360661491751671, -0.034110892564058304, -0.03505668789148331, -0.03962050750851631, -0.04420821741223335, -0.04727528616786003, -0.047338686883449554, -0.050456445664167404], "norm_scores": [0.10222118347883224, 0.10054700821638107, 0.10043760389089584, 0.10038696974515915, 0.10029207170009613, 0.09983540326356888, 0.09937843680381775, 0.0990741029381752, 0.099067822098732, 0.09875942766666412]}}
{"id": "WebQTest-1125", "question": "what area of science did sir isaac newton study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"], ["law.invention.inventor"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by", "people.person.profession"], ["book.written_work.subjects"], ["education.field_of_study.students_majoring"], ["influence.influence_node.influenced_by"], ["people.person.profession", "people.profession.part_of_professional_field"], ["education.education.student", "education.education.students_major_field_of_study"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what area of science did sir isaac newton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> education.field_of_study.students_majoring </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession <SEP> people.profession.part_of_professional_field </PATH>", "<PATH> education.education.student <SEP> education.education.students_major_field_of_study </PATH>"], "scores": [-0.005659246351569891, -0.03272513672709465, -0.04494546726346016, -0.06738089770078659, -0.07023048400878906, -0.07584547251462936, -0.07947556674480438, -0.08145198971033096, -0.08294198662042618, -0.08525153249502182], "norm_scores": [0.10582482814788818, 0.1029990017414093, 0.10174798220396042, 0.09949064254760742, 0.09920753538608551, 0.09865204244852066, 0.09829457849264145, 0.09810050576925278, 0.09795442968606949, 0.09772846847772598]}}
{"id": "WebQTest-1126", "question": "what county is san diego in", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.location.containedby"], ["location.us_county.county_seat"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.county"], ["location.location.contains"], ["location.citytown.postal_codes"]], "ground_paths": [["location.location.containedby"], ["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.county </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes </PATH>"], "scores": [-0.019000550732016563, -0.022794319316744804, -0.025635868310928345, -0.045067403465509415, -0.07914581894874573, -0.08515431731939316, -0.09681317955255508, -0.09990677237510681, -0.10129944980144501, -0.12453777343034744], "norm_scores": [0.1051553338766098, 0.104757159948349, 0.10445991158485413, 0.1024496853351593, 0.0990171879529953, 0.0984240174293518, 0.09728317707777023, 0.0969826802611351, 0.09684771299362183, 0.09462308138608932]}}
{"id": "WebQTest-1127", "question": "which country in north america is divided into provinces", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["location.location.contains"], ["location.administrative_division.first_level_division_of"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.biblioness.bibs_location.country"], ["location.country.administrative_divisions"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in north america is divided into provinces [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.016228262335062027, -0.02136360853910446, -0.022863563150167465, -0.07185329496860504, -0.08297150582075119, -0.0934147834777832, -0.09365474432706833, -0.10972066968679428, -0.11895883828401566, -0.12261492758989334], "norm_scores": [0.10601083934307098, 0.10546783357858658, 0.10530975461006165, 0.10027498751878738, 0.09916629642248154, 0.0981360524892807, 0.09811250865459442, 0.09654884040355682, 0.09566101431846619, 0.09531190246343613]}}
{"id": "WebQTest-1130", "question": "what is real name of santa claus", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.occupation", "people.person.profession"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["fictional_universe.character_occupation.characters_with_this_occupation", "people.person.profession"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes", "symbols.name_source.namesakes"], ["fictional_universe.fictional_character.character_created_by", "people.person.profession"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes", "symbols.namesake.named_after"], ["fictional_universe.fictional_character.character_created_by", "symbols.name_source.namesakes"]], "ground_paths": [["fictional_universe.fictional_character.based_on"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is real name of santa claus [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation <SEP> people.person.profession </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.appears_in_these_fictional_universes <SEP> symbols.name_source.namesakes </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.appears_in_these_fictional_universes <SEP> symbols.namesake.named_after </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> symbols.name_source.namesakes </PATH>"], "scores": [-0.0054016211070120335, -0.04039870575070381, -0.05320395156741142, -0.055681608617305756, -0.06368456780910492, -0.06799549609422684, -0.06882725656032562, -0.07049193233251572, -0.07278241217136383, -0.07497037947177887], "norm_scores": [0.10531012713909149, 0.10168832540512085, 0.10039447993040085, 0.10014604032039642, 0.099347785115242, 0.09892042726278305, 0.09883818030357361, 0.09867378324270248, 0.09844803810119629, 0.09823285788297653]}}
{"id": "WebQTest-1131", "question": "when did kelly slater go pro", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["sports.sports_award_winner.awards", "sports.sports_award_season.awards"], ["sports.pro_sports_played.athlete", "sports.pro_sports_played.seasons"], ["sports.pro_sports_played.athlete", "sports.pro_sports_played.season"], ["sports.sports_league_draft_pick.athlete", "sports.sports_league_draft_pick.draft"], ["sports.pro_athlete.sports_played_professionally", "sports.pro_sports_played.sport"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_league_draft_pick.league", "sports.sports_league_draft_pick.draft"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kelly slater go pro [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award_season.awards </PATH>", "<PATH> sports.pro_sports_played.athlete <SEP> sports.pro_sports_played.seasons </PATH>", "<PATH> sports.pro_sports_played.athlete <SEP> sports.pro_sports_played.season </PATH>", "<PATH> sports.sports_league_draft_pick.athlete <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.pro_athlete.sports_played_professionally <SEP> sports.pro_sports_played.sport </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_league_draft_pick.league <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.01804046705365181, -0.029723869636654854, -0.03886427357792854, -0.03986806422472, -0.042322516441345215, -0.049936648458242416, -0.05060311034321785, -0.05394785478711128, -0.056707873940467834, -0.061553534120321274], "norm_scores": [0.10263795405626297, 0.101445771753788, 0.10052274167537689, 0.10042189061641693, 0.10017570108175278, 0.0994158610701561, 0.09934961795806885, 0.09901787340641022, 0.0987449586391449, 0.09826762974262238]}}
{"id": "WebQTest-1132", "question": "what did coretta scott king died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["influence.influence_node.influenced", "people.deceased_person.cause_of_death"], ["people.person.deceased_person", "people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did coretta scott king died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_person <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0014206385239958763, -0.04457265883684158, -0.11350646615028381, -0.11957873404026031, -0.1311831772327423, -0.13279330730438232, -0.1439879834651947, -0.14684928953647614, -0.1505531221628189, -0.151938796043396], "norm_scores": [0.11174456030130386, 0.10702511668205261, 0.09989600628614426, 0.09929126501083374, 0.09814569354057312, 0.09798779338598251, 0.09689697623252869, 0.0966201201081276, 0.09626290947198868, 0.09612961113452911]}}
{"id": "WebQTest-1133", "question": "what does wh smith stand for", "prediction": [["symbols.name_source.namesakes"], ["common.topic.notable_types", "common.topic.notable_types"], ["symbols.namesake.named_after"], ["organization.organization.founders"], [], ["organization.organization_founder.organizations_founded"], ["symbols.name_source.namesakes", "symbols.namesake.named_after"], ["business.employer.employees", "business.employment_tenure.title"], ["business.business_operation.industry", "business.business_operation.industry"], ["business.business_operation.industry"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does wh smith stand for [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> symbols.name_source.namesakes <SEP> symbols.namesake.named_after </PATH>", "<PATH> business.employer.employees <SEP> business.employment_tenure.title </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> business.business_operation.industry </PATH>"], "scores": [-0.024924540892243385, -0.03991875797510147, -0.041873686015605927, -0.047374095767736435, -0.05382240191102028, -0.05837148800492287, -0.06346961855888367, -0.06893337517976761, -0.06932796537876129, -0.07121575623750687], "norm_scores": [0.10293149203062057, 0.10139963775873184, 0.10120159387588501, 0.1006464809179306, 0.0999995619058609, 0.0995456874370575, 0.0990394800901413, 0.0984998345375061, 0.09846096485853195, 0.09827527403831482]}}
{"id": "WebQTest-1135", "question": "who was vp for nixon", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["government.political_appointer.appointees", "government.politician.government_positions_held"], ["government.us_political_appointer.appointees", "government.us_politician.government_positions_held"], ["government.us_political_appointer.appointees", "government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"], ["government.us_political_appointer.appointees", "government.us_political_appointer.appointees"], ["government.us_political_appointer.appointees", "government.politician.government_positions_held"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for nixon [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> base.americancomedy.impersonated_celebrity.impersonated_by <SEP> base.americancomedy.impersonated_celebrity.impersonated_by </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.us_political_appointer.appointees <SEP> government.us_politician.government_positions_held </PATH>", "<PATH> government.us_political_appointer.appointees <SEP> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> government.us_political_appointer.appointees <SEP> government.us_political_appointer.appointees </PATH>", "<PATH> government.us_political_appointer.appointees <SEP> government.politician.government_positions_held </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.005837418604642153, -0.02258879505097866, -0.07871771603822708, -0.08855672925710678, -0.11016836017370224, -0.11396883428096771, -0.11662551760673523, -0.12060251086950302, -0.1218067929148674, -0.12670376896858215], "norm_scores": [0.10874878615140915, 0.10694226622581482, 0.10110506415367126, 0.10011516511440277, 0.0979747325181961, 0.09760308265686035, 0.09734413027763367, 0.09695775806903839, 0.09684106707572937, 0.09636799246072769]}}
{"id": "WebQTest-1136", "question": "what religion did adolf hitler follow", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"], ["religion.religion.founding_figures"], ["influence.influence_node.influenced_by", "people.person.religion"], ["organization.organization_founder.organizations_founded"], ["base.activism.activist.area_of_activism", "people.person.religion"], ["base.activism.activist.area_of_activism", "religion.religion.notable_figures"], ["base.activism.activist.area_of_activism", "base.activism.activist.religion"], ["symbols.namesake.named_after"], ["base.activism.activist.area_of_activism", "base.activism.activism_issue.activists"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion did adolf hitler follow [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.religion </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> people.person.religion </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> religion.religion.notable_figures </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.religion </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activism_issue.activists </PATH>"], "scores": [-0.0012372777564451098, -0.06009186804294586, -0.0808878168463707, -0.09227660298347473, -0.09373003244400024, -0.10290273278951645, -0.10771968960762024, -0.10854347795248032, -0.10990112274885178, -0.11638431996107101], "norm_scores": [0.10893529653549194, 0.10270898044109344, 0.10059510916471481, 0.09945594519376755, 0.09931150078773499, 0.09840471297502518, 0.09793183952569962, 0.09785119444131851, 0.09771843999624252, 0.09708696603775024]}}
{"id": "WebQTest-1137", "question": "where is the illinois river on a map", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.contains"], ["location.location.primarily_containedby"], ["geography.river.mouth"], ["location.location.containedby", "location.location.containedby"], ["geography.river.origin"], ["geography.river.cities"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the illinois river on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> geography.river.origin </PATH>", "<PATH> geography.river.cities </PATH>"], "scores": [-0.011377012357115746, -0.02655850350856781, -0.03485490381717682, -0.05143457651138306, -0.09008617699146271, -0.12027101218700409, -0.12570247054100037, -0.12624458968639374, -0.13637253642082214, -0.14508934319019318], "norm_scores": [0.10770764946937561, 0.10608483850955963, 0.10520835965871811, 0.10347841680049896, 0.0995551124215126, 0.09659495949745178, 0.0960717424750328, 0.0960196703672409, 0.09505210071802139, 0.0942271500825882]}}
{"id": "WebQTest-1138", "question": "where honduras in the world", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"], ["base.locations.continents.countries_within"], ["geography.river.basin_countries"], ["language.human_language.countries_spoken_in"], ["geography.island.island_group"], ["location.country.containedby"], ["book.written_work.subjects"], ["common.topic.notable_types"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"], ["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where honduras in the world [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> geography.island.island_group </PATH>", "<PATH> location.country.containedby </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.01689583621919155, -0.019182512536644936, -0.03856968507170677, -0.05125761777162552, -0.10432256013154984, -0.12020310014486313, -0.13625293970108032, -0.13768577575683594, -0.13904976844787598, -0.14937667548656464], "norm_scores": [0.10758151859045029, 0.10733579844236374, 0.10527490079402924, 0.10394762456417084, 0.09857544302940369, 0.09702237695455551, 0.09547761082649231, 0.09534090012311935, 0.09521094709634781, 0.09423278272151947]}}
{"id": "WebQTest-1142", "question": "who does ronaldinho play for now 2011", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ronaldinho play for now 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.02463615871965885, -0.02571471408009529, -0.028324773535132408, -0.029043203219771385, -0.030738303437829018, -0.03140799701213837, -0.03487434610724449, -0.040271658450365067, -0.044238604605197906, -0.048479363322257996], "norm_scores": [0.10091491788625717, 0.10080613195896149, 0.10054336488246918, 0.10047116130590439, 0.10030099004507065, 0.10023384541273117, 0.09988699853420258, 0.09934933483600616, 0.09895599633455276, 0.09853723645210266]}}
{"id": "WebQTest-1143", "question": "what galileo galilei was famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"], ["people.profession.people_with_this_profession"], ["symbols.name_source.namesakes"], ["fictional_universe.fictional_character.occupation"], ["law.invention.inventor"], ["law.inventor.inventions"], ["base.argumentmaps.innovator.original_ideas"], ["influence.influence_node.influenced_by"]], "ground_paths": [["astronomy.astronomical_discovery.discoverer", "astronomy.star_system_body.star_system"], ["symbols.name_source.namesakes", "astronomy.star_system_body.star_system"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["astronomy.astronomer.astronomical_objects_discovered", "astronomy.star_system_body.star_system"], ["book.written_work.author", "book.book_subject.works"], ["influence.influence_node.influenced", "base.argumentmaps.innovator.original_ideas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what galileo galilei was famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.0012825573794543743, -0.06419188529253006, -0.07657455652952194, -0.08216939866542816, -0.09668856859207153, -0.09744402021169662, -0.1105419397354126, -0.11554963141679764, -0.11727770417928696, -0.12171373516321182], "norm_scores": [0.1090315654873848, 0.10238376259803772, 0.10112379491329193, 0.10055960714817047, 0.09911011159420013, 0.09903526306152344, 0.09774657338857651, 0.0972583070397377, 0.09709038585424423, 0.09666063636541367]}}
{"id": "WebQTest-1144", "question": "what are the major trading partners of germany", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major trading partners of germany [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.statistical_region.places_imported_from </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.uncommon.topic.exceptions <SEP> base.uncommon.topic.exceptions </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.imports_and_exports.exported_to </PATH>", "<PATH> location.statistical_region.places_imported_from <SEP> location.statistical_region.places_exported_to </PATH>"], "scores": [-0.02850228361785412, -0.03206291422247887, -0.03421831876039505, -0.039108019322156906, -0.04083281382918358, -0.04108307510614395, -0.04181380197405815, -0.0424506738781929, -0.042991772294044495, -0.04389559477567673], "norm_scores": [0.10102332383394241, 0.10066425055265427, 0.10044751316308975, 0.09995755553245544, 0.09978529810905457, 0.09976032376289368, 0.09968745708465576, 0.09962398558855057, 0.09957009553909302, 0.09948014467954636]}}
{"id": "WebQTest-1145", "question": "where did romney graduated college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.parent_institution"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.religion", "religion.religion.organizations"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did romney graduated college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.017436882480978966, -0.019136596471071243, -0.024092024192214012, -0.04764807969331741, -0.0587201751768589, -0.07715332508087158, -0.08265792578458786, -0.08829610794782639, -0.08978663384914398, -0.09354718029499054], "norm_scores": [0.10428756475448608, 0.10411044955253601, 0.10359582304954529, 0.10118402540683746, 0.10006989538669586, 0.09824218600988388, 0.09770288318395615, 0.09715356677770615, 0.09700886160135269, 0.09664473682641983]}}
{"id": "WebQTest-1146", "question": "who did johnny depp play in corpse bride", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.performance.character"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nominee"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["film.performance.film", "film.performance.character"], ["film.actor.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did johnny depp play in corpse bride [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.017849868163466454, -0.024896258488297462, -0.03251646086573601, -0.033582888543605804, -0.03867976740002632, -0.05656440928578377, -0.05777724087238312, -0.05850285291671753, -0.0662413239479065, -0.06955618411302567], "norm_scores": [0.10280017554759979, 0.10207835584878922, 0.10130345821380615, 0.10119548439979553, 0.10068100690841675, 0.09889636933803558, 0.09877650439739227, 0.09870485216379166, 0.0979439839720726, 0.09761983901262283]}}
{"id": "WebQTest-1147", "question": "what is the zip code for jonesboro ar", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"], ["location.citytown.postal_ codes"], ["citytown.citytown.postal_codes"], ["location.us_county.county_seat"], ["location.mailing_address.citytown"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for jonesboro ar [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "</PATH> location.citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_ codes </PATH>", "<PATH> citytown.citytown.postal_codes </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.mailing_address.citytown </PATH>"], "scores": [-7.576466305181384e-05, -0.10537174344062805, -0.1512070894241333, -0.16797597706317902, -0.17207321524620056, -0.1775818020105362, -0.17935313284397125, -0.18640069663524628, -0.18762291967868805, -0.1882372796535492], "norm_scores": [0.11617211252450943, 0.10456164181232452, 0.09987720847129822, 0.09821633994579315, 0.09781475365161896, 0.09727741032838821, 0.09710525721311569, 0.09642330557107925, 0.09630552679300308, 0.0962463766336441]}}
{"id": "WebQTest-1148", "question": "what college did se hinton go", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["book.written_work.author", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did se hinton go [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.author <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.01765904203057289, -0.01979997009038925, -0.02309241332113743, -0.05260979011654854, -0.0615883469581604, -0.06245982274413109, -0.07642480731010437, -0.08138542622327805, -0.08404053002595901, -0.08598948270082474], "norm_scores": [0.10392610728740692, 0.10370384901762009, 0.10336297005414963, 0.1003565639257431, 0.09945952892303467, 0.09937290102243423, 0.09799480438232422, 0.09750989079475403, 0.09725133329629898, 0.09706197679042816]}}
{"id": "WebQTest-1149", "question": "when was taylor swift fearless tour", "prediction": [["music.concert_tour.artist"], ["time.event.locations"], ["music.artist.concert_tours", "time.event.locations"], ["time.event.instance_of_recurring_event"], ["time.event.included_in_event"], ["music.concert_tour.artist", "time.event.locations"], ["music.concert_tour.artist", "music.concert_tour.artist"], ["music.artist.concert_tours"], ["music.album.supporting_tours", "music.concert_tour.artist"], ["music.artist.concert_tours", "music.artist.concert_tours"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was taylor swift fearless tour [/INST]", "raw_output": {"paths": ["<PATH> music.concert_tour.artist </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> music.artist.concert_tours <SEP> time.event.locations </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> music.concert_tour.artist <SEP> time.event.locations </PATH>", "<PATH> music.concert_tour.artist <SEP> music.concert_tour.artist </PATH>", "<PATH> music.artist.concert_tours </PATH>", "<PATH> music.album.supporting_tours <SEP> music.concert_tour.artist </PATH>", "<PATH> music.artist.concert_tours <SEP> music.artist.concert_tours </PATH>"], "scores": [-0.004342695232480764, -0.04949389398097992, -0.06189030036330223, -0.0701334998011589, -0.07430325448513031, -0.07749658823013306, -0.0792156457901001, -0.08531950414180756, -0.09151557087898254, -0.09236029535531998], "norm_scores": [0.10660429298877716, 0.10189802944660187, 0.10064265131950378, 0.09981644153594971, 0.09940110146999359, 0.09908419102430344, 0.0989139974117279, 0.09831207990646362, 0.09770482033491135, 0.09762232005596161]}}
{"id": "WebQTest-1151", "question": "where did charles darwin and his wife live", "prediction": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.parents", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["book.written_work.subjects", "location.location.contains"], ["book.written_work.subjects", "book.written_work.subjects"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.parents", "people.person.place_of_birth"]], "ground_paths": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["people.person.nationality", "location.country.capital"], ["people.deceased_person.place_of_burial", "travel.travel_destination.tourist_attractions"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["book.written_work.subjects", "location.country.capital"], ["book.written_work.subjects", "location.location.contains"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.children", "people.deceased_person.place_of_death"], ["book.written_work.subjects", "location.location.containedby"], ["book.book_subject.works", "book.written_work.subjects"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin and his wife live [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.parents <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.contains </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.subjects </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.03030584566295147, -0.03441267088055611, -0.03447578474879265, -0.03679997846484184, -0.038028616458177567, -0.038818202912807465, -0.040323562920093536, -0.04120027273893356, -0.05837445333600044, -0.06226683780550957], "norm_scores": [0.10112081468105316, 0.10070637613534927, 0.1007000207901001, 0.10046624392271042, 0.10034288465976715, 0.10026368498802185, 0.10011286288499832, 0.10002513974905014, 0.09832195937633514, 0.09793999046087265]}}
{"id": "WebQTest-1152", "question": "who is the voice of kitt", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.performance.special_performance_type", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"]], "ground_paths": [["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of kitt [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_guest_role.character <SEP> tv.tv_guest_role.actor </PATH>"], "scores": [-0.032052744179964066, -0.03235402703285217, -0.040186021476984024, -0.04156876355409622, -0.04254061356186867, -0.04300782456994057, -0.04456014931201935, -0.04474861919879913, -0.04531073197722435, -0.048209115862846375], "norm_scores": [0.10094324499368668, 0.10091283917427063, 0.10012556612491608, 0.0999872237443924, 0.0998900979757309, 0.09984343498945236, 0.0996885746717453, 0.09966978430747986, 0.09961377084255219, 0.09932547062635422]}}
{"id": "WebQTest-1153", "question": "what is eminem 's real full name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.nationality"], ["music.artist.genre", "music.artist.genre"], ["music.artist.album", "music.composer.compositions"], ["music.artist.album"], ["music.artist.label", "music.artist.label"], ["music.recording.artist", "music.composer.compositions"], ["music.artist.album", "music.album.artist"], ["music.artist.origin"], ["music.artist.album", "music.composition.composer"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is eminem 's real full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.album <SEP> music.composer.compositions </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>", "<PATH> music.recording.artist <SEP> music.composer.compositions </PATH>", "<PATH> music.artist.album <SEP> music.album.artist </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> music.artist.album <SEP> music.composition.composer </PATH>"], "scores": [-0.0338619202375412, -0.03976338729262352, -0.04143897071480751, -0.04213494062423706, -0.04720860719680786, -0.04948286712169647, -0.052878186106681824, -0.05517398938536644, -0.05517775937914848, -0.057927995920181274], "norm_scores": [0.10137075185775757, 0.10077427327632904, 0.1006055623292923, 0.1005355641245842, 0.10002677142620087, 0.09979954361915588, 0.09946126490831375, 0.09923318773508072, 0.09923281520605087, 0.0989602729678154]}}
{"id": "WebQTest-1154", "question": "who is amy grant 's first husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.children", "people.person.parents"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.parents", "people.person.children"], ["people.person.parents", "people.person.parents"], ["music.artist.origin", "music.artist.origin"], ["music.artist.genre", "music.artist.genre"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is amy grant 's first husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.025593874976038933, -0.03373536467552185, -0.03375525027513504, -0.038323868066072464, -0.04128415137529373, -0.0429438054561615, -0.04349370300769806, -0.047246504575014114, -0.04906947910785675, -0.05648194998502731], "norm_scores": [0.1015685498714447, 0.10074498504400253, 0.10074298083782196, 0.10028377920389175, 0.09998734295368195, 0.0998215302824974, 0.09976665675640106, 0.09939295798540115, 0.0992119312286377, 0.0984792485833168]}}
{"id": "WebQTest-1155", "question": "what are the seven nations of central america", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["book.written_work.subjects"], ["location.country.form_of_government"], ["book.book_subject.works"], ["book.book_subject.works", "book.written_work.subjects"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the seven nations of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.book_subject.works <SEP> book.written_work.subjects </PATH>"], "scores": [-0.009074516594409943, -0.02583235129714012, -0.043034981936216354, -0.09774386882781982, -0.09877414256334305, -0.1043618842959404, -0.11211767792701721, -0.11476865410804749, -0.11482932418584824, -0.11634411662817001], "norm_scores": [0.10766379535198212, 0.10587461292743683, 0.10406886786222458, 0.09852831065654755, 0.09842686355113983, 0.09787841141223907, 0.09712221473455429, 0.09686509519815445, 0.09685922414064407, 0.09671259671449661]}}
{"id": "WebQTest-1156", "question": "what is jamie spears baby name", "prediction": [["people.person.children"], ["people.person.parents"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["people.person.parent"], ["common.topic.notable_types"], ["base.popstra.celebrity.canoodled_person", "base.popstra.celebrity.canoodled_person"], ["people.person.child"], ["base.popstra.celebrity.canoodled_by", "base.popstra.celebrity.canoodled_by"], ["base.popstra.celebrity.parent", "base.popstra.celebrity.parents"], ["base.popstra.celebrity.parents", "base.popstra.celebrity.parents"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie spears baby name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.celebrity.canoodled </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.popstra.celebrity.canoodled_person <SEP> base.popstra.celebrity.canoodled_person </PATH>", "<PATH> people.person.child </PATH>", "<PATH> base.popstra.celebrity.canoodled_by <SEP> base.popstra.celebrity.canoodled_by </PATH>", "<PATH> base.popstra.celebrity.parent <SEP> base.popstra.celebrity.parents </PATH>", "<PATH> base.popstra.celebrity.parents <SEP> base.popstra.celebrity.parents </PATH>"], "scores": [-0.008983097039163113, -0.02044692263007164, -0.12932640314102173, -0.13307788968086243, -0.13412201404571533, -0.13603389263153076, -0.14180125296115875, -0.1425643265247345, -0.1450197696685791, -0.15163980424404144], "norm_scores": [0.11096291244029999, 0.10969811677932739, 0.09838149696588516, 0.09801310300827026, 0.0979108214378357, 0.09772380441427231, 0.09716182202100754, 0.09708771109580994, 0.09684960544109344, 0.09621057659387589]}}
{"id": "WebQTest-1160", "question": "who is the mother of prince michael jackson", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"], ["music.artist.genre", "music.artist.genre"], ["music.artist.origin", "people.person.place_of_birth"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession"], ["people.person.ethnicity"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["music.artist.origin"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the mother of prince michael jackson [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.origin <SEP> people.person.place_of_birth </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.007474484387785196, -0.032260432839393616, -0.04039706662297249, -0.0859743282198906, -0.10388113558292389, -0.11104931682348251, -0.11707382649183273, -0.12806659936904907, -0.13332241773605347, -0.13411113619804382], "norm_scores": [0.10842777788639069, 0.10577332973480225, 0.10491617769002914, 0.10024172812700272, 0.09846269339323044, 0.09775941073894501, 0.09717223048210144, 0.09610988199710846, 0.09560607373714447, 0.09553069621324539]}}
{"id": "WebQTest-1161", "question": "what is the clemson mascot name", "prediction": [["education.school_mascot.school"], ["education.educational_institution.mascot"], [], ["sports.school_mascot.school"], ["common.topic.notable_types", "common.topic.notable_types"], ["sports.mascot.team"], ["sports.sports_team.team_mascot"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["education.educational_institution.mascot", "education.school_mascot.school"], ["education.educational_institution.mascot", "education.educational_institution.mascot"]], "ground_paths": [["education.school_mascot.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the clemson mascot name [/INST]", "raw_output": {"paths": ["<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.mascot </PATH>", "<PATH> </PATH>", "<PATH> sports.school_mascot.school </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> sports.mascot.team </PATH>", "<PATH> sports.sports_team.team_mascot </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> education.educational_institution.mascot <SEP> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.mascot <SEP> education.educational_institution.mascot </PATH>"], "scores": [-0.012200996279716492, -0.01447519101202488, -0.06718660891056061, -0.08024567365646362, -0.08093038201332092, -0.08687819540500641, -0.10715828835964203, -0.10940443724393845, -0.11431525647640228, -0.1143762469291687], "norm_scores": [0.10680738091468811, 0.10656476020812988, 0.10109306126832962, 0.09978145360946655, 0.09971316158771515, 0.09912183880805969, 0.0971318930387497, 0.09691396355628967, 0.09643919765949249, 0.09643331915140152]}}
{"id": "WebQTest-1163", "question": "what rainforest is in south america", "prediction": [["location.location.containedby"], ["common.topic.notable_types"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.contains"], ["geography.river.basin_countries"], ["book.written_work.subjects"], ["location.location.primarily_containedby"], ["common.image.appears_in_topic_gallery"], ["film.film_location.featured_in_films"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rainforest is in south america [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> common.image.appears_in_topic_gallery </PATH>", "<PATH> film.film_location.featured_in_films </PATH>"], "scores": [-0.0036168124061077833, -0.039948005229234695, -0.062084365636110306, -0.08492736518383026, -0.09046411514282227, -0.11312974244356155, -0.13313443958759308, -0.13526909053325653, -0.1380474865436554, -0.13926734030246735], "norm_scores": [0.10934875905513763, 0.10544729232788086, 0.10313872247934341, 0.10080942511558533, 0.10025281459093094, 0.09800607711076736, 0.0960649773478508, 0.09586013108491898, 0.09559416025876999, 0.0954776257276535]}}
{"id": "WebQTest-1164", "question": "what type of art does claude monet do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"], ["visual_art.artist.associated_periods_or_movements"], ["visual_art.visual_art_form.artists"], ["visual_art.visual_artists.associated_periods_or_movements"], ["film.film_genre.films_in_this_genre"], ["visual_art.art_period_movement.associated_artist"], ["influence.influence_node.influenced_by"], ["art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does claude monet do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.visual_artists.associated_periods_or_movements </PATH>", "<PATH> film.film_genre.films_in_this_genre </PATH>", "<PATH> visual_art.art_period_movement.associated_artist </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> art.art_period_movement.associated_artists </PATH>"], "scores": [-0.005995842162519693, -0.026486027985811234, -0.0358395017683506, -0.08355581015348434, -0.10651768743991852, -0.11649113893508911, -0.12109571695327759, -0.12370730936527252, -0.12636129558086395, -0.12758278846740723], "norm_scores": [0.1083683893084526, 0.10617049038410187, 0.10518205910921097, 0.10028102248907089, 0.09800461679697037, 0.09703203290700912, 0.09658627212047577, 0.09633435308933258, 0.096079021692276, 0.09596173465251923]}}
{"id": "WebQTest-1166", "question": "when did we start war with iraq", "prediction": [["base.culturalevent.event.entity_involved"], ["time.event.locations"], ["time.event.includes_event"], ["time.event.included_in_event"], ["military.military_combatant_group.conflict", "military.military_combatant_group.conflict"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.conflict"], ["location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did we start war with iraq [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.conflict </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> military.military_combatant.military_conflicts <SEP> military.military_combatant_group.conflict </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.025243567302823067, -0.02777145616710186, -0.03233303129673004, -0.037644483149051666, -0.03953330218791962, -0.05439036339521408, -0.05889322981238365, -0.06398753821849823, -0.06767711043357849, -0.06823985278606415], "norm_scores": [0.1022447943687439, 0.10198666155338287, 0.10152250528335571, 0.10098469257354736, 0.10079412907361984, 0.09930769354104996, 0.09886153787374496, 0.09835918247699738, 0.09799695014953613, 0.0979418158531189]}}
{"id": "WebQTest-1168", "question": "what guitar did kurt cobain", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"], ["music.musical_group.member", "music.group_membership.group"], ["music.artist.guitars_played"], ["music.musical_group.member", "music.group_membership.role"], ["music.group_membership.group", "music.group_membership.role"], ["common.topic.image", "music.group_member.instruments_played"], ["music.producer.releases_produced"]], "ground_paths": [["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did kurt cobain [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.artist.guitars_played </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> common.topic.image <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.producer.releases_produced </PATH>"], "scores": [-0.012329488061368465, -0.03187481686472893, -0.03682854771614075, -0.04616466164588928, -0.06633929908275604, -0.07597766816616058, -0.0807289108633995, -0.09588976204395294, -0.09675732254981995, -0.10032402724027634], "norm_scores": [0.10529131442308426, 0.10325334221124649, 0.10274311900138855, 0.10178836435079575, 0.09975539147853851, 0.09879854321479797, 0.09833022952079773, 0.09685070067644119, 0.09676671773195267, 0.09642219543457031]}}
{"id": "WebQTest-1169", "question": "who did jennifer lawrence play in x men", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["award.award_nomination.award_nominee", "award.award_nomination.character"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.film.starring", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jennifer lawrence play in x men [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.character </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.honored_for </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.02128473110496998, -0.027952926233410835, -0.030635809525847435, -0.03501083701848984, -0.03995180130004883, -0.04311292618513107, -0.044814445078372955, -0.05072692036628723, -0.05511118471622467, -0.0566246323287487], "norm_scores": [0.1019359901547432, 0.1012585237622261, 0.10098722577095032, 0.10054636746644974, 0.10005079209804535, 0.09973502159118652, 0.09956546127796173, 0.09897851943969727, 0.09854552149772644, 0.09839649498462677]}}
{"id": "WebQTest-1170", "question": "who is the governor of virginia 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["base.firsts.first_achievement.achievement", "base.firsts.first_achievement_category.firsts", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of virginia 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.appointed_by </PATH>", "<PATH> base.firsts.first_achievement.achievement <SEP> base.firsts.first_achievement_category.firsts <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.appointed_by </PATH>"], "scores": [-0.017289184033870697, -0.01901116408407688, -0.024843288585543633, -0.026368873193860054, -0.029150547459721565, -0.03964822739362717, -0.04931579902768135, -0.05312245339155197, -0.05605131387710571, -0.058249037712812424], "norm_scores": [0.10201019048690796, 0.10183468461036682, 0.10124249011278152, 0.10108816623687744, 0.10080736130475998, 0.09975464642047882, 0.09879491478204727, 0.09841954708099365, 0.09813171625137329, 0.09791628271341324]}}
{"id": "WebQTest-1171", "question": "who did darth vaders voice", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.special_performance_type", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.022656511515378952, -0.02324344962835312, -0.03629082441329956, -0.03828734904527664, -0.0386858806014061, -0.047038089483976364, -0.053006622940301895, -0.05729776620864868, -0.06617239862680435, -0.06904558837413788], "norm_scores": [0.10226497054100037, 0.10220497101545334, 0.10088012367486954, 0.10067890584468842, 0.10063879191875458, 0.09980173408985138, 0.09920784831047058, 0.09878303855657578, 0.09791025519371033, 0.09762934595346451]}}
{"id": "WebQTest-1173", "question": "where is walton county ga", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.hud_county_place.county"], ["location.location.contains"], ["location.administrative_division.first_level_division_of"], ["geography.river.basin_countries"], ["location.county.county_seat"], ["location.location.containedby", "location.location.containedby"], ["location.us_county.geographic_scope"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton county ga [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.county.county_seat </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.us_county.geographic_scope </PATH>"], "scores": [-0.008914444595575333, -0.031891752034425735, -0.04584468901157379, -0.051536526530981064, -0.06559615582227707, -0.08787158876657486, -0.1343936175107956, -0.14097793400287628, -0.14644022285938263, -0.14788292348384857], "norm_scores": [0.1078934594988823, 0.10544262826442719, 0.10398160666227341, 0.10339144617319107, 0.10194797068834305, 0.0997021347284317, 0.0951700285077095, 0.09454546868801117, 0.09403043985366821, 0.09389487653970718]}}
{"id": "WebQTest-1174", "question": "what cancer did jackie kennedy die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.parent_disease"], ["film.film_subject.films", "film.film_subject.films"], ["people.cause_of_death.parent_cause_of_death"], ["film.film.subjects", "film.film_subject.films"], ["people.person.cause_of_death"], ["medicine.cancer_patient.cancer_parent"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did jackie kennedy die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.parent_disease </PATH>", "<PATH> film.film_subject.films <SEP> film.film_subject.films </PATH>", "<PATH> people.cause_of_death.parent_cause_of_death </PATH>", "<PATH> film.film.subjects <SEP> film.film_subject.films </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> medicine.cancer_patient.cancer_parent </PATH>"], "scores": [-0.014282822608947754, -0.014577575959265232, -0.035642385482788086, -0.044517725706100464, -0.10646405816078186, -0.11487675458192825, -0.11510253697633743, -0.11627810448408127, -0.12179271131753922, -0.133323535323143], "norm_scores": [0.10686045140028, 0.1068289652466774, 0.10460216552019119, 0.1036778911948204, 0.09745031595230103, 0.09663393348455429, 0.0966121107339859, 0.09649860113859177, 0.09596791863441467, 0.09486768394708633]}}
{"id": "WebQTest-1175", "question": "what does lauren conrad major in", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.degree"], ["education.education.student", "education.education.degree"], ["education.education.student", "education.education.field_of_study"], ["people.person.education", "education.education.field_of_study"], ["education.education.student", "education.education.degree_field_of_study"], ["people.person.education", "education.field_of_study.students_majoring"], ["people.person.education", "education.field_of_study.students_major_field"], ["education.education.student", "education.field_of_study.students_majoring"]], "ground_paths": [["people.person.education", "education.field_of_study.students_majoring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does lauren conrad major in [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> education.education.student <SEP> education.education.field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.degree_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.field_of_study.students_majoring </PATH>", "<PATH> people.person.education <SEP> education.field_of_study.students_major_field </PATH>", "<PATH> education.education.student <SEP> education.field_of_study.students_majoring </PATH>"], "scores": [-0.008970738388597965, -0.021172644570469856, -0.045240312814712524, -0.04531991481781006, -0.05303458496928215, -0.05681268125772476, -0.06416670978069305, -0.07075673341751099, -0.07245081663131714, -0.07534120976924896], "norm_scores": [0.10430363565683365, 0.10303866118192673, 0.1005883738398552, 0.1005803644657135, 0.09980740398168564, 0.09943103045225143, 0.09870249032974243, 0.0980541780591011, 0.09788820892572403, 0.09760568290948868]}}
{"id": "WebQTest-1176", "question": "what time zone is pennsylvania in right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.time_Zones"], ["locations.location.time_zones"], ["common.topic.notable_types"], ["time.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is pennsylvania in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> time.location.time_zones </PATH>"], "scores": [-1.6276911992463283e-05, -0.12361985445022583, -0.17174272239208221, -0.17782245576381683, -0.18275776505470276, -0.18645597994327545, -0.20484575629234314, -0.20763380825519562, -0.21108710765838623, -0.21432355046272278], "norm_scores": [0.11806295067071915, 0.10433577746152878, 0.09943374246358871, 0.09883104264736176, 0.09834448248147964, 0.09798146784305573, 0.09619607031345367, 0.09592824429273605, 0.09559755027294159, 0.09528864920139313]}}
{"id": "WebQTest-1177", "question": "where did mario lopez go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["film.actor.film", "film.performance.school"], ["film.film.education", "education.education.institution"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mario lopez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> film.actor.film <SEP> film.performance.school </PATH>", "<PATH> film.film.education <SEP> education.education.institution </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.012538527138531208, -0.024186639115214348, -0.02581454999744892, -0.03317849710583687, -0.0467136949300766, -0.07434757798910141, -0.13518531620502472, -0.1382683664560318, -0.1397756189107895, -0.14028744399547577], "norm_scores": [0.10651560127735138, 0.10528209060430527, 0.10511084645986557, 0.10433965176343918, 0.10293691605329514, 0.1001313105225563, 0.09422114491462708, 0.09393110871315002, 0.09378964453935623, 0.09374164044857025]}}
{"id": "WebQTest-1178", "question": "where is tupac from originally", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["music.artist.origin"], ["people.ethnicity.people"], ["people.ethnicity.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.ethnicity"], ["people.ethnicity.people", "people.ethnicity.people"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tupac from originally [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.ethnicity.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people <SEP> people.ethnicity.people </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.004620163235813379, -0.03308533877134323, -0.06985034793615341, -0.093529611825943, -0.11217149347066879, -0.11749669909477234, -0.11901387572288513, -0.13143809139728546, -0.14712585508823395, -0.14754609763622284], "norm_scores": [0.1096273809671402, 0.10655080527067184, 0.10270460695028305, 0.10030120611190796, 0.0984487235546112, 0.09792585670948029, 0.09777740389108658, 0.09657009690999985, 0.09506696462631226, 0.09502701461315155]}}
{"id": "WebQTest-1179", "question": "what role did george lucas play in star wars", "prediction": [["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.writer.film"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.occupation"], ["film.film.produced_by"], ["film.producer.films_executive_produced"], ["film.film.directed_by"], ["film.film_story_contributor.film_story_credits"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film.written_by"], ["film.film.executive_produced_by"]], "ground_paths": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did george lucas play in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.writer.film </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.occupation </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.producer.films_executive_produced </PATH>", "<PATH> film.film.directed_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film.written_by </PATH>", "<PATH> film.film.executive_produced_by </PATH>"], "scores": [-0.029656579717993736, -0.038571055978536606, -0.04027603566646576, -0.04270358756184578, -0.04319227486848831, -0.045412126928567886, -0.047256290912628174, -0.05252738669514656, -0.05444076284766197, -0.05457132309675217], "norm_scores": [0.10152924060821533, 0.10062818229198456, 0.10045675933361053, 0.1002131849527359, 0.10016422718763351, 0.09994212538003922, 0.09975797683000565, 0.09923353791236877, 0.0990438461303711, 0.09903090447187424]}}
{"id": "WebQTest-1180", "question": "what sarah dessen books are movies", "prediction": [["film.film_story_contributor.film_story_credits"], ["book.book_edition.author_editor"], ["book.author.works_written"], ["film.film.story_by"], ["book.written_work.author"], ["film.writer.film"], ["book.author.book_editions_published"], ["media_common.adapted_work.adaptations"], ["film.producer.films_executive_produced"], ["film.author.works_written"]], "ground_paths": [["film.film.story_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sarah dessen books are movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> film.writer.film </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> media_common.adapted_work.adaptations </PATH>", "<PATH> film.producer.films_executive_produced </PATH>", "<PATH> film.author.works_written </PATH>"], "scores": [-0.02515670657157898, -0.0313325896859169, -0.03277745842933655, -0.03385688364505768, -0.03519649803638458, -0.07558644562959671, -0.07705867290496826, -0.09293908625841141, -0.10255945473909378, -0.1054217517375946], "norm_scores": [0.10361947864294052, 0.10298150777816772, 0.10283282399177551, 0.10272188484668732, 0.1025843694806099, 0.09852354973554611, 0.09837860614061356, 0.09682866185903549, 0.09590159356594086, 0.09562748670578003]}}
{"id": "WebQTest-1181", "question": "what is the current time in kauai hawaii", "prediction": [["location.location.time_zones"], ["geography.island.island_group", "location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby", "location.location.time_zones"], ["location.location.time_zones", "location.location.time_zones"], ["base.coinsdaily.coin_type.country", "location.location.time_zones"], ["base.locations.countries.hud_county_place", "location.location.time_zones"], ["geography.island.island_group", "location.location.containedby"], ["location.location.containedby"], ["base.locations.countries.hud_county_place"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in kauai hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> geography.island.island_group <SEP> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.location.time_zones </PATH>", "<PATH> base.locations.countries.hud_county_place <SEP> location.location.time_zones </PATH>", "<PATH> geography.island.island_group <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.hud_county_place </PATH>"], "scores": [-0.00023672558018006384, -0.082008495926857, -0.0934981033205986, -0.09425047039985657, -0.0947880670428276, -0.10570777952671051, -0.1199246495962143, -0.12664656341075897, -0.12669816613197327, -0.12927770614624023], "norm_scores": [0.11012019962072372, 0.10147380828857422, 0.10031459480524063, 0.10023913532495499, 0.10018526762723923, 0.09909722954034805, 0.09769833832979202, 0.09704381972551346, 0.09703881293535233, 0.09678882360458374]}}
{"id": "WebQTest-1183", "question": "where was farrah fawcett buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.person.place_of_burial"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.place_of_interment", "people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here", "people.place_of_interment.location"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was farrah fawcett buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_burial </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_burial <SEP> people.deceased_person.place_of_burial </PATH>", "<PATH> people.person.place_of_interment <SEP> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here <SEP> people.place_of_interment.location </PATH>"], "scores": [-0.002498077927157283, -0.03852180764079094, -0.06855124235153198, -0.09626578539609909, -0.10803402960300446, -0.10805202275514603, -0.10936293751001358, -0.12764163315296173, -0.12888772785663605, -0.14293472468852997], "norm_scores": [0.1093837097287178, 0.10551343858242035, 0.10239202529191971, 0.09959324449300766, 0.09842807054519653, 0.09842629730701447, 0.0982973501086235, 0.0965169295668602, 0.09639673680067062, 0.09505212306976318]}}
{"id": "WebQTest-1185", "question": "what inspired steinbeck", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["influence.influence_node.influenced_by_this_influence_node"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired steinbeck [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <PATH>", "<PATH> influence.influence_node.influenced_by <PATH>", "<PATH> influence.influence_node.influenced_by_this_influence_node </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.010258939117193222, -0.014838872477412224, -0.11977679282426834, -0.11996233463287354, -0.1672661155462265, -0.16812346875667572, -0.17638109624385834, -0.1871935874223709, -0.18772053718566895, -0.18893633782863617], "norm_scores": [0.11293046921491623, 0.11241442710161209, 0.10121575742959976, 0.10119698196649551, 0.09652143716812134, 0.09643872082233429, 0.09564564377069473, 0.09461704641580582, 0.094567209482193, 0.09445229917764664]}}
{"id": "WebQTest-1186", "question": "what kind of government does cuba have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["people.person.nationality", "government.form_of_government.countries"], ["base.oneparty.political_party.politics_in_this_party", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does cuba have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.oneparty.political_party.politics_in_this_party <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> location.country.form_of_government <PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.0118344034999609, -0.011911427602171898, -0.14772929251194, -0.16115011274814606, -0.1638622134923935, -0.16437797248363495, -0.17726410925388336, -0.17734454572200775, -0.17850135266780853, -0.1789785772562027], "norm_scores": [0.113132543861866, 0.11312384158372879, 0.0987572893500328, 0.09744074195623398, 0.09717682749032974, 0.09712671488523483, 0.09588316828012466, 0.09587544202804565, 0.09576459974050522, 0.09571891278028488]}}
{"id": "WebQTest-1187", "question": "where are english speakers distributed", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"], ["people.person.languages"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["people.ethnicity.languages_spoken", "people.ethnicity.languages_spoken"], ["people.person.nationality", "people.person.place_of_birth"]], "ground_paths": [["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are english speakers distributed [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.languages </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.nationality <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.013936190865933895, -0.024715779349207878, -0.031198028475046158, -0.05536431819200516, -0.07920821756124496, -0.09557338058948517, -0.12097835540771484, -0.1324360966682434, -0.13550357520580292, -0.1373268961906433], "norm_scores": [0.10699545592069626, 0.1058482825756073, 0.10516436398029327, 0.1026533991098404, 0.10023467987775803, 0.09860768169164658, 0.09613411128520966, 0.09503892064094543, 0.09474783390760422, 0.09457523375749588]}}
{"id": "WebQTest-1189", "question": "what is the capital of italy today", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"], ["location.location.containedby"], ["location.country.first_level_divisions"], ["base.aareas.schema.administrative_area.capital"], ["location.country.languages_spoken"], ["location.administrative_division.country"], ["location.country.form_of_government"], ["symbols.namesake.named_after"], ["location.administrative_division.first_level_division_of"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of italy today [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.013332568109035492, -0.026645228266716003, -0.02713133580982685, -0.0774172693490982, -0.07966024428606033, -0.10549356043338776, -0.11351469159126282, -0.11984206736087799, -0.12633168697357178, -0.13472329080104828], "norm_scores": [0.10705196112394333, 0.10563625395298004, 0.10558491945266724, 0.10040677338838577, 0.10018181055784225, 0.0976269319653511, 0.09684697538614273, 0.09623612463474274, 0.09561361372470856, 0.09481462836265564]}}
{"id": "WebQTest-1191", "question": "what language do people speak in the bahamas", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["location.country.language_spoken"], ["location.country.currency_used"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.language_spoken </PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken <PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.004798391368240118, -0.033070970326662064, -0.04255181550979614, -0.07623576372861862, -0.16086828708648682, -0.1623474657535553, -0.17092561721801758, -0.18422149121761322, -0.18544533848762512, -0.18698860704898834], "norm_scores": [0.11202315241098404, 0.10890031605958939, 0.10787273198366165, 0.10429966449737549, 0.09583573788404465, 0.09569407999515533, 0.0948767140507698, 0.09362360090017319, 0.09350908547639847, 0.0933648869395256]}}
{"id": "WebQTest-1192", "question": "where did george michael go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["music.artist.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["music.artist.origin", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["music.artist.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george michael go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> music.artist.education <SEP> education.education.institution </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> music.artist.origin <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> music.artist.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.016146009787917137, -0.018843643367290497, -0.025391960516572, -0.030403003096580505, -0.05322035402059555, -0.11040741205215454, -0.11184187233448029, -0.11510279029607773, -0.1213947981595993, -0.12333682924509048], "norm_scores": [0.10570184141397476, 0.10541708022356033, 0.10472902655601501, 0.10420554876327515, 0.10185477882623672, 0.09619341790676117, 0.0960555300116539, 0.09574280679225922, 0.09514228999614716, 0.09495770186185837]}}
{"id": "WebQTest-1194", "question": "what club does cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["soccer.football_player_transfer.player", "soccer.football_player_transfer.purchasing_team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what club does cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player_transfer.player <SEP> soccer.football_player_transfer.purchasing_team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.020347747951745987, -0.02176196500658989, -0.026875538751482964, -0.030384033918380737, -0.03320598229765892, -0.03654443100094795, -0.03701968118548393, -0.03845357149839401, -0.04078519344329834, -0.04435499384999275], "norm_scores": [0.10126762837171555, 0.10112452507019043, 0.10060872882604599, 0.10025636851787567, 0.09997384995222092, 0.09964063763618469, 0.09959329664707184, 0.09945058822631836, 0.09921898692846298, 0.09886541962623596]}}
{"id": "WebQTest-1195", "question": "what time zone is in the bahamas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["locations.location.time_zones"], ["location.location.time_Zones"], ["base.locations.countries.continent"], ["location.location.time_zonessymbol"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.time_zonessymbol </PATH>"], "scores": [-8.129438356263563e-06, -0.15861135721206665, -0.18566429615020752, -0.18757915496826172, -0.1931254267692566, -0.20352976024150848, -0.2071797400712967, -0.21051953732967377, -0.21539287269115448, -0.2322748452425003], "norm_scores": [0.11940129846334457, 0.10188928991556168, 0.09916984289884567, 0.0989801287651062, 0.09843267500400543, 0.09741386026144028, 0.09705894440412521, 0.09673532843589783, 0.09626505523920059, 0.09465355426073074]}}
{"id": "WebQTest-1197", "question": "what language do people speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spokencommon"], ["location.country.languages_spokenlanguage"], ["location. country.languages_spoken"], ["location.country.languages_spokenfilm"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken <PATH>", "<PATH> location.country.languages_spokencommon </PATH>", "<PATH> location.country.languages_spokenlanguage </PATH>", "<PATH> location. country.languages_spoken </PATH>", "<PATH> location.country.languages_spokenfilm </PATH>"], "scores": [-0.0005907337181270123, -0.07430508732795715, -0.0841003879904747, -0.10277122259140015, -0.18730726838111877, -0.1878737211227417, -0.22853974997997284, -0.23009291291236877, -0.2352972775697708, -0.24428676068782806], "norm_scores": [0.11660180240869522, 0.10831572860479355, 0.10725992172956467, 0.10527587682008743, 0.09674205631017685, 0.09668727219104767, 0.09283425658941269, 0.09269018471240997, 0.09220903366804123, 0.09138384461402893]}}
{"id": "WebQTest-1198", "question": "who influenced samuel taylor coleridge", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["people.person.gender", "people.person.gender"], ["people.person.nationality"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced samuel taylor coleridge [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <PATH>"], "scores": [-0.00695195235311985, -0.01870013028383255, -0.09031262993812561, -0.09449168294668198, -0.1020008847117424, -0.1286296546459198, -0.13036789000034332, -0.13346970081329346, -0.13695284724235535, -0.16585947573184967], "norm_scores": [0.10970314592123032, 0.10842187702655792, 0.10092900693416595, 0.10050809383392334, 0.09975618869066238, 0.09713485836982727, 0.09696616232395172, 0.09666585177183151, 0.09632974863052368, 0.09358503669500351]}}
{"id": "WebQTest-1199", "question": "what does kim kardashian work as", "prediction": [["people.person.profession"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"], ["people.person.profession", "people.profession.specializations"], ["freebase.valuenotation.is_reviewed", "rdf-schema#range"], ["business.employment_tenure.person", "business.employment_tenure.title"], ["common.topic.notable_types", "type.type.expected_by"], ["freebase.valuenotation.is_reviewed", "type.property.schema"], ["common.topic.notable_types", "type.type.properties"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does kim kardashian work as [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> rdf-schema#range </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.title </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> type.property.schema </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.properties </PATH>"], "scores": [-0.0009594217990525067, -0.07522324472665787, -0.08749862015247345, -0.08889546245336533, -0.0898962989449501, -0.09148713201284409, -0.09375584125518799, -0.0950009748339653, -0.09629303216934204, -0.09657718986272812], "norm_scores": [0.10835171490907669, 0.10059662908315659, 0.09936931729316711, 0.09923060983419418, 0.09913134574890137, 0.09897377341985703, 0.09874948114156723, 0.09862660616636276, 0.09849925339221954, 0.0984712690114975]}}
{"id": "WebQTest-1200", "question": "where did the mississippi river end", "prediction": [["geography.river.mouth"], ["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.mouth"], ["location.location.partially_contains"], ["geography.river.basin_countries"], ["geography.body_of_water.mouth"], ["location.location.containedby", "location.location.containedby"], ["geography.lake.basin_countries"], ["geography.river.origin"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the mississippi river end [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.mouth </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> geography.body_of_water.mouth </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> geography.river.origin </PATH>"], "scores": [-0.0014141022693365812, -0.06065423786640167, -0.08994537591934204, -0.09839893132448196, -0.11402224749326706, -0.12156683951616287, -0.12169372290372849, -0.1301553100347519, -0.13322491943836212, -0.13607974350452423], "norm_scores": [0.11035092175006866, 0.10400358587503433, 0.10100138932466507, 0.1001511663198471, 0.09859863668680191, 0.09785754233598709, 0.09784512221813202, 0.0970207005739212, 0.09672334045171738, 0.09644760191440582]}}
{"id": "WebQTest-1201", "question": "who played carlton lassiter on psych", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_program_creator.programs_created"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carlton lassiter on psych [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>"], "scores": [-0.01996762305498123, -0.0265556238591671, -0.03144204616546631, -0.039396971464157104, -0.04248170927166939, -0.04508601874113083, -0.04562978446483612, -0.04807054251432419, -0.05276781693100929, -0.05786623805761337], "norm_scores": [0.1021115630865097, 0.10144105553627014, 0.1009465903043747, 0.10014675557613373, 0.09983830153942108, 0.09957863390445709, 0.09952449798583984, 0.09928187727928162, 0.09881661832332611, 0.09831409156322479]}}
{"id": "WebQTest-1203", "question": "who is president of israel", "prediction": [["people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is president of israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.01911536417901516, -0.02006405219435692, -0.024952083826065063, -0.03734155371785164, -0.039183855056762695, -0.04455837979912758, -0.05041379854083061, -0.05637366697192192, -0.05884562060236931, -0.05912773683667183], "norm_scores": [0.10220120847225189, 0.1021042987704277, 0.10160642862319946, 0.10035534203052521, 0.10017062723636627, 0.09963369369506836, 0.09905201196670532, 0.09846342355012894, 0.0982203334569931, 0.09819262474775314]}}
{"id": "WebQTest-1204", "question": "where did the anasazi indians live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.contains"], ["location.location.containedby"], ["location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.person.ethnicity", "people.person.place_of_birth"], ["people.person.place_of_birth"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the anasazi indians live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.000749549362808466, -0.08229527622461319, -0.09492246806621552, -0.09771376848220825, -0.1013571172952652, -0.10610803216695786, -0.11129746586084366, -0.11339417845010757, -0.11538900434970856, -0.11826816201210022], "norm_scores": [0.10972952842712402, 0.10113666951656342, 0.09986762702465057, 0.09958925098180771, 0.09922707825899124, 0.09875677525997162, 0.09824560582637787, 0.09803984314203262, 0.09784445911645889, 0.09756315499544144]}}
{"id": "WebQTest-1205", "question": "who plays harley quinn", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["cvg.game_character.games", "cvg.game_voice_actor.computer_game_voice_performances"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays harley quinn [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> cvg.game_character.games <SEP> cvg.game_voice_actor.computer_game_voice_performances </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.01628473401069641, -0.03168618679046631, -0.0393405444920063, -0.0410471111536026, -0.04252069070935249, -0.04300199821591377, -0.04365105554461479, -0.048203419893980026, -0.05402209609746933, -0.057803455740213394], "norm_scores": [0.10257361084222794, 0.1010059341788292, 0.10023575276136398, 0.10006482899188995, 0.09991749376058578, 0.09986940771341324, 0.09980461746454239, 0.09935129433870316, 0.09877489507198334, 0.09840209037065506]}}
{"id": "WebQTest-1206", "question": "what was francis bacon contributions", "prediction": [["people.person.profession"], ["law.invention.inventor"], ["law.inventor.inventions"], ["base.argumentmaps.innovator.original_ideas"], ["common.topic.notable_types"], ["symbols.namesake.named_after"], ["law.legal_case_party_relationship.case_party", "law.legal_case_party_relationship.case_party"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was francis bacon contributions [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> law.legal_case_party_relationship.case_party <SEP> law.legal_case_party_relationship.case_party </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.006041610613465309, -0.04322916641831398, -0.044495839625597, -0.07235698401927948, -0.08634144812822342, -0.0921153649687767, -0.10130760073661804, -0.10632552206516266, -0.109512098133564, -0.1098082959651947], "norm_scores": [0.10730969160795212, 0.10339239239692688, 0.10326151549816132, 0.10042424499988556, 0.09902963042259216, 0.09845949709415436, 0.09755857288837433, 0.09707025438547134, 0.09676143527030945, 0.09673277288675308]}}
{"id": "WebQTest-1209", "question": "when does ziva start on ncis", "prediction": [["tv.tv_series_season.series"], ["tv.tv_program.seasons"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.seasons"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes", "tv.tv_series_season.series"], ["tv.regular_tv_appearance.series", "tv.tv_series_season.series"], ["tv.tv_series_season.series", "tv.tv_series_season.series"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.regular_tv_appearance.series", "tv.tv_series_season.regular_cast"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes", "tv.tv_program.seasons"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.seasons"]], "ground_paths": [["tv.tv_program.seasons"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does ziva start on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_series_season.series </PATH>", "<PATH> tv.tv_program.seasons </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> fictional_universe.fictional_character.appears_in_these_fictional_universes <SEP> tv.tv_series_season.series </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_series_season.series </PATH>", "<PATH> tv.tv_series_season.series <SEP> tv.tv_series_season.series </PATH>", "<PATH> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_series_season.regular_cast </PATH>", "<PATH> fictional_universe.fictional_character.appears_in_these_fictional_universes <SEP> tv.tv_program.seasons </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.seasons </PATH>"], "scores": [-0.012604300864040852, -0.03599219769239426, -0.045613355934619904, -0.05291251465678215, -0.05559282377362251, -0.060517240315675735, -0.06123385205864906, -0.062095362693071365, -0.06231823191046715, -0.0719529315829277], "norm_scores": [0.1040131002664566, 0.10160867869853973, 0.10063576698303223, 0.0999038890004158, 0.09963647276163101, 0.09914702922105789, 0.09907600283622742, 0.09899068623781204, 0.09896863251924515, 0.09801967442035675]}}
{"id": "WebQTest-1210", "question": "what is the name of the national anthem of canada ( 2 words )", "prediction": [["music.composition.language"], ["music.composition.language", "music.composition.language"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["music.composer.compositions"], ["location.location.containedby", "music.composition.language"], ["music.composition.language", "language.human_language.main_country"], ["location.location.containedby", "location.location.containedby"], ["music.composition.lyricist"], ["music.composition.composer"], ["location.country.national_anthem", "music.composition.language"]], "ground_paths": [["location.country.national_anthem", "government.national_anthem.national_anthem_of"], ["location.country.official_language", "music.composition.language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the national anthem of canada ( 2 words ) [/INST]", "raw_output": {"paths": ["<PATH> music.composition.language </PATH>", "<PATH> music.composition.language <SEP> music.composition.language </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.statistical_region.places_imported_from </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> location.location.containedby <SEP> music.composition.language </PATH>", "<PATH> music.composition.language <SEP> language.human_language.main_country </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> location.country.national_anthem <SEP> music.composition.language </PATH>"], "scores": [-0.011023799888789654, -0.020156526938080788, -0.05732078477740288, -0.059111088514328, -0.059251077473163605, -0.05968007445335388, -0.06268735975027084, -0.06392169743776321, -0.06895823776721954, -0.07179145514965057], "norm_scores": [0.10430766642093658, 0.1033593937754631, 0.09958861768245697, 0.09941048920154572, 0.09939656406641006, 0.09935393929481506, 0.09905560314655304, 0.09893340617418289, 0.0984363779425621, 0.09815788269042969]}}
{"id": "WebQTest-1212", "question": "what did mark zuckerberg study", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.degree"], ["education.education.student", "education.education.field_of_study"], ["people.person.profession", "people.profession.part_of_professional_field"], ["people.person.education", "education.field_of_study.students_major_field"], ["people.person.education", "education.education.degree"], ["education.education.student", "education.education.degree_field_of_study"], ["people.person.education", "education.education.field_of_study"]], "ground_paths": [["people.person.profession", "base.skills.skill.performed_by"], ["people.person.education", "education.field_of_study.students_majoring"], ["education.education.student", "education.education.major_field_of_study"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mark zuckerberg study [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> education.education.student <SEP> education.education.field_of_study </PATH>", "<PATH> people.person.profession <SEP> people.profession.part_of_professional_field </PATH>", "<PATH> people.person.education <SEP> education.field_of_study.students_major_field </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> education.education.student <SEP> education.education.degree_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.field_of_study </PATH>"], "scores": [-0.010135206393897533, -0.01945137232542038, -0.03318065032362938, -0.06196245551109314, -0.06965380907058716, -0.07362054288387299, -0.07396389544010162, -0.07675476372241974, -0.08181075006723404, -0.08454248309135437], "norm_scores": [0.10492091625928879, 0.10394800454378128, 0.10253062099218369, 0.09962166845798492, 0.09885838627815247, 0.09846701472997665, 0.09843321144580841, 0.09815887361764908, 0.09766384214162827, 0.09739741683006287]}}
{"id": "WebQTest-1213", "question": "which country was michael jackson born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"], ["music.artist.origin", "location.location.containedby"], ["people.person.place_of_birth", "base.biblioness.bibs_location.country"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was michael jackson born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.013436727225780487, -0.017054202035069466, -0.04815046861767769, -0.08033932745456696, -0.1035362035036087, -0.10554653406143188, -0.10677428543567657, -0.10793542861938477, -0.11117362976074219, -0.11672016978263855], "norm_scores": [0.10691899061203003, 0.10653290897607803, 0.10327111184597015, 0.09999985992908478, 0.0977068766951561, 0.09751065075397491, 0.09739100933074951, 0.09727798402309418, 0.09696348756551743, 0.09642716497182846]}}
{"id": "WebQTest-1215", "question": "who was stephen r covey", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced_by"], ["people.deceased_person.place_of_death"], ["influence.influence_node.influenced"], ["book.author.works_written"], ["fictional_universe.fictional_character.occupation"], ["book.written_work.author"], ["common.topic.image"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was stephen r covey [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> common.topic.image </PATH>"], "scores": [-0.0010129597503691912, -0.05974128842353821, -0.09951827675104141, -0.12948302924633026, -0.13309091329574585, -0.1375991255044937, -0.1392039656639099, -0.15234950184822083, -0.15492798388004303, -0.16083702445030212], "norm_scores": [0.11214124411344528, 0.10574503242969513, 0.1016213595867157, 0.09862148016691208, 0.09826631098985672, 0.09782429784536362, 0.09766743332147598, 0.0963919386267662, 0.09614371508359909, 0.09557727724313736]}}
{"id": "WebQTest-1216", "question": "what country was stalin in charge of", "prediction": [["people.person.nationality"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.politician.government_positions_held", "government.governmental_jurisdiction.governing_officials"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.people_born_here", "location.location.containedby"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was stalin in charge of [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.01502174511551857, -0.02237503044307232, -0.0434517003595829, -0.0444006472826004, -0.0500488355755806, -0.051027633249759674, -0.05351412296295166, -0.059780605137348175, -0.06439364701509476, -0.06526601314544678], "norm_scores": [0.10322903096675873, 0.10247273743152618, 0.10033556073904037, 0.10024038702249527, 0.09967581182718277, 0.09957829117774963, 0.09933099895715714, 0.09871049225330353, 0.0982561782002449, 0.09817050397396088]}}
{"id": "WebQTest-1217", "question": "what is monta ellis career high points", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.player"], ["basketball.basketball_player.statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_team_roster.player", "sports.sports_team_roster.player"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player_statistics"]], "ground_paths": [["award.award_honor.award_winner", "award.award_honor.award"], ["sports.sports_award_winner.awards", "sports.sports_award.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is monta ellis career high points [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> basketball.basketball_player.statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.player </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player_statistics </PATH>"], "scores": [-0.011883683502674103, -0.017047645524144173, -0.03985310345888138, -0.04424118250608444, -0.04843010753393173, -0.05056726932525635, -0.05646707862615585, -0.07253318279981613, -0.07370652258396149, -0.07554517686367035], "norm_scores": [0.10376115888357162, 0.10322671383619308, 0.10089921951293945, 0.10045744478702545, 0.10003752261400223, 0.09982394427061081, 0.09923673421144485, 0.09765512496232986, 0.09754061698913574, 0.09736143052577972]}}
{"id": "WebQTest-1219", "question": "what nationality is rihanna", "prediction": [["people.person.nationality"], ["people.ethnicity.people"], ["music.artist.origin"], ["people.person.ethnicity"], ["music.album.artist"], ["music.recording.artist"], ["location.location.people_born_here"], ["music.artist.nationality"], ["people.ethnicity.people", "people.ethnicity.people"], ["music.composition.composer"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is rihanna [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.nationality </PATH>", "<PATH> people.ethnicity.people <SEP> people.ethnicity.people </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.01156568992882967, -0.02944023162126541, -0.041486989706754684, -0.05139453709125519, -0.09762623906135559, -0.1146027147769928, -0.1224488839507103, -0.1343194842338562, -0.1372414231300354, -0.13974729180335999], "norm_scores": [0.10782239586114883, 0.1059122309088707, 0.10464399307966232, 0.10361234843730927, 0.0989312157034874, 0.09726589173078537, 0.09650570899248123, 0.09536689519882202, 0.0950886458158493, 0.09485067427158356]}}
{"id": "WebQTest-1220", "question": "what has shannen doherty played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.languages", "film.film.language"]], "ground_paths": [["people.person.languages", "film.film.language"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "tv.tv_program.languages"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has shannen doherty played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>"], "scores": [-0.016142526641488075, -0.031823404133319855, -0.0344136506319046, -0.03641118109226227, -0.03986183926463127, -0.05322793498635292, -0.05387663468718529, -0.05568869039416313, -0.055724773555994034, -0.0586404986679554], "norm_scores": [0.1027727723121643, 0.1011737808585167, 0.10091204941272736, 0.1007106751203537, 0.10036376118659973, 0.09903121739625931, 0.09896699339151382, 0.0987878143787384, 0.09878425300121307, 0.09849664568901062]}}
{"id": "WebQTest-1221", "question": "what college did joakim noah", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.place_of_birth", "location.location.containedby"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["education.education.student", "education.educational_institution.students_graduates"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did joakim noah [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01853523962199688, -0.021750563755631447, -0.02366810478270054, -0.025088241323828697, -0.0521581768989563, -0.06707609444856644, -0.07751703262329102, -0.08749528974294662, -0.09285606443881989, -0.09710683673620224], "norm_scores": [0.10380345582962036, 0.10347022861242294, 0.10327201336622238, 0.10312545299530029, 0.10037130117416382, 0.09888507425785065, 0.0978580042719841, 0.09688640385866165, 0.09636840969324112, 0.0959596335887909]}}
{"id": "WebQTest-1223", "question": "who plays billy elliot", "prediction": [["film.film.country", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["award.award_honor.honored_for", "award.award_honor.award_winner"]], "ground_paths": [["award.award_honor.honored_for", "award.award_winner.awards_won"], ["media_common.netflix_title.netflix_genres", "people.person.nationality"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["film.performance.film", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.film.language", "people.person.languages"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays billy elliot [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_winning_work.awards_won <SEP> award.award_honor.award_winner </PATH>", "<PATH> award.award_winning_work.awards_won <SEP> award.award_winner.awards_won </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_honor.award_winner </PATH>"], "scores": [-0.030194750055670738, -0.033332858234643936, -0.03429516777396202, -0.03518562763929367, -0.036690909415483475, -0.04185148701071739, -0.04197557270526886, -0.046897657215595245, -0.04801243916153908, -0.050307076424360275], "norm_scores": [0.10097047686576843, 0.10065411776304245, 0.10055731236934662, 0.10046780109405518, 0.10031668841838837, 0.09980032593011856, 0.09978795051574707, 0.09929798543453217, 0.09918734431266785, 0.09896000474691391]}}
{"id": "WebQTest-1225", "question": "what kourtney kardashian boyfriends name", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["common.topic.notable_types", "common.topic.notable_types"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"]], "ground_paths": [["people.person.parents", "people.person.children"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.person.children", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kourtney kardashian boyfriends name [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> celebrities.romantic_relationship.celebrity <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> base.popstra.celebrity.breakup <SEP> base.popstra.celebrity.breakup </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> celebrities.romantic_relationship.celebrity <SEP> celebrities.celebrity.sexual_relationships </PATH>"], "scores": [-0.01991991512477398, -0.02349540963768959, -0.02469157613813877, -0.037448734045028687, -0.04213467240333557, -0.047409787774086, -0.048896320164203644, -0.05602942034602165, -0.05785024166107178, -0.058659669011831284], "norm_scores": [0.10218716412782669, 0.10182244330644608, 0.10170071572065353, 0.1004115492105484, 0.09994212538003922, 0.09941630810499191, 0.09926863014698029, 0.0985630676150322, 0.09838375449180603, 0.0983041524887085]}}
{"id": "WebQTest-1226", "question": "what form of government does australia use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["film.film_location.featured_in_films", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent"], ["form_of_government.countries.form_of_government"], ["type.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> film.film_location.featured_in_films <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> form_of_government.countries.form_of_government </PATH>", "<PATH> type.form_of_government.countries </PATH>"], "scores": [-0.00741257518529892, -0.018422767519950867, -0.13990291953086853, -0.14134223759174347, -0.1431649923324585, -0.1459609419107437, -0.15304487943649292, -0.1532270759344101, -0.15412135422229767, -0.15521380305290222], "norm_scores": [0.11187891662120819, 0.11065386980772018, 0.09799602627754211, 0.09785507619380951, 0.09767688065767288, 0.09740415215492249, 0.09671659022569656, 0.09669896215200424, 0.09661252796649933, 0.09650704264640808]}}
{"id": "WebQTest-1227", "question": "what team did howie long", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sport.team_coaches", "sports.sports_team.sport"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_coach.sports_coached", "sports.sports_team.sport"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did howie long [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sport.team_coaches <SEP> sports.sports_team.sport </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.01177408080548048, -0.013020716607570648, -0.05054217204451561, -0.061026375740766525, -0.06182987987995148, -0.06219127029180527, -0.06398814171552658, -0.06466438621282578, -0.06617368757724762, -0.07249012589454651], "norm_scores": [0.10416198521852493, 0.10403221845626831, 0.10020110011100769, 0.09915605187416077, 0.09907641261816025, 0.09904062002897263, 0.09886281937360764, 0.09879598021507263, 0.09864698350429535, 0.0980258509516716]}}
{"id": "WebQTest-1228", "question": "where did the greek alphabet come from", "prediction": [["language.human_language.language_family"], ["language.language_family.languages"], ["language.human_language.writing_system"], ["language.language_writing_system.child_writing_systems"], ["base.rosetta.languoid.parent"], ["language.language_writing_system.parent_writing_systems"], ["language.language_writing_system.languages"], ["language.language_writing_system.language"], ["language.language_writing_system.parent_writing_system"], ["base.rosetta.languoid.child"]], "ground_paths": [["language.language_writing_system.child_writing_systems"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the greek alphabet come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> language.human_language.writing_system </PATH>", "<PATH> language.language_writing_system.child_writing_systems </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> language.language_writing_system.parent_writing_systems </PATH>", "<PATH> language.language_writing_system.languages </PATH>", "<PATH> language.language_writing_system.language </PATH>", "<PATH> language.language_writing_system.parent_writing_system </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.020843269303441048, -0.021529894322156906, -0.04567782208323479, -0.06814149767160416, -0.07069369405508041, -0.07222641259431839, -0.07599940896034241, -0.08907574415206909, -0.08918944746255875, -0.09276729822158813], "norm_scores": [0.10444089025259018, 0.10436920076608658, 0.10187909007072449, 0.09961602091789246, 0.09936211258172989, 0.09920992702245712, 0.09883632510900497, 0.09755231440067291, 0.09754122048616409, 0.09719286113977432]}}
{"id": "WebQTest-1229", "question": "what timezone is elizabethtown kentucky", "prediction": [["location.location.time_zones"], ["location.us_county.hud_county_place", "location.location.time_zones"], ["location.administrative_division.second_level_division_of", "location.location.time_zones"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.time_Zones"], ["location.us_county.county_seat"], ["location.city.time_zones"], ["location.us_county.county_seat", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is elizabethtown kentucky [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.us_county.hud_county_place <SEP> location.location.time_zones </PATH>", "<PATH> location.administrative_division.second_level_division_of <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.city.time_zones </PATH>", "<PATH> location.us_county.county_seat <SEP> location.location.time_zones </PATH>"], "scores": [-9.314419912698213e-06, -0.17291221022605896, -0.17507366836071014, -0.17690663039684296, -0.1806044727563858, -0.18402546644210815, -0.19755379855632782, -0.1996767371892929, -0.2048284113407135, -0.20862124860286713], "norm_scores": [0.11832310259342194, 0.09953567385673523, 0.09932076185941696, 0.09913888573646545, 0.09877295047044754, 0.09843563288450241, 0.09711293131113052, 0.09690698981285095, 0.09640903025865555, 0.09604407101869583]}}
{"id": "WebQTest-1230", "question": "what to see in christchurch new zealand", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["organization.organization.geographic_scope"], ["transportation.road.major_cities"], ["tourist.tourist_attraction.near_travel_destination"], ["film.film_festival.location"], ["base.arthist.helynevek.intezmeny"], ["location.location.contains"], ["film.film_location.featured_in_films"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in christchurch new zealand [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> transportation.road.major_cities </PATH>", "<PATH> tourist.tourist_attraction.near_travel_destination </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> film.film_location.featured_in_films </PATH>"], "scores": [-0.012579825706779957, -0.012629784643650055, -0.04584121331572533, -0.1200801432132721, -0.13211731612682343, -0.13505080342292786, -0.13724680244922638, -0.14266592264175415, -0.1458631306886673, -0.14687635004520416], "norm_scores": [0.10931669175624847, 0.10931123048067093, 0.10574047267436981, 0.09817472100257874, 0.09700006246566772, 0.09671593457460403, 0.09650377929210663, 0.09598222374916077, 0.09567584097385406, 0.09557895362377167]}}
{"id": "WebQTest-1231", "question": "where is chris paul from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.country"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.country"], ["base.schemastaging.athlete_extra.salary", "location.location.people_born_here"], ["people.person.nationality"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.place_of_birth"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.place_of_birth", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris paul from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.country </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.country </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.place_of_birth </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.010661344975233078, -0.017030196264386177, -0.06896880269050598, -0.08642736822366714, -0.09955772012472153, -0.1028735339641571, -0.10641647130250931, -0.10792671144008636, -0.11205027252435684, -0.11896470189094543], "norm_scores": [0.10743636637926102, 0.10675428807735443, 0.10135114938020706, 0.09959706664085388, 0.09829787164926529, 0.09797247499227524, 0.0976259708404541, 0.09747865051031113, 0.09707751125097275, 0.09640859812498093]}}
{"id": "WebQTest-1232", "question": "what year was albert pujols rookie year", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.pro_athlete.teams", "sports.sports_league_season.teams"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["baseball.baseball_player.lifetime_batting_statistics", "baseball.lifetime_batting_statistics.starting_season"], ["sports.pro_athlete.teams", "sports.sports_league_season.year"], ["sports.pro_athlete.teams", "sports.sports_league_season.starting_year"], ["sports.sports_award.award_winner", "sports.sports_league_season.awards"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["sports.sports_award.award_winner", "sports.sports_league_season.awards"], ["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.starting_season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was albert pujols rookie year [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_season.teams </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> baseball.baseball_player.lifetime_batting_statistics <SEP> baseball.lifetime_batting_statistics.starting_season </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_season.year </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_season.starting_year </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_league_season.awards </PATH>"], "scores": [-0.013057257048785686, -0.013233416713774204, -0.03825560212135315, -0.05085357278585434, -0.05219370499253273, -0.0541866272687912, -0.055931203067302704, -0.06018701568245888, -0.061210669577121735, -0.06303650885820389], "norm_scores": [0.10335483402013779, 0.1033366248011589, 0.10078299045562744, 0.09952130168676376, 0.09938802570104599, 0.09919014573097229, 0.09901725500822067, 0.09859675168991089, 0.09849587082862854, 0.09831620007753372]}}
{"id": "WebQTest-1233", "question": "what are the colors of the ny giants", "prediction": [["sports.sports_team.colors"], ["sports.professional_sports_team.owner_s", "sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.professional_sports_team.colors"], ["sports.sports_team. Colors"], ["sports.sports_team.team_colors"], ["sports.sports_team.color"], ["sports.sports_team_colors.team"], ["sports.sports_team_color.teams"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the colors of the ny giants [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.colors </PATH>", "<PATH> sports.sports_team. Colors </PATH>", "<PATH> sports.sports_team.team_colors </PATH>", "<PATH> sports.sports_team.colors <PATH>", "<PATH> sports.sports_team.color </PATH>", "<PATH> sports.sports_team_colors.team </PATH>", "<PATH> sports.sports_team_color.teams </PATH>"], "scores": [-4.326247290009633e-05, -0.10732990503311157, -0.1293898969888687, -0.1412188708782196, -0.1494811624288559, -0.15794813632965088, -0.17269951105117798, -0.1733732670545578, -0.17445369064807892, -0.1780874878168106], "norm_scores": [0.11468493193387985, 0.10301782190799713, 0.10077013075351715, 0.0995851531624794, 0.09876573830842972, 0.09793301671743393, 0.09649898111820221, 0.09643397480249405, 0.09632983803749084, 0.0959804356098175]}}
{"id": "WebQTest-1234", "question": "what business did andrew carnegie dominate", "prediction": [["people.person.profession"], ["book.written_work.subjects"], ["book.written_work.author"], ["book.author.works_written"], ["organization.organization.founders"], ["business.business_person.industry"], ["book.book_subject.works"], ["organization.organization_founder.organizations_founded"], ["business.business_operation.industry"], ["symbols.namesake.named_after"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what business did andrew carnegie dominate [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> business.business_person.industry </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> business.business_operation.industry </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.010489139705896378, -0.04339419677853584, -0.04941924288868904, -0.05652168020606041, -0.06182686612010002, -0.07303453981876373, -0.07417691498994827, -0.0743834525346756, -0.07540054619312286, -0.07733003050088882], "norm_scores": [0.10501240938901901, 0.10161320120096207, 0.10100281983613968, 0.10028800368309021, 0.0997573584318161, 0.09864555299282074, 0.09853292256593704, 0.09851257503032684, 0.09841243922710419, 0.09822273254394531]}}
{"id": "WebQTest-1235", "question": "what was sir isaac newton 's inventions", "prediction": [["law.invention.inventor"], ["law.inventor.inventions"], ["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"], ["symbols.name_source.namesakes"], ["influence.influence_node.influenced_by"], ["law.inventor.us_patents"], ["influence.influence_node.influenced"], ["common.topic.notable_types"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was sir isaac newton 's inventions [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.004299068357795477, -0.031500913202762604, -0.06168445572257042, -0.06454091519117355, -0.0925011858344078, -0.10587258636951447, -0.11790303885936737, -0.12524846196174622, -0.14053858816623688, -0.1422356218099594], "norm_scores": [0.10869092494249344, 0.1057741791009903, 0.10262924432754517, 0.10233651101589203, 0.09951477497816086, 0.0981929823756218, 0.0970187559723854, 0.09630872309207916, 0.09484735131263733, 0.09468653053045273]}}
{"id": "WebQTest-1237", "question": "who did shaq first play for", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did shaq first play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.018998827785253525, -0.020522253587841988, -0.02200770005583763, -0.03879305347800255, -0.04276932403445244, -0.046209219843149185, -0.05029210075736046, -0.05157065764069557, -0.052814826369285583, -0.05685761198401451], "norm_scores": [0.10212124139070511, 0.10196578502655029, 0.10181443393230438, 0.1001197025179863, 0.09972238540649414, 0.09937994182109833, 0.09897501021623611, 0.09884855151176453, 0.09872565418481827, 0.09832732379436493]}}
{"id": "WebQTest-1238", "question": "who was the artist of mickey mouse", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.tv_character.appears_in_tv_episodes", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.character_created_by", "people.person.profession"], ["film.film_character.portrayed_in_films_dubbed", "film.dubbing_performance.actor"], ["fictional_universe.character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by", "symbols.namesake.named_after"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the artist of mickey mouse [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.tv_character.appears_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> people.person.profession </PATH>", "<PATH> film.film_character.portrayed_in_films_dubbed <SEP> film.dubbing_performance.actor </PATH>", "<PATH> fictional_universe.character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> symbols.namesake.named_after </PATH>"], "scores": [-0.0009693152387626469, -0.07032550871372223, -0.07146883755922318, -0.07551640272140503, -0.09322527796030045, -0.09341836720705032, -0.09555184096097946, -0.09635011106729507, -0.09667865186929703, -0.09814881533384323], "norm_scores": [0.10808994621038437, 0.10084730386734009, 0.10073205828666687, 0.10032516717910767, 0.09856416285037994, 0.098545141518116, 0.09833511710166931, 0.09825664758682251, 0.09822437167167664, 0.09808007627725601]}}
{"id": "WebQTest-1239", "question": "what is the zip code for concord", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.us_county.hud_county_place"], ["location.postal_code.citytown"], ["location.town.postal_codes"], ["location.mailing_address.citytown"], ["location.us_county.county_seat"], ["location.citytown.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for concord [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.postal_code.citytown </PATH>", "<PATH> location.town.postal_codes </PATH>", "<PATH> location.mailing_address.citytown </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_codes <PATH>"], "scores": [-0.00014316270244307816, -0.1016465499997139, -0.13789106905460358, -0.15195906162261963, -0.16941478848457336, -0.1698150783777237, -0.17004212737083435, -0.17280198633670807, -0.17319732904434204, -0.17664778232574463], "norm_scores": [0.11511948704719543, 0.10400794446468353, 0.1003057211637497, 0.09890450537204742, 0.09719302505254745, 0.09715413302183151, 0.09713207930326462, 0.09686437994241714, 0.09682608395814896, 0.09649256616830826]}}
{"id": "WebQTest-1240", "question": "what football team is michael vick on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.passing", "american_football.player_passing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team is michael vick on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.passing <SEP> american_football.player_passing_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.01897720992565155, -0.023135539144277573, -0.0350906103849411, -0.037463054060935974, -0.03930524364113808, -0.03975549340248108, -0.042401399463415146, -0.045156750828027725, -0.04619621858000755, -0.04971251264214516], "norm_scores": [0.10188744962215424, 0.1014646515250206, 0.10025884956121445, 0.10002126544713974, 0.09983719140291214, 0.0997922420501709, 0.09952855110168457, 0.09925469756126404, 0.09915157407522202, 0.09880354255437851]}}
{"id": "WebQTest-1241", "question": "where does the columbia river start", "prediction": [["geography.lake.outflow"], ["geography.river.origin"], [], ["location.location.containedby"], ["geography.lake.outflow", "geography.lake.outflow"], ["transportation.bridge.body_of_water_spanned"], ["location.location.containedby", "location.location.containedby"], ["common.topic.notable_types"], ["geography.lake.origin"], ["geography.lake.basin_countries"]], "ground_paths": [["geography.lake.outflow"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the columbia river start [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.outflow </PATH>", "<PATH> geography.river.origin </PATH>", "<PATH> </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.lake.outflow <SEP> geography.lake.outflow </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> geography.lake.origin </PATH>", "<PATH> geography.lake.basin_countries </PATH>"], "scores": [-0.00718305679038167, -0.02907027304172516, -0.06958973407745361, -0.0760338231921196, -0.10539532452821732, -0.11290237307548523, -0.11693576723337173, -0.12486784160137177, -0.12488337606191635, -0.1281019002199173], "norm_scores": [0.10848869383335114, 0.10613998025655746, 0.10192520171403885, 0.10127051174640656, 0.09834028035402298, 0.09760480374097824, 0.09721191227436066, 0.09644387662410736, 0.09644237905740738, 0.09613246470689774]}}
{"id": "WebQTest-1242", "question": "what are all the names of the harry potter books", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["book.book_character.appears_in_book"], ["fictional_universe.fictional_universe.works_set_here"], ["book.book.genre"], ["book.work_of_fiction.part_of_these_fictional_universes"], ["book.written_work.subjects"], ["book.written_work.part_of_series"], ["fictional_universe.book_character.appears_in_these_fictional_universes"], ["book.written_work.previous_in_series"], ["book.book.book_editions_published"]], "ground_paths": [["book.book_character.appears_in_book"], ["fictional_universe.fictional_universe.works_set_here"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the names of the harry potter books [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> book.book_character.appears_in_book </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>", "<PATH> book.book.genre </PATH>", "<PATH> book.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.part_of_series </PATH>", "<PATH> fictional_universe.book_character.appears_in_these_fictional_universes </PATH>", "<PATH> book.written_work.previous_in_series </PATH>", "<PATH> book.book.book_editions_published </PATH>"], "scores": [-0.008158016949892044, -0.026136718690395355, -0.03168225288391113, -0.04353924095630646, -0.07711882144212723, -0.07742354273796082, -0.08048214763402939, -0.08859441429376602, -0.09710567444562912, -0.1065482646226883], "norm_scores": [0.10565527528524399, 0.10377270728349686, 0.1031988263130188, 0.10198242217302322, 0.09861475229263306, 0.09858470410108566, 0.09828363358974457, 0.09748955816030502, 0.09666331857442856, 0.09575486928224564]}}
{"id": "WebQTest-1244", "question": "who was sir walter raleigh and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["people.profession.people_with_this_profession"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["influence.influence_node.influenced_by"], ["base.activism.activist.area_of_activism"], ["influence.influence_node.influenced"], ["symbols.namesake.named_after"], ["film.film_subject.films"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was sir walter raleigh and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> film.film_subject.films </PATH>"], "scores": [-0.0008434345363639295, -0.060390062630176544, -0.0765351951122284, -0.08558709174394608, -0.09226896613836288, -0.13417109847068787, -0.14721383154392242, -0.14908182621002197, -0.14930294454097748, -0.14933325350284576], "norm_scores": [0.11079160124063492, 0.10438691079616547, 0.10271511226892471, 0.10178953409194946, 0.10111165046691895, 0.09696239978075027, 0.09570596367120743, 0.09552735090255737, 0.0955062285065651, 0.09550333023071289]}}
{"id": "WebQTest-1245", "question": "what countries are in the euro dollar", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["finance.currency.countries_used", "olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.coinsdaily.coin_type.country", "olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.coinsdaily.coin_type.country", "base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.coinsdaily.coin_type.country", "base.locations.countries.planet", "base.aareas.schema.administrative_area.administrative_parent"], ["base.coinsdaily.coin_type.country", "olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["finance.currency.countries_used", "base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["finance.currency.countries_used", "base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the euro dollar [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> finance.currency.countries_used <SEP> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> base.locations.countries.planet <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> finance.currency.countries_used <SEP> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> finance.currency.countries_used <SEP> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.011566506698727608, -0.0186463613063097, -0.04876381531357765, -0.05472121015191078, -0.0572359599173069, -0.0616731159389019, -0.062200214713811874, -0.06299708038568497, -0.06334338337182999, -0.06550677120685577], "norm_scores": [0.10396949201822281, 0.10323600471019745, 0.10017315298318863, 0.09957815706729889, 0.09932805597782135, 0.09888829290866852, 0.09883618354797363, 0.09875746071338654, 0.09872326254844666, 0.09850992262363434]}}
{"id": "WebQTest-1246", "question": "what currency can you use in barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["location.country.form_of_government"], ["base.coinsdaily.coin_type.country"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.011846926994621754, -0.013952475041151047, -0.12768401205539703, -0.1349668949842453, -0.13831618428230286, -0.14956988394260406, -0.164951890707016, -0.17396187782287598, -0.17521357536315918, -0.17675752937793732], "norm_scores": [0.11197144538164139, 0.11173593252897263, 0.09972405433654785, 0.09900040924549103, 0.0986693724989891, 0.09756521135568619, 0.09607593715190887, 0.09521418809890747, 0.09509508311748505, 0.09494837373495102]}}
{"id": "WebQTest-1247", "question": "who is angelina jolie", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.person.nationality"], ["people.person.parents"], ["people.person.children"], ["common.topic.subjects"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nominee"], ["common.topic.subject_of"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> common.topic.subjects </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> common.topic.subject_of </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.0019568330608308315, -0.05185995623469353, -0.09337972849607468, -0.10030446201562881, -0.10481058061122894, -0.11042505502700806, -0.11436333507299423, -0.11601743847131729, -0.117988221347332, -0.11951087415218353], "norm_scores": [0.10946694761514664, 0.10413826256990433, 0.09990300238132477, 0.09921359270811081, 0.09876753389835358, 0.09821455180644989, 0.09782851487398148, 0.09766682982444763, 0.09747454524040222, 0.09732624143362045]}}
{"id": "WebQTest-1248", "question": "where is nineveh city", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.biblioness.bibs_location.country"], ["geography.river.cities"], ["base.biblioness.bibs_location.city"], ["base.locations.countries.continent"], ["base.locations.continents.countries_within"], ["book.book_subject.works"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is nineveh city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> geography.river.cities </PATH>", "<PATH> base.biblioness.bibs_location.city </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.0007737336563877761, -0.07405012100934982, -0.10212892293930054, -0.1180066242814064, -0.12214947491884232, -0.14216254651546478, -0.1475428342819214, -0.1506216824054718, -0.15134726464748383, -0.1522349715232849], "norm_scores": [0.11210529506206512, 0.10418438166379929, 0.1012997031211853, 0.09970398992300034, 0.09929178655147552, 0.09732440859079361, 0.09680217504501343, 0.0965045914053917, 0.09643460065126419, 0.09634903073310852]}}
{"id": "WebQTest-1249", "question": "what is the state flower of new mexico", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.governing_officials", "government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.jurisdiction_of_office", "location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.jurisdiction_of_office", "government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.governing_officials", "government.governmental_jurisdiction.governing_officials", "location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of new mexico [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.jurisdiction_of_office <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_jurisdiction.governing_officials <SEP> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>"], "scores": [-0.013426151126623154, -0.016826288774609566, -0.022577518597245216, -0.02506829984486103, -0.04349560663104057, -0.04503358528017998, -0.04673294723033905, -0.04906991496682167, -0.04970834031701088, -0.05146516487002373], "norm_scores": [0.10230748355388641, 0.10196021944284439, 0.1013755053281784, 0.10112331062555313, 0.09927694499492645, 0.09912437945604324, 0.09895607084035873, 0.0987250879406929, 0.09866207093000412, 0.09848889708518982]}}
{"id": "WebQTest-1250", "question": "where is the national ffa foundation located today", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization_foundation.organizations_founded", "location.location.containedby"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.mailing_address.citytown"], ["organization.organization.place_founded"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the national ffa foundation located today [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_foundation.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.0077219316735863686, -0.03531859815120697, -0.04132334887981415, -0.04609476774930954, -0.0475529320538044, -0.06668083369731903, -0.06842203438282013, -0.07935609668493271, -0.08315350860357285, -0.0883106142282486], "norm_scores": [0.10495732724666595, 0.10210045427083969, 0.10148920863866806, 0.10100612044334412, 0.10085894167423248, 0.09894805401563644, 0.09877591580152512, 0.0977017804980278, 0.0973314568400383, 0.0968308076262474]}}
{"id": "WebQTest-1251", "question": "which countries speak germanic languages", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.language_family"], ["language.human_language.main_country"], ["language.language_family.languages"], ["base.rosetta.languoid.languoid_class"], ["base.rosetta.local_name.languoid"], ["language.human_language.region"], ["common.topic.notable_types"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak germanic languages [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.local_name.languoid </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.015174842439591885, -0.03037671372294426, -0.035109516233205795, -0.038430988788604736, -0.04845435544848442, -0.10096936672925949, -0.1216537207365036, -0.12902408838272095, -0.13644638657569885, -0.1597202718257904], "norm_scores": [0.10672546923160553, 0.10511530935764313, 0.10461899638175964, 0.10427207499742508, 0.10323213785886765, 0.09795079380273819, 0.0959455668926239, 0.0952410101890564, 0.09453671425580978, 0.09236188977956772]}}
{"id": "WebQTest-1252", "question": "who plays eowyn in lord of the rings", "prediction": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.performance.character", "film.actor.film"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["award.award_honor.honored_for", "award.award_honor.award_winner"]], "ground_paths": [["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["film.film.starring", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays eowyn in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_honor.award_winner </PATH>"], "scores": [-0.033451590687036514, -0.033896323293447495, -0.0344817191362381, -0.03448839485645294, -0.034714411944150925, -0.034899450838565826, -0.03709482029080391, -0.04155728220939636, -0.04164254292845726, -0.04431195184588432], "norm_scores": [0.10036017000675201, 0.10031554847955704, 0.10025684535503387, 0.10025616735219955, 0.1002335175871849, 0.10021496564149857, 0.0999952033162117, 0.09954997152090073, 0.09954147785902023, 0.0992761179804802]}}
{"id": "WebQTest-1254", "question": "where is augusta masters 2012", "prediction": [["time.event.locations"], ["time.recurring_event.instances"], ["location.location.events"], ["time.recurring_event.current_frequency"], ["sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event"], ["common.topic.image"], ["event.disaster.areas_hosted"], ["tv.tv_program.country_of_origin"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is augusta masters 2012 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> event.disaster.areas_hosted </PATH>", "<PATH> tv.tv_program.country_of_origin </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.0021650262642651796, -0.051965754479169846, -0.06825342029333115, -0.08018981665372849, -0.08665782958269119, -0.09387804567813873, -0.10703091323375702, -0.11257071048021317, -0.1251981556415558, -0.12669508159160614], "norm_scores": [0.10861518234014511, 0.10333855450153351, 0.10166904330253601, 0.10046269744634628, 0.0998149961233139, 0.09909690171480179, 0.09780203551054001, 0.09726172685623169, 0.09604127705097198, 0.0958976224064827]}}
{"id": "WebQTest-1255", "question": "what type of cancer did eva peron die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.parent_cause_of_death"], ["medicine.disease.parent_disease"], ["people.cause_of_death.people", "people.deceased_person.cause_of_death"], ["deceased_person.cause_of_death"], ["film.film_subject.films", "film.film_subject.films"], ["people.person.cause_of_death"]], "ground_paths": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.parent_cause_of_death </PATH>", "<PATH> medicine.disease.parent_disease </PATH>", "<PATH> people.cause_of_death.people <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> film.film_subject.films <SEP> film.film_subject.films </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.008138751611113548, -0.02023893967270851, -0.0373489186167717, -0.0843106210231781, -0.09195733815431595, -0.10856148600578308, -0.11345532536506653, -0.12282641977071762, -0.12338101118803024, -0.1287098377943039], "norm_scores": [0.10776857286691666, 0.10647241026163101, 0.10466615855693817, 0.09986449033021927, 0.09910376369953156, 0.09747181832790375, 0.09699597209692001, 0.09609126299619675, 0.09603799134492874, 0.09552757441997528]}}
{"id": "WebQTest-1256", "question": "who did terrell owens play for in 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did terrell owens play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.018660087138414383, -0.021981049329042435, -0.022006727755069733, -0.027351684868335724, -0.02784603275358677, -0.029312077909708023, -0.04231381416320801, -0.05198189988732338, -0.0743212178349495, -0.08487702161073685], "norm_scores": [0.10213890671730042, 0.10180027037858963, 0.1017976626753807, 0.10125500708818436, 0.10120496898889542, 0.10105670243501663, 0.0997512936592102, 0.09879153221845627, 0.09660906344652176, 0.09559464454650879]}}
{"id": "WebQTest-1257", "question": "where is denmark south carolina", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"], ["location.hud_county_place.county"], ["location.administrative_division.first_level_division_of"], ["location.us_county.hud_county_place"], ["location.administrative_division.second_level_division_of"], ["location.country.administrative_divisions"]], "ground_paths": [["location.location.containedby"], ["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark south carolina [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.administrative_division.second_level_division_of </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.013537474907934666, -0.020953448489308357, -0.05144035443663597, -0.053387317806482315, -0.06625466793775558, -0.07470407336950302, -0.09077993035316467, -0.09475032240152359, -0.09556908160448074, -0.09580624103546143], "norm_scores": [0.10531198233366013, 0.10453387349843979, 0.10139505565166473, 0.10119783133268356, 0.09990403056144714, 0.0990634486079216, 0.09748365730047226, 0.09709736704826355, 0.09701790660619736, 0.09699489176273346]}}
{"id": "WebQTest-1259", "question": "what books did ernest hemingway write", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"], ["book.author.book_editions_published"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"], ["influence.influence_node.influenced", "book.book_edition.author_editor"], ["influence.influence_node.influenced_by"], ["tv.tv_program.program_creator"]], "ground_paths": [["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did ernest hemingway write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>", "<PATH> influence.influence_node.influenced <SEP> book.book_edition.author_editor </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> tv.tv_program.program_creator </PATH>"], "scores": [-0.01664869114756584, -0.022440893575549126, -0.027375347912311554, -0.06933377683162689, -0.06977548450231552, -0.07644154876470566, -0.1104186549782753, -0.12658067047595978, -0.1272556483745575, -0.12771333754062653], "norm_scores": [0.1061682477593422, 0.10555507242679596, 0.10503550618886948, 0.10071954876184464, 0.1006750836968422, 0.10000620037317276, 0.09666536748409271, 0.09511561691761017, 0.09505143761634827, 0.09500794112682343]}}
{"id": "WebQTest-1260", "question": "what year did baltimore ravens win super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship"], ["sports.sports_championship_event.season"], ["sports.sports_winning_work.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did baltimore ravens win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship </PATH>", "<PATH> sports.sports_championship_event.season </PATH>", "<PATH> sports.sports_winning_work.champion </PATH>"], "scores": [-0.012620019726455212, -0.013998365961015224, -0.039012014865875244, -0.07392529398202896, -0.08992152661085129, -0.11162339150905609, -0.11882654577493668, -0.1292530596256256, -0.1486491858959198, -0.1501859426498413], "norm_scores": [0.10778292268514633, 0.10763446986675262, 0.10497552156448364, 0.10137373208999634, 0.09976503252983093, 0.09762327373027802, 0.0969226062297821, 0.09591728448867798, 0.0940747931599617, 0.09393033385276794]}}
{"id": "WebQTest-1261", "question": "where did the battle of passchendaele happen", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "time.event.locations"], ["event.disaster.areas_affected"], ["military.military_conflict.casualties", "location.location.events"], ["military.military_conflict.commanders", "military.military_command.military_combatant"], ["military.military_conflict.casualties", "military.casualties.combatant"], ["military.military_conflict.casualties"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of passchendaele happen [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> time.event.locations </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> military.military_conflict.casualties <SEP> location.location.events </PATH>", "<PATH> military.military_conflict.commanders <SEP> military.military_command.military_combatant </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.combatant </PATH>", "<PATH> military.military_conflict.casualties </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.013153670355677605, -0.013194541446864605, -0.0910104438662529, -0.10250681638717651, -0.11329471319913864, -0.11746460199356079, -0.1208081766963005, -0.1253839135169983, -0.13105615973472595, -0.13171955943107605], "norm_scores": [0.10853065550327301, 0.10852622240781784, 0.1004013791680336, 0.09925373643636703, 0.09818875789642334, 0.09778016805648804, 0.0974537804722786, 0.09700887650251389, 0.09646017849445343, 0.09639620035886765]}}
{"id": "WebQTest-1262", "question": "what part of the country is ohio considered", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.partially_contains"], ["location.administrative_division.first_level_division_of"], ["location.administrative_division.country"], ["base.locations.states_and_provences.country"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.locations.countries.states_provinces_within"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.locations.states_and_provences.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>"], "scores": [-0.012362430803477764, -0.015930334106087685, -0.08107776194810867, -0.08799728006124496, -0.09102445095777512, -0.09655461460351944, -0.09690332412719727, -0.1004205122590065, -0.10098039358854294, -0.10636364668607712], "norm_scores": [0.10682720690965652, 0.10644673556089401, 0.09973306953907013, 0.09904534369707108, 0.09874597191810608, 0.09820140153169632, 0.09816716611385345, 0.09782249480485916, 0.09776774048805237, 0.09724284708499908]}}
{"id": "WebQTest-1263", "question": "what team is cristiano ronaldo play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is cristiano ronaldo play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.021271560341119766, -0.02502415142953396, -0.025199728086590767, -0.037979740649461746, -0.0386592335999012, -0.03969737887382507, -0.04056384041905403, -0.04089219868183136, -0.04145548865199089, -0.046149756759405136], "norm_scores": [0.1014488935470581, 0.10106891393661499, 0.10105116665363312, 0.09976795315742493, 0.09970018267631531, 0.09959673881530762, 0.09951047599315643, 0.09947780519723892, 0.09942179173231125, 0.0989561676979065]}}
{"id": "WebQTest-1264", "question": "what standard time is michigan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.time_Zones"], ["base.biblioness.bibs_location.state"], ["base.locations.states_and_provences.country"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is michigan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.locations.states_and_provences.country </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-1.5511830497416668e-05, -0.14234153926372528, -0.1743004471063614, -0.18874101340770721, -0.19663096964359283, -0.19800876080989838, -0.20589016377925873, -0.20596015453338623, -0.2142016589641571, -0.21615083515644073], "norm_scores": [0.11879406869411469, 0.10303466022014618, 0.09979384392499924, 0.09836312383413315, 0.09759009629487991, 0.09745573252439499, 0.09669066220521927, 0.09668389707803726, 0.0958903580904007, 0.0957036241889]}}
{"id": "WebQTest-1265", "question": "what was john tyler", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["common.topic.notable_types"], ["government.politician.government_positions_held"], ["people.profession.people_with_this_profession"], ["government.politician.government_positions_held", "government.government_position_held.office_holder"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was john tyler [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.politician.government_positions_held </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.basic_title </PATH>"], "scores": [-0.0033497808035463095, -0.04210095852613449, -0.04941906780004501, -0.0525466725230217, -0.05655032396316528, -0.0699256882071495, -0.07901374995708466, -0.08498398959636688, -0.08573894202709198, -0.08790860325098038], "norm_scores": [0.10591751337051392, 0.10189159959554672, 0.10114866495132446, 0.10083280503749847, 0.10042991489171982, 0.09909557551145554, 0.09819906204938889, 0.09761453419923782, 0.09754086285829544, 0.09732946753501892]}}
{"id": "WebQTest-1266", "question": "what did romo do", "prediction": [["american_football.football_player.position_s"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.position"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.position"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.position_s"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.role"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.position_s"], ["sports.pro_athlete.teams", "sports.sports_position_held.office_position_or_title"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.position", "sports.sports_team_roster.position"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did romo do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.position </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.position </PATH>", "<PATH> american_football.player_passing_statistics.player <SEP> american_football.player_passing_statistics.position_s </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.role </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.position_s </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_position_held.office_position_or_title </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.position <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.013870833441615105, -0.039043400436639786, -0.049780670553445816, -0.05141332000494003, -0.05150500312447548, -0.05322810634970665, -0.05366296321153641, -0.061271727085113525, -0.06190914660692215, -0.06360090523958206], "norm_scores": [0.10366161167621613, 0.10108475387096405, 0.10000519454479218, 0.09984204918146133, 0.09983289241790771, 0.09966102242469788, 0.09961768984794617, 0.09886260330677032, 0.09879959374666214, 0.09863258898258209]}}
{"id": "WebQTest-1267", "question": "what currency can you use in jamaica", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["common.topic.notable_types"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in jamaica [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.country.form_of_government </PATH>", "</PATH> location.country.currency_used </PATH>"], "scores": [-0.0006731897592544556, -0.06144287437200546, -0.16163037717342377, -0.16676616668701172, -0.17316129803657532, -0.18187186121940613, -0.1831613928079605, -0.18411731719970703, -0.18572865426540375, -0.19404351711273193], "norm_scores": [0.11579433083534241, 0.10896709561347961, 0.09857901185750961, 0.0980740338563919, 0.09744884073734283, 0.09660369157791138, 0.09647919237613678, 0.09638701379299164, 0.09623182564973831, 0.0954349935054779]}}
{"id": "WebQTest-1268", "question": "where did jay moriarty die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.place_of_death"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["people.place_of_death.people_deceased_here"], ["people.place_of_death.people_deceased"], ["fictional_universe.fictional_character.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jay moriarty die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_death </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_of_death.people_deceased </PATH>", "<PATH> fictional_universe.fictional_character.place_of_death </PATH>"], "scores": [-0.00026708951918408275, -0.09779299795627594, -0.10678994655609131, -0.11245644092559814, -0.12069671601057053, -0.12081113457679749, -0.12129499018192291, -0.12300895154476166, -0.12361276894807816, -0.1299152970314026], "norm_scores": [0.11104017496109009, 0.10072218626737595, 0.09982005506753922, 0.09925603121519089, 0.09844149649143219, 0.09843023121356964, 0.0983826220035553, 0.09821414202451706, 0.09815485030412674, 0.09753818064928055]}}
{"id": "WebQTest-1269", "question": "where was the boston tea party 1773", "prediction": [["time.event.locations"], ["location.location.events"], ["event.disaster.areas_affected"], ["base.culturalevent.event.entity_involved"], ["book.written_work.subjects"], ["symbols.name_source.namesakes", "location.location.containedby"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["event.disaster.areas_thrown"], ["symbols.namesake.named_after"], ["event.public_event.locations"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the boston tea party 1773 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> symbols.name_source.namesakes <SEP> location.location.containedby </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> event.disaster.areas_thrown </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> event.public_event.locations </PATH>"], "scores": [-0.006603521760553122, -0.02535802498459816, -0.06404642760753632, -0.07595932483673096, -0.10509193688631058, -0.11547046154737473, -0.11647212505340576, -0.12689845263957977, -0.12762866914272308, -0.1332499086856842], "norm_scores": [0.1085621565580368, 0.1065450981259346, 0.1025017648935318, 0.10128790885210037, 0.0983796939253807, 0.09736394137144089, 0.09726647287607193, 0.09625759720802307, 0.09618733078241348, 0.0956481546163559]}}
{"id": "WebQTest-1270", "question": "what drugs lindsay lohan abuse", "prediction": [["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["base.popstra.rehab_stay.patient", "base.popstra.rehab_stay.substance_abused"], ["people.person.substance_abuse", "people.substance_abuse.substance"], ["medicine.disease.notable_people_with_this_condition"], ["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"], ["base.popstra.substance_abuse_problem.celebrity", "base.popstra.substance_abuse_problem.substance"], ["base.popstra.substance_abuse_problem.substance_abuser", "base.popstra.substance_abuse_problem.substance"], ["celebrities.celebrity.substance_abuse_problems", "celebrities.substance_abuse_problem.substance"], ["people.person.substance_abuse_problems", "people.substance_abuse_problem.substance"]], "ground_paths": [["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"], ["base.popstra.rehab_stay.patient", "base.popstra.rehab_stay.substance_abused"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drugs lindsay lohan abuse [/INST]", "raw_output": {"paths": ["<PATH> base.popstra.substance_abuse.abuser <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> base.popstra.celebrity.substance_abuse <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> base.popstra.rehab_stay.patient <SEP> base.popstra.rehab_stay.substance_abused </PATH>", "<PATH> people.person.substance_abuse <SEP> people.substance_abuse.substance </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> celebrities.celebrity.substance_abuse_problems <SEP> celebrities.abused_substance.abused_by </PATH>", "<PATH> base.popstra.substance_abuse_problem.celebrity <SEP> base.popstra.substance_abuse_problem.substance </PATH>", "<PATH> base.popstra.substance_abuse_problem.substance_abuser <SEP> base.popstra.substance_abuse_problem.substance </PATH>", "<PATH> celebrities.celebrity.substance_abuse_problems <SEP> celebrities.substance_abuse_problem.substance </PATH>", "<PATH> people.person.substance_abuse_problems <SEP> people.substance_abuse_problem.substance </PATH>"], "scores": [-0.006915871519595385, -0.025230661034584045, -0.04690903052687645, -0.05387071892619133, -0.0629187747836113, -0.06333870440721512, -0.06382853537797928, -0.0647866502404213, -0.06716902554035187, -0.07700192928314209], "norm_scores": [0.10471487045288086, 0.10281449556350708, 0.10060963034629822, 0.0999116599559784, 0.09901171922683716, 0.09897015243768692, 0.09892168641090393, 0.09882695972919464, 0.09859179705381393, 0.09762709587812424]}}
{"id": "WebQTest-1271", "question": "when did carolina panthers go to superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["sports.sports_championship_event.runner_up"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"], ["american_football.football_team.current_head_coach"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event"], ["american_football.football_team.championships"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did carolina panthers go to superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> american_football.football_team.championships </PATH>"], "scores": [-0.009583335369825363, -0.018160900101065636, -0.038441531360149384, -0.070033498108387, -0.09613900631666183, -0.09924391657114029, -0.12562295794487, -0.1262897402048111, -0.12777790427207947, -0.13201524317264557], "norm_scores": [0.1076512560248375, 0.10673181712627411, 0.10458903014659882, 0.1013365164399147, 0.09872529655694962, 0.09841924160718918, 0.09585698693990707, 0.09579309076070786, 0.09565063565969467, 0.09524618834257126]}}
{"id": "WebQTest-1272", "question": "when did the burma cyclone happen", "prediction": [["meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones"], ["time.event.locations"], ["meteorology.tropical_cyclone_season.tropical_cyclones"], ["meteorology.cyclone_affected_area.cyclones", "time.event.locations"], ["time.event.included_in_event"], ["meteorology.tropical_cyclone_season.cyclones"], ["location.location.events"], ["common.topic.notable_types"], ["meteorology.cyclone_season.cyclones"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the burma cyclone happen [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> meteorology.tropical_cyclone_season.tropical_cyclones </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones <SEP> time.event.locations </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> meteorology.tropical_cyclone_season.cyclones </PATH>", "<PATH> location.location.events </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> meteorology.cyclone_season.cyclones </PATH>"], "scores": [-0.008382835425436497, -0.02072555013000965, -0.049758896231651306, -0.065081387758255, -0.102679543197155, -0.11044088006019592, -0.11493586748838425, -0.12477046251296997, -0.1284732222557068, -0.13056863844394684], "norm_scores": [0.10792198777198792, 0.1065981313586235, 0.10354772210121155, 0.10197320580482483, 0.09821038693189621, 0.0974510908126831, 0.09701402485370636, 0.09606461971998215, 0.09570956230163574, 0.09550922363996506]}}
{"id": "WebQTest-1274", "question": "who won the governor election in texas", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.administrative_division.country", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["location.us_state.capital", "people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.containedby", "symbols.name_source.namesakes"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["film.film_location.featured_in_films", "film.film.subjects"], ["government.governmental_jurisdiction.governing_officials", "government.political_appointer.appointees"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["location.administrative_division.country", "people.person.nationality"], ["people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the governor election in texas [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.024416368454694748, -0.024461956694722176, -0.024809952825307846, -0.028082719072699547, -0.029924822971224785, -0.031141037121415138, -0.04708176851272583, -0.04958925396203995, -0.056605104357004166, -0.056649550795555115], "norm_scores": [0.1012859120965004, 0.1012812927365303, 0.10124606639146805, 0.10091524571180344, 0.10072951763868332, 0.10060708224773407, 0.09901604801416397, 0.09876807779073715, 0.09807756543159485, 0.09807320684194565]}}
{"id": "WebQTest-1275", "question": "what does helen flanagan do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["film.film_crewmember.films_crewed", "film.film_job.films_with_this_crew_job"], ["common.topic.notable_types", "type.type.properties"], ["film.film_crew_gig.crewmember", "film.film_job.films_with_this_crew_job"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["fictional_universe.fictional_character.occupation"], ["common.topic.notable_types", "common.topic.notable_types"], ["common.topic.notable_types", "type.type.expected_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does helen flanagan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> film.film_crewmember.films_crewed <SEP> film.film_job.films_with_this_crew_job </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.properties </PATH>", "<PATH> film.film_crew_gig.crewmember <SEP> film.film_job.films_with_this_crew_job </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>"], "scores": [-0.0010685399174690247, -0.05810233950614929, -0.09847109019756317, -0.11668112128973007, -0.11953437328338623, -0.12296847254037857, -0.13921913504600525, -0.14197129011154175, -0.14275795221328735, -0.14558832347393036], "norm_scores": [0.11124754697084427, 0.10508022457361221, 0.10092274844646454, 0.09910157322883606, 0.09881921112537384, 0.09848043322563171, 0.09689299762248993, 0.09662670642137527, 0.09655071794986725, 0.09627783298492432]}}
{"id": "WebQTest-1276", "question": "when are the summer and winter olympics held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.current_frequency"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["olympics.olympic_games.host_city", "olympics.olympic_games.host_city"], ["olympics.olympic_games.competitions", "olympics.olympic_games.competitions"], ["time.event.instance_of_recurrence"], ["olympics.olympic_games.host_city"], ["time.recurring_event.instances", "time.event.instance_of_recurring_event"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when are the summer and winter olympics held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> olympics.olympic_games.host_city <SEP> olympics.olympic_games.host_city </PATH>", "<PATH> olympics.olympic_games.competitions <SEP> olympics.olympic_games.competitions </PATH>", "<PATH> time.event.instance_of_recurrence </PATH>", "<PATH> olympics.olympic_games.host_city </PATH>", "<PATH> time.recurring_event.instances <SEP> time.event.instance_of_recurring_event </PATH>"], "scores": [-0.008637996390461922, -0.017423920333385468, -0.08646771311759949, -0.09892641752958298, -0.10228577256202698, -0.10352268815040588, -0.10390368103981018, -0.11043912172317505, -0.11302287876605988, -0.11616167426109314], "norm_scores": [0.1079750582575798, 0.10703055560588837, 0.0998900979757309, 0.09865330904722214, 0.09832245111465454, 0.09820091724395752, 0.09816350787878036, 0.09752406179904938, 0.09727241098880768, 0.09696756303310394]}}
{"id": "WebQTest-1277", "question": "where is tennessee river", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.river.basin_countries"], ["location.location.containedby"], ["location.administrative_division.first_level_division_of"], ["location.administrative_division.country"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partly_containedby"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tennessee river [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partly_containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.004763987846672535, -0.034017499536275864, -0.07494949549436569, -0.09107732027769089, -0.13874098658561707, -0.1435394585132599, -0.15108446776866913, -0.1539040207862854, -0.15853238105773926, -0.16105449199676514], "norm_scores": [0.11106370389461517, 0.10786176472902298, 0.10353591293096542, 0.10187949240207672, 0.0971374586224556, 0.09667246043682098, 0.0959458127617836, 0.09567566215991974, 0.09523385763168335, 0.0949939712882042]}}
{"id": "WebQTest-1278", "question": "what are some major events in venezuela", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.events"], ["government.form_of_government.countries"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["event.disaster.areas_affected"], ["time.recurring_event.current_frequency"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["book.written_work.subjects"]], "ground_paths": [["location.location.events"], ["base.culturalevent.event.entity_involved"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some major events in venezuela [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.01644010655581951, -0.022174106910824776, -0.024960443377494812, -0.07255370914936066, -0.09125752747058868, -0.10439785569906235, -0.10681309551000595, -0.1122199222445488, -0.11321674287319183, -0.11570118367671967], "norm_scores": [0.10626470297574997, 0.10565713047981262, 0.1053631380200386, 0.10046602785587311, 0.09860438853502274, 0.0973171666264534, 0.09708241373300552, 0.09655892103910446, 0.09646271169185638, 0.09622335433959961]}}
{"id": "WebQTest-1279", "question": "what guitar does corey taylor play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"], ["music.musical_group.member", "music.group_membership.group"], ["common.topic.image", "music.group_member.instruments_played"], ["music.group_member.instruments_played", "music.group_member.instruments_played"], ["music.group_membership.group", "music.group_membership.guitar"], ["common.topic.notable_types"], ["music.music_video.artist", "music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar does corey taylor play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> common.topic.image <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.guitar </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.music_video.artist <SEP> music.group_member.instruments_played </PATH>"], "scores": [-0.00782757718116045, -0.03207268938422203, -0.05181233584880829, -0.06810171902179718, -0.07314690947532654, -0.08545834571123123, -0.08691076934337616, -0.09349007159471512, -0.0976727083325386, -0.09892401099205017], "norm_scores": [0.10632116347551346, 0.10377439111471176, 0.10174600780010223, 0.10010205954313278, 0.09959829598665237, 0.09837961196899414, 0.09823682904243469, 0.09759262204170227, 0.09718527644872665, 0.09706374257802963]}}
{"id": "WebQTest-1280", "question": "who was president after franklin d. roosevelt", "prediction": [["government.us_president.vice_president"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["government.us_vice_president.to_president"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "people.person.nationality"], ["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after franklin d. roosevelt [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> freebase.type_profile.equivalent_topic <SEP> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>"], "scores": [-0.01926257088780403, -0.02379138208925724, -0.032543644309043884, -0.03596577048301697, -0.045296937227249146, -0.045332346111536026, -0.04538910463452339, -0.04865943267941475, -0.05109130218625069, -0.051161471754312515], "norm_scores": [0.1020740196108818, 0.10161279141902924, 0.10072733461856842, 0.1003832146525383, 0.09945088624954224, 0.0994473546743393, 0.09944171458482742, 0.09911704063415527, 0.09887629002332687, 0.09886935353279114]}}
{"id": "WebQTest-1281", "question": "where did kate gosselin grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["tv.tv_personality.tv_regular_appearances", "tv.tv_regular_personal_appearance.place_of_birth"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.place_lived.person", "people.place_lived.location"], ["tv.tv_personality.tv_regular_appearances", "tv.tv_regular_personal_appearance.location"], ["people.person.places_lived", "people.place_lived.location"], ["tv.tv_personality.tv_regular_appearances", "tv.tv_regular_personal_appearance.tv_regular_appearances"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kate gosselin grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> tv.tv_personality.tv_regular_appearances <SEP> tv.tv_regular_personal_appearance.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> tv.tv_personality.tv_regular_appearances <SEP> tv.tv_regular_personal_appearance.location </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> tv.tv_personality.tv_regular_appearances <SEP> tv.tv_regular_personal_appearance.tv_regular_appearances </PATH>"], "scores": [-0.0022413863334804773, -0.03995385020971298, -0.09999141842126846, -0.1320488154888153, -0.13592977821826935, -0.14329101145267487, -0.14384758472442627, -0.15152762830257416, -0.15724946558475494, -0.16062375903129578], "norm_scores": [0.1119738519191742, 0.10782967507839203, 0.10154634714126587, 0.098342664539814, 0.09796174615621567, 0.09724327176809311, 0.09718916565179825, 0.09644559770822525, 0.09589533507823944, 0.09557229280471802]}}
{"id": "WebQTest-1282", "question": "under what political system does the united states government operate", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["government.form_of_government.countries", "location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["base.locations.countries.planet", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.main_country", "location.country.form_of_government"], ["base.locations.countries.planet", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: under what political system does the united states government operate [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.planet <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.0047508724965155125, -0.0232615377753973, -0.1274663209915161, -0.1279774010181427, -0.1439981609582901, -0.1501525640487671, -0.15209078788757324, -0.15218138694763184, -0.1590290069580078, -0.16062285006046295], "norm_scores": [0.11206300556659698, 0.11000772565603256, 0.09912144392728806, 0.09907079488039017, 0.09749625623226166, 0.09689807146787643, 0.0967104360461235, 0.09670168161392212, 0.09604176878929138, 0.09588880836963654]}}
{"id": "WebQTest-1284", "question": "what to do in paris in 1 day", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.geographic_scope"], ["metropolitan_transit.transit_system.area_served"], ["film.film.featured_film_locations"], ["film.film_festival.location"], ["transportation.bridge.locale"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in paris in 1 day [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> transportation.bridge.locale </PATH>"], "scores": [-0.008519328199326992, -0.020289132371544838, -0.039119310677051544, -0.10794553905725479, -0.11880607157945633, -0.1192004531621933, -0.12080950289964676, -0.1255788803100586, -0.12570956349372864, -0.13090501725673676], "norm_scores": [0.10855570435523987, 0.1072855144739151, 0.1052842065691948, 0.09828163683414459, 0.0972200259566307, 0.09718168526887894, 0.09702544659376144, 0.09656379371881485, 0.09655117988586426, 0.09605084359645844]}}
{"id": "WebQTest-1285", "question": "what city is mt lassen in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_range"], ["geography.lake.outflow"], ["geography.lake.basin_county"], ["geography.lake.basin_countries"], ["location.location.partially_containedby"], ["location.location.containedby", "location.location.containedby"], ["geography.mountain_range.mountains"], ["geography.lake.cities"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is mt lassen in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> geography.lake.outflow </PATH>", "<PATH> geography.lake.basin_county </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> geography.lake.cities </PATH>"], "scores": [-0.005568669177591801, -0.03002406843006611, -0.10752695798873901, -0.11752540618181229, -0.12141451984643936, -0.12522897124290466, -0.12940014898777008, -0.13416534662246704, -0.13602225482463837, -0.13798727095127106], "norm_scores": [0.11028575152158737, 0.10762138664722443, 0.09959544986486435, 0.09860461950302124, 0.09822187572717667, 0.09784792363643646, 0.09744063019752502, 0.09697742015123367, 0.09679750353097916, 0.09660747647285461]}}
{"id": "WebQTest-1287", "question": "what is the state motto of ohio", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.administrative_division"], ["location.location_symbol_relationship.administrative_division", "government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state motto of ohio [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.administrative_division </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.017287764698266983, -0.01927926577627659, -0.021636860445141792, -0.023688605055212975, -0.025764238089323044, -0.04076661542057991, -0.042121320962905884, -0.07399196177721024, -0.0753711387515068, -0.07692629843950272], "norm_scores": [0.10244152694940567, 0.10223771631717682, 0.10199696570634842, 0.10178790241479874, 0.10157685726881027, 0.10006433725357056, 0.09992887079715729, 0.09679428488016129, 0.09666088968515396, 0.09651067852973938]}}
{"id": "WebQTest-1289", "question": "what type of music did mozart composed", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.composition.composer"], ["book.author.works_written"], ["music.composer.genre"], ["music.composer.compositions"], ["book.written_work.author"], ["music.music_genre.artists"], ["music.composition.form"], ["music.album.genre"]], "ground_paths": [["music.artist.genre"], ["music.genre.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did mozart composed [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.composer.genre </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> music.music_genre.artists </PATH>", "<PATH> music.composition.form </PATH>", "<PATH> music.album.genre </PATH>"], "scores": [-0.005494080483913422, -0.030637603253126144, -0.06738100200891495, -0.09009990096092224, -0.10777980834245682, -0.11724889278411865, -0.12330290675163269, -0.12509533762931824, -0.12697453796863556, -0.13051500916481018], "norm_scores": [0.10898831486701965, 0.10628212988376617, 0.10244783759117126, 0.1001465767621994, 0.09839154779911041, 0.09746427088975906, 0.09687599539756775, 0.09670250862836838, 0.09652096033096313, 0.09617983549833298]}}
{"id": "WebQTest-1291", "question": "how many kids were there in the kennedy family", "prediction": [["people.person.parents"], ["people.person.children"], ["people.family.members"], ["people.person.nationality", "people.person.nationality"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.sibling_s", "people.person.sibling_s"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.family_member.family"], ["people.person.place_of_birth", "location.location.people_born_here"], ["people.person.parent"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many kids were there in the kennedy family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.family.members </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.sibling_s <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.person.sibling_s <SEP> people.person.sibling_s </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.place_of_birth </PATH>", "<PATH> people.family_member.family </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.people_born_here </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.00952148623764515, -0.017220711335539818, -0.08601215481758118, -0.12958556413650513, -0.1331559121608734, -0.13550259172916412, -0.136712908744812, -0.13833948969841003, -0.15798446536064148, -0.16273681819438934], "norm_scores": [0.1104903295636177, 0.10964290797710419, 0.10235399752855301, 0.09798984974622726, 0.09764062613248825, 0.09741175174713135, 0.097293920814991, 0.09713579714298248, 0.09524619579315186, 0.0947946235537529]}}
{"id": "WebQTest-1292", "question": "which barcelona airport is closest to the port", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transport_terminus.travel_destinations_served", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"], ["location.location.containedby"], ["travel.transport_terminus.travel_destinations_served", "travel.travel_destination.tourist_attractions"], ["travel.transport_terminus.port_served", "travel.transport_terminus.travel_destinations_served"], ["travel.transport_terminus.travel_destinations_served", "travel.travel_destination.tour_operators"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which barcelona airport is closest to the port [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transport_terminus.travel_destinations_served <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transport_terminus.travel_destinations_served <SEP> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.transport_terminus.port_served <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transport_terminus.travel_destinations_served <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.006115391384810209, -0.02913684770464897, -0.04990511015057564, -0.0601748488843441, -0.06226028501987457, -0.07488003373146057, -0.08093822747468948, -0.08710168302059174, -0.08757548034191132, -0.09439676254987717], "norm_scores": [0.10584128648042679, 0.10343249887228012, 0.10130654275417328, 0.10027147084474564, 0.10006257891654968, 0.09880775213241577, 0.09821096807718277, 0.09760750085115433, 0.09756126999855042, 0.09689804911613464]}}
{"id": "WebQTest-1293", "question": "where has the english language evolved from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"], ["language.language_family.languages"], ["common.topic.notable_types"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where has the english language evolved from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.00608193501830101, -0.02575216442346573, -0.06338724493980408, -0.0884149968624115, -0.10696057230234146, -0.1105700135231018, -0.11390655487775803, -0.11723567545413971, -0.11997377127408981, -0.12029172480106354], "norm_scores": [0.10837015509605408, 0.10625932365655899, 0.10233455896377563, 0.09980513900518417, 0.09797126054763794, 0.09761827439069748, 0.09729310125112534, 0.09696974605321884, 0.09670460224151611, 0.09667385369539261]}}
{"id": "WebQTest-1294", "question": "what country did hitler invade that started ww2", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["military.military_command.military_conflict", "military.military_command.military_combatant"], ["military.military_command.military_commander", "military.military_command.military_combatant"], ["military.military_commander.military_commands", "military.military_command.military_combatant"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_conflict.military_personnel_involved"], ["military.military_person.participated_in_conflicts", "people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did hitler invade that started ww2 [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts <SEP> military.military_person.participated_in_conflicts </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_command.military_combatant </PATH>", "<PATH> military.military_command.military_commander <SEP> military.military_command.military_combatant </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_command.military_combatant </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts <SEP> people.person.nationality </PATH>"], "scores": [-0.028920026496052742, -0.031402699649333954, -0.03459366410970688, -0.036776650696992874, -0.038647908717393875, -0.0476297102868557, -0.047663431614637375, -0.04795723408460617, -0.0533454567193985, -0.05393645539879799], "norm_scores": [0.10132165998220444, 0.10107042640447617, 0.10074841976165771, 0.10052873194217682, 0.10034079104661942, 0.09944357722997665, 0.09944022446870804, 0.0994110181927681, 0.09887681156396866, 0.09881839156150818]}}
{"id": "WebQTest-1295", "question": "what does matt damon play in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["music.recording.featured_artists", "music.recording.releases"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.producer.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["tv.tv_program.program_creator"], ["film.actor.film", "film.film.starring"], ["film.film.written_by"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.performance.film"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "tv.tv_program.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does matt damon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.02236633561551571, -0.022455627098679543, -0.04453682526946068, -0.044826608151197433, -0.045118555426597595, -0.046237923204898834, -0.04648030921816826, -0.04787355661392212, -0.05011676996946335, -0.05503297969698906], "norm_scores": [0.10202860832214355, 0.10201950371265411, 0.09979148209095001, 0.09976255893707275, 0.09973344206809998, 0.09962186962366104, 0.09959772974252701, 0.09945905208587646, 0.09923619776964188, 0.09874952584505081]}}
{"id": "WebQTest-1296", "question": "what was scottie pippen known for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"], ["sports.sports_award_winner.awards", "sports.sports_award.category"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position_s"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"], ["people.person.profession"], ["sports.sports_award.award_winner", "sports.sports_award.category"], ["sports.pro_athlete.teams", "sports.sports_position.teams"], ["sports.sports_award_winner.awards", "sports.sports_award.position_s"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen known for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.category </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position_s </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.category </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_position.teams </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.position_s </PATH>"], "scores": [-0.015387666411697865, -0.02712736837565899, -0.033279579132795334, -0.045720286667346954, -0.05087544769048691, -0.05386895313858986, -0.057041820138692856, -0.06089617684483528, -0.06381309032440186, -0.07636505365371704], "norm_scores": [0.10334432870149612, 0.10213818401098251, 0.10151173919439316, 0.10025668144226074, 0.09974117577075958, 0.09944304823875427, 0.09912803769111633, 0.09874669462442398, 0.09845907241106033, 0.0972309485077858]}}
{"id": "WebQTest-1297", "question": "what continent does canada belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.contains"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["base.locations.countries.countries_within"], ["location.location.primarily_containedby"], ["location.location.partially_containedby"], ["book.newspaper.circulation_areas"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.contains"], ["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does canada belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> base.locations.countries.countries_within </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.016724200919270515, -0.01952146366238594, -0.027407361194491386, -0.08320414274930954, -0.12568211555480957, -0.13703051209449768, -0.14577236771583557, -0.1489918828010559, -0.1503135859966278, -0.15251199901103973], "norm_scores": [0.10859302431344986, 0.1082896888256073, 0.1074390858411789, 0.101608507335186, 0.09738276898860931, 0.09628387540578842, 0.09544584900140762, 0.0951390489935875, 0.09501338750123978, 0.09480473399162292]}}
{"id": "WebQTest-1298", "question": "what does george wilson do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["fictional_universe.fictional_character.occupation", "common.topic.notable_types"], ["common.topic.notable_types", "type.type.properties"], ["fictional_universe.fictional_character.occupation", "people.person.profession"], ["fictional_universe.character_occupation.characters_with_this_occupation", "common.topic.notable_types"], ["fictional_universe.fictional_setting.characters_with_this_occupation"], ["people.profession.people_with_this_profession"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does george wilson do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.properties </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation <SEP> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_setting.characters_with_this_occupation </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.004882907494902611, -0.03157437965273857, -0.06618353724479675, -0.06684982776641846, -0.07335299998521805, -0.0794246569275856, -0.08151671290397644, -0.08437761664390564, -0.08750728517770767, -0.08790290355682373], "norm_scores": [0.10630439966917038, 0.10350451618432999, 0.0999835878610611, 0.09991700202226639, 0.0992693230509758, 0.0986684188246727, 0.09846222400665283, 0.09818092733621597, 0.09787413477897644, 0.09783542156219482]}}
{"id": "WebQTest-1300", "question": "what city does fulham play in", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["location.location.containedby"], ["sports.sports_team.location", "location.location.containedby"], ["sports.sports_team_location.teams", "location.location.containedby"], ["base.biblioness.bibs_location.country"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby", "location.location.containedby"], ["sports.sports_team.location", "base.biblioness.bibs_location.country"], ["sports.sports_team.arena_stadium"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does fulham play in [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> sports.sports_team.location <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_team_location.teams <SEP> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_team.location <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>"], "scores": [-0.023446891456842422, -0.02387947030365467, -0.028348153457045555, -0.03129952400922775, -0.08136114478111267, -0.08238854259252548, -0.0834856927394867, -0.08714675158262253, -0.09060438722372055, -0.09497670829296112], "norm_scores": [0.10395680367946625, 0.10391184687614441, 0.1034485325217247, 0.10314366966485977, 0.098107248544693, 0.09800650179386139, 0.09789903461933136, 0.09754127264022827, 0.09720459580421448, 0.09678050875663757]}}
{"id": "WebQTest-1301", "question": "what are fun things to do in sacramento ca", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["base.usnationalparks.us_national_park.state"], ["base.arthist.helynevek.intezmeny"], ["film.film_festival.location"], ["organization.organization.geographic_scope"], ["film.film.featured_film_locations"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are fun things to do in sacramento ca [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination <PATH>"], "scores": [-0.01490082498639822, -0.01664900779724121, -0.025913655757904053, -0.10804308205842972, -0.14230024814605713, -0.15923283994197845, -0.16095824539661407, -0.16118985414505005, -0.16426438093185425, -0.1719689518213272], "norm_scores": [0.11003200709819794, 0.10983981937170029, 0.1088268905878067, 0.10024619102478027, 0.09687019139528275, 0.09524373710155487, 0.09507954865694046, 0.09505753219127655, 0.09476572275161743, 0.09403839707374573]}}
{"id": "WebQTest-1303", "question": "what other movies is josh hutcherson in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.producer.films_executive_produced"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies is josh hutcherson in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.017639119178056717, -0.03487065061926842, -0.03601221740245819, -0.03777386620640755, -0.039074063301086426, -0.0408441461622715, -0.041988905519247055, -0.042347099632024765, -0.04361553117632866, -0.04588523134589195], "norm_scores": [0.10205456614494324, 0.10031107813119888, 0.10019662976264954, 0.10002027451992035, 0.0998903214931488, 0.09971366077661514, 0.09959957003593445, 0.09956390410661697, 0.09943769127130508, 0.09921225905418396]}}
{"id": "WebQTest-1304", "question": "what was hitler the leader of", "prediction": [["organization.organization_founder.organizations_founded"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["organization.organization.founders"], ["base.activism.activist.area_of_activism", "government.governmental_jurisdiction.government_positions"], ["base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism", "base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism"], ["base.activism.activist.area_of_activism", "government.government_office_or_title.office_holders"]], "ground_paths": [["symbols.name_source.namesakes"], ["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was hitler the leader of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activism_issue.activists </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.017575658857822418, -0.03505938872694969, -0.04101511463522911, -0.04240081459283829, -0.04460278898477554, -0.04506715014576912, -0.048540323972702026, -0.05286748334765434, -0.05340869724750519, -0.057931140065193176], "norm_scores": [0.10265588760375977, 0.10087668150663376, 0.10027766972780228, 0.10013881325721741, 0.09991855174303055, 0.0998721569776535, 0.09952589124441147, 0.0990961566567421, 0.09904254227876663, 0.09859564155340195]}}
{"id": "WebQTest-1305", "question": "what awards did marilyn monroe won", "prediction": [["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_winner.awards_won", "award.award_category.winners"], ["people.person.profession", "award.award_discipline.awards_in_this_discipline"], ["award.award_nominee.award_nominations", "award.award_category.nominees"], ["award.award_nomination.award_nominee", "award.award_category.nominees"], ["award.award_nominee.award_nominations", "award.award_honor.award"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_category.nominees"], ["award.award_honor.award_winner", "award.award_honor.award"], ["people.person.profession", "award.award_discipline.awards_in_this_discipline"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nomination.award_nominee", "award.award_category.nominees"], ["award.award_winner.awards_won", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards did marilyn monroe won [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_category.winners </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_category.winners </PATH>", "<PATH> people.person.profession <SEP> award.award_discipline.awards_in_this_discipline </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_category.nominees </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_category.nominees </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_honor.award </PATH>"], "scores": [-0.019685711711645126, -0.023720460012555122, -0.02638038992881775, -0.028648648411035538, -0.031811658293008804, -0.03441217541694641, -0.042577631771564484, -0.044206470251083374, -0.05030946806073189, -0.09825748950242996], "norm_scores": [0.10202907025814056, 0.10161823034286499, 0.10134828835725784, 0.10111866891384125, 0.10079934448003769, 0.10053754597902298, 0.09971995651721954, 0.09955766052007675, 0.09895191341638565, 0.09431931376457214]}}
{"id": "WebQTest-1306", "question": "what kind of government does iran have after 1979", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["people.person.nationality", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["political_form.countries.form_of_government"], ["people.person.nationality", "location.country.form_of_government"], ["base.locations.countries.continent"], ["political_government.form_of_government.countries"], ["form.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does iran have after 1979 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> political_form.countries.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> political_government.form_of_government.countries </PATH>", "<PATH> form.form_of_government.countries </PATH>"], "scores": [-0.004288341384381056, -0.02289487235248089, -0.13172778487205505, -0.13786806166172028, -0.1534470021724701, -0.15476146340370178, -0.15545371174812317, -0.15783807635307312, -0.16183578968048096, -0.16347023844718933], "norm_scores": [0.1125740185379982, 0.11049877852201462, 0.09910416603088379, 0.09849750995635986, 0.09697490930557251, 0.0968475341796875, 0.09678050130605698, 0.09655002504587173, 0.09616481512784958, 0.09600776433944702]}}
{"id": "WebQTest-1307", "question": "where did harper lee attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here", "location.location.containedby"], ["common.topic.notable_types"], ["people.person.place_of_birth", "base.biblioness.bibs_location.state"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harper lee attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.015652086585760117, -0.021571079269051552, -0.022000467404723167, -0.030431795865297318, -0.05140704661607742, -0.058970268815755844, -0.0980762466788292, -0.10143540054559708, -0.1152065172791481, -0.11973673850297928], "norm_scores": [0.10481470078229904, 0.10419613122940063, 0.1041514053940773, 0.10327696055173874, 0.10113326460123062, 0.10037124902009964, 0.09652189165353775, 0.09619820863008499, 0.094882532954216, 0.09445365518331528]}}
{"id": "WebQTest-1308", "question": "what college did jrr tolkien go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.employment_history", "business.employment_tenure.company"], ["education.education.student", "education.education.institution"], ["common.topic.notable_types"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.contains"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did jrr tolkien go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>"], "scores": [-0.015510018914937973, -0.022395582869648933, -0.03081752173602581, -0.032113026827573776, -0.038368258625268936, -0.04793664440512657, -0.07281271368265152, -0.0915064886212349, -0.0920223742723465, -0.0951756089925766], "norm_scores": [0.1038651242852211, 0.10315240919589996, 0.10228731483221054, 0.10215489566326141, 0.10151788592338562, 0.10055115818977356, 0.09808069467544556, 0.09626422822475433, 0.09621457755565643, 0.0959116667509079]}}
{"id": "WebQTest-1309", "question": "what art did wassily kandinsky do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"], ["book.author.school_or_movement"], ["book.author.school_or_movement", "visual_art.visual_artist.associated_periods_or_movements"], ["book.school_or_movement.associated_authors"], ["book.author.school_or_movement", "visual_art.art_period_movement.associated_artists"], ["book.written_work.author"], ["book.author.school_or_movement", "book.school_or_movement.associated_authors"], ["visual_art.art_period_movement.artist"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art did wassily kandinsky do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> book.author.school_or_movement </PATH>", "<PATH> book.author.school_or_movement <SEP> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> book.school_or_movement.associated_authors </PATH>", "<PATH> book.author.school_or_movement <SEP> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.school_or_movement <SEP> book.school_or_movement.associated_authors </PATH>", "<PATH> visual_art.art_period_movement.artist </PATH>"], "scores": [-0.007970496080815792, -0.027703937143087387, -0.04304850846529007, -0.04693087935447693, -0.0646989718079567, -0.0702531635761261, -0.07597921043634415, -0.07952947914600372, -0.08408701419830322, -0.08828313648700714], "norm_scores": [0.10518590360879898, 0.10313057154417038, 0.10156016796827316, 0.10116662830114365, 0.09938496351242065, 0.09883448481559753, 0.09827017784118652, 0.09792190790176392, 0.09747664630413055, 0.09706847369670868]}}
{"id": "WebQTest-1310", "question": "who won the league cup in 2002", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.runner_up"], ["sports.sports_award.award", "sports.sports_award.award_winner"], ["sports.sports_award_type.winners", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship"]], "ground_paths": [["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship.events", "sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the league cup in 2002 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_award.award <SEP> sports.sports_award_winner.awards </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_award.award <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_award_type.winners <SEP> sports.sports_award_winner.awards </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.021649733185768127, -0.023159652948379517, -0.025568615645170212, -0.029018593952059746, -0.03260289877653122, -0.03325771167874336, -0.0384407564997673, -0.03976906090974808, -0.05064396560192108, -0.05553935095667839], "norm_scores": [0.1013346016407013, 0.10118170827627182, 0.10093826055526733, 0.10059062391519547, 0.10023072361946106, 0.10016511380672455, 0.09964729100465775, 0.09951502084732056, 0.09843866527080536, 0.09795795381069183]}}
{"id": "WebQTest-1311", "question": "what was antoni gaudi inspired by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["visual_art.visual_artist.art_forms"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["visual_art.art_period_movement.associated_artists"], ["symbols.namesake.named_after"], ["visual_art.visual_artist.associated_periods_or_movements"], ["architecture.architect.structures_designed"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was antoni gaudi inspired by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.010441498830914497, -0.013567473739385605, -0.07536400854587555, -0.09755071252584457, -0.1021471843123436, -0.10488399118185043, -0.11029835045337677, -0.12298949807882309, -0.12699177861213684, -0.12997514009475708], "norm_scores": [0.10812339931726456, 0.10778594017028809, 0.10132677108049393, 0.0991034135222435, 0.09864894300699234, 0.09837932884693146, 0.09784810245037079, 0.09661415219306946, 0.09622824937105179, 0.09594158083200455]}}
{"id": "WebQTest-1312", "question": "in what country do people speak danish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["language.human_language.region"], ["base.rosetta.local_name.locale"], ["common.topic.notable_types"], ["language.human_language.language_family"], ["language.human_language.country"], ["book.written_work.subjects"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in what country do people speak danish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.01048689428716898, -0.019100205972790718, -0.04687870666384697, -0.08736218512058258, -0.11716525256633759, -0.12152205407619476, -0.1425868272781372, -0.14827387034893036, -0.15532757341861725, -0.15541909635066986], "norm_scores": [0.10925149917602539, 0.10831452161073685, 0.10534711927175522, 0.1011674702167511, 0.0981968566775322, 0.09776996821165085, 0.09573200345039368, 0.09518911689519882, 0.09452004730701447, 0.09451138973236084]}}
{"id": "WebQTest-1313", "question": "who were demeter 's siblings", "prediction": [["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.sibling_relationship.sibling", "fictional_universe.sibling_relationship.sibling"], ["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.children"], ["common.topic.notable_types", "fictional_universe.fictional_character.species"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship.sibling"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.romantic_involvement.partner", "fictional_universe.fictional_character.romantically_involved_with"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "fictional_universe.fictional_character.species"], ["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were demeter 's siblings [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.fictional_character.siblings </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.sibling_relationship_of_fictional_characters.siblings </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.sibling_relationship.sibling <SEP> fictional_universe.sibling_relationship.sibling </PATH>", "<PATH> fictional_universe.fictional_character.children <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> common.topic.notable_types <SEP> fictional_universe.fictional_character.species </PATH>", "<PATH> fictional_universe.sibling_relationship_of_fictional_characters.siblings <SEP> fictional_universe.fictional_character.siblings </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.sibling_relationship.sibling </PATH>"], "scores": [-0.021316591650247574, -0.024597231298685074, -0.025757357478141785, -0.027579769492149353, -0.03469911590218544, -0.042364634573459625, -0.04840727895498276, -0.048417702317237854, -0.04901359602808952, -0.05394205451011658], "norm_scores": [0.10163584351539612, 0.10130296647548676, 0.10118550807237625, 0.10100127011537552, 0.10028477013111115, 0.09951896965503693, 0.09891942143440247, 0.09891839325428009, 0.09885945916175842, 0.09837343543767929]}}
{"id": "WebQTest-1314", "question": "where should a family stay in paris", "prediction": [["travel.travel_destination.accommodation"], ["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["book.newspaper.circulation_areas", "travel.travel_destination.accommodation"], ["film.film.featured_film_locations"], ["base.aareas.schema.administrative_area.administrative_parent"], ["book.book_subject.works", "travel.travel_destination.accommodation"], ["travel.accommodation.accommodation"], ["base.aareas.schema.administrative_area.administrative_children"], ["book.book_subject.works", "book.book_subject.works"]], "ground_paths": [["travel.travel_destination.accommodation"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where should a family stay in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> travel.travel_destination.accommodation </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> book.book_subject.works <SEP> travel.travel_destination.accommodation </PATH>", "<PATH> travel.accommodation.accommodation </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> book.book_subject.works <SEP> book.book_subject.works </PATH>"], "scores": [-0.0036800780799239874, -0.03282957524061203, -0.0973774641752243, -0.13018479943275452, -0.1429462432861328, -0.14349564909934998, -0.14659690856933594, -0.148954376578331, -0.15176531672477722, -0.1518918126821518], "norm_scores": [0.11162292957305908, 0.10841614753007889, 0.10163918882608414, 0.09835877269506454, 0.09711155295372009, 0.09705821424722672, 0.0967576801776886, 0.09652984887361526, 0.09625888615846634, 0.09624671190977097]}}
{"id": "WebQTest-1316", "question": "what was the name of benjamin franklin wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of benjamin franklin wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>"], "scores": [-0.02702263370156288, -0.033836908638477325, -0.039058711379766464, -0.04028607904911041, -0.04063112661242485, -0.042793530970811844, -0.04374747723340988, -0.046702370047569275, -0.04721228405833244, -0.04868769273161888], "norm_scores": [0.10140533745288849, 0.10071668028831482, 0.100192129611969, 0.10006923228502274, 0.10003471374511719, 0.0998186245560646, 0.0997234508395195, 0.09942921996116638, 0.09937853366136551, 0.0992320105433464]}}
{"id": "WebQTest-1317", "question": "what the largest city in spain", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.capital"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.country.administrative_divisions"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the largest city in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.020570583641529083, -0.02697545848786831, -0.04168399050831795, -0.047272395342588425, -0.04759342968463898, -0.04772762209177017, -0.049347102642059326, -0.07225421816110611, -0.08079052716493607, -0.089422807097435], "norm_scores": [0.10320761799812317, 0.10254870355129242, 0.10105139762163162, 0.10048826038837433, 0.10045600682497025, 0.10044252872467041, 0.10027998685836792, 0.09800897538661957, 0.09717590361833572, 0.09634065628051758]}}
{"id": "WebQTest-1319", "question": "who has pudge rodriguez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_coach.sports_coached", "sports.sports_team.sport"], ["sports.sports_team_coach_tenure.coach", "sports.sports_team.coaches"], ["sports.sports_team_coach.teams_coached", "sports.sports_team.coaches"], ["sports.sport.team_coaches", "sports.sports_team.sport"], ["batting.batting_statistics.player", "batting.batting_statistics.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has pudge rodriguez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.sports_team_coach_tenure.coach <SEP> sports.sports_team.coaches </PATH>", "<PATH> sports.sports_team_coach.teams_coached <SEP> sports.sports_team.coaches </PATH>", "<PATH> sports.sport.team_coaches <SEP> sports.sports_team.sport </PATH>", "<PATH> batting.batting_statistics.player <SEP> batting.batting_statistics.team </PATH>"], "scores": [-0.011714472435414791, -0.01723507046699524, -0.029265420511364937, -0.032687172293663025, -0.06849656254053116, -0.07213802635669708, -0.07419634610414505, -0.08271463215351105, -0.08275308459997177, -0.08446738868951797], "norm_scores": [0.10444238036870956, 0.10386737436056137, 0.10262531042098999, 0.10227475315332413, 0.0986771509051323, 0.09831847995519638, 0.09811630845069885, 0.09728407859802246, 0.0972803384065628, 0.0971137136220932]}}
{"id": "WebQTest-1320", "question": "what is the milwaukee brewers stadium called", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], [], ["sports.sports_team.arena_stadium", "sports.sports_facility.teams"], ["sports.sports_facility.teams", "sports.sports_team.arena_stadium"], ["sports.sports_facility.teams", "sports.sports_facility.teams"], ["sports.sports_team.arena_stadium", "sports.sports_team.arena_stadium"], ["location.location.containedby"], ["sports.professional_sports_team.arena_stadium"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers stadium called [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> </PATH>", "<PATH> sports.sports_team.arena_stadium <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> sports.professional_sports_team.arena_stadium </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.002930466551333666, -0.030664902180433273, -0.11443173140287399, -0.11781515926122665, -0.12122432887554169, -0.12141808867454529, -0.13032680749893188, -0.13570259511470795, -0.13764353096485138, -0.13816523551940918], "norm_scores": [0.11063379049301147, 0.10760758817195892, 0.09896085411310196, 0.09862659126520157, 0.09829092770814896, 0.09827189147472382, 0.09740028530359268, 0.09687808901071548, 0.09669024497270584, 0.09663981199264526]}}
{"id": "WebQTest-1321", "question": "where did the gallipoli war take place", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["book.written_work.subjects"], ["military.military_conflict.casualties"], ["language.human_language.countries_spoken_in"], ["book.book_subject.works"], ["military.military_command.military_conflict"], ["film.film_subject.films"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gallipoli war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> military.military_conflict.casualties </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> military.military_command.military_conflict </PATH>", "<PATH> film.film_subject.films </PATH>"], "scores": [-0.007837396115064621, -0.022083261981606483, -0.09108670800924301, -0.11977571249008179, -0.12614338099956512, -0.12715966999530792, -0.1494813710451126, -0.14986972510814667, -0.14994904398918152, -0.1501941829919815], "norm_scores": [0.110541433095932, 0.10897783190011978, 0.10171157121658325, 0.0988350361585617, 0.09820768237113953, 0.09810791909694672, 0.09594225138425827, 0.09590499848127365, 0.09589739143848419, 0.0958738848567009]}}
{"id": "WebQTest-1322", "question": "what other movies has tom hardy played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.languages", "film.film.language"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.actor.film", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has tom hardy played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.022687362506985664, -0.034997548907995224, -0.03854313865303993, -0.03864587843418121, -0.0395495630800724, -0.04030352830886841, -0.04088796675205231, -0.04103980213403702, -0.04277082160115242, -0.043291062116622925], "norm_scores": [0.10156901180744171, 0.10032633692026138, 0.09997124969959259, 0.09996098279953003, 0.09987068921327591, 0.09979541599750519, 0.09973710775375366, 0.09972196817398071, 0.09954950213432312, 0.0994977280497551]}}
{"id": "WebQTest-1323", "question": "what to visit in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"], ["olympics.olympic_host_city.olympics_hosted"], ["olympics.olympic_participating_country.olympics_participated_in"], ["transportation.bridge.locale"], ["geography.lake.lake_type"], ["geography.lake.cities"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> olympics.olympic_host_city.olympics_hosted </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> transportation.bridge.locale </PATH>", "<PATH> geography.lake.lake_type </PATH>", "<PATH> geography.lake.cities </PATH>"], "scores": [-0.015414688736200333, -0.01858852244913578, -0.026386721059679985, -0.11720141768455505, -0.1283053159713745, -0.13514430820941925, -0.13756678998470306, -0.1449313908815384, -0.14680270850658417, -0.14854508638381958], "norm_scores": [0.10886946320533752, 0.10852447152137756, 0.10768146812915802, 0.09833330661058426, 0.09724746644496918, 0.09658466279506683, 0.09635096788406372, 0.09564398974180222, 0.09546517580747604, 0.09529899060726166]}}
{"id": "WebQTest-1324", "question": "what college did john nash go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.education", "education.education.institution"], ["book.written_work.author", "education.dissertation.institution"], ["common.topic.notable_types"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["book.author.education", "education.education.institution"]], "ground_paths": [["people.person.employment_history", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["book.written_work.author", "education.dissertation.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john nash go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> book.written_work.author <SEP> education.dissertation.institution </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> book.author.education <SEP> education.education.institution </PATH>"], "scores": [-0.0173427052795887, -0.01741841994225979, -0.03556722775101662, -0.03985544294118881, -0.04115355759859085, -0.05190804600715637, -0.07374057918787003, -0.07884044945240021, -0.09206867963075638, -0.09403742849826813], "norm_scores": [0.1037152111530304, 0.10370735079050064, 0.10184217244386673, 0.10140638053417206, 0.10127483308315277, 0.10019151121377945, 0.0980277806520462, 0.09752912819385529, 0.09624747931957245, 0.0960581824183464]}}
{"id": "WebQTest-1325", "question": "what language do maltese speak", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken"], ["language.human_language.writing_system"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"], ["people.person.nationality", "people.ethnicity.people", "people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do maltese speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.writing_system </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.nationality <SEP> people.ethnicity.people <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>"], "scores": [-0.015493867918848991, -0.029646990820765495, -0.03320993483066559, -0.03362179175019264, -0.07217185199260712, -0.10485070943832397, -0.11602319777011871, -0.12470754235982895, -0.12937961518764496, -0.1318073868751526], "norm_scores": [0.10645893216133118, 0.10496282577514648, 0.10458949953317642, 0.1045464426279068, 0.10059286653995514, 0.09735874086618423, 0.09627705067396164, 0.09544456750154495, 0.09499968588352203, 0.09476932883262634]}}
{"id": "WebQTest-1326", "question": "what did thomas edison do for the world", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["people.person.profession"], ["law.inventor.us_patents"], ["base.argumentmaps.original_idea.innovator"], ["symbols.namesake.named_after"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.inventions"], ["law.inventor.us_patents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas edison do for the world [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.013517365790903568, -0.020606718957424164, -0.036455146968364716, -0.05949464067816734, -0.07469215989112854, -0.08024904876947403, -0.08438526093959808, -0.09042571485042572, -0.10251469165086746, -0.10681720823049545], "norm_scores": [0.10543233156204224, 0.10468752682209015, 0.10304147750139236, 0.10069458931684494, 0.09917585551738739, 0.0986262708902359, 0.09821917116641998, 0.0976276770234108, 0.09645456075668335, 0.09604045748710632]}}
{"id": "WebQTest-1327", "question": "where is shakira from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"], ["people.person.ethnicity"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["people.ethnicity.people_with_this_ethnicity"], ["symbols.namesake.named_after"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced", "people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is shakira from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.011544568464159966, -0.01855609379708767, -0.059844277799129486, -0.08542563766241074, -0.13030008971691132, -0.13861341774463654, -0.13996833562850952, -0.14278239011764526, -0.14358441531658173, -0.15257711708545685], "norm_scores": [0.1093541756272316, 0.10859012603759766, 0.10419793426990509, 0.10156621038913727, 0.09710922837257385, 0.09630528092384338, 0.09617488086223602, 0.09590461850166321, 0.09582773596048355, 0.09496983885765076]}}
{"id": "WebQTest-1328", "question": "what did hayes do", "prediction": [["people.person.profession"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["common.topic.notable_types"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["people.profession.people_with_this_profession"], ["government.government_position_held.appointed_by", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held"], ["government.government_position_held.appointed_by", "government.government_position_held.office_position_or_title"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hayes do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.006124021485447884, -0.04653662070631981, -0.04773417487740517, -0.052075132727622986, -0.05615965276956558, -0.05915655568242073, -0.0593472421169281, -0.07387760281562805, -0.07858442515134811, -0.08118095248937607], "norm_scores": [0.10510027408599854, 0.10093757510185242, 0.1008167713880539, 0.10038008540868759, 0.09997090697288513, 0.09967175126075745, 0.09965275973081589, 0.09821523725986481, 0.09775403141975403, 0.09750054776668549]}}
{"id": "WebQTest-1329", "question": "who played bilbo in the fellowship of the ring", "prediction": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.starring", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["people.person.gender", "people.person.gender"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film.country", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film.starring", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in the fellowship of the ring [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.034812238067388535, -0.03523360937833786, -0.03552853688597679, -0.03797511011362076, -0.04012465849518776, -0.04020611196756363, -0.040362950414419174, -0.0410468615591526, -0.041439395397901535, -0.042188651859760284], "norm_scores": [0.10040844231843948, 0.10036613792181015, 0.10033654421567917, 0.1000913679599762, 0.09987644851207733, 0.0998683050274849, 0.09985265135765076, 0.09978438168764114, 0.09974522143602371, 0.09967051446437836]}}
{"id": "WebQTest-1330", "question": "what is the most populated state in united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.locations.states_and_provences.country"], ["base.biblioness.bibs_location.country"], ["base.biblioness.bibs_location.state"], ["base.locations.countries.states_provinces_within"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children", "location.location.containedby"]], "ground_paths": [["location.administrative_division.first_level_division_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most populated state in united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.locations.states_and_provences.country </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> location.location.containedby </PATH>"], "scores": [-0.008639209903776646, -0.02713107317686081, -0.03496070206165314, -0.0685405507683754, -0.06913550943136215, -0.0693664625287056, -0.06986066699028015, -0.0864727571606636, -0.08690490573644638, -0.0894303023815155], "norm_scores": [0.105343297123909, 0.10341319441795349, 0.10260667651891708, 0.09921837598085403, 0.0991593524813652, 0.09913645684719086, 0.09908747673034668, 0.09745501726865768, 0.09741291403770447, 0.09716722369194031]}}
{"id": "WebQTest-1331", "question": "who is my state senator arizona", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "people.person.nationality"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "ground_paths": [["location.country.first_level_divisions", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.capital", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.capital", "people.deceased_person.place_of_death"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my state senator arizona [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.02314767800271511, -0.02664060890674591, -0.029513677582144737, -0.029987061396241188, -0.030543409287929535, -0.034973155707120895, -0.04254508763551712, -0.04308939352631569, -0.04586154967546463, -0.04704282060265541], "norm_scores": [0.10122275352478027, 0.1008698046207428, 0.10058040916919708, 0.10053280740976334, 0.10047689825296402, 0.10003279149532318, 0.09927821159362793, 0.09922418743371964, 0.09894950687885284, 0.09883268922567368]}}
{"id": "WebQTest-1332", "question": "when does the vietnam war end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["time.event.locations"], ["book.written_work.subjects"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.military_conflict.casualties"], ["time.event.instance_of_recurring_event"], ["military.military_conflict.casualties", "time.event.included_in_event"], ["military.military_conflict.combatants"], ["military.military_conflict.casualties", "military.military_conflict.time_of_casualties"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does the vietnam war end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> military.military_conflict.casualties <SEP> time.event.included_in_event </PATH>", "<PATH> military.military_conflict.combatants </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.time_of_casualties </PATH>"], "scores": [-0.015371479094028473, -0.03251504898071289, -0.0332065187394619, -0.05856204777956009, -0.05885125696659088, -0.07693424820899963, -0.07755982875823975, -0.08378316462039948, -0.08659042418003082, -0.08987582474946976], "norm_scores": [0.10466990619897842, 0.10289078205823898, 0.10281965136528015, 0.1002453938126564, 0.10021640360355377, 0.09842047840356827, 0.09835892915725708, 0.09774871170520782, 0.09747468680143356, 0.09715496748685837]}}
{"id": "WebQTest-1333", "question": "where does the tennessee river go", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"], ["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.partially_contains"], ["geography.river.cities"], ["geography.river.origin"], ["location.location.containedby", "location.location.containedby"], ["transportation.bridge.body_of_water_spanned"], ["geography.lake.basin_countries"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the tennessee river go [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.cities </PATH>", "<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>", "<PATH> geography.lake.basin_countries </PATH>"], "scores": [-0.012539335526525974, -0.03341306746006012, -0.037282057106494904, -0.04492213949561119, -0.06475501507520676, -0.08715483546257019, -0.10005790740251541, -0.12692993879318237, -0.12862306833267212, -0.13450659811496735], "norm_scores": [0.10656489431858063, 0.10436354577541351, 0.1039605438709259, 0.10316929966211319, 0.10114331543445587, 0.09890291094779968, 0.09763495624065399, 0.09504624456167221, 0.0948854610323906, 0.09432882815599442]}}
{"id": "WebQTest-1334", "question": "who did tom hanks play in apollo 13", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.film.country", "people.person.nationality"], ["film.actor.film", "film.performance.character"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.performance.character"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["people.person.nationality", "people.person.nationality"], ["spaceflight.astronaut.missions"], ["people.person.gender", "people.person.gender"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom hanks play in apollo 13 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.performance.character </PATH>"], "scores": [-0.0233998391777277, -0.02813921868801117, -0.03188193216919899, -0.039851848036050797, -0.04045946151018143, -0.04294959828257561, -0.04365892335772514, -0.045575354248285294, -0.05286434665322304, -0.053866367787122726], "norm_scores": [0.10169623792171478, 0.1012154072523117, 0.10083729028701782, 0.10003682225942612, 0.0999760553240776, 0.09972741454839706, 0.09965669363737106, 0.09946589171886444, 0.09874352067708969, 0.09864463657140732]}}
{"id": "WebQTest-1335", "question": "what language do they in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.005501938052475452, -0.03507710620760918, -0.04306326434016228, -0.08713466674089432, -0.1814887672662735, -0.18475039303302765, -0.18977469205856323, -0.19028030335903168, -0.20555035769939423, -0.21292008459568024], "norm_scores": [0.11332087963819504, 0.11001846939325333, 0.10914334654808044, 0.1044376939535141, 0.0950341746211052, 0.09472472220659256, 0.09424999356269836, 0.09420233964920044, 0.09277479350566864, 0.0920935869216919]}}
{"id": "WebQTest-1336", "question": "what is utah 's state capitol", "prediction": [["base.aareas.schema.administrative_area.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.state"], ["location.us_state.capital"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.district_represented"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.capital"], ["government.governmental_jurisdiction.government_bodies"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.location_of_office"]], "ground_paths": [["location.location.containedby"], ["base.biblioness.bibs_location.city"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is utah 's state capitol [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.us_state.capital </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.capital </PATH>", "<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.location_of_office </PATH>"], "scores": [-0.019894320517778397, -0.025265585631132126, -0.029903745278716087, -0.03712954744696617, -0.03965196758508682, -0.049335382878780365, -0.05035692825913429, -0.06786748021841049, -0.06868928670883179, -0.07075773179531097], "norm_scores": [0.10261712968349457, 0.10206742584705353, 0.10159511864185333, 0.10086365789175034, 0.10060955584049225, 0.09964001178741455, 0.09953827410936356, 0.09781047701835632, 0.0977301299571991, 0.09752818942070007]}}
{"id": "WebQTest-1337", "question": "what all did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["law.inventor.us_patents"], ["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"], ["influence.influence_node.influenced_by"], ["symbols.name_source.namesakes"], ["law.us_patent.inventor"], ["book.written_work.author"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> law.us_patent.inventor </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.0065823327749967575, -0.025508059188723564, -0.07690857350826263, -0.07834179699420929, -0.08746352046728134, -0.12268064171075821, -0.12545867264270782, -0.1433318704366684, -0.14358390867710114, -0.1505129337310791], "norm_scores": [0.10923218727111816, 0.10718432813882828, 0.10181419551372528, 0.10166837275028229, 0.10074520111083984, 0.09725899249315262, 0.09698917716741562, 0.09527107328176498, 0.0952470675110817, 0.09458937495946884]}}
{"id": "WebQTest-1339", "question": "what team does jeremy lin play for 2013", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jeremy lin play for 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.01452622003853321, -0.01802825555205345, -0.022285915911197662, -0.030291398987174034, -0.03373720869421959, -0.04410891979932785, -0.06236982345581055, -0.06694601476192474, -0.07940059155225754, -0.08030200004577637], "norm_scores": [0.10308539867401123, 0.10272501409053802, 0.10228857398033142, 0.10147298127412796, 0.10112392157316208, 0.10008051991462708, 0.09826954454183578, 0.09782087057828903, 0.09661011397838593, 0.09652306139469147]}}
{"id": "WebQTest-1340", "question": "what is steve nash nationality", "prediction": [["people.person.nationality"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.country"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.country"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["people.person.place_of_birth", "location.location.containedby"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_award.award_winner", "sports.sports_award.country"], ["sports.sports_award_winner.awards", "sports.sports_award.country"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is steve nash nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.country </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.country </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.country </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.country </PATH>"], "scores": [-0.0021620052866637707, -0.05676992982625961, -0.059567198157310486, -0.06205211207270622, -0.07361439615488052, -0.07535292208194733, -0.07938078790903091, -0.08142859488725662, -0.0841543972492218, -0.08767256140708923], "norm_scores": [0.10658478736877441, 0.10092047601938248, 0.10063856840133667, 0.10038880258798599, 0.09923475980758667, 0.09906238317489624, 0.09866418689489365, 0.09846234321594238, 0.09819431602954865, 0.09784946590662003]}}
{"id": "WebQTest-1341", "question": "who are the u s senators of pennsylvania", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.locations.countries.states_provinces_within", "people.person.nationality"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_parent", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.location.containedby", "people.deceased_person.place_of_burial"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.place_of_birth"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_burial"], ["location.location.containedby", "people.person.nationality"], ["location.location.containedby", "people.person.place_of_birth"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the u s senators of pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.locations.countries.states_provinces_within <SEP> people.person.nationality </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.02356664277613163, -0.028117604553699493, -0.028215983882546425, -0.03244035691022873, -0.034927528351545334, -0.0349661149084568, -0.036386217921972275, -0.038669463247060776, -0.041757430881261826, -0.042311891913414], "norm_scores": [0.10106082260608673, 0.10060194879770279, 0.10059204697608948, 0.10016800463199615, 0.09991918504238129, 0.09991533309221268, 0.09977354109287262, 0.09954598546028137, 0.09923906624317169, 0.0991840586066246]}}
{"id": "WebQTest-1342", "question": "what kind of language does france speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["base.rosetta.languoid.languoid_class"], ["film.film.featured_film_locations"], ["location.country.language_spoken"], ["location.country.internet_tld"], ["location.country.currency_used"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does france speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> location.country.language_spoken </PATH>", "<PATH> location.country.internet_tld </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.014216496609151363, -0.021788258105516434, -0.02716189995408058, -0.07837773859500885, -0.16382266581058502, -0.16611473262310028, -0.17044971883296967, -0.17728795111179352, -0.18291670083999634, -0.18411193788051605], "norm_scores": [0.11072998493909836, 0.10989473760128021, 0.10930578410625458, 0.10384853184223175, 0.09534372389316559, 0.09512543678283691, 0.0947139710187912, 0.0940685048699379, 0.0935405045747757, 0.09342876076698303]}}
{"id": "WebQTest-1343", "question": "what degree did martin luther king earn", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"], ["education.education.degree_recipient", "education.education.degree"], ["common.topic.notable_types"], ["education.education.degree_recipient"], ["education.education.student", "education.education.degree_or_title"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.education.degree_conferred"], ["education.education.student", "education.education.degree_level"]], "ground_paths": [["people.person.education", "education.education.degree"], ["education.education.student", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degree did martin luther king earn [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.degree_recipient <SEP> education.education.degree </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> education.education.degree_recipient </PATH>", "<PATH> education.education.student <SEP> education.education.degree_or_title </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.degree_conferred </PATH>", "<PATH> education.education.student <SEP> education.education.degree_level </PATH>"], "scores": [-0.0048283664509654045, -0.023356033489108086, -0.05248859524726868, -0.09275805205106735, -0.1060393676161766, -0.10717090964317322, -0.11225926876068115, -0.11310526728630066, -0.11400658637285233, -0.1143488734960556], "norm_scores": [0.10815791040658951, 0.10617244243621826, 0.1031239926815033, 0.0990537479519844, 0.09774688631296158, 0.09763633459806442, 0.09714078903198242, 0.09705864638090134, 0.09697120636701584, 0.09693801403045654]}}
{"id": "WebQTest-1344", "question": "what two states is st louis in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.time_zones"], ["base.biblioness.bibs_location.state"], ["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.second_level_division_of"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two states is st louis in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.time_zones </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.second_level_division_of </PATH>"], "scores": [-0.003577533643692732, -0.057682111859321594, -0.06102698668837547, -0.0650910809636116, -0.07266885787248611, -0.08189116418361664, -0.08207454532384872, -0.08276455849409103, -0.08777102828025818, -0.09429880976676941], "norm_scores": [0.1067161113023758, 0.10109569877386093, 0.10075811296701431, 0.10034945607185364, 0.09959189593791962, 0.09867765009403229, 0.09865956008434296, 0.09859150648117065, 0.09809914231300354, 0.09746085852384567]}}
{"id": "WebQTest-1348", "question": "what team did peyton manning 's dad play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.professional_sports_team.owner_s", "sports.sports_team.sport"], ["sports.professional_sports_team.owner_s", "sports.sports_team.owner_s"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did peyton manning 's dad play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.owner_s </PATH>"], "scores": [-0.008511664345860481, -0.016198959201574326, -0.0381762757897377, -0.04158485680818558, -0.0497790202498436, -0.052845291793346405, -0.05644102394580841, -0.05744926258921623, -0.06869882345199585, -0.07754775881767273], "norm_scores": [0.10387331992387772, 0.10307787358760834, 0.10083720833063126, 0.10049407929182053, 0.09967398643493652, 0.09936882555484772, 0.09901215881109238, 0.09891238063573837, 0.09780590236186981, 0.09694423526525497]}}
{"id": "WebQTest-1350", "question": "what did jesse owens won", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_medal_honor.event"], ["people.person.nationality", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"], ["people.person.nationality", "olympics.olympic_games.participating_countries"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_event_competition.medalists"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_honor.event"], ["people.person.nationality", "olympics.olympic_medal_honor.country"], ["people.person.nationality", "olympics.olympic_games.country"], ["olympics.olympic_athlete_affiliation.athlete", "olympics.olympic_medal_honor.event"]], "ground_paths": [["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesse owens won [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_honor.event </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_medal_honor.country </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.country </PATH>", "<PATH> olympics.olympic_athlete_affiliation.athlete <SEP> olympics.olympic_medal_honor.event </PATH>"], "scores": [-0.018945660442113876, -0.0206015482544899, -0.03506194427609444, -0.038884323090314865, -0.04374966397881508, -0.044831082224845886, -0.050545550882816315, -0.053039055317640305, -0.06105664744973183, -0.06236932426691055], "norm_scores": [0.10241483896970749, 0.10224539786577225, 0.1007775217294693, 0.10039304941892624, 0.09990579634904861, 0.0997978076338768, 0.09922914206981659, 0.09898202866315842, 0.09819159656763077, 0.09806279093027115]}}
{"id": "WebQTest-1351", "question": "what is the name of the currency used in peru", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_formerly_used"], ["common.topic.notable_types"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country"], ["government.form_of_government.countries"], ["finance.currency.countries_used", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in peru [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.currency_used <PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>"], "scores": [-0.0092011084780097, -0.016304124146699905, -0.11731233447790146, -0.13372355699539185, -0.15030446648597717, -0.16884107887744904, -0.16888755559921265, -0.17442305386066437, -0.17980626225471497, -0.18185967206954956], "norm_scores": [0.11262638866901398, 0.1118292361497879, 0.10108531266450882, 0.09943991899490356, 0.09780470281839371, 0.09600843489170074, 0.09600397199392319, 0.095474012196064, 0.09496143460273743, 0.09476664662361145]}}
{"id": "WebQTest-1353", "question": "what are the four nations of the united kingdom", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.first_level_division_of"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_divisions"], ["location.country.first_level_divisions"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.administrative_divisions"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four nations of the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_divisions </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.011138788424432278, -0.022836558520793915, -0.024595629423856735, -0.07079781591892242, -0.09685594588518143, -0.10696816444396973, -0.11258953809738159, -0.11505447328090668, -0.11789985001087189, -0.13314613699913025], "norm_scores": [0.10715504735708237, 0.10590887069702148, 0.10572274029254913, 0.1009492427110672, 0.09835266321897507, 0.09736311435699463, 0.09681733697652817, 0.09657897800207138, 0.09630457311868668, 0.0948474183678627]}}
{"id": "WebQTest-1354", "question": "who is judy garland father", "prediction": [["people.person.parents"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.gender"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is judy garland father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.has_value <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.007421004585921764, -0.02415533922612667, -0.09835740923881531, -0.14346234500408173, -0.14355909824371338, -0.1477484107017517, -0.15233810245990753, -0.15670479834079742, -0.1594550460577011, -0.16581271588802338], "norm_scores": [0.1117321327328682, 0.10987792164087296, 0.10201990604400635, 0.09752054512500763, 0.09751110523939133, 0.09710345417261124, 0.09665880352258682, 0.09623764455318451, 0.09597332775592804, 0.09536509215831757]}}
{"id": "WebQTest-1355", "question": "what school did robert f kennedy go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["book.written_work.subjects", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.children", "people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did robert f kennedy go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.person.children <SEP> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.015265248715877533, -0.021422406658530235, -0.025736959651112556, -0.02707509696483612, -0.041671160608530045, -0.07907988876104355, -0.08357701450586319, -0.08640581369400024, -0.11476018279790878, -0.12177067995071411], "norm_scores": [0.10467489808797836, 0.10403238236904144, 0.10358449816703796, 0.10344598442316055, 0.10194703936576843, 0.09820378571748734, 0.09776313602924347, 0.09748698025941849, 0.09476161748170853, 0.09409961104393005]}}
{"id": "WebQTest-1356", "question": "what drug killed marilyn monroe", "prediction": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"], ["medicine.disease.parent_disease"], ["medicine.disease.notable_people_with_this_condition"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["music.artist.track", "music.artist.track"], ["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"], ["people.cause_of_death.people", "people.deceased_person.cause_of_death"], ["film.film_subject.films", "film.film.country", "people.person.nationality"], ["people.person.cause_of_death"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drug killed marilyn monroe [/INST]", "raw_output": {"paths": ["<PATH> people.cause_of_death.people </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.parent_disease </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> base.popstra.celebrity.substance_abuse <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> music.artist.track <SEP> music.artist.track </PATH>", "<PATH> base.popstra.substance_abuse.abuser <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> people.cause_of_death.people <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> film.film_subject.films <SEP> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.007082459982484579, -0.023506032302975655, -0.057361118495464325, -0.059439998120069504, -0.08861260861158371, -0.09504661709070206, -0.0967879444360733, -0.10701842606067657, -0.11010966449975967, -0.1257980912923813], "norm_scores": [0.1071762964129448, 0.10543044656515121, 0.10192084312438965, 0.10170917958021164, 0.0987849161028862, 0.09815137833356857, 0.09798061102628708, 0.09698333591222763, 0.09668400138616562, 0.0951790139079094]}}
{"id": "WebQTest-1357", "question": "where is the fukushima daiichi nuclear power station", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.contains"], ["location.location.street_address", "location.mailing_address.citytown"], ["location.location.street_address", "location.mailing_address.state_province_region"], ["location.location.containedby", "location.location.contains"], ["location.location.street_address", "location.mailing_address.country"], ["location.location.street_address"], ["location.location.contains", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear power station [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> location.location.containedby <SEP> location.location.contains </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.country </PATH>", "<PATH> location.location.street_address </PATH>", "<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.005129960831254721, -0.04163585975766182, -0.04461902752518654, -0.049042362719774246, -0.073496513068676, -0.0787617489695549, -0.08754204213619232, -0.09079686552286148, -0.1039230078458786, -0.1073286160826683], "norm_scores": [0.10646254569292068, 0.1026461198925972, 0.10234036296606064, 0.10188867896795273, 0.09942729771137238, 0.09890516847372055, 0.09804055094718933, 0.09772196412086487, 0.09644763171672821, 0.09611973166465759]}}
{"id": "WebQTest-1358", "question": "what is los angeles california time zone", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.time_Zones"], ["base.biblioness.bibs_location.country"], ["location.location.time_zonessymbol"], ["locations.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is los angeles california time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.time_zonessymbol </PATH>", "<PATH> locations.location.time_zones </PATH>"], "scores": [-1.1356400136719458e-05, -0.16325591504573822, -0.16973109543323517, -0.1836051046848297, -0.184245303273201, -0.1902298480272293, -0.1974545568227768, -0.21668444573879242, -0.2240191400051117, -0.2255978137254715], "norm_scores": [0.11894205212593079, 0.10102741420269012, 0.10037534683942795, 0.09899235516786575, 0.09892900288105011, 0.09833873063325882, 0.0976308211684227, 0.09577132016420364, 0.09507144242525101, 0.09492147713899612]}}
{"id": "WebQTest-1359", "question": "what did nicolas cage name his son", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.nationality", "people.person.nationality"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["people.person.gender"], ["film.producer.film", "film.film_story_contributor.film_story_credits"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["music.artist.track", "music.artist.track"], ["influence.influence_node.influenced_by", "people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicolas cage name his son [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> film.producer.film <SEP> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> music.artist.track <SEP> music.artist.track </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.children </PATH>"], "scores": [-0.011589218862354755, -0.015416929498314857, -0.10965996980667114, -0.11503610759973526, -0.11780849099159241, -0.11910156160593033, -0.12585261464118958, -0.1274915188550949, -0.12796415388584137, -0.1314527988433838], "norm_scores": [0.10915184020996094, 0.10873483866453171, 0.09895540028810501, 0.09842482954263687, 0.09815233945846558, 0.09802550077438354, 0.09736595302820206, 0.09720651060342789, 0.09716058522462845, 0.09682220220565796]}}
{"id": "WebQTest-1361", "question": "what currency should take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "finance.currency.countries_used"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.country.languages_spoken"], ["film.film.featured_film_locations", "film.film.country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> finance.currency.countries_used </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> film.film.featured_film_locations <SEP> film.film.country </PATH>"], "scores": [-0.007808876223862171, -0.032314933836460114, -0.04920833185315132, -0.05534499138593674, -0.05735109746456146, -0.06319563090801239, -0.06999454647302628, -0.07172725349664688, -0.07551918178796768, -0.08037176728248596], "norm_scores": [0.10494361072778702, 0.10240311175584793, 0.10068770498037338, 0.10007171332836151, 0.09987115859985352, 0.09928916394710541, 0.09861639142036438, 0.09844567626714706, 0.0980730727314949, 0.09759832173585892]}}
{"id": "WebQTest-1362", "question": "who is the state governor of florida", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of florida [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.appointed_by </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.appointed_by </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.014487463980913162, -0.022850172594189644, -0.024804353713989258, -0.02757568284869194, -0.036324020475149155, -0.04950534552335739, -0.053030144423246384, -0.05716020613908768, -0.059629373252391815, -0.06016053259372711], "norm_scores": [0.1026269942522049, 0.10177233815193176, 0.101573646068573, 0.10129253566265106, 0.10041026026010513, 0.0990954041481018, 0.09874673187732697, 0.09833974391222, 0.09809722006320953, 0.09804513305425644]}}
{"id": "WebQTest-1363", "question": "who was jessica simpson married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.infidelity_participant", "base.popstra.infidelity.victim"], ["people.marriage.spouse", "people.marriage.spouse"], ["broadcast.artist.content", "broadcast.artist.content"], ["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.artist.genre", "music.artist.genre"], ["people.person.languages", "people.person.languages"], ["people.person.parents", "people.person.children"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.parents", "people.person.children"], ["people.person.languages", "people.person.languages"], ["broadcast.artist.content", "broadcast.artist.content"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["base.saturdaynightlive.snl_episode.host", "base.saturdaynightlive.snl_host.episodes_hosted"], ["people.person.spouse_s", "people.person.spouse_s"], ["music.artist.genre", "music.artist.genre"], ["people.person.nationality", "people.person.nationality"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["base.popstra.celebrity.infidelity_participant", "base.popstra.celebrity.infidelity_victim"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.spouse_s", "people.marriage.spouse"], ["music.artist.album", "music.album.featured_artists"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was jessica simpson married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.infidelity_participant <SEP> base.popstra.infidelity.victim </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> broadcast.artist.content <SEP> broadcast.artist.content </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.languages <SEP> people.person.languages </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.023228147998452187, -0.03238770738244057, -0.03815304860472679, -0.04339823126792908, -0.04356580972671509, -0.04380660504102707, -0.044251199811697006, -0.05025278404355049, -0.05191397666931152, -0.06722785532474518], "norm_scores": [0.1020740494132042, 0.10114336758852005, 0.10056192427873611, 0.10003583133220673, 0.10001906752586365, 0.09999498724937439, 0.09995054453611374, 0.09935247153043747, 0.09918756783008575, 0.09768019616603851]}}
{"id": "WebQTest-1364", "question": "what do they speak in scotland", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["location.country.first_level_divisions"], ["base.rosetta.languoid.languoid_class"], ["location.country.currency_used"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.014866829849779606, -0.015578309074044228, -0.03814326226711273, -0.08009947836399078, -0.16738417744636536, -0.17062419652938843, -0.17119301855564117, -0.1824021190404892, -0.1889452338218689, -0.19492895901203156], "norm_scores": [0.11106276512145996, 0.11098377406597137, 0.10850748419761658, 0.10404909402132034, 0.09535227715969086, 0.09504382312297821, 0.09498977661132812, 0.09393098205327988, 0.0933183878660202, 0.09276165813207626]}}
{"id": "WebQTest-1365", "question": "what did john dalton study", "prediction": [["people.person.profession"], ["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.profession", "people.profession.part_of_professional_field"], ["people.person.education", "education.education.field_of_study"], ["people.person.profession", "people.profession.specializations"], ["education.education.student", "education.education.field_of_study"], ["education.education.student", "education.education.degree"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.education.degree"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john dalton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.profession <SEP> people.profession.part_of_professional_field </PATH>", "<PATH> people.person.education <SEP> education.education.field_of_study </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> education.education.student <SEP> education.education.field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>"], "scores": [-0.01284732110798359, -0.023936130106449127, -0.03461203724145889, -0.061366595327854156, -0.06348765641450882, -0.0667431652545929, -0.06779836863279343, -0.06983478367328644, -0.07300083339214325, -0.07848317921161652], "norm_scores": [0.10430288314819336, 0.10315268486738205, 0.10205729305744171, 0.09936299175024033, 0.09915246069431305, 0.0988301932811737, 0.09872596710920334, 0.09852512925863266, 0.09821368008852005, 0.09767670929431915]}}
{"id": "WebQTest-1366", "question": "what songs do zac brown band sing", "prediction": [["music.recording.artist"], ["music.artist.album"], ["music.composer.compositions"], ["music.album.artist"], ["music.composition.composer"], ["music.artist.track"], ["music.producer.tracks_produced"], ["music.recording.producer"], ["music.featured_artist.recordings"], ["music.artist.genre"]], "ground_paths": [["music.artist.album"], ["music.recording.featured_artists"], ["music.album.artist"], ["music.music_video.artist"], ["music.recording.artist"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["music.featured_artist.recordings"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs do zac brown band sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.producer.tracks_produced </PATH>", "<PATH> music.recording.producer </PATH>", "<PATH> music.featured_artist.recordings </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.01417000126093626, -0.02982928231358528, -0.051873914897441864, -0.05200886353850365, -0.06502998620271683, -0.06710922718048096, -0.08095093071460724, -0.08421874046325684, -0.08864904940128326, -0.09557075053453445], "norm_scores": [0.10496508330106735, 0.1033342108130455, 0.10108117014169693, 0.10106752812862396, 0.09976004809141159, 0.09955283999443054, 0.09818434715270996, 0.09786403179168701, 0.09743141382932663, 0.09675934910774231]}}
{"id": "WebQTest-1367", "question": "where is glastonbury england", "prediction": [["location.location.containedby"], ["location.location.contains"], ["time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.events"], ["location.place_with_neighborhoods.neighborhoods"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.hud_county_place.county"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is glastonbury england [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.events </PATH>", "<PATH> location.place_with_neighborhoods.neighborhoods </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011342111974954605, -0.017108909785747528, -0.09330393373966217, -0.1188255101442337, -0.12023748457431793, -0.12123849242925644, -0.12150989472866058, -0.1319890022277832, -0.13393612205982208, -0.14321357011795044], "norm_scores": [0.10929550975561142, 0.1086670458316803, 0.10069473087787628, 0.0981573611497879, 0.09801886230707169, 0.09792079776525497, 0.09789422899484634, 0.09687374532222748, 0.09668529033660889, 0.09579245746135712]}}
{"id": "WebQTest-1368", "question": "who formed the american federation of labor", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "people.person.nationality"], ["business.employment_tenure.company", "business.employment_tenure.person"], ["organization.organization.spun_off_from"], ["business.employment_tenure.company", "people.person.employment_history"], ["business.employer.employees", "business.employment_tenure.person"], ["organization.organization.spun_off_from", "organization.organization_founder.organizations_founded"], ["business.employer.employees", "people.person.employment_history"], ["people.person.profession"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who formed the american federation of labor [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>", "<PATH> business.employment_tenure.company <SEP> business.employment_tenure.person </PATH>", "<PATH> organization.organization.spun_off_from </PATH>", "<PATH> business.employment_tenure.company <SEP> people.person.employment_history </PATH>", "<PATH> business.employer.employees <SEP> business.employment_tenure.person </PATH>", "<PATH> organization.organization.spun_off_from <SEP> organization.organization_founder.organizations_founded </PATH>", "<PATH> business.employer.employees <SEP> people.person.employment_history </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.001560967997647822, -0.049679677933454514, -0.07777293771505356, -0.09299646317958832, -0.10114499926567078, -0.10560286790132523, -0.10658019036054611, -0.1084912121295929, -0.11129157245159149, -0.12016458064317703], "norm_scores": [0.1089111864566803, 0.10379461199045181, 0.10091926157474518, 0.0993945449590683, 0.09858792275190353, 0.09814941138029099, 0.09805352985858917, 0.09786632657051086, 0.09759265184402466, 0.096730537712574]}}
{"id": "WebQTest-1369", "question": "what to visit in denver", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"], ["base.arthist.helynevek.intezmeny"], ["film.film_festival.location"], ["location.neighborhood.neighborhood_of"], ["transportation.bridge.locale"], ["transportation.road.major_cities"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in denver [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> location.neighborhood.neighborhood_of </PATH>", "<PATH> transportation.bridge.locale </PATH>", "<PATH> transportation.road.major_cities </PATH>"], "scores": [-0.013482152484357357, -0.023951375856995583, -0.02462385967373848, -0.12225841730833054, -0.13710154592990875, -0.14280648529529572, -0.1445617973804474, -0.14867882430553436, -0.15049439668655396, -0.1560656875371933], "norm_scores": [0.10955771058797836, 0.10841671377420425, 0.10834383219480515, 0.09826571494340897, 0.09681791812181473, 0.09626714885234833, 0.09609831869602203, 0.09570349752902985, 0.09552989900112152, 0.09499914944171906]}}
{"id": "WebQTest-1370", "question": "where did thomas hobbes live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.person.parents", "people.person.place_of_birth"], ["people.person.nationality"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.parents", "people.deceased_person.place_of_death"]], "ground_paths": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas hobbes live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> people.person.parents <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.025959627702832222, -0.03182578086853027, -0.031860098242759705, -0.033757634460926056, -0.04242737963795662, -0.055707696825265884, -0.05572693794965744, -0.05843210220336914, -0.059337686747312546, -0.05973626673221588], "norm_scores": [0.1019623875617981, 0.10136601328849792, 0.10136253386735916, 0.1011703759431839, 0.10029704868793488, 0.09897387772798538, 0.09897197782993317, 0.09870459884405136, 0.09861525148153305, 0.09857595711946487]}}
{"id": "WebQTest-1373", "question": "where was john f kennedy when he got shot", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.place_of_birth", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["time.event.locations"], ["people.person.nationality", "location.country.administrative_divisions"], ["people.place_lived.person", "people.place_lived.location"], ["base.crime.crime_victim.crime_type"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john f kennedy when he got shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> people.person.nationality <SEP> location.country.administrative_divisions </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> base.crime.crime_victim.crime_type </PATH>"], "scores": [-0.0041746338829398155, -0.05148550868034363, -0.052163854241371155, -0.0603354386985302, -0.06396407634019852, -0.06768149882555008, -0.06790505349636078, -0.07333870232105255, -0.081101194024086, -0.0857677236199379], "norm_scores": [0.1058000847697258, 0.10091115534305573, 0.10084272176027298, 0.10002203285694122, 0.0996597558259964, 0.09928996115922928, 0.09926776587963104, 0.09872984141111374, 0.09796642512083054, 0.09751033037900925]}}
{"id": "WebQTest-1374", "question": "what type of government does italy have 2011", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.form_of_government"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_parent", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does italy have 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.form_of_government </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> government.form_of_government.countries </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.country.form_of_government </PATH>"], "scores": [-0.00902493018656969, -0.013568492606282234, -0.11435982584953308, -0.12393639981746674, -0.1388693004846573, -0.13986262679100037, -0.14213408529758453, -0.14249737560749054, -0.1427866667509079, -0.14285437762737274], "norm_scores": [0.11058905720710754, 0.11008772999048233, 0.09953270107507706, 0.09858407080173492, 0.09712286293506622, 0.09702643007040024, 0.09680629521608353, 0.09677112847566605, 0.0967431366443634, 0.09673658758401871]}}
{"id": "WebQTest-1376", "question": "what province is montreal canada located", "prediction": [["location.province.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.state"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.first_level_division_of"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.biblioness.bibs_location.province"], ["location.administrative_division.country"], ["base.biblioness.bibs_location.province_of"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is montreal canada located [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.biblioness.bibs_location.province </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.biblioness.bibs_location.province_of </PATH>"], "scores": [-0.0243783388286829, -0.02842571586370468, -0.030418232083320618, -0.0347493551671505, -0.04377521947026253, -0.053633913397789, -0.062343575060367584, -0.06801366806030273, -0.07463914155960083, -0.08077587932348251], "norm_scores": [0.10258752107620239, 0.10217315703630447, 0.10196977108716965, 0.1015290841460228, 0.10061682015657425, 0.09962974488735199, 0.0987657755613327, 0.09820733964443207, 0.09755881875753403, 0.09696196019649506]}}
{"id": "WebQTest-1378", "question": "who does kellan lutz play in prom night", "prediction": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["film.actor.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.performance.character"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.performance.character"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kellan lutz play in prom night [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.023660501465201378, -0.024314478039741516, -0.02932482399046421, -0.030365318059921265, -0.03051716834306717, -0.05510353296995163, -0.06358834356069565, -0.06450986862182617, -0.0661153644323349, -0.06962761282920837], "norm_scores": [0.10221220552921295, 0.10214538127183914, 0.10163487493991852, 0.10152918100357056, 0.10151376575231552, 0.09904834628105164, 0.09821149706840515, 0.0981210246682167, 0.09796362370252609, 0.0976201519370079]}}
{"id": "WebQTest-1379", "question": "what did franz liszt do to earn a living", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["music.artist.genre"], ["influence.influence_node.influenced_by", "people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["music.group_member.instruments_played"], ["influence.influence_node.influenced", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"], ["influence.influence_node.influenced"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did franz liszt do to earn a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.001025717006996274, -0.0602673776447773, -0.08562392741441727, -0.0951787680387497, -0.0957956537604332, -0.10426722466945648, -0.11388901621103287, -0.12034335732460022, -0.12348819524049759, -0.1281532496213913], "norm_scores": [0.10953917354345322, 0.10323837399482727, 0.1006535068154335, 0.09969636052846909, 0.09963487833738327, 0.0987943783402443, 0.09784835577011108, 0.0972188413143158, 0.09691359102725983, 0.09646254032850266]}}
{"id": "WebQTest-1380", "question": "what was nikola tesla known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["law.inventor.inventions"], ["people.profession.people_with_this_profession"], ["symbols.namesake.named_after"], ["law.invention.inventor"], ["symbols.name_source.namesakes"], ["base.argumentmaps.innovator.original_ideas"], ["influence.influence_node.influenced_by"], ["base.activism.activist.area_of_activism"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.0022857191506773233, -0.04977366328239441, -0.06798480451107025, -0.08590391278266907, -0.0859154611825943, -0.1062936782836914, -0.10714147239923477, -0.11213605105876923, -0.12304890900850296, -0.13059692084789276], "norm_scores": [0.10877779871225357, 0.10373289883136749, 0.1018608957529068, 0.10005190223455429, 0.10005075484514236, 0.09803252667188644, 0.09794944524765015, 0.09746145457029343, 0.09640365093946457, 0.09567873924970627]}}
{"id": "WebQTest-1381", "question": "what is the zip code for princeton nj", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.town.postal_codes"], ["location.citytown.postal_ codes"], ["location.Citytown.postal_codes"], ["location.mailing_address.citytown"], ["location.citytown.postal_code"]], "ground_paths": [["location.location.contains"], ["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for princeton nj [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.town.postal_codes </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.citytown.postal_ codes </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.mailing_address.citytown </PATH>", "</PATH> location.citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_code </PATH>"], "scores": [-0.0003167038375977427, -0.07845945656299591, -0.11783353239297867, -0.16737385094165802, -0.17191268503665924, -0.1794469654560089, -0.18051518499851227, -0.18254992365837097, -0.18304967880249023, -0.18391068279743195], "norm_scores": [0.11531098186969757, 0.10664332658052444, 0.10252593457698822, 0.0975705236196518, 0.09712866693735123, 0.09639962762594223, 0.09629671275615692, 0.09610097110271454, 0.09605295211076736, 0.0959702879190445]}}
{"id": "WebQTest-1382", "question": "what other countries border argentina", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_contains"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["time.event.locations", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["film.film.featured_film_locations", "media_common.netflix_title.netflix_genres"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.country.languages_spoken", "location.country.official_language"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["time.event.locations", "time.event.locations"], ["location.location.contains", "location.location.containedby"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.country.form_of_government", "government.form_of_government.countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.events", "location.location.events"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.country.form_of_government", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.containedby", "location.location.containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["book.book_subject.works", "book.book_subject.works"], ["location.location.partially_containedby", "location.location.containedby"], ["language.human_language.main_country", "language.language_family.geographic_distribution"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.contains", "location.location.contains"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["film.film.featured_film_locations", "film.film.country"], ["location.location.primarily_containedby", "location.location.containedby"], ["base.locations.continents.counties_within", "base.locations.counties_parishes_and_boroughs.coninent"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["geography.river.basin_countries", "location.location.partially_contains"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries border argentina [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.03779810667037964, -0.03968469053506851, -0.03981439769268036, -0.040155310183763504, -0.042358919978141785, -0.045149434357881546, -0.045297328382730484, -0.04686226695775986, -0.04837208613753319, -0.04960396885871887], "norm_scores": [0.10057202726602554, 0.10038246959447861, 0.10036944597959518, 0.10033523291349411, 0.1001143828034401, 0.09983539581298828, 0.09982063621282578, 0.09966453909873962, 0.0995141789317131, 0.09939166903495789]}}
{"id": "WebQTest-1383", "question": "what language does people in iceland speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.human_language.language_family"], ["book.written_work.original_language"], ["book.written_work.subjects"], ["language.human_language.region"], ["language.language_family.languages"], ["book.book_subject.works"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does people in iceland speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> book.written_work.original_language </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.016056202352046967, -0.017218708992004395, -0.04183982312679291, -0.042602986097335815, -0.14803218841552734, -0.16346094012260437, -0.16360034048557281, -0.17040246725082397, -0.17287315428256989, -0.18375539779663086], "norm_scores": [0.10980851203203201, 0.10968093574047089, 0.10701343417167664, 0.10693179816007614, 0.09623201936483383, 0.09475867450237274, 0.0947454646229744, 0.09410317987203598, 0.09387096017599106, 0.09285497665405273]}}
{"id": "WebQTest-1384", "question": "what is the predominant religion in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["location.country.administrative_divisions"], ["book.book_subject.works", "base.argumentmaps.thing_of_disputed_value.disparagement"], ["symbols.namesake.named_after", "religion.religion.notable_figures"], ["film.film.country", "media_common.netflix_title.netflix_genres"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["film.film_location.featured_in_films", "film.film.country", "people.person.nationality"]], "ground_paths": [["people.person.quotations", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["symbols.namesake.named_after", "religion.religion.notable_figures"], ["people.person.nationality", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the predominant religion in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>", "<PATH> film.film_location.featured_in_films <SEP> media_common.netflix_title.netflix_genres </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> book.book_subject.works <SEP> base.argumentmaps.thing_of_disputed_value.disparagement </PATH>", "<PATH> symbols.namesake.named_after <SEP> religion.religion.notable_figures </PATH>", "<PATH> film.film.country <SEP> media_common.netflix_title.netflix_genres </PATH>", "<PATH> location.country.administrative_divisions <SEP> religion.religion.sacred_sites </PATH>", "<PATH> film.film_location.featured_in_films <SEP> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.003986255265772343, -0.028237879276275635, -0.05100175365805626, -0.08122748136520386, -0.09000769257545471, -0.093777135014534, -0.09557643532752991, -0.09665290266275406, -0.0996953696012497, -0.10018184781074524], "norm_scores": [0.10719874501228333, 0.10463027656078339, 0.10227539390325546, 0.09923029690980911, 0.09836284071207047, 0.09799277037382126, 0.09781660884618759, 0.09771136939525604, 0.0974145382642746, 0.09736715257167816]}}
{"id": "WebQTest-1385", "question": "who has coached the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.historical_coaching_staff", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team.sport", "sports.sport.team_coaches"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["american_football.football_team.historical_coaching_staff", "american_football.football_coach.coaching_history"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["american_football.football_historical_coach_position.team", "american_football.football_coach.coaching_history"], ["sports.sports_team.sport", "sports.sports_team_coach_tenure.coach"], ["american_football.football_team.current_head_coach"]], "ground_paths": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.historical_coaching_staff", "american_football.football_coach.coaching_history"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has coached the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.team_coaches </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_coach.coaching_history </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_coach.coaching_history </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>"], "scores": [-0.0066589778289198875, -0.03275153040885925, -0.03444313257932663, -0.03868386149406433, -0.04002870246767998, -0.0401371568441391, -0.04044368863105774, -0.04539601132273674, -0.06858149915933609, -0.06880561262369156], "norm_scores": [0.10354039072990417, 0.10087369382381439, 0.10070320218801498, 0.10027705878019333, 0.10014229267835617, 0.10013142973184586, 0.10010073333978653, 0.0996062308549881, 0.09732338786125183, 0.09730158001184464]}}
{"id": "WebQTest-1387", "question": "what time zone is toronto gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["base.locations.time_zones.locations_in_this_time_zone"], ["base.locations.countries.continent"], ["location.location.containedby"], ["language.human_language.countries_spoken_in"], ["location.location.time_zones", "location.location.time_zones"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["language.human_language.countries_spoken_in", "location.country.official_language"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is toronto gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> base.locations.time_zones.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.official_language </PATH>"], "scores": [-0.0002129094791598618, -0.07553865760564804, -0.1540721356868744, -0.1614755094051361, -0.16688272356987, -0.16712361574172974, -0.16986221075057983, -0.1701405644416809, -0.17876742780208588, -0.18272320926189423], "norm_scores": [0.1151278093457222, 0.10677428543567657, 0.09870974719524384, 0.09798165410757065, 0.097453273832798, 0.0974298045039177, 0.09716334193944931, 0.09713630378246307, 0.09630192816257477, 0.09592173248529434]}}
{"id": "WebQTest-1388", "question": "what country did toussaint l'ouverture help free", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved"], ["base.activism.activist.area_of_activism"], ["book.written_work.subjects"], ["film.film_subject.films"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["base.activism.activism_issue.activists"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.place_of_birth"]], "ground_paths": [["military.military_service.military_person", "military.armed_force.personnel"], ["military.military_service.military_person", "military.military_service.military_force"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did toussaint l'ouverture help free [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> film.film_subject.films </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.person.nationality </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.01807582564651966, -0.02610253542661667, -0.04917754605412483, -0.05333126708865166, -0.05566611886024475, -0.0609419047832489, -0.06239825114607811, -0.06447251886129379, -0.06514335423707962, -0.06653191894292831], "norm_scores": [0.1034562736749649, 0.10262918472290039, 0.10028812289237976, 0.09987242519855499, 0.09963951259851456, 0.09911521524190903, 0.09897097945213318, 0.09876590222120285, 0.0986996665596962, 0.09856270253658295]}}
{"id": "WebQTest-1389", "question": "what jobs did robert burns have", "prediction": [["people.person.profession"], ["influence.influence_node.influenced", "people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"], ["common.topic.notable_types", "type.type.expected_by"], ["fictional_universe.fictional_character.occupation"], ["common.topic.notable_types", "rdf-schema#domain"], ["film.film_story_contributor.film_story_credits"], ["people.profession.people_with_this_profession"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did robert burns have [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.notable_types <SEP> rdf-schema#domain </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.0024043445009738207, -0.055564019829034805, -0.06398218125104904, -0.06670330464839935, -0.07506100833415985, -0.0832967609167099, -0.09790468215942383, -0.10417575389146805, -0.10527541488409042, -0.10937415808439255], "norm_scores": [0.10762645304203033, 0.10205447673797607, 0.10119897872209549, 0.10092397779226303, 0.10008399188518524, 0.09926310926675797, 0.0978236198425293, 0.09721209108829498, 0.0971052497625351, 0.09670804440975189]}}
{"id": "WebQTest-1390", "question": "who won utah attorney general", "prediction": [["base.aareas.schema.administrative_area.administrative_children", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "ground_paths": [["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.city", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won utah attorney general [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.015799840912222862, -0.025760717689990997, -0.0288242157548666, -0.031094010919332504, -0.03281611576676369, -0.03593048080801964, -0.049671560525894165, -0.05254606530070305, -0.054026197642087936, -0.055058278143405914], "norm_scores": [0.10225175321102142, 0.10123829543590546, 0.10092862695455551, 0.10069979727268219, 0.10052652657032013, 0.10021394491195679, 0.09884630888700485, 0.0985625833272934, 0.09841681271791458, 0.0983152836561203]}}
{"id": "WebQTest-1392", "question": "what the language spoken in indonesia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.language_family"], ["book.written_work.subjects"], ["language.human_language.country"], ["language.human_language.region"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the language spoken in indonesia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.01520559098571539, -0.023398123681545258, -0.026288772001862526, -0.05322853475809097, -0.1488143503665924, -0.1735704094171524, -0.1903935968875885, -0.19393746554851532, -0.19436874985694885, -0.19924671947956085], "norm_scores": [0.11092031747102737, 0.11001530289649963, 0.10969775170087814, 0.1067819744348526, 0.09704776853322983, 0.0946747437119484, 0.0930953323841095, 0.09276600182056427, 0.09272599965333939, 0.09227479249238968]}}
{"id": "WebQTest-1394", "question": "who coaches the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team.sport", "sports.sport.team_coaches"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["american_football.football_historical_coach_position.team", "american_football.football_coach.coaching_history"], ["american_football.football_team.current_head_coach"], ["american_football.football_coach.current_team_head_coached"], ["sports.sports_team.sport", "sports.sports_team_coach_tenure.coach"], ["american_football.football_team.historical_coaching_staff", "american_football.football_historical_coach_position.coach"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who coaches the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.team_coaches </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_coach.coaching_history </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>", "<PATH> american_football.football_coach.current_team_head_coached </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_historical_coach_position.coach </PATH>"], "scores": [-0.006297075189650059, -0.03678632527589798, -0.036821119487285614, -0.03739718720316887, -0.04199475049972534, -0.04410088062286377, -0.04566027224063873, -0.055865101516246796, -0.06835442036390305, -0.07677149772644043], "norm_scores": [0.10392911732196808, 0.10080821812152863, 0.10080470889806747, 0.10074665397405624, 0.10028453171253204, 0.10007353872060776, 0.09991760551929474, 0.09890314191579819, 0.09767559915781021, 0.09685690701007843]}}
{"id": "WebQTest-1395", "question": "what other countries does south africa share borders with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.partially_containedby", "location.location.partially_contains"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_contains", "location.location.partially_containedby"]], "ground_paths": [["time.event.locations", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.official_language", "location.country.official_language"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["government.form_of_government.countries", "location.country.form_of_government"], ["time.event.locations", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.containedby", "base.locations.continents.countries_within"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["film.film.featured_film_locations", "film.film.subjects"], ["location.country.official_language", "language.human_language.main_country"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.country.languages_spoken", "language.human_language.main_country"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.containedby", "location.location.containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.containedby", "base.locations.countries.continent"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.currency_used", "location.country.currency_used"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.time_zones", "location.location.time_zones"], ["organization.organization.founders", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries does south africa share borders with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.adjoining_relationship.adjoins </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03294404223561287, -0.03408269211649895, -0.04314876347780228, -0.04385970160365105, -0.044029995799064636, -0.044077932834625244, -0.04654640704393387, -0.046861015260219574, -0.04977249726653099, -0.05081648752093315], "norm_scores": [0.10107111185789108, 0.10095609724521637, 0.10004495829343796, 0.09997384995222092, 0.09995682537555695, 0.09995203465223312, 0.09970561414957047, 0.09967425465583801, 0.09938447177410126, 0.09928077459335327]}}
{"id": "WebQTest-1396", "question": "who played darth vader at the end of return of the jedi", "prediction": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film.starring", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader at the end of return of the jedi [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.022479433566331863, -0.02650531381368637, -0.02769296243786812, -0.03216370567679405, -0.03368114307522774, -0.03840525820851326, -0.04344598203897476, -0.04411493241786957, -0.04670945182442665, -0.05007820203900337], "norm_scores": [0.10141067951917648, 0.1010032370686531, 0.1008833572268486, 0.1004333347082138, 0.10028104484081268, 0.09980842471122742, 0.09930658340454102, 0.09924018383026123, 0.0989830270409584, 0.09865014255046844]}}
{"id": "WebQTest-1398", "question": "what movies did joan crawford play in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.deceased_person.place_of_death", "film.film.featured_film_locations"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.producer.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.character"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did joan crawford play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.023774528875947, -0.028202416375279427, -0.034777071326971054, -0.038264136761426926, -0.04145507141947746, -0.04239296913146973, -0.0457768477499485, -0.04706088453531265, -0.04819539561867714, -0.04856019467115402], "norm_scores": [0.1016167402267456, 0.10116779059171677, 0.10050482302904129, 0.10015497356653214, 0.09983588755130768, 0.09974230080842972, 0.09940534830093384, 0.0992777943611145, 0.09916521608829498, 0.09912905842065811]}}
{"id": "WebQTest-1399", "question": "what products does the united states export", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.locations.countries.planet", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.founders", "organization.organization.founders"], ["location.country.form_of_government", "location.country.form_of_government"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products does the united states export [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.locations.countries.planet <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> organization.organization.founders <SEP> organization.organization.founders </PATH>", "<PATH> location.country.form_of_government <SEP> location.country.form_of_government </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.023277437314391136, -0.026255005970597267, -0.03031265176832676, -0.03645307943224907, -0.0423465296626091, -0.045967888087034225, -0.04708484560251236, -0.04770851135253906, -0.04864580184221268, -0.051756374537944794], "norm_scores": [0.10167958587408066, 0.10137728601694107, 0.10096675157546997, 0.10034867376089096, 0.09975901991128922, 0.0993984043598175, 0.09928744286298752, 0.09922554343938828, 0.09913258254528046, 0.09882470220327377]}}
{"id": "WebQTest-1402", "question": "what is the kentucky state bird", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.location_symbol_relationship.Kind_of_symbol", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.Kind_of_symbol", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the kentucky state bird [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.location_symbol_relationship.Kind_of_symbol <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.Kind_of_symbol <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.014083092100918293, -0.022399568930268288, -0.028362972661852837, -0.028477145358920097, -0.03505827859044075, -0.037648603320121765, -0.03920547664165497, -0.04023217409849167, -0.04399796575307846, -0.0586407370865345], "norm_scores": [0.10208737105131149, 0.10124187916517258, 0.10063993185758591, 0.10062844306230545, 0.09996836632490158, 0.09970974922180176, 0.0995546355843544, 0.09945247322320938, 0.09907866269350052, 0.09763845056295395]}}
{"id": "WebQTest-1403", "question": "who was canadian prime minister in 1993", "prediction": [["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["people.person.nationality"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.government_position_held.appointed_by"]], "ground_paths": [["base.firsts.first_achievement.achievement", "base.firsts.first_achievement.first"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was canadian prime minister in 1993 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.appointed_by </PATH>"], "scores": [-0.0076833972707390785, -0.016738900914788246, -0.02849312126636505, -0.03319946676492691, -0.04176272824406624, -0.07350322604179382, -0.07811945676803589, -0.08573628962039948, -0.08662304282188416, -0.08948968350887299], "norm_scores": [0.10470721125602722, 0.10376331210136414, 0.10255079716444016, 0.10206928849220276, 0.10119897872209549, 0.09803731739521027, 0.09758578985929489, 0.09684532880783081, 0.09675948321819305, 0.09648250043392181]}}
{"id": "WebQTest-1405", "question": "what there to do in salt lake city utah", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["location.location.contains"], ["base.usnationalparks.us_national_park.state"], ["film.film_festival.location"], ["film.film.featured_film_locations"], ["organization.organization.geographic_scope"], ["olympics.olympic_games.host_city"], ["base.biblioness.bibs_location.city"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what there to do in salt lake city utah [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> olympics.olympic_games.host_city </PATH>", "<PATH> base.biblioness.bibs_location.city </PATH>"], "scores": [-0.014396583661437035, -0.014627929776906967, -0.031228099018335342, -0.10785389691591263, -0.12125159800052643, -0.13752847909927368, -0.14139173924922943, -0.1557767540216446, -0.15982367098331451, -0.16374735534191132], "norm_scores": [0.10927200317382812, 0.10924672335386276, 0.10744818300008774, 0.0995224118232727, 0.09819794446229935, 0.09661252051591873, 0.09623999893665314, 0.09486549347639084, 0.09448236227035522, 0.09411235898733139]}}
{"id": "WebQTest-1408", "question": "who is the current ohio state senator", "prediction": [["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current ohio state senator [/INST]", "raw_output": {"paths": ["<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.country.first_level_divisions <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.person.place_of_birth </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.023610414937138557, -0.02714828960597515, -0.02719506435096264, -0.03147566691040993, -0.031564030796289444, -0.03738972917199135, -0.04435177519917488, -0.04499315470457077, -0.04563218355178833, -0.04601550102233887], "norm_scores": [0.10123682767152786, 0.10087929666042328, 0.10087458044290543, 0.10044369846582413, 0.1004348173737526, 0.09985141456127167, 0.09915865957736969, 0.09909508377313614, 0.09903178364038467, 0.09899383038282394]}}
{"id": "WebQTest-1409", "question": "what are the major languages spoken in the united kingdom", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.first_level_divisions"], ["base.rosetta.languoid.languoid_class"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.015778256580233574, -0.02240089885890484, -0.02317654900252819, -0.048917919397354126, -0.126717209815979, -0.12703543901443481, -0.13416461646556854, -0.13597716391086578, -0.1372050791978836, -0.14460603892803192], "norm_scores": [0.1077236533164978, 0.10701259225606918, 0.10692962259054184, 0.10421223193407059, 0.09641195833683014, 0.0963812842965126, 0.09569660574197769, 0.09552330523729324, 0.09540608525276184, 0.0947025939822197]}}
{"id": "WebQTest-1410", "question": "what did st. nicholas do in his life", "prediction": [["people.person.profession"], ["book.written_work.subjects"], ["symbols.namesake.named_after"], ["base.saints.saint.venerated_in"], ["symbols.name_source.namesakes"], ["base.activism.activist.area_of_activism"], ["base.saints.saint.venerated_by"], ["common.topic.notable_types"], ["religion.religion.notable_figures"], ["book.book_subject.works"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st. nicholas do in his life [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.saints.saint.venerated_in </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.saints.saint.venerated_by </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.017359448596835136, -0.032967135310173035, -0.03681829944252968, -0.03839346393942833, -0.049000855535268784, -0.05972539260983467, -0.05997857078909874, -0.07543230056762695, -0.08292483538389206, -0.08619420975446701], "norm_scores": [0.10369487851858139, 0.10208900272846222, 0.10169660300016403, 0.10153654217720032, 0.10046519339084625, 0.09939350187778473, 0.09936834126710892, 0.09784454107284546, 0.09711417555809021, 0.09679719805717468]}}
{"id": "WebQTest-1414", "question": "what happened to pope john paul ii", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["deceased_person.cause_of_death"], ["people.person.deceased_people"], ["people.deceased_person.place_of_death"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["people.person.deceased_cause"], ["people.person.deceased_cause_of_death"], ["film.film_subject.films", "film.film.subjects"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to pope john paul ii [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_people </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_cause </PATH>", "<PATH> people.person.deceased_cause_of_death </PATH>", "<PATH> film.film_subject.films <SEP> film.film.subjects </PATH>"], "scores": [-0.0006449734210036695, -0.06532208621501923, -0.11350467056035995, -0.11396519094705582, -0.1149723008275032, -0.12007944285869598, -0.12429429590702057, -0.12950918078422546, -0.1296968013048172, -0.14201408624649048], "norm_scores": [0.11095356941223145, 0.10400455445051193, 0.09911216050386429, 0.09906652569770813, 0.0989667996764183, 0.09846265614032745, 0.09804852306842804, 0.0975385457277298, 0.09752023965120316, 0.09632642567157745]}}
{"id": "WebQTest-1416", "question": "what was franklin pierce famous for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["common.topic.notable_types"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_office_category.officeholders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_position_held.office_holder_position_or_title"], ["government.politician.government_positions_held", "people.person.profession"], ["government.politician.government_positions_held"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin pierce famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_category.officeholders </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_holder_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held </PATH>"], "scores": [-0.011407028883695602, -0.031475044786930084, -0.03856290876865387, -0.040889937430620193, -0.042195986956357956, -0.04301805421710014, -0.043849650770425797, -0.06351412087678909, -0.06494694203138351, -0.06600196659564972], "norm_scores": [0.10336030274629593, 0.1013067439198494, 0.10059123486280441, 0.10035742819309235, 0.10022644698619843, 0.10014408081769943, 0.1000608429312706, 0.09811241179704666, 0.09797193109989166, 0.09786862134933472]}}
{"id": "WebQTest-1417", "question": "when was the last time the boston bruins went to the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_championship.events"], ["sports.professional_sports_team.owner_s", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event"], ["sports.professional_sports_team.owner_s", "sports.sports_team.owner_s", "sports.sports_championship_event.champion"], ["time.recurring_event.instances"], ["sports.professional_sports_team.owner_s", "sports.sports_team.owner_s"]], "ground_paths": [["sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the boston bruins went to the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.owner_s <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.owner_s </PATH>"], "scores": [-0.011144505813717842, -0.015054825693368912, -0.02939865179359913, -0.08331161737442017, -0.10055933147668839, -0.1025589182972908, -0.10813894122838974, -0.1193140298128128, -0.12936340272426605, -0.12961037456989288], "norm_scores": [0.10732711106538773, 0.10690824687480927, 0.10538571327924728, 0.09985450655221939, 0.09814701229333878, 0.09795095771551132, 0.09740591049194336, 0.09632344543933868, 0.09536030888557434, 0.09533675760030746]}}
{"id": "WebQTest-1418", "question": "who is meredith gray married to in real life", "prediction": [["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"], ["fictional_universe.marriage_of_fictional_characters.spouses", "fictional_universe.marriage_of_fictional_characters.spouses"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character_creator.fictional_characters_created", "people.person.nationality", "people.person.nationality"], ["fictional_universe.fictional_character.spouses_s", "fictional_universe.marriage_of_fictional_characters.spouses"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is meredith gray married to in real life [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>", "<PATH> fictional_universe.marriage_of_fictional_characters.spouses <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> fictional_universe.fictional_character.children <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.spouses_s <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>"], "scores": [-0.02067899890244007, -0.026755915954709053, -0.029377177357673645, -0.03475193306803703, -0.036756403744220734, -0.038244716823101044, -0.038470011204481125, -0.04765726998448372, -0.04890536144375801, -0.052217088639736176], "norm_scores": [0.10167958587408066, 0.1010635644197464, 0.10079899430274963, 0.10025867819786072, 0.10005791485309601, 0.09990910440683365, 0.09988659620285034, 0.0989731103181839, 0.09884966909885406, 0.0985228419303894]}}
{"id": "WebQTest-1419", "question": "what inspired michelangelo buonarroti", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["symbols.namesake.named_after"], ["people.person.profession"], ["visual_art.visual_artist.art_forms"], ["people.person.gender", "people.person.gender"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michelangelo buonarroti [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> influence.influence_node.influenced <PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.005572856869548559, -0.020034445449709892, -0.09723467379808426, -0.098769910633564, -0.12560753524303436, -0.12630099058151245, -0.14378243684768677, -0.15266728401184082, -0.15442660450935364, -0.1560356616973877], "norm_scores": [0.11063943803310394, 0.1090509369969368, 0.10094894468784332, 0.10079407691955566, 0.09812498092651367, 0.09805695712566376, 0.09635767340660095, 0.09550534188747406, 0.09533746540546417, 0.09518418461084366]}}
{"id": "WebQTest-1420", "question": "what does monsanto own", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization.child"], ["business.business_operation.industry"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["organization.organization_relationship.parent", "organization.organization_relationship.parent"], ["business.industry.companies"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization.spun_off_from", "organization.organization.spin_offs"], ["organization.organization.place_founded"]], "ground_paths": [["business.industry.companies"], ["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does monsanto own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>", "<PATH> business.business_operation.industry </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization_relationship.parent </PATH>", "<PATH> organization.organization_relationship.child <SEP> organization.organization_relationship.parent </PATH>", "<PATH> organization.organization_relationship.parent <SEP> organization.organization_relationship.parent </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.spun_off_from <SEP> organization.organization.spin_offs </PATH>", "<PATH> organization.organization.place_founded </PATH>"], "scores": [-0.03389982506632805, -0.03962422534823418, -0.04119016230106354, -0.04757295176386833, -0.05100400373339653, -0.05406028404831886, -0.058936815708875656, -0.059317898005247116, -0.061998188495635986, -0.06599389761686325], "norm_scores": [0.10175615549087524, 0.10117532312870026, 0.10101702064275742, 0.10037429630756378, 0.10003049671649933, 0.09972523897886276, 0.09924011677503586, 0.09920229762792587, 0.0989367663860321, 0.0985422357916832]}}
{"id": "WebQTest-1421", "question": "who did woody harrelson play on cheers", "prediction": [["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.character"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["people.person.gender", "people.person.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.profession", "fictional_universe.fictional_character.occupation"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did woody harrelson play on cheers [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.024876488372683525, -0.025878649204969406, -0.025921139866113663, -0.03066324256360531, -0.03650951012969017, -0.042964786291122437, -0.04804938659071922, -0.051107291132211685, -0.055691055953502655, -0.058832015842199326], "norm_scores": [0.10152112692594528, 0.10141944140195847, 0.10141512751579285, 0.10093533992767334, 0.10034698247909546, 0.09970128536224365, 0.09919564425945282, 0.09889276325702667, 0.0984405130147934, 0.09813179075717926]}}
{"id": "WebQTest-1422", "question": "what language do they speak in sweden yahoo", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in sweden yahoo [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.012787774205207825, -0.01945783570408821, -0.03347502276301384, -0.0571196973323822, -0.17438946664333344, -0.18453015387058258, -0.18482552468776703, -0.18864251673221588, -0.18918439745903015, -0.193450465798378], "norm_scores": [0.11140597611665726, 0.11066536605358124, 0.10912496596574783, 0.10657501220703125, 0.09478197991847992, 0.09382568299770355, 0.093797966837883, 0.09344062954187393, 0.09339001029729843, 0.09299244731664658]}}
{"id": "WebQTest-1423", "question": "what language did they speak in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did they speak in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.004399253521114588, -0.03738269582390785, -0.047549132257699966, -0.0790676474571228, -0.17721714079380035, -0.1817118376493454, -0.18510748445987701, -0.19138431549072266, -0.19870689511299133, -0.2055555135011673], "norm_scores": [0.1131540909409523, 0.10948275774717331, 0.10837534815073013, 0.10501279681921005, 0.09519550204277039, 0.09476858377456665, 0.09444732218980789, 0.09385635703802109, 0.09317159652709961, 0.09253567457199097]}}
{"id": "WebQTest-1424", "question": "what form of government do we have in the philippines", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["people.person.nationality", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["form.form_of_government.countries"], ["base.locations.countries.continent", "location.country.form_of_government"], ["political_form.countries.form_of_government"], ["government.form_of_government.countries", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government do we have in the philippines [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent <SEP> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> political_form.countries.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>"], "scores": [-0.008599764667451382, -0.014145580120384693, -0.1347820907831192, -0.1454116702079773, -0.14642487466335297, -0.15602801740169525, -0.1584903597831726, -0.16369009017944336, -0.1644423007965088, -0.1648491770029068], "norm_scores": [0.11222824454307556, 0.11160757392644882, 0.09892404824495316, 0.097878098487854, 0.09777897596359253, 0.09684448689222336, 0.0966063141822815, 0.09610529243946075, 0.09603302925825119, 0.09599396586418152]}}
{"id": "WebQTest-1425", "question": "what colleges did mitt romney attend", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.religion", "religion.religious_organization.associated_with"], ["people.person.place_of_birth", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["people.person.nationality", "base.biblioness.bibs_location.country"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.religion", "religion.religion.organizations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did mitt romney attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.religion <SEP> religion.religious_organization.associated_with </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.018040550872683525, -0.02336501143872738, -0.024977553635835648, -0.037848033010959625, -0.043946996331214905, -0.049739379435777664, -0.0685669481754303, -0.09418412297964096, -0.09622308611869812, -0.09641040116548538], "norm_scores": [0.10375361144542694, 0.10320264846086502, 0.10303635895252228, 0.10171873867511749, 0.10110024362802505, 0.10051632672548294, 0.09864155203104019, 0.09614672511816025, 0.0959508866071701, 0.09593291580677032]}}
{"id": "WebQTest-1426", "question": "where is ibrahim afellay from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["sports.sports_team_roster.player", "sports.sports_team_roster.place_of_birth"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["location.location.people_born_here", "location.location.containedby"], ["sports.pro_athlete.teams", "sports.sports_team_roster.place_of_birth"], ["people.ethnicity.people_with_this_ethnicity"], ["people.person.place_of_birth", "location.location.containedby"], ["sports.pro_athlete.teams", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ibrahim afellay from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.place_of_birth </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.place_of_birth </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> sports.pro_athlete.teams <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.004898421466350555, -0.03783608600497246, -0.04910638555884361, -0.06581787019968033, -0.0715799555182457, -0.09512332826852798, -0.0958673506975174, -0.09707038104534149, -0.10050417482852936, -0.1036035344004631], "norm_scores": [0.10690273344516754, 0.10343896597623825, 0.10227972269058228, 0.10058467835187912, 0.10000676661729813, 0.09767977893352509, 0.09760712087154388, 0.09748976677656174, 0.09715558588504791, 0.09685492515563965]}}
{"id": "WebQTest-1427", "question": "who was the secretary of state when andrew jackson was president", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.appointed_by", "government.government_position_held.office_holder"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["people.person.gender", "people.person.gender"], ["government.government_office_category.officeholders", "government.government_position_held.office_holder"], ["government.us_vice_president.to_president"], ["government.government_office_category.officeholders", "government.politician.government_positions_held"], ["people.person.profession", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.appointed_by", "government.government_position_held.office_holder"], ["government.political_appointer.appointees", "government.government_position_held.office_holder"], ["people.person.profession", "common.topic.notable_types"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["people.person.nationality", "people.person.nationality"], ["government.us_vice_president.to_president"], ["people.person.gender", "people.person.gender"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the secretary of state when andrew jackson was president [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.politician.government_positions_held </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> government.government_office_category.officeholders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.government_office_category.officeholders <SEP> government.politician.government_positions_held </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.021954040974378586, -0.024727409705519676, -0.02913803979754448, -0.029812291264533997, -0.03610102832317352, -0.03867151960730553, -0.03924889117479324, -0.0406750813126564, -0.04122592508792877, -0.04722755402326584], "norm_scores": [0.10129781067371368, 0.10101726651191711, 0.10057269781827927, 0.10050490498542786, 0.09987484663724899, 0.09961844235658646, 0.09956094622612, 0.09941905736923218, 0.09936429560184479, 0.09876974672079086]}}
{"id": "WebQTest-1428", "question": "who does ron stoppable 's voice", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ron stoppable 's voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.021483922377228737, -0.030249113216996193, -0.033573925495147705, -0.03811575472354889, -0.038350701332092285, -0.038931313902139664, -0.041232604533433914, -0.0434412844479084, -0.047835949808359146, -0.048710472881793976], "norm_scores": [0.10168181359767914, 0.10079444944858551, 0.10045988112688065, 0.100004643201828, 0.0999811515212059, 0.09992311894893646, 0.0996934249997139, 0.09947347640991211, 0.0990372821688652, 0.09895072132349014]}}
{"id": "WebQTest-1429", "question": "what currency is accepted in new zealand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "finance.currency.countries_used"], ["language.human_language.main_country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is accepted in new zealand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> finance.currency.countries_used </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.008141851983964443, -0.020163599401712418, -0.1195642501115799, -0.12081144005060196, -0.12135832011699677, -0.12549935281276703, -0.1297052651643753, -0.136711984872818, -0.13827432692050934, -0.14349763095378876], "norm_scores": [0.11019827425479889, 0.10888142883777618, 0.0985790565609932, 0.09845619648694992, 0.09840235859155655, 0.09799570590257645, 0.09758441895246506, 0.09690306335687637, 0.09675177931785583, 0.09624774008989334]}}
{"id": "WebQTest-1432", "question": "what places are in the west midlands", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.first_level_division_of"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_area"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.containedby", "common.topic.image"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.contains"], ["location.location.containedby"], []], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places are in the west midlands [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.005474678240716457, -0.03694426640868187, -0.048012733459472656, -0.06241694092750549, -0.07420733571052551, -0.08902719616889954, -0.09918803721666336, -0.11461657285690308, -0.12256322056055069, -0.12449885159730911], "norm_scores": [0.10741275548934937, 0.10408515483140945, 0.10293944925069809, 0.1014673113822937, 0.10027799755334854, 0.098802849650383, 0.0978040099143982, 0.09630661457777023, 0.09554433077573776, 0.09535958617925644]}}
{"id": "WebQTest-1433", "question": "what to do with kids in asheville nc", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["base.usnationalparks.us_national_park.state"], ["travel.family_travel_destination.tourist_attractions"], ["film.film_festival.location"], ["organization.organization.geographic_scope"], ["travel.location.containedby"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in asheville nc [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>", "<PATH> travel.family_travel_destination.tourist_attractions </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> travel.travel_destination.tourist_attractions <PATH>", "<PATH> travel.location.containedby </PATH>"], "scores": [-0.007943369448184967, -0.020322559401392937, -0.03910097852349281, -0.07527437806129456, -0.1367824226617813, -0.1432674080133438, -0.15047957003116608, -0.1524209976196289, -0.16480371356010437, -0.16756077110767365], "norm_scores": [0.1100790947675705, 0.10872480273246765, 0.10670217126607895, 0.10291136801242828, 0.0967722237110138, 0.09614669531583786, 0.09545576572418213, 0.09527062624692917, 0.09409818798303604, 0.0938391163945198]}}
{"id": "WebQTest-1434", "question": "who is in the american league in baseball", "prediction": [["baseball.baseball_team.league"], ["baseball.baseball_league.teams"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["sports.sports_position.sport", "baseball.baseball_player.position_s"], ["sports.sports_position.sport", "sports.pro_athlete.teams"], ["sports.sports_position.sport", "american_football.football_player.position_s"], ["sports.sports_position.sport", "sports.sport.positions", "people.person.nationality"], ["sports.sports_league_participation.league", "sports.sports_league_participation.team"], ["sports.sports_position.sport", "sports.sport.positions", "baseball.baseball_player.position_s"], ["sports.sports_league_participation.league", "sports.sports_team.league"]], "ground_paths": [["baseball.baseball_team.league", "sports.sports_team.previously_known_as"], ["baseball.baseball_team.league"], ["sports.sports_league.sport", "sports.sports_team.sport"], ["sports.sports_league.teams", "sports.sports_team.league"], ["baseball.baseball_team.league", "sports.defunct_sports_team.later_known_as"], ["sports.sports_league.teams", "sports.sports_league_participation.team"], ["sports.sports_league_participation.league", "sports.sports_team.league"], ["baseball.baseball_league.teams"], ["baseball.baseball_league.teams", "sports.sports_team.previously_known_as"], ["sports.sports_league.sport", "sports.defunct_sports_team.sport"], ["baseball.baseball_league.teams", "sports.defunct_sports_team.later_known_as"], ["sports.sports_league_participation.league", "sports.sports_league_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in the american league in baseball [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.league </PATH>", "<PATH> baseball.baseball_league.teams </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.sports_position.sport <SEP> baseball.baseball_player.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.pro_athlete.teams </PATH>", "<PATH> sports.sports_position.sport <SEP> american_football.football_player.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sport.positions <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_league_participation.team </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sport.positions <SEP> baseball.baseball_player.position_s </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_team.league </PATH>"], "scores": [-0.011136634275317192, -0.02670878730714321, -0.04367890954017639, -0.05043446272611618, -0.06006474420428276, -0.06325556337833405, -0.06967184692621231, -0.0726282149553299, -0.0728430524468422, -0.07529496401548386], "norm_scores": [0.10441704094409943, 0.10280364006757736, 0.10107377171516418, 0.10039325803518295, 0.09943109005689621, 0.0991143211722374, 0.09848042577505112, 0.09818969666957855, 0.09816860407590866, 0.09792821109294891]}}
{"id": "WebQTest-1435", "question": "who did kurt warner play 4", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.football_player.passing", "american_football.player_passing_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "ground_paths": [["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.passing", "american_football.player_passing_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kurt warner play 4 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_passing_statistics.player <SEP> american_football.player_passing_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.passing <SEP> american_football.player_passing_statistics.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.024518851190805435, -0.025521717965602875, -0.02915453352034092, -0.030690666288137436, -0.03340815007686615, -0.03359152376651764, -0.03530770167708397, -0.04801240190863609, -0.05036129057407379, -0.051381248980760574], "norm_scores": [0.10116980969905853, 0.1010684072971344, 0.10070190578699112, 0.10054732859134674, 0.1002744734287262, 0.10025607794523239, 0.10008417069911957, 0.09882067888975143, 0.09858882427215576, 0.09848832339048386]}}
{"id": "WebQTest-1436", "question": "who is dirk nowitzki wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.marriage.spouse"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dirk nowitzki wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.03076859936118126, -0.03342778608202934, -0.03982684016227722, -0.04016824811697006, -0.04162251576781273, -0.04797527939081192, -0.04847389832139015, -0.04859062656760216, -0.05294950678944588, -0.05399938300251961], "norm_scores": [0.10130681842565536, 0.10103778541088104, 0.10039329528808594, 0.10035902261734009, 0.1002131775021553, 0.09957857429981232, 0.09952893853187561, 0.09951731562614441, 0.09908447414636612, 0.09898050129413605]}}
{"id": "WebQTest-1441", "question": "where all did picasso live", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.children", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death"], ["people.person.children", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"], ["book.written_work.subjects", "book.written_work.subjects"], ["people.person.nationality", "location.location.containedby"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["symbols.name_source.namesakes", "location.location.containedby"], ["people.person.parents", "people.person.nationality"], ["people.person.places_lived", "people.place_lived.location"], ["symbols.namesake.named_after", "film.film.country"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced", "people.person.nationality"], ["influence.influence_node.influenced_by", "people.person.nationality"], ["film.film_subject.films", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where all did picasso live [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> people.person.children <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.02469380758702755, -0.027279864996671677, -0.031051546335220337, -0.04062088206410408, -0.0500907339155674, -0.050270188599824905, -0.05550997704267502, -0.06098100543022156, -0.062235742807388306, -0.06424858421087265], "norm_scores": [0.10221460461616516, 0.10195062309503555, 0.10156681388616562, 0.10059952735900879, 0.09965135157108307, 0.09963347017765045, 0.09911277890205383, 0.0985720157623291, 0.09844841808080673, 0.09825044870376587]}}
{"id": "WebQTest-1442", "question": "what currency shall i take to croatia", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["common.topic.notable_types"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency shall i take to croatia [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.008627718314528465, -0.019541975110769272, -0.08003286272287369, -0.1393769234418869, -0.13945606350898743, -0.1484493762254715, -0.15211926400661469, -0.17011813819408417, -0.1727759838104248, -0.17725153267383575], "norm_scores": [0.11166703701019287, 0.11045490205287933, 0.1039714515209198, 0.09798087179660797, 0.09797311574220657, 0.09709597378969193, 0.09674029797315598, 0.09501464664936066, 0.09476245194673538, 0.0943392813205719]}}
{"id": "WebQTest-1443", "question": "what team is raul ibanez on", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["people.person.nationality", "sports.sports_team_location.teams"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is raul ibanez on [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>"], "scores": [-0.015617625787854195, -0.019956182688474655, -0.024459294974803925, -0.026771415024995804, -0.05076716095209122, -0.053660210222005844, -0.05857934057712555, -0.07034846395254135, -0.07162028551101685, -0.08659256994724274], "norm_scores": [0.10324584692716599, 0.10279888659715652, 0.10233700275421143, 0.1021006628870964, 0.0996798500418663, 0.0993918851017952, 0.09890415519475937, 0.09774696826934814, 0.09762272983789444, 0.09617198258638382]}}
{"id": "WebQTest-1445", "question": "what tv series did mark harmon star in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.languages", "tv.tv_program.languages"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.languages", "film.film.language"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["people.person.languages", "film.film.language"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv series did mark harmon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.languages <SEP> tv.tv_program.languages </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_program.regular_cast </PATH>"], "scores": [-0.020587394014000893, -0.024913297966122627, -0.031978677958250046, -0.03809720277786255, -0.03966500610113144, -0.04206357151269913, -0.043596383184194565, -0.04505879804491997, -0.04546594247221947, -0.054729800671339035], "norm_scores": [0.10181436687707901, 0.10137487947940826, 0.10066115111112595, 0.10004712641239166, 0.09989040344953537, 0.09965110570192337, 0.09949846565723419, 0.09935306757688522, 0.09931262582540512, 0.09839685261249542]}}
{"id": "WebQTest-1446", "question": "what year did jimmy the rev sullivan die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"], ["people.person.deceased_place_of_death"], ["people.person.nationality", "location.location.events"], ["people.person.deceased_people"], ["people.cause_of_death.people"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.deceased_place_of_death"], ["people.person.deceased_year"], ["people.person.gender", "people.person.gender"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did jimmy the rev sullivan die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.events </PATH>", "<PATH> people.person.deceased_people </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.deceased_place_of_death </PATH>", "<PATH> people.person.deceased_year </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.0007665989687666297, -0.06449174880981445, -0.0764055848121643, -0.09040690958499908, -0.1026124507188797, -0.10618264228105545, -0.10866989940404892, -0.1117895096540451, -0.12768469750881195, -0.12835846841335297], "norm_scores": [0.1094517707824707, 0.10269453376531601, 0.10147830098867416, 0.1000673770904541, 0.09885342419147491, 0.09850112348794937, 0.0982564315199852, 0.09795039147138596, 0.0964057520031929, 0.09634082019329071]}}
{"id": "WebQTest-1447", "question": "who plays anakin skywalker in star wars 1", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film.country", "people.person.nationality"], ["film.film.starring", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.film.language", "people.person.languages"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["film.film.starring", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.film.language", "people.person.languages"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays anakin skywalker in star wars 1 [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film.language <SEP> people.person.languages </PATH>"], "scores": [-0.01597019098699093, -0.02262280322611332, -0.024287760257720947, -0.04192069172859192, -0.04498780891299248, -0.04777766764163971, -0.048900678753852844, -0.05789276212453842, -0.059892185032367706, -0.060642778873443604], "norm_scores": [0.1026751846075058, 0.10199438780546188, 0.10182472318410873, 0.10004499554634094, 0.09973861277103424, 0.09946074336767197, 0.09934910386800766, 0.09845975786447525, 0.09826309233903885, 0.09818936884403229]}}
{"id": "WebQTest-1448", "question": "where does michelle pfeiffer live now", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.children", "people.person.place_of_birth"], ["people.person.children", "people.deceased_person.place_of_death"], ["people.person.parents", "people.person.place_of_birth"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.country.administrative_divisions"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does michelle pfeiffer live now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.children <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.country.administrative_divisions </PATH>"], "scores": [-0.015975942835211754, -0.03156210109591484, -0.03227672353386879, -0.032872024923563004, -0.03801121935248375, -0.05498252809047699, -0.057653509080410004, -0.06611901521682739, -0.08450207859277725, -0.08517863601446152], "norm_scores": [0.10342607647180557, 0.10182656347751617, 0.10175381600856781, 0.10169326514005661, 0.10117197781801224, 0.09946945309638977, 0.09920412302017212, 0.0983678549528122, 0.09657607972621918, 0.09651076048612595]}}
{"id": "WebQTest-1449", "question": "what was laura ingalls wilder famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["book.written_work.author"], ["fictional_universe.fictional_character.occupation"], ["book.book_edition.author_editor"], ["book.author.works_written"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced_by", "people.person.profession"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["influence.influence_node.influenced", "people.person.profession"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was laura ingalls wilder famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>"], "scores": [-0.0034748450852930546, -0.04657414183020592, -0.05326646938920021, -0.08108421415090561, -0.08348658680915833, -0.08886735886335373, -0.09945681691169739, -0.11394002288579941, -0.11554329842329025, -0.11630059778690338], "norm_scores": [0.10790961980819702, 0.10335758328437805, 0.10266819596290588, 0.09985155612230301, 0.09961196035146713, 0.09907741099596024, 0.09803377091884613, 0.09662416577339172, 0.09646937996149063, 0.09639634937047958]}}
{"id": "WebQTest-1452", "question": "where is dwight howard now", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team.location"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwight howard now [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.location </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.01964777708053589, -0.021799080073833466, -0.02370183728635311, -0.02936505898833275, -0.05176804959774017, -0.053991686552762985, -0.056529320776462555, -0.06317427009344101, -0.06318998336791992, -0.06840644031763077], "norm_scores": [0.10256657004356384, 0.10234615951776505, 0.10215160250663757, 0.10157472640275955, 0.09932445734739304, 0.09910383820533752, 0.09885267168283463, 0.09819797426462173, 0.09819643199443817, 0.09768553078174591]}}
{"id": "WebQTest-1453", "question": "who is nick lachey married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.parents", "people.person.children"], ["people.person.languages", "people.person.languages"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nick lachey married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02069292962551117, -0.035647619515657425, -0.03823069483041763, -0.0431290902197361, -0.04716486856341362, -0.04741666838526726, -0.048432037234306335, -0.053205400705337524, -0.05388062074780464, -0.05784167721867561], "norm_scores": [0.10241039842367172, 0.10089027881622314, 0.1006300076842308, 0.10013829171657562, 0.09973496198654175, 0.09970986098051071, 0.09960866719484329, 0.09913433343172073, 0.09906741976737976, 0.09867577999830246]}}
{"id": "WebQTest-1454", "question": "where did madonna grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["influence.influence_node.influenced_by", "location.location.people_born_here"], ["influence.influence_node.influenced_by"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["music.composition.composer"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did madonna grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.007764448877424002, -0.03448609635233879, -0.0379609540104866, -0.10410632938146591, -0.11593297123908997, -0.12063378095626831, -0.12438933551311493, -0.12735199928283691, -0.13254475593566895, -0.13384240865707397], "norm_scores": [0.10888232290744781, 0.10601133853197098, 0.1056436076760292, 0.09888187050819397, 0.09771931171417236, 0.09726103395223618, 0.0968964472413063, 0.09660979360342026, 0.09610942751169205, 0.0959847941994667]}}
{"id": "WebQTest-1455", "question": "what capital city of brazil", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.second_level_division_of"], ["location.administrative_division.country"], ["location.administrative_division.first_level_division_of"], ["base.biblioness.bibs_location.state"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["base.aareas.schema.administrative_area.capital"], ["people.person.place_of_birth"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital city of brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.second_level_division_of </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> people.person.place_of_birth <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.004068559966981411, -0.04246269538998604, -0.06849844753742218, -0.07777336239814758, -0.07978960126638412, -0.07985804975032806, -0.1273745745420456, -0.13082019984722137, -0.134171724319458, -0.14301146566867828], "norm_scores": [0.10874062031507492, 0.10464474558830261, 0.10195540636777878, 0.101014144718647, 0.10081068426370621, 0.10080379247665405, 0.09612595289945602, 0.09579531848430634, 0.09547478705644608, 0.0946345329284668]}}
{"id": "WebQTest-1456", "question": "what college did john stockton go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["common.topic.notable_types"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth", "location.location.contains"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john stockton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.012797328643500805, -0.0215945802628994, -0.025270694866776466, -0.045731741935014725, -0.05724892020225525, -0.062497179955244064, -0.0674917921423912, -0.07520335167646408, -0.07850831001996994, -0.09980958700180054], "norm_scores": [0.10423368960618973, 0.10332074016332626, 0.10294162482023239, 0.1008567288517952, 0.09970180690288544, 0.09917991608381271, 0.09868578612804413, 0.09792768955230713, 0.09760457277297974, 0.09554746001958847]}}
{"id": "WebQTest-1457", "question": "what kind of currency does mexico use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "finance.currency.countries_used"], ["film.film.featured_film_locations", "film.film.country"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["film.film_location.featured_in_films", "finance.currency.countries_used"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does mexico use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> finance.currency.countries_used </PATH>", "<PATH> film.film.featured_film_locations <SEP> film.film.country </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> film.film_location.featured_in_films <SEP> finance.currency.countries_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>"], "scores": [-0.002503747586160898, -0.04506918787956238, -0.07158003002405167, -0.09096555411815643, -0.09883307665586472, -0.09915348887443542, -0.10439183562994003, -0.10443625599145889, -0.10514376312494278, -0.10738664120435715], "norm_scores": [0.10831815004348755, 0.10380429029464722, 0.1010885089635849, 0.099147729575634, 0.0983707457780838, 0.09833922237157822, 0.09782543033361435, 0.09782109409570694, 0.09775190055370331, 0.09753290563821793]}}
{"id": "WebQTest-1458", "question": "who owns chrysler corporation 2011", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["organization.organization.parent", "organization.organization.child"], ["business.employer.employees", "business.employment_tenure.person"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["common.topic.notable_types", "common.topic.notable_types"], ["business.employment_tenure.company", "business.employment_tenure.person"], ["organization.organization_board_membership.organization", "organization.organization_board_membership.member"], ["business.employer.employees", "people.person.employment_history"], ["business.employer.employees", "business.employer.employees"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["organization.organization.parent", "organization.organization.child"], ["organization.organization.parent", "organization.organization_relationship.parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns chrysler corporation 2011 [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization_relationship.child <SEP> organization.organization_relationship.parent </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>", "<PATH> business.employer.employees <SEP> business.employment_tenure.person </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization_relationship.parent </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> business.employment_tenure.company <SEP> business.employment_tenure.person </PATH>", "<PATH> organization.organization_board_membership.organization <SEP> organization.organization_board_membership.member </PATH>", "<PATH> business.employer.employees <SEP> people.person.employment_history </PATH>", "<PATH> business.employer.employees <SEP> business.employer.employees </PATH>"], "scores": [-0.016606267541646957, -0.022689757868647575, -0.027978748083114624, -0.03605063632130623, -0.038957078009843826, -0.04648319631814957, -0.051589664071798325, -0.053659480065107346, -0.06050487235188484, -0.07346979528665543], "norm_scores": [0.102639339864254, 0.10201682895421982, 0.1014786884188652, 0.10066285729408264, 0.10037071257829666, 0.09961814433336258, 0.09911075234413147, 0.09890581667423248, 0.09823108464479446, 0.0969657450914383]}}
{"id": "WebQTest-1459", "question": "what are some books that mark twain wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["film.film_story_contributor.film_story_credits"], ["book.author.book_editions_published"], ["film.film.story_by"], ["influence.influence_node.influenced", "book.written_work.author"], ["influence.influence_node.influenced_by", "book.written_work.author"], ["influence.influence_node.influenced_by"], []], "ground_paths": [["book.book_subject.works"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["book.written_work.subjects"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some books that mark twain wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> </PATH>"], "scores": [-0.011466564610600471, -0.024721093475818634, -0.03373022750020027, -0.07758025079965591, -0.09617774188518524, -0.09786662459373474, -0.11017180979251862, -0.11509709060192108, -0.11628161370754242, -0.12288116663694382], "norm_scores": [0.10707243531942368, 0.10566261410713196, 0.10471494495868683, 0.10022241622209549, 0.09837576001882553, 0.09820976108312607, 0.09700866788625717, 0.09653204679489136, 0.09641776978969574, 0.09578356146812439]}}
{"id": "WebQTest-1460", "question": "what was gregor mendel contribution", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["law.invention.inventor"], ["symbols.namesake.named_after"], ["people.profession.people_with_this_profession"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.name_source.namesakes"], ["law.inventor.inventions"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was gregor mendel contribution [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.008920661173760891, -0.045003537088632584, -0.05460145324468613, -0.05601579323410988, -0.05962386727333069, -0.05974855273962021, -0.06788906455039978, -0.07734714448451996, -0.08337301760911942, -0.08508006483316422], "norm_scores": [0.10519200563430786, 0.10146403312683105, 0.1004948616027832, 0.10035282373428345, 0.09999138861894608, 0.09997892379760742, 0.09916835278272629, 0.09823483228683472, 0.09764465689659119, 0.09747811406850815]}}
{"id": "WebQTest-1461", "question": "who has ray allen dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["common.topic.notable_types", "common.topic.notable_types"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has ray allen dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> base.popstra.dated.participant <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.celebrity.sexual_relationships </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> celebrities.romantic_relationship.celebrity <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>"], "scores": [-0.01869072951376438, -0.03134371340274811, -0.03306523337960243, -0.043740805238485336, -0.04882907122373581, -0.05260991305112839, -0.05398141220211983, -0.056698717176914215, -0.05694909393787384, -0.058589186519384384], "norm_scores": [0.10270357877016068, 0.10141225904226303, 0.10123782604932785, 0.1001628041267395, 0.0996544361114502, 0.09927837550640106, 0.09914231300354004, 0.09887327253818512, 0.09884851425886154, 0.09868653863668442]}}
{"id": "WebQTest-1462", "question": "who is running for vice president with barack obama 2012", "prediction": [["government.election_campaign.election", "government.election_campaign.candidate"], ["government.election_campaign.election", "government.politician.election_campaigns"], ["government.election_campaign.candidate", "government.election_campaign.candidate"], ["government.election.campaigns", "government.election_campaign.candidate"], ["time.event.locations", "people.person.nationality"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.presidential_nominee"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is running for vice president with barack obama 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.election <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election_campaign.election <SEP> government.politician.election_campaigns </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election.campaigns <SEP> government.election_campaign.candidate </PATH>", "<PATH> time.event.locations <SEP> people.person.nationality </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.presidential_nominee </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.016033684834837914, -0.030385617166757584, -0.03107408620417118, -0.03146347030997276, -0.03241544961929321, -0.038691069930791855, -0.04604397714138031, -0.05016782134771347, -0.056867264211177826, -0.05702338367700577], "norm_scores": [0.10231680423021317, 0.10085885971784592, 0.10078943520784378, 0.10075020045042038, 0.10065432637929916, 0.10002464056015015, 0.0992918610572815, 0.098883256316185, 0.0982230082154274, 0.09820766746997833]}}
{"id": "WebQTest-1463", "question": "what language do people from the uk speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["location.country.first_languages"], ["language.human_language.countries_spoken_in_"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from the uk speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.form_of_government </PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.first_languages </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> language.human_language.countries_spoken_in_ </PATH>"], "scores": [-0.015520247630774975, -0.0172909926623106, -0.04031800478696823, -0.04647190123796463, -0.14883287250995636, -0.18590998649597168, -0.18786948919296265, -0.18837253749370575, -0.18948306143283844, -0.20020103454589844], "norm_scores": [0.11090860515832901, 0.11071239411830902, 0.10819213837385178, 0.10752838850021362, 0.09706626832485199, 0.0935332328081131, 0.0933501273393631, 0.09330318123102188, 0.09319961816072464, 0.09220603853464127]}}
{"id": "WebQTest-1464", "question": "what to do with my kids in toronto", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["organization.organization.geographic_scope"], ["film.film_festival.location"], ["film.film.featured_film_locations"], ["base.biblioness.bibs_location.country"], ["location.neighborhood.neighborhood_of"], ["book.newspaper.circulation_areas"], ["base.biblioness.bibs_location.city"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["film.film_festival.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with my kids in toronto [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.neighborhood.neighborhood_of </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> base.biblioness.bibs_location.city </PATH>"], "scores": [-0.013914004899561405, -0.014189939945936203, -0.03354654833674431, -0.12204480916261673, -0.13361361622810364, -0.13999736309051514, -0.14230450987815857, -0.14451812207698822, -0.15197890996932983, -0.16574904322624207], "norm_scores": [0.10948383808135986, 0.10945363342761993, 0.10735535621643066, 0.09826286882162094, 0.0971326231956482, 0.09651453047990799, 0.09629211574792862, 0.09607919305562973, 0.09536504000425339, 0.09406084567308426]}}
{"id": "WebQTest-1465", "question": "what kind of government is south korea", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["people.person.nationality", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent", "location.country.form_of_government"], ["locations.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is south korea [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.country.form_of_government </PATH>", "<PATH> locations.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.009752889163792133, -0.014378665946424007, -0.13462291657924652, -0.13619309663772583, -0.14143960177898407, -0.15373098850250244, -0.16018782556056976, -0.16124200820922852, -0.16210347414016724, -0.16513951122760773], "norm_scores": [0.11190342158079147, 0.1113869771361351, 0.0987672507762909, 0.09861229360103607, 0.09809628129005432, 0.09689792990684509, 0.09627428650856018, 0.09617285430431366, 0.09609003365039825, 0.09579873830080032]}}
{"id": "WebQTest-1467", "question": "who did ben franklin marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben franklin marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.027747076004743576, -0.032118577510118484, -0.032986342906951904, -0.04154571518301964, -0.04448774829506874, -0.046676937490701675, -0.0469302274286747, -0.05059647932648659, -0.05105195939540863, -0.05646723508834839], "norm_scores": [0.10153915733098984, 0.1010962501168251, 0.10100855678319931, 0.10014767944812775, 0.09985347092151642, 0.09963511675596237, 0.09960988163948059, 0.09924536198377609, 0.09920015931129456, 0.09866441786289215]}}
{"id": "WebQTest-1468", "question": "what sri lanka capital", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.capital"], ["location.administrative_division.country"], ["language.human_language.countries_spoken_in"], ["common.topic.notable_types"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sri lanka capital [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.009181493893265724, -0.027266494929790497, -0.042294710874557495, -0.10685492306947708, -0.11320266127586365, -0.12227120995521545, -0.1319075971841812, -0.14110960066318512, -0.14384005963802338, -0.1450282484292984], "norm_scores": [0.10918665677309036, 0.10722975432872772, 0.10563033819198608, 0.09902629256248474, 0.09839969128370285, 0.09751138836145401, 0.09657624363899231, 0.09569162875413895, 0.09543070942163467, 0.09531737118959427]}}
{"id": "WebQTest-1470", "question": "what kind of government does poland have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["locations.country.form_of_government"], ["political_form.countries.form_of_government"], ["geography.river.basin_countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does poland have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> locations.country.form_of_government </PATH>", "<PATH> political_form.countries.form_of_government </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.country.form_of_government <PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.01080840639770031, -0.013435604982078075, -0.1464325338602066, -0.14664480090141296, -0.16970670223236084, -0.17393828928470612, -0.17434045672416687, -0.18039777874946594, -0.18219514191150665, -0.18226505815982819], "norm_scores": [0.11332499235868454, 0.1130276620388031, 0.09895207732915878, 0.09893107414245605, 0.09667563438415527, 0.09626741707324982, 0.0962287113070488, 0.09564757347106934, 0.09547582268714905, 0.0954691544175148]}}
{"id": "WebQTest-1471", "question": "what famous people came from delaware", "prediction": [["people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people came from delaware [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.01320132426917553, -0.040139343589544296, -0.042651575058698654, -0.04651443660259247, -0.046750061213970184, -0.04685821756720543, -0.04872952774167061, -0.055484332144260406, -0.05553780868649483, -0.05600537359714508], "norm_scores": [0.10324299335479736, 0.10049895197153091, 0.10024679452180862, 0.0998603105545044, 0.09983677417039871, 0.09982597827911377, 0.09963934868574142, 0.09896857291460037, 0.09896328300237656, 0.09891701489686966]}}
{"id": "WebQTest-1472", "question": "when did romney become governor", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["base.onephylogeny.type_of_thing.things_of_this_type", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.nationality", "location.location.containedby"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office", "location.location.events"], ["government.government_position_held.office_holder", "government.political_district.representatives"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did romney become governor [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location.events </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.political_district.representatives </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location.events </PATH>"], "scores": [-0.02072945423424244, -0.031239207834005356, -0.032027825713157654, -0.033328209072351456, -0.04376381263136864, -0.04523255676031113, -0.04555242136120796, -0.0501093715429306, -0.05177460238337517, -0.05188861861824989], "norm_scores": [0.10199816524982452, 0.1009318083524704, 0.10085223615169525, 0.10072117298841476, 0.09967555105686188, 0.09952925890684128, 0.09949743002653122, 0.09904506057500839, 0.09888026118278503, 0.09886898845434189]}}
{"id": "WebQTest-1473", "question": "what currency does cyprus use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country"], ["location.country.currency_formerly_used"], ["finance.currency.countries_used", "location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_used", "finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does cyprus use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_used <PATH>", "</PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_used <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.0028253779746592045, -0.03905203938484192, -0.11093368381261826, -0.17074567079544067, -0.17982234060764313, -0.1869443953037262, -0.19234247505664825, -0.19334718585014343, -0.19493287801742554, -0.19838261604309082], "norm_scores": [0.11522764712572098, 0.11112804710865021, 0.10342031717300415, 0.09741590172052383, 0.09653569757938385, 0.09585059434175491, 0.09533458203077316, 0.09523884952068329, 0.09508795291185379, 0.0947604849934578]}}
{"id": "WebQTest-1475", "question": "what type of guitar does johnny depp play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"], ["people.person.nationality", "music.group_member.instruments_played"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nomine"], ["music.musical_group.member", "music.group_membership.role"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of guitar does johnny depp play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> people.person.nationality <SEP> music.group_member.instruments_played </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nomine </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.013423625379800797, -0.03165377676486969, -0.0355500653386116, -0.03600588068366051, -0.05506217107176781, -0.0781884416937828, -0.07901651412248611, -0.07948668301105499, -0.0855766013264656, -0.08702395856380463], "norm_scores": [0.10453425347805023, 0.10264584422111511, 0.10224667936563492, 0.10220009088516235, 0.10027097910642624, 0.09797869622707367, 0.09789758175611496, 0.09785156697034836, 0.09725747257471085, 0.09711681306362152]}}
{"id": "WebQTest-1477", "question": "what episode of how i met your mother is carrie underwood on", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.seasons"], ["tv.tv_series_season.series", "tv.tv_series_season.episodes"], ["music.recording.artist", "music.recording.releases"], ["tv.regular_tv_appearance.actor", "tv.tv_series_season.regular_cast"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what episode of how i met your mother is carrie underwood on [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> tv.tv_guest_role.actor <SEP> tv.tv_guest_role.episodes_appeared_in </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> tv.tv_series_season.series <SEP> tv.tv_series_season.episodes </PATH>", "<PATH> music.recording.artist <SEP> music.recording.releases </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_series_season.regular_cast </PATH>"], "scores": [-0.01798255182802677, -0.02722341939806938, -0.029800686985254288, -0.03748699277639389, -0.04007876291871071, -0.053622402250766754, -0.05380222946405411, -0.054218292236328125, -0.054447416216135025, -0.05687269940972328], "norm_scores": [0.10247846692800522, 0.1015358418226242, 0.10127449035644531, 0.10049904882907867, 0.10023891925811768, 0.09889047592878342, 0.09887269139289856, 0.09883156418800354, 0.09880892187356949, 0.09856957197189331]}}
{"id": "WebQTest-1478", "question": "who are the 9 justices of the supreme court 2011", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"], ["book.written_work.subjects", "book.written_work.author"], ["people.appointment.appointed_role", "people.appointee.position"], ["people.appointed_role.appointment", "people.appointee.position"], ["people.appointed_role.appointment", "people.appointment.appointee"], ["people.appointment.appointed_role", "people.appointment.appointee"], ["government.governmental_jurisdiction.government_bodies", "people.person.nationality"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"], ["book.book_subject.works", "book.author.works_written"]], "ground_paths": [["book.written_work.subjects", "book.author.works_written"], ["law.court.jurisdiction", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the 9 justices of the supreme court 2011 [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.author.works_written </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>", "<PATH> people.appointment.appointed_role <SEP> people.appointee.position </PATH>", "<PATH> people.appointed_role.appointment <SEP> people.appointee.position </PATH>", "<PATH> people.appointed_role.appointment <SEP> people.appointment.appointee </PATH>", "<PATH> people.appointment.appointed_role <SEP> people.appointment.appointee </PATH>", "<PATH> government.governmental_jurisdiction.government_bodies <SEP> people.person.nationality </PATH>", "<PATH> common.image.appears_in_topic_gallery <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> book.book_subject.works <SEP> book.author.works_written </PATH>"], "scores": [-0.00789754930883646, -0.032979510724544525, -0.03300253301858902, -0.03385379910469055, -0.04161172732710838, -0.04753057658672333, -0.048598624765872955, -0.06109106168150902, -0.06981216371059418, -0.07217050343751907], "norm_scores": [0.10374703258275986, 0.10117721557617188, 0.10117488354444504, 0.10108879208564758, 0.10030759125947952, 0.09971564263105392, 0.09960918873548508, 0.09837256371974945, 0.09751839190721512, 0.09728866815567017]}}
{"id": "WebQTest-1479", "question": "who is dustin johnson the golfer", "prediction": [["people.person.profession"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft_pick.player", "sports.drafted_athlete.drafted"], ["sports.pro_athlete.sports_played_professionally", "sports.pro_sports_played.athlete"], ["sports.pro_athlete.sports_played_professionally", "sports.pro_athlete.sports_played_professionally"], ["sports.pro_athlete.sports_played_professionally", "sports.pro_athlete.sports_played"], ["sports.sports_team_roster.player", "sports.sports_team_roster.player"], ["sports.pro_athlete.sports_played_professionally", "people.person.profession"], ["sports.sports_award_winner.awards", "sports.sports_award.discipline_or_subjects"], ["sports.pro_athlete.sports_played_professionally", "sports.pro_sports_played.sports_played"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dustin johnson the golfer [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.pro_athlete.sports_played_professionally <SEP> sports.pro_sports_played.athlete </PATH>", "<PATH> sports.pro_athlete.sports_played_professionally <SEP> sports.pro_athlete.sports_played_professionally </PATH>", "<PATH> sports.pro_athlete.sports_played_professionally <SEP> sports.pro_athlete.sports_played </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.player </PATH>", "<PATH> sports.pro_athlete.sports_played_professionally <SEP> people.person.profession </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.discipline_or_subjects </PATH>", "<PATH> sports.pro_athlete.sports_played_professionally <SEP> sports.pro_sports_played.sports_played </PATH>"], "scores": [-0.002378889126703143, -0.06020769849419594, -0.065900519490242, -0.06894844770431519, -0.0740949884057045, -0.07550603896379471, -0.08096185326576233, -0.0852518081665039, -0.08836906403303146, -0.09004129469394684], "norm_scores": [0.10687518864870071, 0.1008700355887413, 0.10029743611812592, 0.09999219328165054, 0.09947890788316727, 0.09933863580226898, 0.09879813343286514, 0.09837520122528076, 0.09806901961565018, 0.09790516644716263]}}
{"id": "WebQTest-1480", "question": "who did cher have a son with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["music.artist.genre", "music.artist.genre"], ["people.person.parents", "people.person.parents"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nominee"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["people.person.gender"], ["award.award_nomination.award_nominee", "award.award_nominee.award_nominations"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominee.award_nominations"], ["people.person.parents", "people.person.children"], ["music.album.artist", "music.album.artist"], ["music.artist.concert_tours", "music.concert_tour.artist"], ["people.person.spouse_s", "people.person.spouse_s"], ["music.artist.genre", "music.artist.genre"], ["base.americancomedy.double_act.comic", "base.americancomedy.double_act.straight_man"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"], ["people.person.profession", "people.person.profession"], ["music.composer.compositions", "music.composer.compositions"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["people.marriage.spouse", "people.marriage.spouse"], ["music.artist.label", "music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cher have a son with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> base.popstra.celebrity.breakup <SEP> base.popstra.celebrity.breakup </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.021472634747624397, -0.03305819258093834, -0.039711300283670425, -0.04862702265381813, -0.04918123409152031, -0.051824454218149185, -0.052643921226263046, -0.054812829941511154, -0.05523722618818283, -0.05566952005028725], "norm_scores": [0.10250003635883331, 0.10131936520338058, 0.10064752399921417, 0.09975416213274002, 0.09969889372587204, 0.0994357094168663, 0.09935426712036133, 0.099139004945755, 0.09909693896770477, 0.09905410557985306]}}
{"id": "WebQTest-1481", "question": "what city is the university of oregon state in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.contains"], ["location.location.containedby", "location.us_county.county_seat"], ["location.location.containedby", "base.biblioness.bibs_location.country"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is the university of oregon state in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>"], "scores": [-0.002619053004309535, -0.05439792945981026, -0.05663343891501427, -0.057128168642520905, -0.0784318670630455, -0.08426231890916824, -0.08603835105895996, -0.08812030404806137, -0.09005482494831085, -0.09094876796007156], "norm_scores": [0.10681191831827164, 0.10142205655574799, 0.10119558870792389, 0.10114552825689316, 0.099013552069664, 0.09843792766332626, 0.09826325625181198, 0.09805888682603836, 0.0978693813085556, 0.09778193384408951]}}
{"id": "WebQTest-1482", "question": "who played carmen cortez in spy kids", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.film", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.starring", "film.actor.film"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carmen cortez in spy kids [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>"], "scores": [-0.0254600141197443, -0.029153745621442795, -0.03320959582924843, -0.03429275378584862, -0.03564563766121864, -0.039328332990407944, -0.03998759016394615, -0.045743074268102646, -0.05084846168756485, -0.06935233622789383], "norm_scores": [0.10148801654577255, 0.10111383348703384, 0.10070456564426422, 0.10059554129838943, 0.10045954585075378, 0.10009026527404785, 0.10002429783344269, 0.09945026785135269, 0.09894382953643799, 0.09712981432676315]}}
{"id": "WebQTest-1483", "question": "what is the political system in germany", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["government.form_of_government.countries", "location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["book.newspaper.circulation_areas", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system in germany [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.010025224648416042, -0.01446913368999958, -0.1285579353570938, -0.1357618272304535, -0.14797478914260864, -0.1567181944847107, -0.1573309302330017, -0.16126863658428192, -0.1633874475955963, -0.16416536271572113], "norm_scores": [0.11188214272260666, 0.11138605326414108, 0.09937626868486404, 0.09866294264793396, 0.09746530652046204, 0.09661684185266495, 0.09655765444040298, 0.09617818892002106, 0.0959746241569519, 0.09589998424053192]}}
{"id": "WebQTest-1484", "question": "what prompted shakespeare to write poetry", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["people.person.gender", "people.person.gender"], ["influence.peer_relationship.peers", "influence.influence_node.peers"], ["common.topic.notable_types", "common.topic.notable_types"], ["influence.influence_node.influenced_by", "symbols.name_source.namesakes"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what prompted shakespeare to write poetry [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> influence.peer_relationship.peers <SEP> influence.influence_node.peers </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> symbols.name_source.namesakes </PATH>"], "scores": [-0.016624506562948227, -0.026043007150292397, -0.028219250962138176, -0.03225492686033249, -0.036846939474344254, -0.04256244748830795, -0.04888782277703285, -0.071607805788517, -0.07279732078313828, -0.07876165211200714], "norm_scores": [0.10290344059467316, 0.10193879157304764, 0.10171719640493393, 0.10130751878023148, 0.1008433923125267, 0.1002686619758606, 0.09963642805814743, 0.09739820659160614, 0.09728241711854935, 0.0967039242386818]}}
{"id": "WebQTest-1485", "question": "what is the currency in turkey called", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used", "location.country.currency_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_formerly_used"], ["location.country.currency_formerly_used", "finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in turkey called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used <SEP> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.016382163390517235, -0.019637202844023705, -0.032638370990753174, -0.04676814004778862, -0.13982278108596802, -0.15206806361675262, -0.16093334555625916, -0.16119854152202606, -0.16654905676841736, -0.16730061173439026], "norm_scores": [0.10918506234884262, 0.10883024334907532, 0.10742447525262833, 0.10591726750135422, 0.09650585055351257, 0.09533131867647171, 0.09448990970849991, 0.09446486085653305, 0.09396076947450638, 0.09389018267393112]}}
{"id": "WebQTest-1486", "question": "what school did mark zuckerberg attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["education.school_mascot.school"], ["organization.organization_founder.organizations_founded"], ["common.topic.notable_types"]], "ground_paths": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did mark zuckerberg attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> education.school_mascot.school </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.015397422015666962, -0.018333883956074715, -0.022307980805635452, -0.04778820648789406, -0.06636734306812286, -0.09888680279254913, -0.12683585286140442, -0.12769438326358795, -0.1282622069120407, -0.14613547921180725], "norm_scores": [0.10652393102645874, 0.1062115877866745, 0.10579033195972443, 0.1031288206577301, 0.10123045742511749, 0.0979914516210556, 0.09529060870409012, 0.09520883113145828, 0.0951547846198082, 0.09346916526556015]}}
{"id": "WebQTest-1487", "question": "where does kate middleton live 2012", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality", "location.country.first_level_divisions"], ["people.person.children", "people.person.place_of_birth"], ["people.person.nationality", "location.administrative_division.first_level_division_of"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kate middleton live 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.country.first_level_divisions </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.011184519156813622, -0.021507451310753822, -0.03416479006409645, -0.05056069791316986, -0.052932173013687134, -0.06038316339254379, -0.08110745251178741, -0.08209041506052017, -0.08675313740968704, -0.08951129019260406], "norm_scores": [0.10465288907289505, 0.103578120470047, 0.10227535665035248, 0.10061213374137878, 0.10037381201982498, 0.09962871670722961, 0.09758522361516953, 0.09748934954404831, 0.09703584760427475, 0.0967685729265213]}}
{"id": "WebQTest-1488", "question": "what type of art does marc chagall do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.associated_artist.associated_periods_or_movements"], ["visual_art.visual_art_form.associated_artists"], ["visual_art.visual_artists.associated_periods_or_movements"], ["visual_art.artist.associated_periods_or_movements"], ["book.school_or_movement.associated_authors"], ["visual_art.associated_period_movement.associated_artists"], ["art.visual_artist.associated_periods_or_movements"], ["visual_art.school_or_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["book.school_or_movement.associated_authors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does marc chagall do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.associated_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_art_form.associated_artists </PATH>", "<PATH> visual_art.visual_artists.associated_periods_or_movements </PATH>", "<PATH> visual_art.artist.associated_periods_or_movements </PATH>", "<PATH> book.school_or_movement.associated_authors </PATH>", "<PATH> visual_art.associated_period_movement.associated_artists </PATH>", "<PATH> art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.school_or_movement.associated_artists </PATH>"], "scores": [-0.001612517866306007, -0.037972237914800644, -0.10684784501791, -0.11151164025068283, -0.11584275960922241, -0.11938352137804031, -0.12106943130493164, -0.12110082060098648, -0.1272842288017273, -0.13001005351543427], "norm_scores": [0.1101628765463829, 0.10622933506965637, 0.09915900230407715, 0.09869762510061264, 0.09827107191085815, 0.09792374074459076, 0.09775878489017487, 0.09775571525096893, 0.09715311974287033, 0.09688866138458252]}}
{"id": "WebQTest-1489", "question": "what character did billy d williams play in star wars", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.film.directed_by", "fictional_universe.fictional_character_creator.fictional_characters_created"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what character did billy d williams play in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film.directed_by <SEP> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.character </PATH>"], "scores": [-0.02182927541434765, -0.022155944257974625, -0.02862728387117386, -0.02924744039773941, -0.034720394760370255, -0.035387638956308365, -0.04879463464021683, -0.05087985470890999, -0.056504931300878525, -0.05971763655543327], "norm_scores": [0.10170111060142517, 0.10166789591312408, 0.10101208835840225, 0.1009494736790657, 0.100398488342762, 0.10033152252435684, 0.09899535030126572, 0.09878914058208466, 0.09823500365018845, 0.09791990369558334]}}
{"id": "WebQTest-1490", "question": "what language is spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.language_family.geographic_distribution"], ["location.country.currency_formerly_used"], ["base.rosetta.languoid.languoid_class"], ["location.country.language_spoken"], ["location.country.currency_used"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.language_spoken </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.0074693202041089535, -0.028399497270584106, -0.048525530844926834, -0.057192716747522354, -0.14588962495326996, -0.16446413099765778, -0.17996057868003845, -0.1978348195552826, -0.20143936574459076, -0.20198361575603485], "norm_scores": [0.11196784675121307, 0.1096487045288086, 0.10746395587921143, 0.10653657466173172, 0.09749406576156616, 0.09569986909627914, 0.09422829747200012, 0.09255900233983994, 0.09222596883773804, 0.09217578172683716]}}
{"id": "WebQTest-1493", "question": "what school did wilfred owen go to", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["book.written_work.author", "people.person.education", "education.education.institution"], ["book.written_work.author", "people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did wilfred owen go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.author <SEP> people.person.education <SEP> education.education.institution </PATH>", "<PATH> book.written_work.author <SEP> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.015135901048779488, -0.024315187707543373, -0.024690110236406326, -0.02883172407746315, -0.050454165786504745, -0.06269627064466476, -0.06498556584119797, -0.08436565101146698, -0.08739879727363586, -0.09184286743402481], "norm_scores": [0.1038687452673912, 0.10291966795921326, 0.10288109630346298, 0.10245587676763535, 0.10026431083679199, 0.09904433786869049, 0.0988178625702858, 0.09692120552062988, 0.09662766754627228, 0.09619919955730438]}}
{"id": "WebQTest-1494", "question": "who is kevin jonas married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is kevin jonas married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>"], "scores": [-0.028485804796218872, -0.034601736813783646, -0.03967992588877678, -0.04240655526518822, -0.04474402591586113, -0.045936841517686844, -0.04861646518111229, -0.0508028045296669, -0.05089350789785385, -0.053380876779556274], "norm_scores": [0.10155609250068665, 0.1009368747472763, 0.1004256010055542, 0.1001521497964859, 0.09991831332445145, 0.09979920834302902, 0.09953214228153229, 0.09931477159261703, 0.09930576384067535, 0.09905906021595001]}}
{"id": "WebQTest-1495", "question": "where does airtran airways fly", "prediction": [["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airline_airport_presence.airline", "aviation.airline_airport_presence.airport"], ["aviation.airport.hub_for", "aviation.airport.serves"], ["aviation.airline.hubs", "aviation.airport.serves"], ["organization.organization.place_founded", "location.location.containedby"], ["aviation.airline.hubs", "aviation.airline_airport_presence.airport"], ["common.topic.notable_types", "common.topic.notable_types"], ["aviation.airline_airport_presence.airline", "aviation.airline_airport_presence.airports_served"], ["aviation.airport.hub_for", "location.location.nearby_airports"], ["aviation.airline.focus_cities"]], "ground_paths": [["aviation.airport.focus_city_for"], ["aviation.airline_airport_presence.airline", "aviation.airport.airlines"], ["aviation.airline.focus_cities"], ["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does airtran airways fly [/INST]", "raw_output": {"paths": ["<PATH> aviation.airline.airports_served <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airline_airport_presence.airline <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airport.hub_for <SEP> aviation.airport.serves </PATH>", "<PATH> aviation.airline.hubs <SEP> aviation.airport.serves </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> aviation.airline.hubs <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> aviation.airline_airport_presence.airline <SEP> aviation.airline_airport_presence.airports_served </PATH>", "<PATH> aviation.airport.hub_for <SEP> location.location.nearby_airports </PATH>", "<PATH> aviation.airline.focus_cities </PATH>"], "scores": [-0.019127748906612396, -0.024400759488344193, -0.038689229637384415, -0.04542318359017372, -0.04971348121762276, -0.051480986177921295, -0.05225105211138725, -0.056479714810848236, -0.057488057762384415, -0.059522394090890884], "norm_scores": [0.1026589497923851, 0.10211905837059021, 0.10067030787467957, 0.09999466687440872, 0.09956658631563187, 0.09939075261354446, 0.09931425005197525, 0.09889517724514008, 0.09879550337791443, 0.09859472513198853]}}
{"id": "WebQTest-1496", "question": "where does the city of paris get its name", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["location.location.containedby"], ["symbols.namesake.named_after", "symbols.namesake.named_after"], [], ["location.location.containedby", "symbols.namesake.named_after"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"], ["symbols.armorial_grant.armiger"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the city of paris get its name [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> symbols.namesake.named_after </PATH>", "<PATH> </PATH>", "<PATH> location.location.containedby <SEP> symbols.namesake.named_after </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> symbols.armorial_grant.armiger </PATH>"], "scores": [-0.011059488169848919, -0.014972871169447899, -0.07754358649253845, -0.12090368568897247, -0.12487997114658356, -0.12703150510787964, -0.127401664853096, -0.1289108395576477, -0.1290009319782257, -0.1314810961484909], "norm_scores": [0.1091112568974495, 0.10868509858846664, 0.10209298878908157, 0.09776082634925842, 0.097372867166996, 0.0971635952591896, 0.09712763130664825, 0.09698116779327393, 0.09697242826223373, 0.09673220664262772]}}
{"id": "WebQTest-1499", "question": "who did sandra bullock marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.infidelity.participant", "base.popstra.infidelity.perpetrator"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.infidelity.victim", "base.popstra.celebrity.infidelity_perpetrator"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sandra bullock marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.01814868673682213, -0.03727167472243309, -0.03949262201786041, -0.04398056119680405, -0.048537205904722214, -0.05364817753434181, -0.05574971064925194, -0.0577128529548645, -0.05940021574497223, -0.060740452259778976], "norm_scores": [0.10296723246574402, 0.10101689398288727, 0.100792795419693, 0.10034145414829254, 0.09988527745008469, 0.09937607496976852, 0.09916745126247406, 0.09897296130657196, 0.09880609065294266, 0.0986737608909607]}}
{"id": "WebQTest-1500", "question": "where is olympic national park located", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.contains"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby", "location.location.containedby"], ["geography.lake.basin_countries"]], "ground_paths": [["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> geography.lake.basin_countries </PATH>"], "scores": [-0.01132685411721468, -0.024352647364139557, -0.029154330492019653, -0.0934259220957756, -0.0981602594256401, -0.10429835319519043, -0.10942494869232178, -0.11063160002231598, -0.11303301900625229, -0.1191350668668747], "norm_scores": [0.10716114938259125, 0.10577434301376343, 0.10526766628026962, 0.09871477633714676, 0.09824852645397186, 0.09764732420444489, 0.09714800864458084, 0.09703086316585541, 0.0967981219291687, 0.09620925784111023]}}
{"id": "WebQTest-1501", "question": "where is the un based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded", "base.locations.continents.countries_within"], ["organization.organization_founder.organizations_founded", "base.locations.countries.continent"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization_founder.organizations_founded", "location.country.capital"], ["book.book_subject.works", "base.reviews.review.mentions"], ["location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["book.book_subject.works", "base.reviews.review.mentions"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the un based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.locations.continents.countries_within </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.locations.countries.continent </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.country.capital </PATH>", "<PATH> book.book_subject.works <SEP> base.reviews.review.mentions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01326944213360548, -0.041515905410051346, -0.042041320353746414, -0.04674815759062767, -0.0471772626042366, -0.05909103900194168, -0.060099925845861435, -0.06641576439142227, -0.06787662953138351, -0.06876065582036972], "norm_scores": [0.10386261343955994, 0.10096991062164307, 0.10091687738895416, 0.10044299811124802, 0.10039989650249481, 0.09921085834503174, 0.09911081939935684, 0.09848681837320328, 0.09834304451942444, 0.09825614839792252]}}
{"id": "WebQTest-1502", "question": "what city and state is yale located in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["education.educational_institution.parent_institution"], ["education.educational_institution.parent_institution", "location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city and state is yale located in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.contains </PATH>"], "scores": [-0.0017386188264936209, -0.05883905291557312, -0.06591728329658508, -0.06696298718452454, -0.08134175837039948, -0.08943640440702438, -0.1009850949048996, -0.10120568424463272, -0.10674604773521423, -0.10774397850036621], "norm_scores": [0.10788311809301376, 0.1018955260515213, 0.10117682814598083, 0.1010710746049881, 0.09962819516658783, 0.09882499277591705, 0.09769026935100555, 0.09766871482133865, 0.09712909162044525, 0.09703221172094345]}}
{"id": "WebQTest-1503", "question": "where did thomas paine die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people_deceased_here"], ["location.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"], ["deceased_person.place_of_death"], ["people.person.place_of_death"], ["people.person.place_of_birth"], ["people.place_of_death.person", "people.deceased_place.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas paine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_death.person <SEP> people.deceased_place.place_of_death </PATH>"], "scores": [-0.0003801215498242527, -0.09492208063602448, -0.10277016460895538, -0.11149097234010696, -0.11164958775043488, -0.1159851998090744, -0.11687155812978745, -0.12124954164028168, -0.12297068536281586, -0.12382341176271439], "norm_scores": [0.11064987629652023, 0.10066810995340347, 0.0998811423778534, 0.09901388734579086, 0.09899818897247314, 0.09856989979743958, 0.09848257154226303, 0.09805235266685486, 0.09788373857736588, 0.09780029952526093]}}
{"id": "WebQTest-1504", "question": "where was barack hussein obama sr born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.person.parents"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.parents", "people.person.place_of_birth"], ["people.person.children"], ["influence.influence_node.influenced", "people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was barack hussein obama sr born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.0007478264160454273, -0.06390102207660675, -0.1015896201133728, -0.1035127192735672, -0.10802566260099411, -0.11337292939424515, -0.11589270830154419, -0.12148074805736542, -0.1283746361732483, -0.12878380715847015], "norm_scores": [0.11019905656576157, 0.1034548431634903, 0.09962833672761917, 0.099436916410923, 0.09898918122053146, 0.09846127033233643, 0.09821347892284393, 0.0976661816239357, 0.09699520468711853, 0.09695552289485931]}}
{"id": "WebQTest-1506", "question": "where is the home stadium of the new england patriots", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_facility.teams", "sports.sports_team.arena_stadium"], ["location.location.containedby"], ["sports.sports_facility.teams", "sports.sports_facility.teams"], ["sports.professional_sports_team.owner_s", "sports.sports_team.arena_stadium"], ["sports.sports_team.arena_stadium", "sports.sports_facility.teams"], ["sports.sports_championship_event.champion"], ["sports.sports_team.arena_stadium", "sports.sports_team.arena_stadium"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the home stadium of the new england patriots [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team.arena_stadium <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.arena_stadium <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.0034832933451980352, -0.02746119536459446, -0.10026970505714417, -0.10526321083307266, -0.11059393733739853, -0.1159190759062767, -0.11592470854520798, -0.11689184606075287, -0.12287911772727966, -0.1235818937420845], "norm_scores": [0.10940810292959213, 0.10681592673063278, 0.09931518882513046, 0.09882050007581711, 0.09829511493444443, 0.09777306765317917, 0.0977725237607956, 0.09767800569534302, 0.09709492325782776, 0.09702671319246292]}}
{"id": "WebQTest-1507", "question": "who was kate chopin", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["influence.influence_node.influenced_by", "people.person.profession"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["symbols.namesake.named_after"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["influence.influence_node.influenced", "people.person.profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kate chopin [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>"], "scores": [-0.0004957611672580242, -0.07928083837032318, -0.0974929928779602, -0.1267772614955902, -0.1306336522102356, -0.1319969743490219, -0.13780640065670013, -0.13861164450645447, -0.14879287779331207, -0.15572714805603027], "norm_scores": [0.11199455708265305, 0.10350968688726425, 0.10164161771535873, 0.09870826452970505, 0.0983283519744873, 0.09819438308477402, 0.09762559086084366, 0.0975470021367073, 0.09655889868736267, 0.09589164704084396]}}
{"id": "WebQTest-1508", "question": "when did bill clinton go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.students_graduates"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did bill clinton go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>"], "scores": [-0.014923649840056896, -0.020386479794979095, -0.02197878435254097, -0.0563579723238945, -0.0574299618601799, -0.1067216694355011, -0.1133078932762146, -0.11394420266151428, -0.11835739016532898, -0.12080492079257965], "norm_scores": [0.10603441298007965, 0.10545673966407776, 0.10528896003961563, 0.10173072665929794, 0.10162173211574554, 0.09673406928777695, 0.09609904885292053, 0.09603792428970337, 0.09561502188444138, 0.09538128226995468]}}
{"id": "WebQTest-1510", "question": "what are staffordshire terriers", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_of_breed.breed_of"], ["biology.breed_breed.breed_of"], ["biology.breed_origin.breeds_originating_here"], ["biology.breed_of_origin.breed_of"], ["biology.breed_of.breed_of"], ["biology.breed_of_breed_of.breed_of"], ["biology.breed_of_organization.breed_of"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["biology.breed_of_breed_of_this_breed.breed_of"]], "ground_paths": [["biology.animal_breed.breed_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are staffordshire terriers [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_of_breed.breed_of </PATH>", "<PATH> biology.breed_breed.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.breed_of_origin.breed_of </PATH>", "<PATH> biology.breed_of.breed_of </PATH>", "<PATH> biology.breed_of_breed_of.breed_of </PATH>", "<PATH> biology.breed_of_organization.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> biology.animal_breed.place_of_origin </PATH>", "<PATH> biology.breed_of_breed_of_this_breed.breed_of </PATH>"], "scores": [-0.007226771209388971, -0.03929806500673294, -0.04995884746313095, -0.05978698283433914, -0.06647693365812302, -0.06656540930271149, -0.08463308960199356, -0.08803118020296097, -0.08980540186166763, -0.09806211292743683], "norm_scores": [0.10590913146734238, 0.10256636887788773, 0.10147875547409058, 0.10048627853393555, 0.09981627762317657, 0.09980745613574982, 0.09802035242319107, 0.09768783301115036, 0.09751466661691666, 0.09671283513307571]}}
{"id": "WebQTest-1513", "question": "what to do downtown sacramento", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.neighborhood.neighborhood_of"], ["location.location.contains"], ["base.biblioness.bibs_location.state"], ["organization.organization.geographic_scope"], ["base.usnationalparks.us_national_park.state"], ["film.film.featured_film_locations"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do downtown sacramento [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.neighborhood.neighborhood_of </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.017064735293388367, -0.019507788121700287, -0.02200663834810257, -0.08219825476408005, -0.10899975150823593, -0.14362405240535736, -0.1439105123281479, -0.1465092897415161, -0.15751956403255463, -0.15764544904232025], "norm_scores": [0.10845836251974106, 0.10819371789693832, 0.10792369395494461, 0.10161923617124557, 0.09893187135457993, 0.09556503593921661, 0.09553765505552292, 0.09528970718383789, 0.09424629807472229, 0.09423442929983139]}}
{"id": "WebQTest-1515", "question": "who is michael j fox parents", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"], ["influence.influence_node.influenced", "people.person.parents"], ["people.person.place_of_birth"], ["people.person.parent"], ["people.person.offspring"], ["influence.influence_node.influenced_by", "people.person.parents"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.parents </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> people.person.offspring </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.parents </PATH>"], "scores": [-0.0117415189743042, -0.016039669513702393, -0.14658288657665253, -0.14882323145866394, -0.16895897686481476, -0.17248061299324036, -0.17734381556510925, -0.18024703860282898, -0.183879092335701, -0.1904601752758026], "norm_scores": [0.11340425163507462, 0.11291787773370743, 0.0990988239645958, 0.09887705743312836, 0.09690599888563156, 0.09656533598899841, 0.09609685838222504, 0.09581827372312546, 0.09547088295221329, 0.09484464675188065]}}
{"id": "WebQTest-1516", "question": "where did sanjay gupta go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.honorary_degree_recipient.honorary_degrees", "education.honorary_degree.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.honorary_degree_recipient.honorary_degrees", "education.educational_institution.honorary_degrees_awarded"], ["education.education.student", "education.educational_institution.students_graduates"], ["education.honorary_degree_recipient.honorary_degrees", "education.educational_institution.honorary_degrees"], ["education.honorary_degree_recipient.honorary_degrees", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["education.honorary_degree_recipient.honorary_degrees", "education.honorary_degree.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did sanjay gupta go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.honorary_degree_recipient.honorary_degrees <SEP> education.honorary_degree.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.honorary_degree_recipient.honorary_degrees <SEP> education.educational_institution.honorary_degrees_awarded </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.honorary_degree_recipient.honorary_degrees <SEP> education.educational_institution.honorary_degrees </PATH>", "<PATH> education.honorary_degree_recipient.honorary_degrees <SEP> education.education.institution </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.016958188265562057, -0.019512442871928215, -0.023945817723870277, -0.02926943264901638, -0.039852481335401535, -0.053489990532398224, -0.05756000429391861, -0.07934615015983582, -0.0813436284661293, -0.08213704824447632], "norm_scores": [0.10315641760826111, 0.10289327055215836, 0.10243810713291168, 0.10189422219991684, 0.10082155466079712, 0.09945593029260635, 0.09905196726322174, 0.09691734611988068, 0.09672394394874573, 0.0966472327709198]}}
{"id": "WebQTest-1518", "question": "what countries are involved in the nafta", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["common.topic.notable_types"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.founders"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.membership_organization.members", "organization.organization_member.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are involved in the nafta [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_membership.member </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.continents.countries_within </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.011482881382107735, -0.02500598132610321, -0.04155617207288742, -0.04188859835267067, -0.04708307236433029, -0.05564066395163536, -0.06007929891347885, -0.060156337916851044, -0.06736607104539871, -0.0697246789932251], "norm_scores": [0.10370268672704697, 0.10230974107980728, 0.10063043236732483, 0.10059698671102524, 0.10007579624652863, 0.09922303259372711, 0.09878359735012054, 0.09877599030733109, 0.09806639701128006, 0.09783537685871124]}}
{"id": "WebQTest-1519", "question": "what continent is cayman islands located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["common.topic.notable_types"], ["geography.island.island_group"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.locations.countries.continent"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.country.languages_spoken", "language.human_language.region"], ["book.book_subject.works", "book.written_work.subjects"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent is cayman islands located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> geography.island.island_group </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.country.languages_spoken <SEP> language.human_language.region </PATH>", "<PATH> book.book_subject.works <SEP> book.written_work.subjects </PATH>"], "scores": [-0.009142586961388588, -0.021379537880420685, -0.07815314084291458, -0.09101136028766632, -0.09762789309024811, -0.10119448602199554, -0.10657570511102676, -0.1104697659611702, -0.11720281839370728, -0.11818718165159225], "norm_scores": [0.10781686753034592, 0.10650556534528732, 0.10062730312347412, 0.09934169799089432, 0.0986865684390068, 0.09833522140979767, 0.09780748188495636, 0.0974273532629013, 0.09677357226610184, 0.09667835384607315]}}
{"id": "WebQTest-1520", "question": "what language speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spokencommon"], ["location.country.languages_spokenlanguage"], ["location.country.form_of_government"], ["location. country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken <PATH>", "<PATH> location.country.languages_spokencommon </PATH>", "<PATH> location.country.languages_spokenlanguage </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location. country.languages_spoken </PATH>"], "scores": [-0.0010883338982239366, -0.06728658825159073, -0.07549218088388443, -0.09361814707517624, -0.19535070657730103, -0.1966261863708496, -0.23825399577617645, -0.24218972027301788, -0.24570120871067047, -0.24785646796226501], "norm_scores": [0.11681356281042099, 0.10933110117912292, 0.10843764245510101, 0.10648981481790543, 0.09618917107582092, 0.09606656432151794, 0.09214961528778076, 0.09178765118122101, 0.09146590530872345, 0.09126898646354675]}}
{"id": "WebQTest-1522", "question": "who does kroy biermann play football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kroy biermann play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.020038768649101257, -0.023317541927099228, -0.027360187843441963, -0.028769515454769135, -0.029446301981806755, -0.03886810690164566, -0.04109979048371315, -0.052330132573843, -0.05325702205300331, -0.06262490153312683], "norm_scores": [0.10177348554134369, 0.10144034028053284, 0.10103107988834381, 0.10088878870010376, 0.10082054138183594, 0.0998750850558281, 0.09965244680643082, 0.09853958338499069, 0.09844828397035599, 0.09753034263849258]}}
{"id": "WebQTest-1523", "question": "when is the last time the pittsburgh steelers won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["common.topic.notable_types"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the pittsburgh steelers won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>"], "scores": [-0.01047142967581749, -0.01598665490746498, -0.034626588225364685, -0.059618979692459106, -0.07848814874887466, -0.08548883348703384, -0.0957324355840683, -0.11308688670396805, -0.11482111364603043, -0.12983009219169617], "norm_scores": [0.10645326972007751, 0.10586776584386826, 0.10391268134117126, 0.10134783387184143, 0.09945341944694519, 0.09875960648059845, 0.0977531224489212, 0.09607131034135818, 0.09590484201908112, 0.09447615593671799]}}
{"id": "WebQTest-1524", "question": "who invented islamic religion", "prediction": [["religion.religion.notable_figures"], ["religion.religion.founding_figures"], ["religion.belief.belief_of"], ["people.person.religion"], ["symbols.namesake.named_after"], ["religion.religion.notable_figures", "religion.religion.founding_figures"], ["religion.deity.deity_of"], ["religion.religion.religion"], ["religion.religion.deities"], ["religion.founding_figure.religion_founded"]], "ground_paths": [["religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who invented islamic religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>", "<PATH> religion.belief.belief_of </PATH>", "<PATH> people.person.religion </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> religion.religion.notable_figures <SEP> religion.religion.founding_figures </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> religion.religion.religion </PATH>", "<PATH> religion.religion.deities </PATH>", "<PATH> religion.founding_figure.religion_founded </PATH>"], "scores": [-0.0024794423952698708, -0.04386702924966812, -0.06788332015275955, -0.09639705717563629, -0.09817303717136383, -0.10385202616453171, -0.11382245272397995, -0.125862255692482, -0.1281527429819107, -0.13195215165615082], "norm_scores": [0.10919579863548279, 0.10476869344711304, 0.10228250920772552, 0.09940724074840546, 0.09923085570335388, 0.09866891801357269, 0.09769003838300705, 0.09652091562747955, 0.09630009531974792, 0.0959349051117897]}}
{"id": "WebQTest-1525", "question": "who were we fighting in the gulf war", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["time.event.locations"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["military.casualties.military_conflict", "military.casualties.combatant"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["film.film_genre.films_in_this_genre", "film.film.country"]], "ground_paths": [["military.military_conflict.casualties", "military.military_combatant.casualties"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["film.film_genre.films_in_this_genre", "film.film.country"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["time.event.locations"], ["film.film_subject.films", "tv.tv_program.country_of_origin"], ["film.film.subjects", "film.film.country"], ["film.film.genre", "film.film.country"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were we fighting in the gulf war [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>", "<PATH> military.casualties.military_conflict <SEP> military.military_combatant.casualties </PATH>", "<PATH> time.event.included_in_event <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.casualties.military_conflict <SEP> military.casualties.combatant </PATH>", "<PATH> time.event.includes_event <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.person.nationality </PATH>", "<PATH> film.film_genre.films_in_this_genre <SEP> film.film.country </PATH>"], "scores": [-0.02467142418026924, -0.027242988348007202, -0.02929099276661873, -0.03412581607699394, -0.04044849053025246, -0.041658248752355576, -0.042653072625398636, -0.047253020107746124, -0.04970801621675491, -0.05254005640745163], "norm_scores": [0.10143472254276276, 0.10117421299219131, 0.10096722841262817, 0.10048024356365204, 0.09984693676233292, 0.09972622245550156, 0.0996270626783371, 0.09916984289884567, 0.09892667084932327, 0.09864690899848938]}}
{"id": "WebQTest-1526", "question": "how many languages do they speak in switzerland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["book.newspaper.circulation_areas"], ["film.film.featured_film_locations"], ["location.country.form_of_government"], ["location.country.currency_used"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in switzerland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.015119338408112526, -0.022454671561717987, -0.023449165746569633, -0.07502573728561401, -0.15613922476768494, -0.16769815981388092, -0.17116805911064148, -0.1761406809091568, -0.1847822070121765, -0.18725916743278503], "norm_scores": [0.11054836213588715, 0.10974042117595673, 0.10963132977485657, 0.10412027686834335, 0.09600816667079926, 0.09490480273962021, 0.09457606822252274, 0.09410693496465683, 0.09329721331596375, 0.09306640923023224]}}
{"id": "WebQTest-1527", "question": "what political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["base.politicalconventions.presidential_nominee.nominated_at", "base.politicalconventions.political_convention.party"], ["organization.organization.founders"], ["government.politician.party", "government.political_party.politicians_in_this_party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"]], "ground_paths": [["government.election_campaign.candidate", "government.election_campaign.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.party", "government.political_party_tenure.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.politician.election_campaigns <SEP> government.election_campaign.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> base.politicalconventions.presidential_nominee.nominated_at <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> government.politician.party <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>"], "scores": [-0.018990306183695793, -0.020908817648887634, -0.0235141571611166, -0.034726545214653015, -0.04020398110151291, -0.04960561916232109, -0.0498381182551384, -0.06151876598596573, -0.06245111674070358, -0.07213212549686432], "norm_scores": [0.10245358943939209, 0.10225722193717957, 0.10199115425348282, 0.10085397958755493, 0.10030306875705719, 0.09936446696519852, 0.09934136271476746, 0.09818775206804276, 0.09809624403715134, 0.09715115278959274]}}
{"id": "WebQTest-1528", "question": "what movies does logan lerman play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does logan lerman play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>"], "scores": [-0.01648774929344654, -0.036467526108026505, -0.03673143684864044, -0.03796203434467316, -0.03838792070746422, -0.0405128188431263, -0.04306497797369957, -0.04459337145090103, -0.047939714044332504, -0.055770233273506165], "norm_scores": [0.10235302150249481, 0.10032831877470016, 0.10030185431241989, 0.10017850250005722, 0.10013584047555923, 0.0999232828617096, 0.09966859221458435, 0.0995163768529892, 0.09918391704559326, 0.09841028600931168]}}
{"id": "WebQTest-1529", "question": "where did emperor constantine die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["location.location.people_born_here"], ["location.location.people_born_here", "location.location.containedby"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.deceased_person.place_of_burial"], ["people.person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor constantine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.person.place_of_death </PATH>"], "scores": [-0.0006750273169018328, -0.07461033761501312, -0.08244812488555908, -0.09828581660985947, -0.10098458081483841, -0.13452763855457306, -0.13578343391418457, -0.1365501880645752, -0.13926775753498077, -0.14292718470096588], "norm_scores": [0.11085186153650284, 0.10295164585113525, 0.10214788466691971, 0.10054284334182739, 0.10027186572551727, 0.09696422517299652, 0.09684253484010696, 0.09676831215620041, 0.09650569409132004, 0.09615317732095718]}}
{"id": "WebQTest-1531", "question": "where is mali located", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["geography.river.basin_countries"], ["base.aareas.schema.administrative_area.administrative_parent"], ["common.topic.notable_types"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.partially_containedby"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mali located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.017039546743035316, -0.022102491930127144, -0.041048627346754074, -0.05121589079499245, -0.06314897537231445, -0.11555425822734833, -0.1253286898136139, -0.13013753294944763, -0.1385764181613922, -0.13918688893318176], "norm_scores": [0.10684039443731308, 0.10630083829164505, 0.10430580377578735, 0.10325067490339279, 0.10202589631080627, 0.09681688994169235, 0.09587516635656357, 0.09541522711515427, 0.09461341798305511, 0.09455567598342896]}}
{"id": "WebQTest-1533", "question": "what planet did luke skywalker came from", "prediction": [["fictional_universe.fictional_setting.characters_that_have_lived_here"], ["fictional_universe.fictional_character.place_of_birth"], ["fictional_universe.fictional_character.places_lived"], ["fictional_universe.fictional_character.places_lived", "fictional_universe.fictional_setting.fictional_characters_born_here"], ["fictional_universe.fictional_character.place_of_birth", "fictional_universe.fictional_setting.fictional_characters_born_here"], ["fictional_universe.fictional_character.places_lived", "fictional_universe.fictional_setting.characters_that_have_lived_here"], ["fictional_universe.fictional_setting.characters_that_have_lived_here", "fictional_universe.fictional_setting.characters_that_have_lived_here"], ["fictional_universe.fictional_character.places_lived", "fictional_universe.fictional_character.place_of_birth"], ["film.film_character.portrayed_in_films", "film.performance.film", "film.film_location.featured_in_films"], ["fictional_universe.fictional_character.place_of_birth", "fictional_universe.fictional_character.place_of_birth"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what planet did luke skywalker came from [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> fictional_universe.fictional_character.places_lived </PATH>", "<PATH> fictional_universe.fictional_character.places_lived <SEP> fictional_universe.fictional_setting.fictional_characters_born_here </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth <SEP> fictional_universe.fictional_setting.fictional_characters_born_here </PATH>", "<PATH> fictional_universe.fictional_character.places_lived <SEP> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> fictional_universe.fictional_setting.characters_that_have_lived_here <SEP> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> fictional_universe.fictional_character.places_lived <SEP> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.film <SEP> film.film_location.featured_in_films </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth <SEP> fictional_universe.fictional_character.place_of_birth </PATH>"], "scores": [-0.012144708074629307, -0.018901804462075233, -0.020135929808020592, -0.05922919884324074, -0.06966209411621094, -0.07363694906234741, -0.07673470675945282, -0.0852585956454277, -0.0869075208902359, -0.08921700716018677], "norm_scores": [0.10477225482463837, 0.10406668484210968, 0.10393832623958588, 0.09995344281196594, 0.09891606122255325, 0.09852366149425507, 0.09821892529726028, 0.09738527983427048, 0.09722483158111572, 0.09700055420398712]}}
{"id": "WebQTest-1534", "question": "who are the coaches for the oakland raiders", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.current_head_coach"], ["american_football.football_coach.current_team_head_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["sports.sports_team.sport", "sports.sport.team_coaches"], ["american_football.football_historical_coach_position.team", "american_football.football_coach.coaching_history"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["american_football.football_team.historical_coaching_staff", "american_football.football_coach.coaching_history"], ["american_football.football_team.historical_coaching_staff", "american_football.football_historical_coach_position.coach"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the coaches for the oakland raiders [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>", "<PATH> american_football.football_coach.current_team_head_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.team_coaches </PATH>", "<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_coach.coaching_history </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_coach.coaching_history </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_historical_coach_position.coach </PATH>"], "scores": [-0.015950381755828857, -0.021620793268084526, -0.0254563856869936, -0.03593302145600319, -0.036899637430906296, -0.041884757578372955, -0.04195535182952881, -0.04382088780403137, -0.053218498826026917, -0.055958766490221024], "norm_scores": [0.10214700549840927, 0.10156942903995514, 0.10118059813976288, 0.10012610256671906, 0.1000293642282486, 0.09953194111585617, 0.09952491521835327, 0.09933941811323166, 0.0984102413058281, 0.09814094007015228]}}
{"id": "WebQTest-1535", "question": "where was the first microsoft headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["location.location.contains"], ["symbols.namesake.named_after", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.founders"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization_founder.organizations_founded"], ["organization.organization.place_founded"], ["organization.organization.founders", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["base.engineering1.rfid_company.location", "base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first microsoft headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.founders <SEP> location.location.containedby </PATH>"], "scores": [-0.017900971695780754, -0.034066714346408844, -0.042448077350854874, -0.04853525385260582, -0.04948851838707924, -0.0501369908452034, -0.05752500519156456, -0.05780884623527527, -0.058294352144002914, -0.06317230314016342], "norm_scores": [0.10304063558578491, 0.10138829797506332, 0.10054207593202591, 0.09993191063404083, 0.09983670711517334, 0.09977198392152786, 0.09903758019208908, 0.09900947660207748, 0.09896142035722733, 0.09847985953092575]}}
{"id": "WebQTest-1536", "question": "where did john steinbeck go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["people.person.education", "education.educational_institution.parent_institution"], ["common.topic.notable_types"], ["book.written_work.author", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john steinbeck go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.author <SEP> location.location.containedby </PATH>"], "scores": [-0.016373595222830772, -0.018276896327733994, -0.018415184691548347, -0.04904739558696747, -0.08558075875043869, -0.08768775314092636, -0.10586079210042953, -0.10759538412094116, -0.11211884766817093, -0.1178257092833519], "norm_scores": [0.10562281310558319, 0.10542198270559311, 0.10540740191936493, 0.10222748667001724, 0.09856017678976059, 0.09835272282361984, 0.09658150374889374, 0.09641411900520325, 0.09597897529602051, 0.09543279558420181]}}
{"id": "WebQTest-1537", "question": "what did niels bohr discover about the atomic structure", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["base.nobelprizes.nobel_honor.nobel_prize_winner"], ["base.nobelprizes.nobel_honor.nobel_prize_winner", "base.nobelprizes.nobel_subject_area.nobel_awards"], ["base.nobelprizes.nobel_prize_winner.nobel_honor"], ["base.nobelprizes.nobel_honor.nobel_prize_winner", "base.nobelprizes.nobel_honor.subject_area"], ["base.nobelprizes.nobel_prize_winner.nobel_honor", "base.nobelprizes.nobel_subject_area.nobel_awards"], ["base.nobelprizes.nobel_honor.nobel_honor"], ["base.nobelprizes.nobel_prize_winner.nobel_honor", "base.nobelprizes.nobel_honor.subject_area"], ["base.nobelprizes.nobel_honor.nobel_prize_winner", "base.nobelprizes.nobel_honor.nobel_subject_area"]], "ground_paths": [["education.academic.advisees", "book.author.works_written"], ["base.nobelprizes.nobel_honor.nobel_prize_winner", "base.nobelprizes.nobel_honor.subject_area"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did niels bohr discover about the atomic structure [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.nobelprizes.nobel_honor.nobel_prize_winner </PATH>", "<PATH> base.nobelprizes.nobel_honor.nobel_prize_winner <SEP> base.nobelprizes.nobel_subject_area.nobel_awards </PATH>", "<PATH> base.nobelprizes.nobel_prize_winner.nobel_honor </PATH>", "<PATH> base.nobelprizes.nobel_honor.nobel_prize_winner <SEP> base.nobelprizes.nobel_honor.subject_area </PATH>", "<PATH> base.nobelprizes.nobel_prize_winner.nobel_honor <SEP> base.nobelprizes.nobel_subject_area.nobel_awards </PATH>", "<PATH> base.nobelprizes.nobel_honor.nobel_honor </PATH>", "<PATH> base.nobelprizes.nobel_prize_winner.nobel_honor <SEP> base.nobelprizes.nobel_honor.subject_area </PATH>", "<PATH> base.nobelprizes.nobel_honor.nobel_prize_winner <SEP> base.nobelprizes.nobel_honor.nobel_subject_area </PATH>"], "scores": [-0.012283938005566597, -0.0248945914208889, -0.03447183594107628, -0.04614847153425217, -0.04780410975217819, -0.05156270042061806, -0.05214614421129227, -0.06109685078263283, -0.06181037798523903, -0.07256153970956802], "norm_scores": [0.10346314311027527, 0.10216660052537918, 0.10119279474020004, 0.10001807659864426, 0.09985262155532837, 0.09947802126407623, 0.09941999614238739, 0.09853408485651016, 0.09846380352973938, 0.09741088002920151]}}
{"id": "WebQTest-1538", "question": "what books did louis sachar wrote", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"], ["book.author.book_editions_published"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["influence.influence_node.influenced", "book.book_edition.author_editor"], ["influence.influence_node.influenced_by", "book.book_edition.author_editor"], ["book.writer.works_written"], ["people.person.nationality", "book.book_edition.place_of_publication"]], "ground_paths": [["book.author.works_written"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did louis sachar wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> influence.influence_node.influenced <SEP> book.book_edition.author_editor </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.book_edition.author_editor </PATH>", "<PATH> book.writer.works_written </PATH>", "<PATH> people.person.nationality <SEP> book.book_edition.place_of_publication </PATH>"], "scores": [-0.012168943881988525, -0.025658616796135902, -0.029335005208849907, -0.07480616867542267, -0.07685838639736176, -0.08600753545761108, -0.12661367654800415, -0.12922728061676025, -0.1320398896932602, -0.1366056352853775], "norm_scores": [0.10722170770168304, 0.10578504204750061, 0.10539684444665909, 0.1007116511464119, 0.10050518810749054, 0.09958983957767487, 0.09562688320875168, 0.09537728875875473, 0.09510939568281174, 0.09467614442110062]}}
{"id": "WebQTest-1539", "question": "who is ryan reynolds married to 2012", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.spouse_s", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is ryan reynolds married to 2012 [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.01750221662223339, -0.030103055760264397, -0.03176543861627579, -0.042232103645801544, -0.04545813798904419, -0.04580042511224747, -0.049539756029844284, -0.052176911383867264, -0.054760005325078964, -0.054990511387586594], "norm_scores": [0.10251739621162415, 0.10123369842767715, 0.10106554627418518, 0.10001324862241745, 0.09969111531972885, 0.09965699911117554, 0.09928504377603531, 0.09902356564998627, 0.09876810014247894, 0.09874534606933594]}}
{"id": "WebQTest-1540", "question": "what is newt gingrich 's wife 's name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.marriage.spouse", "people.person.spouse_s"]], "ground_paths": [["people.person.profession", "common.topic.notable_types"], ["people.person.religion", "people.person.religion"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.person.spouse_s"], ["film.film.executive_produced_by", "film.film.executive_produced_by"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is newt gingrich 's wife 's name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>"], "scores": [-0.023930080235004425, -0.032760899513959885, -0.03628723323345184, -0.03940017521381378, -0.04010697081685066, -0.04405631497502327, -0.04574589431285858, -0.04574717581272125, -0.04787512123584747, -0.055814195424318314], "norm_scores": [0.10173559188842773, 0.10084113478660583, 0.100486159324646, 0.10017384588718414, 0.10010306537151337, 0.09970850497484207, 0.09954017400741577, 0.09954005479812622, 0.09932845830917358, 0.09854301065206528]}}
{"id": "WebQTest-1541", "question": "what type of music did antonio vivaldi play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["book.author.school_or_movement"], ["book.written_work.author"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["music.album.genre"], ["music.composer.compositions"], ["music.composer.genre"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did antonio vivaldi play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.school_or_movement </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> music.album.genre </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.composer.genre </PATH>"], "scores": [-0.0024872480425983667, -0.037672098726034164, -0.11023597419261932, -0.12056095898151398, -0.12117219716310501, -0.12424305081367493, -0.12728114426136017, -0.13450907170772552, -0.1353343427181244, -0.13749708235263824], "norm_scores": [0.11069697886705399, 0.10686984658241272, 0.09938963502645493, 0.09836871922016144, 0.09830860793590546, 0.09800717979669571, 0.09770987182855606, 0.0970061868429184, 0.09692616015672684, 0.09671676158905029]}}
{"id": "WebQTest-1542", "question": "what school does james franco go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["book.written_work.author", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["common.topic.notable_types"], ["book.written_work.subjects", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school does james franco go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.author <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>"], "scores": [-0.01476635504513979, -0.016234952956438065, -0.024177663028240204, -0.05381154641509056, -0.07196492701768875, -0.08918043226003647, -0.12970994412899017, -0.1302676796913147, -0.13299719989299774, -0.13994121551513672], "norm_scores": [0.10664692521095276, 0.10649041831493378, 0.10564794391393661, 0.10256312042474747, 0.10071804374456406, 0.09899897873401642, 0.09506681561470032, 0.0950138121843338, 0.09475482255220413, 0.09409911930561066]}}
{"id": "WebQTest-1543", "question": "what is samuel langhorne clemens well known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["book.written_work.author"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["symbols.namesake.named_after"], ["fictional_universe.fictional_character.character_created_by"], ["book.author.works_written"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is samuel langhorne clemens well known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0014749806141480803, -0.04641782492399216, -0.09009163081645966, -0.09955932945013046, -0.1033378392457962, -0.10898961871862411, -0.12138132750988007, -0.12382029742002487, -0.12420226633548737, -0.12556712329387665], "norm_scores": [0.1096649169921875, 0.10484537482261658, 0.10036493092775345, 0.09941918402910233, 0.09904424101114273, 0.0984860509634018, 0.09727317094802856, 0.0970362052321434, 0.0969991534948349, 0.09686684608459473]}}
{"id": "WebQTest-1544", "question": "who was president after william henry harrison", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.gender", "people.person.gender"], ["people.person.profession", "people.person.profession"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["freebase.type_profile.equivalent_topic", "common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after william henry harrison [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.015163461677730083, -0.022569354623556137, -0.04375792294740677, -0.05486474186182022, -0.05497504025697708, -0.057255201041698456, -0.05786172300577164, -0.05964966118335724, -0.06200491264462471, -0.06287254393100739], "norm_scores": [0.10343828052282333, 0.10267505794763565, 0.10052241384983063, 0.09941209852695465, 0.09940113872289658, 0.09917474538087845, 0.09911461174488068, 0.09893756359815598, 0.09870481491088867, 0.09861921519041061]}}
{"id": "WebQTest-1546", "question": "where did rick santorum attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here", "location.location.containedby"], ["organization.organization.founders", "people.person.education", "education.educational_institution.students_graduates"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rick santorum attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.founders <SEP> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>"], "scores": [-0.01612420566380024, -0.022237036377191544, -0.02764984779059887, -0.03188182786107063, -0.03526861220598221, -0.046191681176424026, -0.07995692640542984, -0.10417260229587555, -0.10486944764852524, -0.10550521314144135], "norm_scores": [0.10414838790893555, 0.1035136878490448, 0.1029549092054367, 0.10252012312412262, 0.10217349231243134, 0.10106351971626282, 0.09770805388689041, 0.09537040442228317, 0.095303975045681, 0.09524340182542801]}}
{"id": "WebQTest-1547", "question": "what is the title of george bush 's new book", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["book.written_work.subjects"], ["book.author.book_editions_published"], ["book.book_subject.works"], ["film.film.subjects"], ["common.topic.notable_types"], ["film.producer.films_executive_produced"], ["media_common.quotation.author"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the title of george bush 's new book [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> film.producer.films_executive_produced </PATH>", "<PATH> media_common.quotation.author </PATH>"], "scores": [-0.009585092775523663, -0.023976029828190804, -0.04575701430439949, -0.0552772656083107, -0.06774533540010452, -0.09032665193080902, -0.09759505093097687, -0.10683732479810715, -0.12101205438375473, -0.1280248761177063], "norm_scores": [0.10663942992687225, 0.10511577874422073, 0.1028510108590126, 0.10187648981809616, 0.10061416774988174, 0.09836762398481369, 0.09765524417161942, 0.09675684571266174, 0.09539502114057541, 0.09472837299108505]}}
{"id": "WebQTest-1548", "question": "where did antoine lavoisier die", "prediction": [["people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["people.place_of_death.people", "people.place_of_death.location"], ["people.place_lived.person", "people.place_lived.location"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did antoine lavoisier die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.00023476124624721706, -0.09301795810461044, -0.103901207447052, -0.10476364195346832, -0.10780230909585953, -0.10930511355400085, -0.10983788222074509, -0.11501488834619522, -0.11563632637262344, -0.12496627867221832], "norm_scores": [0.11025548726320267, 0.10048587620258331, 0.0993981808423996, 0.09931249916553497, 0.09901117533445358, 0.09886249154806137, 0.09880984574556351, 0.09829962253570557, 0.09823855757713318, 0.09732625633478165]}}
{"id": "WebQTest-1550", "question": "what is the name of king george vi wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["royalty.monarch.kingdom", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"], ["common.topic.notable_types", "common.topic.notable_types"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.spouse_s", "people.person.spouse_s"], ["film.film.subjects", "film.film_subject.films"], ["people.person.religion", "people.person.religion"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.family.members", "people.family.members"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of king george vi wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> common.image.appears_in_topic_gallery <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> common.image.appears_in_topic_gallery <SEP> common.topic.image </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.0364699624478817, -0.036818794906139374, -0.03903219476342201, -0.041680190712213516, -0.04240330681204796, -0.04545017331838608, -0.04664210230112076, -0.047411464154720306, -0.049423377960920334, -0.05046948790550232], "norm_scores": [0.10071238875389099, 0.10067726671695709, 0.1004546731710434, 0.10018902271986008, 0.10011659562587738, 0.09981201589107513, 0.09969311952590942, 0.09961644560098648, 0.09941623359918594, 0.09931229054927826]}}
{"id": "WebQTest-1551", "question": "what year did seattle seahawks go to the super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["time.recurring_event.current_frequency"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.season"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did seattle seahawks go to the super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.season </PATH>"], "scores": [-0.018026765435934067, -0.021706730127334595, -0.025176966562867165, -0.034407440572977066, -0.039855360984802246, -0.04987475275993347, -0.08691877126693726, -0.09990037232637405, -0.11832726001739502, -0.12220051884651184], "norm_scores": [0.10437904298305511, 0.10399563610553741, 0.10363537818193436, 0.1026831716299057, 0.10212527960538864, 0.1011071652173996, 0.0974302738904953, 0.09617364406585693, 0.09441769123077393, 0.09405269473791122]}}
{"id": "WebQTest-1553", "question": "where does sami yusuf live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.places_lived", "people.place_lived.location"], ["music.artist.origin"], ["people.person.nationality"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.children", "people.deceased_person.place_of_death"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sami yusuf live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.children <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.024533508345484734, -0.028636563569307327, -0.032860834151506424, -0.03936975076794624, -0.040943846106529236, -0.04317016154527664, -0.05352984741330147, -0.05893436446785927, -0.06149943545460701, -0.08009551465511322], "norm_scores": [0.1021929383277893, 0.10177449882030487, 0.1013454794883728, 0.10068797320127487, 0.10052960366010666, 0.10030604153871536, 0.09927226603031158, 0.0987372025847435, 0.09848425537347794, 0.0966697558760643]}}
{"id": "WebQTest-1554", "question": "what is the official language of brazil now", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["location.country.languages_spoken", "language.human_language.main_country"], ["location. country.languages_spoken"], ["location.country.languages_spokencommon"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the official language of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken <SEP> location.country.languages_spoken </PATH>", "</PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken <PATH>", "<PATH> location.country.languages_spoken <SEP> language.human_language.main_country </PATH>", "<PATH> location. country.languages_spoken </PATH>", "<PATH> location.country.languages_spokencommon </PATH>"], "scores": [-0.0005195639678277075, -0.07373546063899994, -0.09046180546283722, -0.09902818500995636, -0.18490846455097198, -0.18496300280094147, -0.18624399602413177, -0.20939970016479492, -0.2194318026304245, -0.2247122824192047], "norm_scores": [0.11550946533679962, 0.10735451430082321, 0.10557381063699722, 0.10467327386140823, 0.09605909883975983, 0.09605386108160019, 0.09593089669942856, 0.09373506903648376, 0.09279941022396088, 0.09231067448854446]}}
{"id": "WebQTest-1555", "question": "what is the parliament of nepal called", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.governing_officials", "government.governmental_body.members"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.governmental_body"], ["location.location.containedby", "government.governmental_jurisdiction.government_bodies"], ["location.location.containedby", "government.governmental_body.jurisdiction"], ["government.governmental_body.jurisdiction"], ["location.country.parliament"], ["location.location.containedby", "location.location.containedby"], ["location.country.legislative_sessions", "government.governmental_jurisdiction.government_bodies"], ["location.country.form_of_government"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the parliament of nepal called [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_body.members </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.governmental_body </PATH>", "<PATH> location.location.containedby <SEP> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> location.location.containedby <SEP> government.governmental_body.jurisdiction </PATH>", "<PATH> government.governmental_body.jurisdiction </PATH>", "<PATH> location.country.parliament </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.country.legislative_sessions <SEP> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.01677842065691948, -0.018535062670707703, -0.026834113523364067, -0.02957283891737461, -0.05195118486881256, -0.05326974019408226, -0.07053244113922119, -0.07099167257547379, -0.07299391180276871, -0.07575024664402008], "norm_scores": [0.10321946442127228, 0.10303831100463867, 0.10218672454357147, 0.10190724581480026, 0.09965206682682037, 0.09952075034379959, 0.09781749546527863, 0.09777257591485977, 0.09757701307535172, 0.0973084345459938]}}
{"id": "WebQTest-1556", "question": "when 's the last time the mets won the world series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["time.recurring_event.instances"], ["sports.sports_world_championship_event.champion"], ["sports.sports_club.championships"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the mets won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_world_championship_event.champion </PATH>", "<PATH> sports.sports_club.championships </PATH>"], "scores": [-0.010963247157633305, -0.011658947914838791, -0.07771005481481552, -0.08644428104162216, -0.11367620527744293, -0.13669376075267792, -0.1439274251461029, -0.14703956246376038, -0.1582409292459488, -0.1582496166229248], "norm_scores": [0.10964136570692062, 0.10956510901451111, 0.10256204009056091, 0.10167014598846436, 0.09893883019685745, 0.09668751060962677, 0.0959906205534935, 0.09569235891103745, 0.09462644904851913, 0.09462562203407288]}}
{"id": "WebQTest-1557", "question": "where do florida panthers play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["sports.sports_team.location", "location.location.containedby"], ["sports.sports_team_roster.team", "sports.sports_facility.teams"], ["sports.sports_team_roster.team"], ["sports.sports_facility.teams", "sports.sports_team.arena_stadium"], ["sports.sports_facility.teams", "sports.sports_facility.teams"], ["sports.sports_team_roster.team", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do florida panthers play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.006944823078811169, -0.024351738393306732, -0.06389713287353516, -0.06553636491298676, -0.11867710947990417, -0.12139874696731567, -0.12724152207374573, -0.12980546057224274, -0.1317482441663742, -0.14140644669532776], "norm_scores": [0.10887893289327621, 0.10700009018182755, 0.10285129398107529, 0.10268284380435944, 0.09736865013837814, 0.09710400551557541, 0.09653830528259277, 0.09629109501838684, 0.0961042046546936, 0.09518048912286758]}}
{"id": "WebQTest-1558", "question": "who is john cena married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is john cena married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.01994488015770912, -0.033729396760463715, -0.03513356298208237, -0.04504698887467384, -0.04706699773669243, -0.0495869517326355, -0.05079114809632301, -0.05392853915691376, -0.054335616528987885, -0.054818443953990936], "norm_scores": [0.10247354954481125, 0.10107069462537766, 0.10092887282371521, 0.09993326663970947, 0.09973160177469254, 0.09948059916496277, 0.0993608832359314, 0.0990496426820755, 0.09900932013988495, 0.09896153211593628]}}
{"id": "WebQTest-1559", "question": "what to do san jose ca", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["location.location.contains"], ["base.usnationalparks.us_national_park.state"], ["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"], ["base.arthist.helynevek.intezmeny"], ["film.film_festival.location"], ["film.film.featured_film_locations"]], "ground_paths": [["film.film_festival.location"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.neighborhood.neighborhood_of"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do san jose ca [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> film.film.featured_film_locations </PATH>"], "scores": [-0.01636010967195034, -0.016370946541428566, -0.02826683036983013, -0.10850480943918228, -0.1283680945634842, -0.14655466377735138, -0.15523222088813782, -0.15947292745113373, -0.1603858321905136, -0.16801385581493378], "norm_scores": [0.10947823524475098, 0.10947705805301666, 0.10818244516849518, 0.09984121471643448, 0.0978776067495346, 0.0961136445403099, 0.09528321772813797, 0.09488000720739365, 0.0947934240102768, 0.0940730944275856]}}
{"id": "WebQTest-1560", "question": "who founded new york university", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["education.educational_institution.parent_institution", "people.person.parents"], ["education.educational_institution.parent_institution", "people.person.education"], ["education.educational_institution.subsidiary_or_constituent_schools", "symbols.namesake.named_after"], ["education.educational_institution.parent_institution"], ["education.education.student", "education.education.student"], ["education.educational_institution.subsidiary_or_constituent_schools", "people.person.nationality"], ["education.educational_institution.subsidiary_or_constituent_schools", "symbols.name_source.namesakes"], ["education.educational_institution.parent_institution", "people.person.children"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded new york university [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> people.person.parents </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> people.person.education </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> symbols.namesake.named_after </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>", "<PATH> education.education.student <SEP> education.education.student </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> people.person.nationality </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> symbols.name_source.namesakes </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> people.person.children </PATH>"], "scores": [-0.026223329827189445, -0.031582362949848175, -0.035808123648166656, -0.038020532578229904, -0.04295707121491432, -0.04338366538286209, -0.04704109579324722, -0.052148133516311646, -0.05615587905049324, -0.05858564004302025], "norm_scores": [0.10170609503984451, 0.10116250813007355, 0.10073591768741608, 0.10051330178976059, 0.10001833736896515, 0.09997567534446716, 0.09961068630218506, 0.09910327196121216, 0.09870687872171402, 0.09846734255552292]}}
{"id": "WebQTest-1561", "question": "who created arthur", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.program_creator"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["film.film_story_contributor.film_story_credits"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "ground_paths": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.program_creator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who created arthur [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.program_creator </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>"], "scores": [-0.007045959588140249, -0.03735663369297981, -0.04721178114414215, -0.052097707986831665, -0.07704537361860275, -0.07923148572444916, -0.08043210208415985, -0.08542633801698685, -0.0934629887342453, -0.09434898942708969], "norm_scores": [0.10596655309200287, 0.1028028205037117, 0.10179466754198074, 0.1012985110282898, 0.09880261868238449, 0.09858685731887817, 0.09846856445074081, 0.09797801822423935, 0.09719375520944595, 0.09710767865180969]}}
{"id": "WebQTest-1562", "question": "what countries constitute the eu", "prediction": [["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization.geographic_scope", "location.location.containedby"], ["organization.organization.geographic_scope", "base.locations.countries.continent"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_membership.member"], ["government.governmental_jurisdiction.governing_officials", "government.political_district.representatives"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization_founder.organizations_founded"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"], ["organization.organization.geographic_scope", "location.location.containedby"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.district_represented"], ["government.governmental_jurisdiction.governing_officials", "government.political_district.representatives"], ["organization.organization.geographic_scope", "location.location.primarily_containedby"], ["organization.organization.founders"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["government.government_position_held.jurisdiction_of_office", "government.political_district.representatives"], ["organization.organization.geographic_scope", "base.locations.countries.continent"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["base.industrystandards.standard_organization.standards", "base.aareas.coding_scheme.geo_scope"], ["organization.organization_membership.organization", "organization.organization_membership.member"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_membership.member"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries constitute the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.continents.countries_within </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.countries.continent </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_membership.member </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.political_district.representatives </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_membership.member </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.district_represented </PATH>"], "scores": [-0.02957964316010475, -0.03206333890557289, -0.033405791968107224, -0.03602335974574089, -0.03846056014299393, -0.03867455571889877, -0.04228430241346359, -0.04248490929603577, -0.04630047455430031, -0.046697065234184265], "norm_scores": [0.10090429335832596, 0.1006539911031723, 0.10051895678043365, 0.10025618225336075, 0.10001213103532791, 0.09999073296785355, 0.0996304452419281, 0.09961046278476715, 0.09923111647367477, 0.09919176995754242]}}
{"id": "WebQTest-1563", "question": "what are all the countries in north africa", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["common.topic.notable_types"], ["language.human_language.countries_spoken_in"], ["geography.river.basin_countries"], ["location.location.primarily_containedby"], ["language.human_language.main_country"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the countries in north africa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.005129879340529442, -0.033244892954826355, -0.06876234710216522, -0.07088364660739899, -0.08114293217658997, -0.1119718924164772, -0.13009370863437653, -0.13418349623680115, -0.1468174159526825, -0.14743605256080627], "norm_scores": [0.10906121134757996, 0.10603765398263931, 0.10233756899833679, 0.10212070494890213, 0.10107837617397308, 0.09800978004932404, 0.09624965488910675, 0.09585683047771454, 0.09465339779853821, 0.09459485858678818]}}
{"id": "WebQTest-1564", "question": "what did bobby darin die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.people.deceased_person.cause_of_death"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["people.marriage.spouse", "people.marriage.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bobby darin die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.cause_of_death </PATH>"], "scores": [-0.0014594978420063853, -0.04433314502239227, -0.110251784324646, -0.11554685235023499, -0.13869498670101166, -0.14669230580329895, -0.14963030815124512, -0.15162599086761475, -0.15751932561397552, -0.1672450602054596], "norm_scores": [0.11224226653575897, 0.10753173381090164, 0.10067196190357208, 0.10014031082391739, 0.09784887731075287, 0.09706946462392807, 0.09678468108177185, 0.09659172594547272, 0.09602416306734085, 0.09509477764368057]}}
{"id": "WebQTest-1565", "question": "where does houston dynamo play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"], ["sports.sports_team.location"], ["travel.travel_destination.tourist_attractions"], ["sports.sports_team_location.teams"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["sports.professional_sports_team.owner_s", "sports.sports_team.arena_stadium"], ["sports.sports_facility.teams", "sports.sports_team.arena_stadium"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does houston dynamo play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_team.arena_stadium </PATH>"], "scores": [-0.012090618722140789, -0.013345258310437202, -0.08303108811378479, -0.08312774449586868, -0.09617437422275543, -0.10979387909173965, -0.11634176969528198, -0.11967237293720245, -0.12198188155889511, -0.12422196567058563], "norm_scores": [0.10779552161693573, 0.10766036808490753, 0.10041340440511703, 0.10040369629859924, 0.09910227358341217, 0.09776169806718826, 0.0971236526966095, 0.09680071473121643, 0.09657741338014603, 0.09636131674051285]}}
{"id": "WebQTest-1566", "question": "what kind of guitar john mayer", "prediction": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"], ["music.instrument.family"], ["music.group_member.instruments_played"], ["music.musical_group.member", "music.group_membership.role"], ["music.guitar_player.guitars_played"], ["music.artist.guitars_played"], ["music.producer.releases_produced"], ["music.musical_group.member", "music.group_membership.group"], ["music.instrument.guitarists"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar john mayer [/INST]", "raw_output": {"paths": ["<PATH> music.guitar.guitarists </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.guitar_player.guitars_played </PATH>", "<PATH> music.artist.guitars_played </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.instrument.guitarists </PATH>"], "scores": [-0.014699302613735199, -0.01579175516963005, -0.05213035270571709, -0.05710628628730774, -0.10391629487276077, -0.10766013711690903, -0.10989715903997421, -0.11783010512590408, -0.12325914949178696, -0.12328312546014786], "norm_scores": [0.10692951083183289, 0.10681276023387909, 0.10300101339817047, 0.10248975455760956, 0.09780276566743851, 0.0974372923374176, 0.0972195640206337, 0.09645137935876846, 0.09592915326356888, 0.09592685103416443]}}
{"id": "WebQTest-1567", "question": "what cancer did patrick swayze", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition", "medicine.disease.notable_people_with_this_condition"], ["medicine.cancer_patient.cancer"], ["people.person.medical_condition", "medical_condition.condition"], ["medicine.cancer_patient.notable_people_with_this_condition"], ["medicine.cancer_patient.cancer", "medicine.cancer_patient.cancer_type"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did patrick swayze [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.cancer_patient.cancer </PATH>", "<PATH> people.person.medical_condition <SEP> medical_condition.condition </PATH>", "<PATH> medicine.cancer_patient.notable_people_with_this_condition </PATH>", "<PATH> medicine.cancer_patient.cancer <SEP> medicine.cancer_patient.cancer_type </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.0016499822959303856, -0.046036992222070694, -0.058525316417217255, -0.11361005157232285, -0.11885321140289307, -0.11958710849285126, -0.12997502088546753, -0.13061600923538208, -0.13094772398471832, -0.1315321922302246], "norm_scores": [0.11002293229103088, 0.10524614155292511, 0.1039399728178978, 0.098369300365448, 0.09785488247871399, 0.09778310358524323, 0.09677258878946304, 0.09671058505773544, 0.09667850285768509, 0.09662202000617981]}}
{"id": "WebQTest-1568", "question": "what university did obama graduated from", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"], ["book.written_work.subjects", "location.location.containedby"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.014082531444728374, -0.016146663576364517, -0.02639957144856453, -0.0863635316491127, -0.09371750801801682, -0.09919483959674835, -0.10818999260663986, -0.11179844290018082, -0.1203441247344017, -0.12908945977687836], "norm_scores": [0.10677521675825119, 0.1065550446510315, 0.10546812415122986, 0.09932971745729446, 0.09860192984342575, 0.09806333482265472, 0.09718520194292068, 0.09683513641357422, 0.09601113945245743, 0.09517516195774078]}}
{"id": "WebQTest-1569", "question": "what jamaican money called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["location.country.currency_formerly_used"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.currency_used", "location.country.currency_used"], ["location.country.currency_used", "common.topic.notable_types", "common.topic.notable_types"], ["language.human_language.countries_spoken_in", "location.country.currency_used"], ["common.topic.notable_types"], ["location.country.currency_used", "location.country.currency_used", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jamaican money called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.0005220065359026194, -0.07259882986545563, -0.1339200735092163, -0.13523979485034943, -0.14119434356689453, -0.14561253786087036, -0.1650286465883255, -0.17065152525901794, -0.17368733882904053, -0.17736762762069702], "norm_scores": [0.11384294182062149, 0.10592623054981232, 0.0996258556842804, 0.09949446469545364, 0.09890378266572952, 0.09846777468919754, 0.0965743437409401, 0.09603284299373627, 0.09574174880981445, 0.09539003670215607]}}
{"id": "WebQTest-1570", "question": "where is the arizona cardinals stadium located", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"], ["location.location.containedby"], ["sports.sports_team.location"], ["sports.sports_team.arena_stadium", "location.location.containedby"], ["sports.professional_sports_team.owner_s", "sports.sports_team.arena_stadium"], ["sports.sports_team_location.teams", "location.location.containedby"], ["sports.sports_team.arena_stadium", "sports.sports_facility.teams"], ["sports.sports_team.location", "location.location.containedby"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the arizona cardinals stadium located [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.arena_stadium <SEP> location.location.containedby </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team_location.teams <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_team.arena_stadium <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location <SEP> location.location.containedby </PATH>"], "scores": [-0.0060797156766057014, -0.025792177766561508, -0.05311385914683342, -0.07298991084098816, -0.09138046950101852, -0.09340044111013412, -0.09684278815984726, -0.10222212225198746, -0.1041860431432724, -0.104408398270607], "norm_scores": [0.107078917324543, 0.10498880594968796, 0.10215916484594345, 0.10014868527650833, 0.09832372516393661, 0.09812531620264053, 0.09778812527656555, 0.09726349264383316, 0.09707266092300415, 0.09705108404159546]}}
{"id": "WebQTest-1571", "question": "what is sir isaac newton theory", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["influence.influence_node.influenced_by"], ["people.person.profession"], ["base.argumentmaps.original_idea.innovator"], ["influence.influence_node.influenced"], ["common.topic.notable_types"], ["astronomy.astronomer.astronomical_objects_discovered"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is sir isaac newton theory [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> astronomy.astronomer.astronomical_objects_discovered </PATH>"], "scores": [-0.007438700180500746, -0.023357655853033066, -0.06965965032577515, -0.08345925062894821, -0.09458040446043015, -0.09556154161691666, -0.10915498435497284, -0.11166603118181229, -0.1138404905796051, -0.12376059591770172], "norm_scores": [0.10780005902051926, 0.10609757155179977, 0.1012970507144928, 0.09990879148244858, 0.09880384057760239, 0.09870694577693939, 0.09737426042556763, 0.09713005274534225, 0.09691908210515976, 0.09596239030361176]}}
{"id": "WebQTest-1572", "question": "what should i do today in san francisco", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["metropolitan_transit.transit_system.area_served"], ["location.neighborhood.neighborhood_of"], ["film.film_festival.location"], ["base.arthist.helynevek.intezmeny"], ["organization.organization.geographic_scope"], ["film.film.featured_film_locations"], ["transportation.bridge.locale"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.neighborhood.neighborhood_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what should i do today in san francisco [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> location.neighborhood.neighborhood_of </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> transportation.bridge.locale </PATH>"], "scores": [-0.015543638728559017, -0.019191615283489227, -0.021863140165805817, -0.10309531539678574, -0.1157955452799797, -0.1195753961801529, -0.12112700194120407, -0.12721410393714905, -0.13351745903491974, -0.14417915046215057], "norm_scores": [0.10782614350318909, 0.1074335053563118, 0.10714688152074814, 0.09878724068403244, 0.09754055738449097, 0.0971725583076477, 0.09702190011739731, 0.0964331179857254, 0.09582717716693878, 0.09481092542409897]}}
{"id": "WebQTest-1573", "question": "who influenced picasso art", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["visual_art.visual_artist.art_forms"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["visual_art.art_period_movement.associated_artists"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["people.person.profession"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["visual_art.visual_artist.associated_periods_or_movements"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced picasso art [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>"], "scores": [-0.01204010471701622, -0.01299904566258192, -0.1106279045343399, -0.1155729666352272, -0.11892224848270416, -0.13090217113494873, -0.13551470637321472, -0.137843057513237, -0.13801217079162598, -0.13892532885074615], "norm_scores": [0.10963141173124313, 0.10952632874250412, 0.09933879226446152, 0.09884876757860184, 0.0985182449221611, 0.09734504669904709, 0.09689707309007645, 0.09667173027992249, 0.09665537625551224, 0.09656716138124466]}}
{"id": "WebQTest-1574", "question": "what is my timezone in pennsylvania", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.time_Zones"], ["time.location.time_zones"], ["location.location.time_zonessymbol"], ["location.location.time_zonescommon_in_this_time_zone"], ["locations.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> time.location.time_zones </PATH>", "<PATH> location.location.time_zonessymbol </PATH>", "<PATH> location.location.time_zonescommon_in_this_time_zone </PATH>", "<PATH> locations.location.time_zones </PATH>"], "scores": [-1.4750757145520765e-05, -0.1316731572151184, -0.1804019659757614, -0.18913619220256805, -0.19363677501678467, -0.2131633460521698, -0.23085032403469086, -0.23194481432437897, -0.23225751519203186, -0.23321597278118134], "norm_scores": [0.11986243724822998, 0.10507626086473465, 0.1000787615776062, 0.09920845925807953, 0.09876296669244766, 0.09685316681861877, 0.09515519440174103, 0.0950511023402214, 0.09502138942480087, 0.09493035823106766]}}
{"id": "WebQTest-1575", "question": "what products and\\/or services does google offer customers", "prediction": [["organization.organization.sectors"], ["business.business_operation.industry"], ["organization.organization_founder.organizations_founded"], ["computer.software_developer.software"], ["business.brand.products"], ["organization.organization.sectors", "organization.organization.sectors"], ["organization.organization.founders"], ["business.industry.companies"], ["internet.website.category", "internet.website_category.sites"], ["organization.organization.sectors", "business.business_operation.industry"]], "ground_paths": [["computer.software.developer"], ["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["business.consumer_company.products", "business.consumer_product.company"], ["business.consumer_company.products"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"], ["computer.software_developer.software"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["computer.computer_manufacturer_brand.computer_models"], ["internet.website.owner"], ["projects.project_participation.participant", "projects.project_participation.project"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products and\\/or services does google offer customers [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> business.business_operation.industry </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> computer.software_developer.software </PATH>", "<PATH> business.brand.products </PATH>", "<PATH> organization.organization.sectors <SEP> organization.organization.sectors </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> internet.website.category <SEP> internet.website_category.sites </PATH>", "<PATH> organization.organization.sectors <SEP> business.business_operation.industry </PATH>"], "scores": [-0.04366827383637428, -0.04803187772631645, -0.05013440176844597, -0.052701666951179504, -0.052916791290044785, -0.05540815368294716, -0.05662047490477562, -0.0583483949303627, -0.06839726120233536, -0.07180020958185196], "norm_scores": [0.10121740400791168, 0.10077670216560364, 0.10056503117084503, 0.10030718892812729, 0.100285604596138, 0.10003607720136642, 0.09991487860679626, 0.09974237531423569, 0.09874510020017624, 0.09840964525938034]}}
{"id": "WebQTest-1577", "question": "who did cliff lee play for last year", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cliff lee play for last year [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.014329949393868446, -0.014728107489645481, -0.0297795832157135, -0.033327531069517136, -0.059607814997434616, -0.060052499175071716, -0.07045584172010422, -0.07103148102760315, -0.07936853915452957, -0.08903004974126816], "norm_scores": [0.10382231324911118, 0.10378098487854004, 0.10223062336444855, 0.10186855494976044, 0.09922630339860916, 0.09918218106031418, 0.09815570712089539, 0.09809921681880951, 0.09728476405143738, 0.09634937345981598]}}
{"id": "WebQTest-1580", "question": "what type of democracy is the united states", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet", "base.aareas.schema.administrative_area.administrative_parent"], ["film.film.country", "government.form_of_government.countries"], ["form.form_of_government.countries"], ["film.film.country", "location.country.form_of_government"], ["base.locations.countries.planet"], ["type.form_of_government.countries"], ["book.written_work.subjects"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of democracy is the united states [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> film.film.country <SEP> government.form_of_government.countries </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> film.film.country <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> type.form_of_government.countries </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.008717945776879787, -0.015363924205303192, -0.14319320023059845, -0.1506449431180954, -0.15442338585853577, -0.1601475030183792, -0.16211985051631927, -0.16560831665992737, -0.16578637063503265, -0.1683923602104187], "norm_scores": [0.11262813955545425, 0.11188210546970367, 0.09845665097236633, 0.09772570431232452, 0.09735715389251709, 0.09680145978927612, 0.09661072492599487, 0.09627428650856018, 0.09625715017318726, 0.0960066169500351]}}
{"id": "WebQTest-1582", "question": "what time in china hong kong", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.schemastaging.disputed_location_claimant.disputed_territories", "location.location.time_zones"], ["base.locations.countries.continent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.schemastaging.disputed_location_claimant.disputed_territories", "time.time_zone.locations_in_this_time_zone"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.locations.time_zones.locations_in_this_time_zone"]], "ground_paths": [["time.time_zone.locations_in_this_time_zone"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in china hong kong [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories <SEP> location.location.time_zones </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories <SEP> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.locations.time_zones.locations_in_this_time_zone </PATH>"], "scores": [-0.005408159457147121, -0.02459416724741459, -0.07119680941104889, -0.08470629155635834, -0.08500836044549942, -0.08992671221494675, -0.09391822665929794, -0.097244493663311, -0.09826011955738068, -0.10107707977294922], "norm_scores": [0.10716767609119415, 0.10513114929199219, 0.10034417361021042, 0.09899769723415375, 0.0989677906036377, 0.09848222881555557, 0.09808991849422455, 0.09776418656110764, 0.09766494482755661, 0.09739021211862564]}}
{"id": "WebQTest-1583", "question": "what all movies has taylor lautner been in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all movies has taylor lautner been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.honored_for </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_winning_work.awards_won </PATH>"], "scores": [-0.024184634909033775, -0.029255103319883347, -0.02984103187918663, -0.030857635661959648, -0.030942201614379883, -0.038241200149059296, -0.03990175947546959, -0.04230262711644173, -0.043191034346818924, -0.04498698562383652], "norm_scores": [0.10112252086400986, 0.10061107575893402, 0.10055214166641235, 0.10044997930526733, 0.10044147074222565, 0.09971102327108383, 0.09954559057950974, 0.0993068739771843, 0.0992186889052391, 0.09904065728187561]}}
{"id": "WebQTest-1584", "question": "what disease did bernie mac die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"], ["medicine.notable_person_with_medical_condition.condition"], ["people.person.cause_of_death"], ["deceased_person.cause_of_death"], ["influence.influence_node.influenced_by", "medicine.disease.notable_people_with_this_condition"], ["media_common.quotation.author", "medicine.disease.notable_people_with_this_condition"], ["people.person.deceased_person", "medicine.disease.notable_people_with_this_condition"], ["influence.influence_node.influenced", "medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did bernie mac die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> media_common.quotation.author <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.deceased_person <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> influence.influence_node.influenced <SEP> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.0045680636540055275, -0.03465868905186653, -0.039989978075027466, -0.06123039871454239, -0.10432904213666916, -0.10738348215818405, -0.10894962400197983, -0.10942761600017548, -0.1290183812379837, -0.13122902810573578], "norm_scores": [0.10807114839553833, 0.10486765950918198, 0.10431007295846939, 0.10211785137653351, 0.09781020134687424, 0.0975119024515152, 0.097359299659729, 0.09731277078390121, 0.09542489796876907, 0.09521417319774628]}}
{"id": "WebQTest-1585", "question": "who plays spike spiegel", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["base.schemastaging.tv_star_dubbing_performance.character", "base.schemastaging.tv_star_dubbing_performance.actor"]], "ground_paths": [["base.schemastaging.tv_character_extra.regular_dubbing_performances", "base.schemastaging.tv_star_dubbing_performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays spike spiegel [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> base.schemastaging.tv_star_dubbing_performance.character <SEP> base.schemastaging.tv_star_dubbing_performance.actor </PATH>"], "scores": [-0.016694873571395874, -0.03532005473971367, -0.03533497825264931, -0.03974105045199394, -0.041249774396419525, -0.04473059996962547, -0.047248225659132004, -0.050729040056467056, -0.055844224989414215, -0.057043686509132385], "norm_scores": [0.10259667783975601, 0.10070347785949707, 0.10070197284221649, 0.10025925189256668, 0.10010810196399689, 0.09976024925708771, 0.09950941056013107, 0.09916363656520844, 0.09865768998861313, 0.09853942692279816]}}
{"id": "WebQTest-1586", "question": "who was queen isabella 's mother", "prediction": [["people.person.parents"], ["people.person.children"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.parent"], ["people.person.mother"], ["people.person.gender"], ["common.topic.notable_types"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen isabella 's mother [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> people.person.mother </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.008783118799328804, -0.019985556602478027, -0.12129886448383331, -0.12601086497306824, -0.1299322545528412, -0.14530231058597565, -0.1479872316122055, -0.15418745577335358, -0.15534403920173645, -0.1564241647720337], "norm_scores": [0.11121918261051178, 0.10998021066188812, 0.09938360005617142, 0.09891641139984131, 0.09852927178144455, 0.09702645987272263, 0.09676629304885864, 0.09616817533969879, 0.09605702012777328, 0.0959533229470253]}}
{"id": "WebQTest-1587", "question": "who does chris hemsworth have a baby with", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nominee"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["celebrities.friendship.friend", "celebrities.friendship.friend"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does chris hemsworth have a baby with [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.021731384098529816, -0.024875110015273094, -0.026885075494647026, -0.03146884962916374, -0.05660008639097214, -0.06321969628334045, -0.0731002613902092, -0.07413427531719208, -0.0743631049990654, -0.07549594342708588], "norm_scores": [0.10306747257709503, 0.1027439683675766, 0.10253766179084778, 0.10206872224807739, 0.09953558444976807, 0.098878875374794, 0.097906693816185, 0.09780550748109818, 0.09778313338756561, 0.09767242521047592]}}
{"id": "WebQTest-1589", "question": "where is the mts iceplex in winnipeg", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"], ["business.asset_ownership.owner", "business.asset_ownership.owned_asset"], ["business.asset_ownership.owned_asset", "business.asset_ownership.owned_asset"], ["sports.sports_facility.teams"], ["location.location.containedby", "location.location.containedby"], ["geography.lake.basin_countries"], ["business.asset.owners", "business.asset.owners"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the mts iceplex in winnipeg [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> business.asset_ownership.owner <SEP> business.asset_ownership.owned_asset </PATH>", "<PATH> business.asset_ownership.owned_asset <SEP> business.asset_ownership.owned_asset </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> business.asset.owners <SEP> business.asset.owners </PATH>"], "scores": [-0.014012566767632961, -0.02601121924817562, -0.029221292585134506, -0.030141763389110565, -0.10758119076490402, -0.10905338823795319, -0.12379837781190872, -0.12723492085933685, -0.13173708319664001, -0.1342429220676422], "norm_scores": [0.10704774409532547, 0.10577099025249481, 0.10543200373649597, 0.10533500462770462, 0.09748575836420059, 0.0973423421382904, 0.09591756761074066, 0.09558851271867752, 0.0951591208577156, 0.09492096304893494]}}
{"id": "WebQTest-1590", "question": "what are the most popular religions in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["symbols.namesake.named_after", "religion.religion.notable_figures"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["film.film_location.featured_in_films", "film.film.country", "people.person.nationality"], ["people.person.place_of_birth", "people.person.religion"], ["people.deceased_person.place_of_death", "people.person.religion"], ["film.film_location.featured_in_films", "film.film_location.featured_in_films"]], "ground_paths": [["people.person.quotations", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["symbols.namesake.named_after", "religion.religion.notable_figures"], ["people.person.nationality", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the most popular religions in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>", "<PATH> location.country.administrative_divisions <SEP> religion.religion.sacred_sites </PATH>", "<PATH> symbols.namesake.named_after <SEP> religion.religion.notable_figures </PATH>", "<PATH> film.film_location.featured_in_films <SEP> media_common.netflix_title.netflix_genres </PATH>", "<PATH> film.film_location.featured_in_films <SEP> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.religion </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.person.religion </PATH>", "<PATH> film.film_location.featured_in_films <SEP> film.film_location.featured_in_films </PATH>"], "scores": [-0.003100902307778597, -0.030314331874251366, -0.058768536895513535, -0.0772806704044342, -0.08584900945425034, -0.09243465214967728, -0.09818120300769806, -0.0991140827536583, -0.1031402200460434, -0.1113583967089653], "norm_scores": [0.10749699175357819, 0.10461107641458511, 0.10167640447616577, 0.09981147199869156, 0.09895990043878555, 0.09831033647060394, 0.09774701297283173, 0.09765587002038956, 0.09726348519325256, 0.09646743535995483]}}
{"id": "WebQTest-1591", "question": "what was obama 's former occupation", "prediction": [["people.person.profession"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.profession", "people.profession.specializations"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession", "common.topic.notable_types"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["common.topic.notable_types", "type.type.expected_by"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was obama 's former occupation [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.007735425140708685, -0.03505250811576843, -0.03999072685837746, -0.045656099915504456, -0.05841153487563133, -0.0610639713704586, -0.06566422432661057, -0.06822724640369415, -0.0756615549325943, -0.07691133767366409], "norm_scores": [0.10465431958436966, 0.10183416306972504, 0.10133253037929535, 0.10076005756855011, 0.09948298335075378, 0.09921946376562119, 0.0987640768289566, 0.09851126372814178, 0.09778162091970444, 0.09765949100255966]}}
{"id": "WebQTest-1592", "question": "what are all the songs justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"], ["music.composition.composer"], ["music.lyricist.lyrics_written"], ["music.album.artist"], ["music.artist.album"], ["music.composition.lyricist"], ["music.artist.track"], ["music.recording.producer"], ["music.recording.featured_artists"]], "ground_paths": [["music.artist.album"], ["music.recording.featured_artists"], ["music.album.artist"], ["music.composer.compositions"], ["music.lyricist.lyrics_written"], ["music.recording.artist"], ["music.composition.lyricist"], ["music.composition.composer"], ["music.featured_artist.albums"], ["music.featured_artist.recordings"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the songs justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.recording.producer </PATH>", "<PATH> music.recording.featured_artists </PATH>"], "scores": [-0.017548346891999245, -0.022034378722310066, -0.03340061753988266, -0.05004186928272247, -0.0695711299777031, -0.07623128592967987, -0.07856445759534836, -0.08181995898485184, -0.0839679092168808, -0.0869000256061554], "norm_scores": [0.10430319607257843, 0.10383634269237518, 0.10266278684139252, 0.10096848756074905, 0.09901577979326248, 0.09835851192474365, 0.0981292873620987, 0.09781035035848618, 0.09760048985481262, 0.0973147302865982]}}
{"id": "WebQTest-1593", "question": "where is the thames river source", "prediction": [["geography.river.origin"], ["geography.river.basin_countries"], ["geography.river.mouth"], ["location.location.containedby"], ["geography.river.cities"], ["geography.lake.basin_countries"], ["geography.river.source"], ["geography.lake.outflow"], ["transportation.bridge.body_of_water_spanned"], []], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the thames river source [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.cities </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> geography.river.source </PATH>", "<PATH> geography.lake.outflow </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>", "<PATH> </PATH>"], "scores": [-0.005067152436822653, -0.0516783781349659, -0.053196072578430176, -0.056159887462854385, -0.09815741330385208, -0.10006751865148544, -0.10403946042060852, -0.11510389298200607, -0.12209783494472504, -0.1249377503991127], "norm_scores": [0.10803446173667908, 0.10311439633369446, 0.10295802354812622, 0.10265332460403442, 0.09843141585588455, 0.0982435792684555, 0.0978541374206543, 0.09677740186452866, 0.09610291570425034, 0.09583036601543427]}}
{"id": "WebQTest-1595", "question": "what new movies is robert pattinson in", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["music.artist.track", "music.recording.releases"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["music.recording.artist", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what new movies is robert pattinson in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.honored_for </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.02847115881741047, -0.030623048543930054, -0.03239331766963005, -0.03274674713611603, -0.034827135503292084, -0.0357193648815155, -0.03645284101366997, -0.03953021764755249, -0.042291488498449326, -0.042940862476825714], "norm_scores": [0.10071433335542679, 0.10049784183502197, 0.10032008588314056, 0.100284643471241, 0.10007622092962265, 0.0999869704246521, 0.09991366416215897, 0.09960666298866272, 0.09933199733495712, 0.09926751255989075]}}
{"id": "WebQTest-1596", "question": "what language do they use in pakistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["film.film.featured_film_locations", "film.film.language"], ["location.country.language_spoken"], ["film.film.featured_film_locations"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they use in pakistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> film.film.featured_film_locations <SEP> film.film.language </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> location.country.language_spoken </PATH>", "<PATH> film.film.featured_film_locations </PATH>"], "scores": [-0.015978487208485603, -0.01774400845170021, -0.03429371118545532, -0.053041595965623856, -0.1418876200914383, -0.17229118943214417, -0.17959460616111755, -0.18805553019046783, -0.18892443180084229, -0.19012688100337982], "norm_scores": [0.11046028882265091, 0.11026544123888016, 0.1084555983543396, 0.10644122958183289, 0.09739228338003159, 0.09447576850652695, 0.09378828853368759, 0.09299809485673904, 0.09291733056306839, 0.09280566871166229]}}
{"id": "WebQTest-1597", "question": "what kind of products does nike sell", "prediction": [["business.business_operation.industry"], ["business.industry.companies"], ["business.industry.parent_industry"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["business.brand.products"], ["business.business_operation.industry", "business.business_operation.industry"], ["business.business_operation.industry", "business.industry.parent_industry"], ["business.product_line.companies"], ["business.product.company"], ["business.industry.industry"]], "ground_paths": [["business.industry.companies"], ["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of products does nike sell [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> business.industry.parent_industry </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> business.brand.products </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> business.business_operation.industry <SEP> business.industry.parent_industry </PATH>", "<PATH> business.product_line.companies </PATH>", "<PATH> business.product.company </PATH>", "<PATH> business.industry.industry </PATH>"], "scores": [-0.011600473895668983, -0.029139062389731407, -0.053279366344213486, -0.05725724995136261, -0.07524120807647705, -0.07725480943918228, -0.08180389553308487, -0.08252791315317154, -0.08313996344804764, -0.08455628156661987], "norm_scores": [0.10530423372983932, 0.10347344726324081, 0.10100547969341278, 0.10060448944568634, 0.09881139546632767, 0.09861262887716293, 0.09816505014896393, 0.09809400141239166, 0.09803397953510284, 0.09789523482322693]}}
{"id": "WebQTest-1598", "question": "who did lbj marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"]], "ground_paths": [["people.person.parents", "people.person.parents"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["book.written_work.subjects", "people.person.nationality"], ["people.person.profession", "common.topic.notable_types"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did lbj marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>"], "scores": [-0.02639496512711048, -0.0406169667840004, -0.04120670258998871, -0.0460701622068882, -0.04840889945626259, -0.04981660842895508, -0.05098506808280945, -0.05187170207500458, -0.05449806526303291, -0.05501823499798775], "norm_scores": [0.10202629119157791, 0.10058554261922836, 0.10052623599767685, 0.10003852844238281, 0.0998048335313797, 0.09966443479061127, 0.09954804182052612, 0.09945981949567795, 0.09919894486665726, 0.09914735704660416]}}
{"id": "WebQTest-1599", "question": "where did kim jong il die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people"], ["people.place_of_death.people_deceased_here"], ["people.place_of_death.people", "people.place_of_death.location"], ["location.deceased_person.place_of_death"], ["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim jong il die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_death </PATH>"], "scores": [-0.00036983840982429683, -0.0961492508649826, -0.09663654118776321, -0.10952223092317581, -0.11073338985443115, -0.11600583046674728, -0.11654198169708252, -0.12016186118125916, -0.12032785266637802, -0.12118794769048691], "norm_scores": [0.1104927510023117, 0.10040083527565002, 0.10035192221403122, 0.09906711429357529, 0.09894721210002899, 0.09842688590288162, 0.0983741283416748, 0.09801866114139557, 0.09800239652395248, 0.09791813790798187]}}
{"id": "WebQTest-1603", "question": "what is the sacred text of daoism", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"], ["location.statistical_region.religions", "location.religion_text.religious_text_of"], ["religion.religion.sacred_sites"], ["religion.religion.religious_texts"], ["common.topic.notable_types"], ["location.statistical_region.religions", "location.religion_text.religion_text_of"], ["religion.religion.sacred_texts"], ["religion.belief.belief_of"], ["religion.religions.texts"]], "ground_paths": [["religion.religion.texts"], ["book.written_work.subjects"], ["religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the sacred text of daoism [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_text.religious_text_of </PATH>", "<PATH> religion.religion.sacred_sites </PATH>", "<PATH> religion.religion.religious_texts </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_text.religion_text_of </PATH>", "<PATH> religion.religion.sacred_texts </PATH>", "<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religions.texts </PATH>"], "scores": [-0.009298587217926979, -0.014652595855295658, -0.11456916481256485, -0.11517894268035889, -0.12627600133419037, -0.13219931721687317, -0.1352902501821518, -0.14236097037792206, -0.14289842545986176, -0.1437080055475235], "norm_scores": [0.1101996973156929, 0.10961125791072845, 0.09918864816427231, 0.09912817925214767, 0.09803423285484314, 0.09745525568723679, 0.09715449810028076, 0.09646996855735779, 0.09641813486814499, 0.096340112388134]}}
{"id": "WebQTest-1604", "question": "what countries speak english natively", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["base.rosetta.local_name.languoid"], ["languages.human_language.countries_spoken_in"], ["base.rosetta.local_name.locale"], ["location.country.language_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak english natively [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.local_name.languoid </PATH>", "<PATH> languages.human_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> location.country.language_spoken </PATH>"], "scores": [-0.016523348167538643, -0.021450765430927277, -0.0251688864082098, -0.07702747732400894, -0.15382836759090424, -0.15632683038711548, -0.16067412495613098, -0.17249487340450287, -0.17701373994350433, -0.18549101054668427], "norm_scores": [0.11005213856697083, 0.10951119661331177, 0.10910477489233017, 0.1035909578204155, 0.09593292325735092, 0.09569353610277176, 0.09527843445539474, 0.09415879845619202, 0.0937342569231987, 0.09294302016496658]}}
{"id": "WebQTest-1607", "question": "who won fedex cup 2012", "prediction": [["award.competition.winner", "people.person.profession"], ["award.competition.winner", "people.person.gender", "people.person.gender"], ["award.competitor.competitions", "people.person.profession"], ["award.competitor.competitions", "people.person.gender", "people.person.gender"], ["award.competition.winner"], ["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["sports.sports_award.season", "sports.sports_award_winner.awards"], ["sports.sports_award.season", "sports.sports_award.award_winner"], ["sports.sports_award_type.winners", "sports.sports_award_winner.awards"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fedex cup 2012 [/INST]", "raw_output": {"paths": ["<PATH> award.competition.winner <SEP> people.person.profession </PATH>", "<PATH> award.competition.winner <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> award.competitor.competitions <SEP> people.person.profession </PATH>", "<PATH> award.competitor.competitions <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> award.competition.winner </PATH>", "<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_award.season <SEP> sports.sports_award_winner.awards </PATH>", "<PATH> sports.sports_award.season <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_award_type.winners <SEP> sports.sports_award_winner.awards </PATH>"], "scores": [-0.017956698313355446, -0.027843032032251358, -0.029737843200564384, -0.040481749922037125, -0.04375986009836197, -0.05496224761009216, -0.06155300885438919, -0.06778077036142349, -0.06878289580345154, -0.06962686032056808], "norm_scores": [0.1030588150024414, 0.1020449548959732, 0.10185178369283676, 0.10076335817575455, 0.1004335880279541, 0.09931476414203644, 0.0986623615026474, 0.0980498269200325, 0.09795161336660385, 0.09786897897720337]}}
{"id": "WebQTest-1608", "question": "what language is mainly spoken in egypt", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["base.culturalevent.event.entity_involved", "location.location.events", "language.human_language.countries_spoken_in"], ["base.culturalevent.event.entity_involved", "location.location.events", "language.human_language.main_country"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["base.culturalevent.event.entity_involved", "location.location.events", "location.country.languages_spoken"], ["base.culturalevent.event.entity_involved", "location.location.events", "location.country.official_language"], ["base.militaryinfiction.location_in_fiction.languages"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is mainly spoken in egypt [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> location.country.languages_spoken </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> location.country.official_language </PATH>", "<PATH> base.militaryinfiction.location_in_fiction.languages </PATH>"], "scores": [-0.022689715027809143, -0.02355593629181385, -0.02659563161432743, -0.02945111319422722, -0.1106327548623085, -0.1196586862206459, -0.12205059826374054, -0.1225539892911911, -0.1269555240869522, -0.1291753351688385], "norm_scores": [0.10613182932138443, 0.10603994131088257, 0.10571809858083725, 0.10541664808988571, 0.09719691425561905, 0.09632357209920883, 0.09609344601631165, 0.09604508429765701, 0.09562326967716217, 0.09541123360395432]}}
{"id": "WebQTest-1609", "question": "who played alf on tv show", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program_writer_relationship.tv_program", "tv.tv_writer.tv_programs"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played alf on tv show [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program_writer_relationship.tv_program <SEP> tv.tv_writer.tv_programs </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.01766175776720047, -0.020786253735423088, -0.047567564994096756, -0.047987986356019974, -0.0485006608068943, -0.05752091482281685, -0.058371853083372116, -0.0609099455177784, -0.061524052172899246, -0.06444022059440613], "norm_scores": [0.10312182456254959, 0.10280011594295502, 0.10008353739976883, 0.1000414714217186, 0.09999018907546997, 0.09909231215715408, 0.09900803118944168, 0.0987570509314537, 0.09869642555713654, 0.09840903431177139]}}
{"id": "WebQTest-1610", "question": "who was gerald ford vp", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.politician.government_positions_held", "government.politician.government_positions_held"], ["government.political_appointer.appointees", "government.politician.government_positions_held"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["government.politician.government_positions_held", "government.government_position_held.office_holder"], ["people.person.profession", "common.topic.notable_types"], ["people.person.gender", "people.person.gender"], ["influence.influence_node.influenced_by", "government.us_vice_president.to_president"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was gerald ford vp [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.politician.government_positions_held </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_holder </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> government.us_vice_president.to_president </PATH>"], "scores": [-0.005344250239431858, -0.02502722479403019, -0.07773168385028839, -0.08297806978225708, -0.09069985896348953, -0.09276620298624039, -0.09302086383104324, -0.11755138635635376, -0.12243969738483429, -0.1276133507490158], "norm_scores": [0.10805220901966095, 0.10594621300697327, 0.1005069687962532, 0.09998104721307755, 0.09921199083328247, 0.09900719672441483, 0.09898199141025543, 0.09658344835042953, 0.09611247479915619, 0.09561649709939957]}}
{"id": "WebQTest-1611", "question": "what is armenian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["common.topic.notable_types", "common.topic.notable_types"], ["language.human_language.countries_spoken_in", "location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_used", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is armenian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>"], "scores": [-0.00662654172629118, -0.02594579942524433, -0.07909294217824936, -0.09272336959838867, -0.13923761248588562, -0.14114691317081451, -0.14970095455646515, -0.1621129810810089, -0.169138565659523, -0.17218436300754547], "norm_scores": [0.11112932860851288, 0.1090029925107956, 0.10336106270551682, 0.10196175426244736, 0.09732769429683685, 0.0971420481801033, 0.09631463140249252, 0.09512656182050705, 0.09446058422327042, 0.09417331218719482]}}
{"id": "WebQTest-1612", "question": "what type of art did raphael sanzio create", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"], ["influence.influence_node.influenced_by", "visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.visual_artists.art_forms"], ["art.visual_artist.art_forms"], ["influence.influence_node.influenced", "visual_art.visual_artist.associated_periods_or_movements"], ["art.art_period_movement.associated_artists"], ["visual_art.art_period_movement.associated_artists", "visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did raphael sanzio create [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_artists.art_forms </PATH>", "<PATH> art.visual_artist.art_forms </PATH>", "<PATH> influence.influence_node.influenced <SEP> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.art_period_movement.associated_artists <SEP> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.0046220035292208195, -0.022530166432261467, -0.058808207511901855, -0.1022198423743248, -0.11582159996032715, -0.12883247435092926, -0.13148847222328186, -0.1329861581325531, -0.13560059666633606, -0.13782863318920135], "norm_scores": [0.10956047475337982, 0.10761591792106628, 0.10378178209066391, 0.09937284141778946, 0.09803035110235214, 0.09676314145326614, 0.09650648385286331, 0.09636205434799194, 0.09611044824123383, 0.09589655697345734]}}
{"id": "WebQTest-1613", "question": "what language do they speak in malta", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["finance.currency.countries_formerly_used"], ["film.film.featured_film_locations"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in malta [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "</PATH> location.country.official_language </PATH>"], "scores": [-0.018240783363580704, -0.021638456732034683, -0.027162322774529457, -0.0420488603413105, -0.1678549349308014, -0.19580332934856415, -0.19580990076065063, -0.19815148413181305, -0.20084400475025177, -0.21233341097831726], "norm_scores": [0.11121149361133575, 0.1108342707157135, 0.11022371798753738, 0.10859503597021103, 0.09575755894184113, 0.0931183397769928, 0.09311772882938385, 0.09289994090795517, 0.09265013784170151, 0.0915917381644249]}}
{"id": "WebQTest-1614", "question": "what religion was king louis xiv", "prediction": [["people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["religion.religion.notable_figures"], ["royalty.kingdom.rulers", "people.person.religion"], ["people.person.nationality", "people.person.religion"], ["royalty.noble_title_tenure.noble_person", "people.person.religion"], ["influence.influence_node.influenced_by", "people.person.religion"], ["royalty.noble_title_tenure.noble_person", "royalty.noble_title.holders"], ["symbols.namesake.named_after"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion was king louis xiv [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> royalty.noble_title_tenure.noble_person <SEP> people.person.religion </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.religion </PATH>", "<PATH> royalty.noble_title_tenure.noble_person <SEP> royalty.noble_title.holders </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0017375980969518423, -0.0528222918510437, -0.06590894609689713, -0.07178032398223877, -0.08892285078763962, -0.09897360950708389, -0.1026248186826706, -0.1033807173371315, -0.10430173575878143, -0.11545956879854202], "norm_scores": [0.10814717411994934, 0.10276125371456146, 0.10142520815134048, 0.10083144903182983, 0.09911768138408661, 0.09812646359205246, 0.09776882827281952, 0.09769495576620102, 0.09760501980781555, 0.0965220108628273]}}
{"id": "WebQTest-1615", "question": "what football team does mike wright play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_coach.sports_coached", "sports.sports_team.sport"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.sport.team_coaches", "sports.sports_team.sport"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team does mike wright play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team.sport </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> sports.sport.team_coaches <SEP> sports.sports_team.sport </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.01560436375439167, -0.016584692522883415, -0.030399605631828308, -0.053663741797208786, -0.05510503053665161, -0.05580757185816765, -0.05654079094529152, -0.056661851704120636, -0.058452825993299484, -0.06167786195874214], "norm_scores": [0.10307636111974716, 0.10297536849975586, 0.10156255215406418, 0.09922704845666885, 0.09908414632081985, 0.09901455044746399, 0.09894198179244995, 0.0989300087094307, 0.09875298291444778, 0.09843501448631287]}}
{"id": "WebQTest-1616", "question": "what did shannon hoon die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["people.people.deceased_person.cause_of_death"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["people.marriage.spouse", "people.marriage.cause_of_death"], ["common.topic.notable_types"], ["people.people.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shannon hoon die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.cause_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.people.cause_of_death </PATH>"], "scores": [-0.0024608939420431852, -0.03612837567925453, -0.12236319482326508, -0.12511765956878662, -0.1338612586259842, -0.14520017802715302, -0.15150955319404602, -0.15675778687000275, -0.15771108865737915, -0.1597224324941635], "norm_scores": [0.11221405118703842, 0.10849897563457489, 0.09953466057777405, 0.09926087409257889, 0.09839675575494766, 0.09728734940290451, 0.09667545557022095, 0.09616940468549728, 0.09607777744531631, 0.09588472545146942]}}
{"id": "WebQTest-1617", "question": "what did nicholas copernicus discover", "prediction": [["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"], ["base.argumentmaps.innovator.original_ideas"], ["astronomy.astronomer.astronomical_objects_discovered"], ["symbols.name_source.namesakes"], ["astronomy.astronomical_discovery.discoverer"], ["common.topic.notable_types"], ["base.nobelprizes.nobel_prize_winner.nobel_honor"], ["people.person.profession"], ["base.argumentmaps.original_idea.first_appears_in_work"]], "ground_paths": [["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicholas copernicus discover [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> astronomy.astronomer.astronomical_objects_discovered </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> astronomy.astronomical_discovery.discoverer </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.nobelprizes.nobel_prize_winner.nobel_honor </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> base.argumentmaps.original_idea.first_appears_in_work </PATH>"], "scores": [-0.01559657882899046, -0.023002387955784798, -0.03550103306770325, -0.0446212962269783, -0.05523958429694176, -0.07880102843046188, -0.0813002958893776, -0.10601639002561569, -0.1110105961561203, -0.12267594039440155], "norm_scores": [0.10524546355009079, 0.10446891188621521, 0.10317131876945496, 0.10223464667797089, 0.10115483403205872, 0.09879934042692184, 0.09855271875858307, 0.09614674001932144, 0.0956677570939064, 0.09455825388431549]}}
{"id": "WebQTest-1618", "question": "what teams has manny ramirez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["freebase.domain_profile.featured_topic", "sports.sports_team.sport"], ["sports.sports_team_coach.teams_coached", "sports.sports_team.coaches"], ["sports.sports_team_coach_tenure.coach", "sports.sports_team.coaches"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_team_coach.sports_coached", "sports.sports_team.sport"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_coach.teams_coached", "sports.sports_team.coaches"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what teams has manny ramirez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> freebase.domain_profile.featured_topic <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.sports_team_coach.teams_coached <SEP> sports.sports_team.coaches </PATH>", "<PATH> sports.sports_team_coach_tenure.coach <SEP> sports.sports_team.coaches </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.018610743805766106, -0.021770870313048363, -0.02190198376774788, -0.023839281871914864, -0.05055929347872734, -0.05331893637776375, -0.06215241178870201, -0.06686632335186005, -0.07045406848192215, -0.0732591450214386], "norm_scores": [0.10278161615133286, 0.10245732218027115, 0.1024438887834549, 0.10224562138319016, 0.0995497927069664, 0.09927545487880707, 0.09840236604213715, 0.09793959558010101, 0.09758883714675903, 0.0973154827952385]}}
{"id": "WebQTest-1619", "question": "where do the appalachian mountains run", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.mountain_range.mountains"], ["geography.river.basin_countries"], ["location.location.containedby"], ["geography.mountain_pass.range"], ["location.location.partial_containedby"], ["geography.mountain.mountain_range"], ["location.location.partial_contains"], ["geography.mountain_range.passes"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.mountain_pass.range </PATH>", "<PATH> location.location.partial_containedby </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> location.location.partial_contains </PATH>", "<PATH> geography.mountain_range.passes </PATH>"], "scores": [-0.008314982987940311, -0.020486103370785713, -0.08190038055181503, -0.09477836638689041, -0.09718790650367737, -0.09876453876495361, -0.11345258355140686, -0.11365723609924316, -0.11908495426177979, -0.1287168711423874], "norm_scores": [0.1081719920039177, 0.10686339437961578, 0.10049791634082794, 0.09921201318502426, 0.09897324442863464, 0.09881732612848282, 0.0973764955997467, 0.09735656529664993, 0.09682957828044891, 0.09590139985084534]}}
{"id": "WebQTest-1620", "question": "when was president wilson in office", "prediction": [["book.written_work.subjects", "base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "location.location.events"], ["government.government_position_held.office_holder", "government.political_district.representatives", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.government_position_held.office_holder", "government.government_position_held.office_term"], ["book.written_work.subjects", "location.location.events"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.political_district.representatives", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was president wilson in office [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location.events </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.political_district.representatives <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_term </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.events </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.political_district.representatives <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> location.location.events </PATH>"], "scores": [-0.031387653201818466, -0.033256929367780685, -0.03354176878929138, -0.040967363864183426, -0.04143519699573517, -0.043751027435064316, -0.04611488804221153, -0.04695675149559975, -0.051999881863594055, -0.05337733402848244], "norm_scores": [0.1010923758149147, 0.10090357810258865, 0.10087484121322632, 0.10012856125831604, 0.10008172690868378, 0.09985022246837616, 0.09961447864770889, 0.0995306447148323, 0.09902995824813843, 0.0988936498761177]}}
{"id": "WebQTest-1621", "question": "what political party is jerry brown", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.party", "government.political_party.politicians_in_this_party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["base.politicalconventions.presidential_nominee.nominated_at", "base.politicalconventions.political_convention.party"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party is jerry brown [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.politician.party <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> government.politician.election_campaigns <SEP> government.election_campaign.party </PATH>", "<PATH> base.politicalconventions.presidential_nominee.nominated_at <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.013719865120947361, -0.01862363889813423, -0.027369746938347816, -0.038681838661432266, -0.049630600959062576, -0.06182871386408806, -0.06432587653398514, -0.07188604772090912, -0.07900459319353104, -0.08156589418649673], "norm_scores": [0.10373437404632568, 0.1032269224524498, 0.10232803225517273, 0.10117700695991516, 0.10007529705762863, 0.09886197745800018, 0.09861541539430618, 0.09787267446517944, 0.09717843681573868, 0.09692984819412231]}}
{"id": "WebQTest-1622", "question": "what ocean is around hawaii", "prediction": [["location.location.contains", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "location.location.contains"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["geography.island.island_group"], ["geography.island.island_group", "location.location.containedby"], ["location.location.contains"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["geography.island.island_group", "location.location.contains"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what ocean is around hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.contains </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.adjoining_relationship.adjoins </PATH>", "<PATH> geography.island.island_group </PATH>", "<PATH> geography.island.island_group <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.location.adjoin_s </PATH>", "<PATH> geography.island.island_group <SEP> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.027380308136343956, -0.028970366343855858, -0.03931773826479912, -0.04253716021776199, -0.04331132024526596, -0.045144181698560715, -0.04759971797466278, -0.05482316017150879, -0.06738071888685226, -0.06933542340993881], "norm_scores": [0.10192946344614029, 0.10176751762628555, 0.10071992129087448, 0.10039617866277695, 0.10031848400831223, 0.10013478994369507, 0.09988920390605927, 0.0991702601313591, 0.09793271124362946, 0.09774146229028702]}}
{"id": "WebQTest-1623", "question": "what is the most common language spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.language_family.geographic_distribution"], ["location.country.currency_formerly_used"], ["language.human_language.language_family"], ["location.country.language_spoken"], ["location.country.main_language"], ["location.country.currency_used"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> location.country.language_spoken </PATH>", "<PATH> location.country.main_language </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.007309342734515667, -0.027959998697042465, -0.045580942183732986, -0.04863593354821205, -0.1256381720304489, -0.1386183649301529, -0.17365996539592743, -0.17725585401058197, -0.18268965184688568, -0.185344859957695], "norm_scores": [0.11070043593645096, 0.10843783617019653, 0.10654380172491074, 0.10621881484985352, 0.09834669530391693, 0.09707839041948318, 0.09373551607131958, 0.0933990553021431, 0.09289292246103287, 0.09264660626649857]}}
{"id": "WebQTest-1624", "question": "what language tunisia", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["language.language_family.geographic_distribution"], ["country.languages_spoken.countries_spoken_in"], ["film.film.language"], ["language.human_language.language_family"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language tunisia [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>", "<PATH> country.languages_spoken.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> film.film.language </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.016961315646767616, -0.01698983460664749, -0.04555802792310715, -0.04889257252216339, -0.14605386555194855, -0.1875334531068802, -0.19285373389720917, -0.1986374408006668, -0.19885437190532684, -0.2028830498456955], "norm_scores": [0.11112300306558609, 0.1111198365688324, 0.10799025744199753, 0.10763075947761536, 0.09766519069671631, 0.0936969444155693, 0.09319977462291718, 0.09266228973865509, 0.09264218807220459, 0.09226971864700317]}}
{"id": "WebQTest-1625", "question": "what are some inventions that leonardo da vinci invented", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"], ["influence.influence_node.influenced_by"], ["law.inventor.us_patents"], ["influence.influence_node.influenced"], ["film.film.subjects"], ["engineering.inventor.inventions"]], "ground_paths": [["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some inventions that leonardo da vinci invented [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> engineering.inventor.inventions </PATH>"], "scores": [-0.007108320947736502, -0.0188688263297081, -0.07571659982204437, -0.08525974303483963, -0.09849361330270767, -0.12459857761859894, -0.1281537413597107, -0.14553628861904144, -0.15769360959529877, -0.1624310314655304], "norm_scores": [0.10962864011526108, 0.10834690928459167, 0.10235942155122757, 0.10138723254203796, 0.10005432367324829, 0.09747621417045593, 0.09713029116392136, 0.09545649588108063, 0.09430302679538727, 0.09385734051465988]}}
{"id": "WebQTest-1626", "question": "who wrote st trinians", "prediction": [["film.film.written_by"], ["fictional_universe.fictional_universe.locations"], ["film.film_story_contributor.film_story_credits"], ["film.film.directed_by"], ["film.film.produced_by"], ["film.film.story_by"], ["film.film.executive_produced_by"], ["fictional_universe.fictional_universe.works_set_here"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["fictional_universe.fictional_character.character_created_by"]], "ground_paths": [["fictional_universe.fictional_universe.created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote st trinians [/INST]", "raw_output": {"paths": ["<PATH> film.film.written_by </PATH>", "<PATH> fictional_universe.fictional_universe.locations </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> film.film.executive_produced_by </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>"], "scores": [-0.022302426397800446, -0.034807149320840836, -0.04298608377575874, -0.05464715510606766, -0.058633953332901, -0.0598071813583374, -0.0712878629565239, -0.071710504591465, -0.07217736542224884, -0.07361146062612534], "norm_scores": [0.10343287140130997, 0.10214752703905106, 0.10131547600030899, 0.10014089196920395, 0.09974244236946106, 0.09962549805641174, 0.09848826378583908, 0.09844664484262466, 0.09840069711208344, 0.09825967997312546]}}
{"id": "WebQTest-1627", "question": "what are malaysian coins called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["location.country.coins_used"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["common.topic.notable_types"], ["location.country.currency_used", "location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in", "location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are malaysian coins called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.coins_used </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.currency_used </PATH>"], "scores": [-0.006657336372882128, -0.02698626182973385, -0.063897043466568, -0.07828516513109207, -0.09619047492742538, -0.10376110672950745, -0.10895755141973495, -0.1141018196940422, -0.12244442105293274, -0.12373030930757523], "norm_scores": [0.1080145612359047, 0.10584090650081635, 0.10200545936822891, 0.10054830461740494, 0.09876397997140884, 0.09801909327507019, 0.09751106798648834, 0.09701073169708252, 0.09620478004217148, 0.09608114510774612]}}
{"id": "WebQTest-1628", "question": "what type government does mexico have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["film.film_location.featured_in_films", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["film.film.featured_film_locations", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["film.film_location.featured_in_films", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type government does mexico have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> film.film_location.featured_in_films <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> film.film.featured_film_locations <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> film.film_location.featured_in_films <SEP> location.country.form_of_government </PATH>"], "scores": [-0.005958814173936844, -0.02247898280620575, -0.14112216234207153, -0.14281724393367767, -0.14324325323104858, -0.14381660521030426, -0.15105657279491425, -0.1527405083179474, -0.1570311188697815, -0.16197767853736877], "norm_scores": [0.11215822398662567, 0.1103205680847168, 0.09797842055559158, 0.0978124812245369, 0.0977708250284195, 0.09771478176116943, 0.09700988233089447, 0.09684666246175766, 0.09643202275037766, 0.09595619142055511]}}
{"id": "WebQTest-1629", "question": "what currency does dominican republic take", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"], ["travel.tourist_attraction.near_travel_destination"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["travel.tourist_attraction.near_travel_destination", "location.country.currency_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["location.country.languages_spoken"], ["travel.tourist_attraction.near_travel_destination", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does dominican republic take [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.0004488307167775929, -0.07322879135608673, -0.13398052752017975, -0.14529867470264435, -0.152043417096138, -0.15876133739948273, -0.16530703008174896, -0.17617540061473846, -0.17981365323066711, -0.18642055988311768], "norm_scores": [0.11447101086378098, 0.10643576830625534, 0.10016211122274399, 0.0990348532795906, 0.09836912900209427, 0.0977105051279068, 0.0970730111002922, 0.09602370113134384, 0.09567497670650482, 0.09504494816064835]}}
{"id": "WebQTest-1630", "question": "what inspired michael jackson to become a singer", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.profession"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["music.artist.genre"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["people.person.gender", "people.person.gender"], ["people.person.nationality", "people.person.nationality"], ["base.danceporn.video.dancer"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michael jackson to become a singer [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.danceporn.video.dancer </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.006169929169118404, -0.019963424652814865, -0.09112878888845444, -0.10086248815059662, -0.1036132425069809, -0.10386444628238678, -0.1132548600435257, -0.13485699892044067, -0.13761845231056213, -0.14373894035816193], "norm_scores": [0.10923492908477783, 0.10773853957653046, 0.10033774375915527, 0.09936583042144775, 0.09909287840127945, 0.09906798601150513, 0.09814205020666122, 0.09604471176862717, 0.09577985107898712, 0.09519542753696442]}}
{"id": "WebQTest-1631", "question": "who is the texas state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "ground_paths": [["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["location.us_state.capital", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"], ["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.political_district.elections", "government.election.winner"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.events", "military.military_person.participated_in_conflicts"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the texas state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.016881311312317848, -0.025765080004930496, -0.028658295050263405, -0.03162334859371185, -0.03641359135508537, -0.03693162649869919, -0.04372114688158035, -0.046814944595098495, -0.047109585255384445, -0.04738185927271843], "norm_scores": [0.10193854570388794, 0.10103696584701538, 0.1007450595498085, 0.10044679045677185, 0.09996677190065384, 0.09991500526666641, 0.09923893213272095, 0.09893237799406052, 0.09890323132276535, 0.09887630492448807]}}
{"id": "WebQTest-1632", "question": "what to visit in london city", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["organization.organization.geographic_scope"], ["metropolitan_transit.transit_system.area_served"], ["transportation.bridge.locale"], ["organization.organization_scope.organizations_with_this_scope"], ["base.arthist.helynevek.intezmeny"], ["film.film_festival.location"], ["film.film.featured_film_locations"]], "ground_paths": [["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["transportation.bridge.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in london city [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> transportation.bridge.locale </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> film.film.featured_film_locations </PATH>"], "scores": [-0.012310437858104706, -0.020327281206846237, -0.031229151412844658, -0.07790906727313995, -0.08598029613494873, -0.0971626490354538, -0.10322027653455734, -0.11040589213371277, -0.1349901705980301, -0.1350376307964325], "norm_scores": [0.10699626058340073, 0.10614191740751266, 0.10499106347560883, 0.10020271688699722, 0.099397212266922, 0.09829190373420715, 0.09769828617572784, 0.09699878841638565, 0.09464321285486221, 0.09463872015476227]}}
{"id": "WebQTest-1633", "question": "what genre of art is the mona lisa", "prediction": [["visual_art.artwork.art_form"], ["visual_art.visual_artist.art_forms"], ["visual_art.art_subject.artwork_on_the_subject"], ["visual_art.artwork.genre"], ["visual_art.artwork.period_or_movement"], ["visual_art.artwork.artist"], ["visual_art.visual_artist.art_forms", "visual_art.visual_artist.art_forms"], ["visual_art.art_subject.artworks"], ["visual_art.artwork.art_form", "visual_art.visual_artist.art_forms"], ["visual_art.art_subject.artwork_on_the_subject", "visual_art.art_subject.artwork_on_the_subject"]], "ground_paths": [["visual_art.artwork.art_genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what genre of art is the mona lisa [/INST]", "raw_output": {"paths": ["<PATH> visual_art.artwork.art_form </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_subject.artwork_on_the_subject </PATH>", "<PATH> visual_art.artwork.genre </PATH>", "<PATH> visual_art.artwork.period_or_movement </PATH>", "<PATH> visual_art.artwork.artist </PATH>", "<PATH> visual_art.visual_artist.art_forms <SEP> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_subject.artworks </PATH>", "<PATH> visual_art.artwork.art_form <SEP> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_subject.artwork_on_the_subject <SEP> visual_art.art_subject.artwork_on_the_subject </PATH>"], "scores": [-0.018750939518213272, -0.028398094698786736, -0.03941231593489647, -0.042054206132888794, -0.049106914550065994, -0.05313718318939209, -0.05697274208068848, -0.059095654636621475, -0.06145070120692253, -0.06861506402492523], "norm_scores": [0.1029258668422699, 0.10193770378828049, 0.1008211001753807, 0.10055508464574814, 0.09984839707612991, 0.09944678843021393, 0.09906609356403351, 0.09885600954294205, 0.09862346947193146, 0.09791941940784454]}}
{"id": "WebQTest-1634", "question": "who does peter griffin voice", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.character_created_by", "influence.influence_node.influenced_by"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peter griffin voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.01774996891617775, -0.033386923372745514, -0.03349848464131355, -0.03740482032299042, -0.03927857056260109, -0.041334118694067, -0.05316736176609993, -0.053933318704366684, -0.059177689254283905, -0.059449225664138794], "norm_scores": [0.10253220796585083, 0.10094138979911804, 0.1009301245212555, 0.10053662955760956, 0.10034842789173126, 0.10014237463474274, 0.09896434098482132, 0.0988885760307312, 0.09837131947278976, 0.09834461659193039]}}
{"id": "WebQTest-1635", "question": "when did tony romo got drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"], ["sports.sports_league_draft.picks", "sports.sports_league_draft_pick.draft"], ["sports.sports_league_draft.picks", "sports.sports_league_draft.picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft.picks"], ["sports.pro_athlete.teams", "sports.sports_league_draft_pick.draft"], ["sports.pro_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.draft"], ["sports.pro_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did tony romo got drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_league_draft.picks <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_league_draft.picks <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.pro_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.pro_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.012781307101249695, -0.01491736900061369, -0.02610884979367256, -0.03528531640768051, -0.0562053881585598, -0.05974990501999855, -0.06910005211830139, -0.07118776440620422, -0.07152260839939117, -0.07304837554693222], "norm_scores": [0.10365954786539078, 0.1034383624792099, 0.1022871807217598, 0.10135284066200256, 0.0992545634508133, 0.09890337288379669, 0.09798292070627213, 0.0977785736322403, 0.09774584323167801, 0.09759681671857834]}}
{"id": "WebQTest-1637", "question": "what are the catholic holy days", "prediction": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"], ["time.recurring_event.holidays"], ["time.recurring_event.current_frequency"], ["time.recurring_event.instances"], ["base.saints.saint.venerated_in"], ["location.location.events"], ["symbols.namesake.named_after"], ["religion.holiday.religions"], ["common.topic.notable_types"]], "ground_paths": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the catholic holy days [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> time.recurring_event.holidays </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> base.saints.saint.venerated_in </PATH>", "<PATH> location.location.events </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> religion.holiday.religions </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.011512177996337414, -0.01450249832123518, -0.09229887276887894, -0.09505852311849594, -0.11988592892885208, -0.12250953912734985, -0.12727774679660797, -0.1337791532278061, -0.13588115572929382, -0.13866163790225983], "norm_scores": [0.10904247313737869, 0.10871688276529312, 0.10057973861694336, 0.100302554666996, 0.09784296154975891, 0.09758660197257996, 0.09712238609790802, 0.09649300575256348, 0.09629038721323013, 0.09602303057909012]}}
{"id": "WebQTest-1638", "question": "what language is most commonly spoken in belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["country.languages_spoken.countries_spoken_in"], ["base.rosetta.languoid.languoid_class"], ["book.newspaper.circulation_areas"], ["book.newspaper.circulation_areas", "book.periodical.language"], ["country.languages_spoken"], ["people.person.nationality", "people.person.languages"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is most commonly spoken in belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> country.languages_spoken.countries_spoken_in </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.language </PATH>", "<PATH> country.languages_spoken </PATH>", "<PATH> people.person.nationality <SEP> people.person.languages </PATH>"], "scores": [-0.011136836372315884, -0.01947680115699768, -0.035487331449985504, -0.08308883011341095, -0.14213982224464417, -0.14856557548046112, -0.15030255913734436, -0.1627846509218216, -0.16612260043621063, -0.17057400941848755], "norm_scores": [0.11006607860326767, 0.10915195196866989, 0.10741828382015228, 0.10242480039596558, 0.09655163437128067, 0.0959332063794136, 0.09576672315597534, 0.09457877278327942, 0.09426359832286835, 0.0938449278473854]}}
{"id": "WebQTest-1639", "question": "where did frank baum die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"], ["people.place_of_death.person", "people.place_of_death.location"], ["people.place_of_death.people", "people.place_of_death.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.place_lived.person", "people.place_lived.location"], ["people.place_of_death.people_deceased_person"], ["people.place_of_death.people_deceased_person", "base.biblioness.bibs_location.state"], ["people.place_of_death.people_deceased_person", "base.biblioness.bibs_location.city"], ["people.deceased_person.place_of_death", "base.biblioness.bibs_location.state"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did frank baum die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_of_death.person <SEP> people.place_of_death.location </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.place_of_death.people_deceased_person </PATH>", "<PATH> people.place_of_death.people_deceased_person <SEP> base.biblioness.bibs_location.state </PATH>", "<PATH> people.place_of_death.people_deceased_person <SEP> base.biblioness.bibs_location.city </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.0003427203628234565, -0.10139618813991547, -0.10618861764669418, -0.10774711519479752, -0.10993900150060654, -0.11338302493095398, -0.11740336567163467, -0.11744015663862228, -0.11956202238798141, -0.1216655820608139], "norm_scores": [0.11057895421981812, 0.0999506264925003, 0.0994727611541748, 0.09931786358356476, 0.09910040348768234, 0.09875968843698502, 0.09836343675851822, 0.09835981577634811, 0.09815134108066559, 0.09794508665800095]}}
{"id": "WebQTest-1640", "question": "where is tunis on a map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.country.capital"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.country"], ["location.location.containedby"], [], ["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tunis on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.013778229244053364, -0.035773877054452896, -0.03879101201891899, -0.043486904352903366, -0.051535289734601974, -0.05873237922787666, -0.06659536063671112, -0.09709008038043976, -0.09914536029100418, -0.09975224733352661], "norm_scores": [0.10473760217428207, 0.10245898365974426, 0.1021503135561943, 0.10167175531387329, 0.10085675120353699, 0.10013347119092941, 0.09934921562671661, 0.0963653177022934, 0.09616746008396149, 0.09610912203788757]}}
{"id": "WebQTest-1641", "question": "who is nicolas cage married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.parents"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nicolas cage married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.01738668419420719, -0.031628645956516266, -0.042323511093854904, -0.04316781833767891, -0.049775850027799606, -0.05024431273341179, -0.051105204969644547, -0.055685676634311676, -0.05656679719686508, -0.05741916596889496], "norm_scores": [0.1028469130396843, 0.10139255225658417, 0.1003139466047287, 0.10022928565740585, 0.0995691642165184, 0.09952251613140106, 0.09943687915802002, 0.09898245334625244, 0.09889528155326843, 0.09881102293729782]}}
{"id": "WebQTest-1643", "question": "who does the voice for chucky in child 's play", "prediction": [["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film.country", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.film", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["film.performance.film", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["film.performance.film", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for chucky in child 's play [/INST]", "raw_output": {"paths": ["<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.029536254703998566, -0.02995879016816616, -0.030755145475268364, -0.03234529867768288, -0.04095124080777168, -0.04615749046206474, -0.04840388894081116, -0.05146065726876259, -0.05280866473913193, -0.05374693125486374], "norm_scores": [0.10121026635169983, 0.10116751492023468, 0.10108697414398193, 0.10092636197805405, 0.10006152093410492, 0.09954192489385605, 0.09931857138872147, 0.09901543706655502, 0.09888205677270889, 0.09878931939601898]}}
{"id": "WebQTest-1644", "question": "what did mary wollstonecraft fight for", "prediction": [["people.person.profession"], ["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"], ["influence.influence_node.influenced_by", "base.activism.activism_issue.activists"], ["book.written_work.author"], ["influence.influence_node.influenced_by", "base.activism.activist.area_of_activism"], ["influence.influence_node.influenced"], ["book.author.works_written"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by", "organization.organization.founders"]], "ground_paths": [["base.activism.activist.area_of_activism"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mary wollstonecraft fight for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> base.activism.activism_issue.activists </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> base.activism.activist.area_of_activism </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> organization.organization.founders </PATH>"], "scores": [-0.018416177481412888, -0.022043900564312935, -0.04269515350461006, -0.05278154835104942, -0.05495965853333473, -0.056516874581575394, -0.07185542583465576, -0.07372896373271942, -0.07755233347415924, -0.08220597356557846], "norm_scores": [0.10373146831989288, 0.10335583984851837, 0.10124330967664719, 0.10022725909948349, 0.10000919550657272, 0.09985356777906418, 0.09833364933729172, 0.09814959019422531, 0.09777504950761795, 0.09732109308242798]}}
{"id": "WebQTest-1646", "question": "what year did the phillies begin", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_team.previously_known"], ["sports.sports_team.previously_known_as"], ["sports.sports_team_roster.team"], ["sports.sports_team.prequel"], ["sports.sports_team_championship_event.champion"], ["sports.professional_sports_team.owner_s"], ["sports.sports_team.later_known_as"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the phillies begin [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.previously_known </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>", "<PATH> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team.prequel </PATH>", "<PATH> sports.sports_team_championship_event.champion </PATH>", "<PATH> sports.professional_sports_team.owner_s </PATH>", "<PATH> sports.sports_team.later_known_as </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.003972324077039957, -0.03527584671974182, -0.05905286222696304, -0.0741366595029831, -0.09272869676351547, -0.11283939331769943, -0.11495921015739441, -0.11534997075796127, -0.11614855378866196, -0.12868273258209229], "norm_scores": [0.10839005559682846, 0.10504961758852005, 0.10258131474256516, 0.10104560852050781, 0.09918432682752609, 0.09720958024263382, 0.09700373560190201, 0.09696583449840546, 0.09688843041658401, 0.0956815853714943]}}
{"id": "WebQTest-1648", "question": "what language does iceland speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["language.human_language.language_family"], ["book.written_work.subjects"], ["book.written_work.original_language"], ["language.language_family.languages"], ["language.human_language.region"], ["language.human_language.country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does iceland speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.original_language </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> language.human_language.country </PATH>"], "scores": [-0.014642992988228798, -0.018113428726792336, -0.04709960147738457, -0.049253322184085846, -0.14029935002326965, -0.17061012983322144, -0.17123378813266754, -0.1734749972820282, -0.1766929179430008, -0.1823182851076126], "norm_scores": [0.11022554337978363, 0.1098436713218689, 0.1067054271697998, 0.10647585988044739, 0.0972098708152771, 0.09430757164955139, 0.09424877166748047, 0.09403778612613678, 0.09373566508293152, 0.09320984780788422]}}
{"id": "WebQTest-1649", "question": "what was dr seuss real name", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by", "people.person.nationality"], ["book.book_edition.author_editor"], ["book.book.interior_illustrations_by"], ["book.written_work.author"], ["book.book.interior_illustrations_by", "people.person.profession"], ["book.book.interior_illustrations_by", "people.person.interred_names"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["book.book_edition.author_editor", "people.person.profession"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss real name [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.book.interior_illustrations_by </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.interred_names </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> book.book_edition.author_editor <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>"], "scores": [-0.02547583542764187, -0.027804866433143616, -0.03848767280578613, -0.045454833656549454, -0.04564487189054489, -0.04758517071604729, -0.0559503510594368, -0.06330651044845581, -0.06785804778337479, -0.07168891280889511], "norm_scores": [0.10236111283302307, 0.10212299227714539, 0.10103783756494522, 0.10033633559942245, 0.10031726956367493, 0.10012280941009521, 0.09928876161575317, 0.09856105595827103, 0.09811347723007202, 0.09773833304643631]}}
{"id": "WebQTest-1650", "question": "what did gloria steinem founded", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.founder.organizations_founded"], ["base.activism.activist.area_of_activism"], ["book.written_work.author"], ["influence.influence_node.influenced_by", "organization.organization.founders"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by", "organization.organization_founder.organizations_founded"], ["common.topic.notable_types"], ["book.author.works_written"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gloria steinem founded [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.founder.organizations_founded </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> organization.organization.founders </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> organization.organization_founder.organizations_founded </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.005600225646048784, -0.02409018948674202, -0.10644922405481339, -0.11473027616739273, -0.11867349594831467, -0.120220847427845, -0.1268385648727417, -0.13320836424827576, -0.1378306895494461, -0.13939975202083588], "norm_scores": [0.11008270829916, 0.10806597769260406, 0.0995224341750145, 0.09870167821645737, 0.09831324219703674, 0.0981612429022789, 0.09751377999782562, 0.09689459949731827, 0.09644776582717896, 0.09629654884338379]}}
{"id": "WebQTest-1652", "question": "what language they speak in scotland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["base.rosetta.languoid.languoid_class"], ["location.country.first_level_divisions"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken <PATH>"], "scores": [-0.01653633639216423, -0.016728518530726433, -0.030901392921805382, -0.07621017843484879, -0.17754852771759033, -0.1776275634765625, -0.17886334657669067, -0.18437038362026215, -0.1896425187587738, -0.20364981889724731], "norm_scores": [0.11115843802690506, 0.11113707721233368, 0.10957305133342743, 0.10471922904253006, 0.09462714195251465, 0.09461966156959534, 0.09450279921293259, 0.09398380666971207, 0.0934896245598793, 0.09218920022249222]}}
{"id": "WebQTest-1653", "question": "what boarding school did mark zuckerberg go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["organization.organization_founder.organizations_founded"], ["common.topic.notable_types"], ["organization.organization.founders"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what boarding school did mark zuckerberg go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.014456071890890598, -0.019910749047994614, -0.025339694693684578, -0.02955426648259163, -0.052560511976480484, -0.09207765758037567, -0.11210846155881882, -0.11382012069225311, -0.11949480324983597, -0.11983001232147217], "norm_scores": [0.10560405999422073, 0.10502959787845612, 0.1044609397649765, 0.1040216013789177, 0.10165578126907349, 0.09771697223186493, 0.09577909857034683, 0.09561529755592346, 0.09507424384355545, 0.095042385160923]}}
{"id": "WebQTest-1654", "question": "what are bigos", "prediction": [["food.dish.cuisine"], ["dining.cuisine.dishes"], ["food.diet_follower.follows_diet"], ["food.diet.dishes"], ["food.diet_type.diet"], ["food.diet_based_dish.diet"], ["food.diet_and_soup.diet"], ["food.diet_dish.dishes_served"], ["food.diet_based_dish.dishes"], ["food.diet_dish.dishes"]], "ground_paths": [["food.type_of_dish.dishes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are bigos [/INST]", "raw_output": {"paths": ["<PATH> food.dish.cuisine </PATH>", "<PATH> dining.cuisine.dishes </PATH>", "<PATH> food.diet_follower.follows_diet </PATH>", "<PATH> food.diet.dishes </PATH>", "<PATH> food.diet_type.diet </PATH>", "<PATH> food.diet_based_dish.diet </PATH>", "<PATH> food.diet_and_soup.diet </PATH>", "<PATH> food.diet_dish.dishes_served </PATH>", "<PATH> food.diet_based_dish.dishes </PATH>", "<PATH> food.diet_dish.dishes </PATH>"], "scores": [-0.032836705446243286, -0.060708098113536835, -0.06515469402074814, -0.07000552862882614, -0.07846096158027649, -0.08138589560985565, -0.08837227523326874, -0.08887476474046707, -0.09334678947925568, -0.0947565957903862], "norm_scores": [0.10433010756969452, 0.10146242380142212, 0.10101226717233658, 0.1005234643816948, 0.09967707842588425, 0.09938595443964005, 0.09869402647018433, 0.0986444503068924, 0.09820428490638733, 0.09806594252586365]}}
{"id": "WebQTest-1655", "question": "where are ike and tina turner 's children", "prediction": [["people.person.children"], ["people.person.parents"], ["music.artist.genre", "music.artist.genre"], ["people.person.place_of_birth"], ["music.artist.origin", "people.person.place_of_birth"], ["music.recording.artist", "music.recording.artist"], ["music.artist.origin"], ["music.artist.label", "music.artist.label"], ["music.artist.album", "music.album.artist"], ["music.album.artist", "people.person.children"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are ike and tina turner 's children [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin <SEP> people.person.place_of_birth </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>", "<PATH> music.artist.album <SEP> music.album.artist </PATH>", "<PATH> music.album.artist <SEP> people.person.children </PATH>"], "scores": [-0.008240614086389542, -0.020026184618473053, -0.08189063519239426, -0.08981247246265411, -0.09751030057668686, -0.10024437308311462, -0.10714996606111526, -0.10941950231790543, -0.11964619904756546, -0.1350158303976059], "norm_scores": [0.1080993264913559, 0.10683278739452362, 0.10042392462491989, 0.09963152557611465, 0.0988675206899643, 0.09859758615493774, 0.0979190543293953, 0.09769707173109055, 0.09670305252075195, 0.09522811323404312]}}
{"id": "WebQTest-1656", "question": "what are some of leonardo da vinci most famous inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"], ["influence.influence_node.influenced_by"], ["law.inventor.us_patents"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "law.inventor.inventions"], ["common.topic.notable_types"]], "ground_paths": [["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some of leonardo da vinci most famous inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> law.inventor.inventions </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.00688915466889739, -0.019439488649368286, -0.0756010189652443, -0.08262930810451508, -0.09490052610635757, -0.10943705588579178, -0.12692272663116455, -0.1295052468776703, -0.14383621513843536, -0.15337832272052765], "norm_scores": [0.10900712758302689, 0.107647605240345, 0.10176858305931091, 0.10105583071708679, 0.09982332587242126, 0.09838274121284485, 0.09667740762233734, 0.09642805904150009, 0.09505601227283478, 0.09415328502655029]}}
{"id": "WebQTest-1657", "question": "when did shaq come into the nba", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"], ["sports.sports_award.award_winner", "sports.sports_league_season.awards"], ["sports.pro_athlete.teams", "sports.sports_league_season.teams"], ["common.topic.image", "common.image.appears_in_topic_gallery"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shaq come into the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_league_season.awards </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_league_season.awards </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_season.teams </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>"], "scores": [-0.01561965886503458, -0.021321440115571022, -0.02164144068956375, -0.03135468438267708, -0.039545346051454544, -0.04892021045088768, -0.055261723697185516, -0.06333082914352417, -0.06634477525949478, -0.06813003122806549], "norm_scores": [0.10277246683835983, 0.10218815505504608, 0.10215545445680618, 0.10116799920797348, 0.1003427505493164, 0.09940645098686218, 0.09877804666757584, 0.0979842022061348, 0.09768933057785034, 0.09751509130001068]}}
{"id": "WebQTest-1658", "question": "where do they speak turkish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["languages.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"], ["people.ethnicity.geographic_distribution"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak turkish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> languages.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.01390758529305458, -0.022168729454278946, -0.028005195781588554, -0.10369505733251572, -0.159586563706398, -0.16775254905223846, -0.1852816641330719, -0.18820737302303314, -0.1910378783941269, -0.19181329011917114], "norm_scores": [0.11147049814462662, 0.11055342108011246, 0.10991005599498749, 0.10189802199602127, 0.096359021961689, 0.09557535499334335, 0.09391460567712784, 0.09364023059606552, 0.093375563621521, 0.09330318868160248]}}
{"id": "WebQTest-1659", "question": "what is the zip code for walnut creek", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"], ["location.us_county.hud_county_place"], ["location.citytown.postal_ codes"], ["location.citytown.postal_code"], ["location.postal_code.citytown"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for walnut creek [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.citytown.postal_ codes </PATH>", "</PATH> location.citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_code </PATH>", "<PATH> location.postal_code.citytown </PATH>"], "scores": [-7.519400969613343e-05, -0.10707017034292221, -0.16075348854064941, -0.17076435685157776, -0.17218449711799622, -0.17570224404335022, -0.1794845163822174, -0.18249712884426117, -0.18701769411563873, -0.19278112053871155], "norm_scores": [0.11631657183170319, 0.10451395064592361, 0.09905123710632324, 0.0980646014213562, 0.09792543202638626, 0.09758155792951584, 0.09721317887306213, 0.09692074358463287, 0.09648361057043076, 0.09592912346124649]}}
{"id": "WebQTest-1661", "question": "who was the voice of darth vader in the original star wars movie", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["film.performance.character", "film.actor.film"], ["film.performance.film", "film.performance.actor"], ["film.performance.special_performance_type", "film.actor.film"], ["film.film.starring", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the voice of darth vader in the original star wars movie [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>"], "scores": [-0.01754690520465374, -0.026522373780608177, -0.029345562681555748, -0.03073696605861187, -0.038873687386512756, -0.04175650328397751, -0.05151306092739105, -0.05319735035300255, -0.05807841196656227, -0.06173001974821091], "norm_scores": [0.10235569626092911, 0.10144112259149551, 0.10115513950586319, 0.10101448744535446, 0.10019589960575104, 0.09990745782852173, 0.09893744438886642, 0.09877095371484756, 0.09829001873731613, 0.09793175756931305]}}
{"id": "WebQTest-1662", "question": "where is english spoken around the world", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["languages.human_language.countries_spoken_in"], ["location.country.form_of_government"], ["common.topic.notable_types"], ["language.human_language.region"], ["book.written_work.subjects"]], "ground_paths": [["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken around the world [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> languages.human_language.countries_spoken_in </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.012621053494513035, -0.025364039465785027, -0.028495950624346733, -0.05898357927799225, -0.14311912655830383, -0.16109170019626617, -0.16620193421840668, -0.18136940896511078, -0.18250301480293274, -0.18470275402069092], "norm_scores": [0.11044847965240479, 0.1090499684214592, 0.10870897024869919, 0.10544469952583313, 0.09693600982427597, 0.09520938992500305, 0.09472408890724182, 0.09329819679260254, 0.09319249540567398, 0.09298771619796753]}}
{"id": "WebQTest-1663", "question": "what music did louis armstrong play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["book.written_work.author"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["music.genre.artist"], ["music.album.genre"], ["music.musical_group.member", "music.group_membership.group"], ["music.musical_group.member"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> music.genre.artist </PATH>", "<PATH> music.album.genre </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.musical_group.member </PATH>"], "scores": [-0.0017086798325181007, -0.04962047562003136, -0.09254063665866852, -0.11853551119565964, -0.13514062762260437, -0.1357216089963913, -0.14219459891319275, -0.146310955286026, -0.15198788046836853, -0.153193399310112], "norm_scores": [0.11160720139741898, 0.1063859686255455, 0.10191647708415985, 0.0993013083934784, 0.09766601026058197, 0.09760928899049759, 0.09697950631380081, 0.09658113121986389, 0.09603439271450043, 0.0959186926484108]}}
{"id": "WebQTest-1664", "question": "what form of currency does germany use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country"], ["finance.currency.countries_used", "location.country.currency_used"], ["time.event.locations", "location.location.events"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does germany use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used <PATH>", "</PATH> finance.currency.countries_used </PATH>", "<PATH> time.event.locations <SEP> location.location.events </PATH>"], "scores": [-0.005690320394933224, -0.02572883851826191, -0.11970096826553345, -0.14481998980045319, -0.15614189207553864, -0.17736516892910004, -0.1869693398475647, -0.18805433809757233, -0.1950892060995102, -0.20522752404212952], "norm_scores": [0.11416614800691605, 0.11190120130777359, 0.10186457633972168, 0.09933770447969437, 0.09821934998035431, 0.0961567834019661, 0.09523768723011017, 0.09513441473245621, 0.09446751326322556, 0.09351460635662079]}}
{"id": "WebQTest-1665", "question": "where is the latin language from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"], ["language.language_family.languages"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.language_family", "language.human_language.language_family"], ["symbols.namesake.named_after"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.language_family", "language.language_family.languages"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the latin language from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.language_family <SEP> language.human_language.language_family </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.language_family <SEP> language.language_family.languages </PATH>"], "scores": [-0.0019484455697238445, -0.052531369030475616, -0.0687437653541565, -0.09407170861959457, -0.14282746613025665, -0.1472775787115097, -0.15438906848430634, -0.15565572679042816, -0.16167311370372772, -0.1623961627483368], "norm_scores": [0.11171112209558487, 0.10620097815990448, 0.10449308902025223, 0.10187973082065582, 0.09703165292739868, 0.09660080820322037, 0.0959162712097168, 0.09579485654830933, 0.09522014856338501, 0.09515132755041122]}}
{"id": "WebQTest-1666", "question": "what are the top tourist attractions in rome", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["base.arthist.int_zm_nyek.helynev"], ["base.arthist.helynevek.intezmeny"], ["olympics.olympic_games.host_city"], ["film.film_location.featured_in_films"], ["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"], ["film.film.featured_film_locations"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["base.arthist.int_zm_nyek.helynev"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the top tourist attractions in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> base.arthist.int_zm_nyek.helynev </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> olympics.olympic_games.host_city </PATH>", "<PATH> film.film_location.featured_in_films </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film.featured_film_locations </PATH>"], "scores": [-0.017415247857570648, -0.018649466335773468, -0.019091010093688965, -0.09930886328220367, -0.10168185085058212, -0.1103828027844429, -0.11144869774580002, -0.11510223150253296, -0.12821342051029205, -0.1302030384540558], "norm_scores": [0.10690023750066757, 0.10676838457584381, 0.10672125220298767, 0.09849467128515244, 0.09826121479272842, 0.09740997105836868, 0.09730619192123413, 0.09695132076740265, 0.09568848460912704, 0.09549828618764877]}}
{"id": "WebQTest-1667", "question": "who were the children of king solomon", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"], ["fictional_universe.fictional_character.parents"], ["film.film_subject.films", "film.film_subject.films"], ["film.film_subject.films", "film.film_story_contributor.film_story_credits"], ["people.family.members"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.children"], ["people.deceased_person.place_of_burial"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were the children of king solomon [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> film.film_subject.films <SEP> film.film_subject.films </PATH>", "<PATH> film.film_subject.films <SEP> film.film_story_contributor.film_story_credits </PATH>", "<PATH> people.family.members </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>"], "scores": [-0.013553272001445293, -0.01413730252534151, -0.13514409959316254, -0.1463690549135208, -0.15413591265678406, -0.15777559578418732, -0.15918321907520294, -0.16851070523262024, -0.17500287294387817, -0.17585676908493042], "norm_scores": [0.11214380711317062, 0.11207833141088486, 0.09930453449487686, 0.09819607436656952, 0.09743636101484299, 0.09708236902952194, 0.09694580733776093, 0.09604574739933014, 0.09542421996593475, 0.09534277021884918]}}
{"id": "WebQTest-1668", "question": "what did andy irons die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["deceased_person.cause_of_death"], ["people.people.deceased_person.cause_of_death"], ["people.marriage.spouse", "people.marriage.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.people.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did andy irons die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>", "<PATH> people.people.cause_of_death </PATH>"], "scores": [-0.001999364700168371, -0.03981946036219597, -0.12492672353982925, -0.12956978380680084, -0.13135240972042084, -0.1317334622144699, -0.1418016254901886, -0.14907033741474152, -0.15798896551132202, -0.16629242897033691], "norm_scores": [0.11209175735712051, 0.10793159902095795, 0.09912586212158203, 0.09866669028997421, 0.09849096089601517, 0.09845343232154846, 0.09746716171503067, 0.09676127135753632, 0.09590213000774384, 0.09510910511016846]}}
{"id": "WebQTest-1669", "question": "where is auburn university located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.contains"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_parent"], ["education.educational_institution.parent_institution", "location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is auburn university located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.contains </PATH>"], "scores": [-0.002378251403570175, -0.04520997777581215, -0.11741062253713608, -0.1275169849395752, -0.13089051842689514, -0.13499455153942108, -0.13748182356357574, -0.13958802819252014, -0.14713118970394135, -0.1515052318572998], "norm_scores": [0.11161839216947556, 0.10693853348493576, 0.09948964416980743, 0.09848921746015549, 0.09815752506256104, 0.09775550663471222, 0.09751266986131668, 0.09730749577283859, 0.0965762585401535, 0.09615474939346313]}}
{"id": "WebQTest-1670", "question": "who is tatyana ali parents", "prediction": [["people.person.children"], ["people.person.parents"], ["music.artist.album", "music.album.artist"], ["people.person.parent"], ["music.artist.origin", "people.person.place_of_birth"], ["influence.influence_node.influenced_by"], ["music.artist.genre", "music.artist.genre"], ["common.topic.notable_types", "common.topic.notable_types"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "people.person.parents"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is tatyana ali parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> music.artist.album <SEP> music.album.artist </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> music.artist.origin <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.parents </PATH>"], "scores": [-0.012090157717466354, -0.016222374513745308, -0.1547079086303711, -0.15564383566379547, -0.15913060307502747, -0.16180476546287537, -0.16512633860111237, -0.16610243916511536, -0.17219874262809753, -0.17599011957645416], "norm_scores": [0.1127435490489006, 0.11227863281965256, 0.09775829315185547, 0.09766683727502823, 0.09732688963413239, 0.0970669686794281, 0.0967450886964798, 0.09665070474147797, 0.0960632860660553, 0.09569976478815079]}}
{"id": "WebQTest-1671", "question": "where did leif ericson grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes", "location.location.containedby"], ["fictional_universe.fictional_character.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced_by", "location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did leif ericson grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes <SEP> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> location.location.people_born_here </PATH>"], "scores": [-0.0016780615551397204, -0.048947155475616455, -0.08573387563228607, -0.10379265993833542, -0.11449619382619858, -0.1151643916964531, -0.11683468520641327, -0.11706612259149551, -0.11797445267438889, -0.11926186084747314], "norm_scores": [0.10960476845502853, 0.10454439371824265, 0.10076841711997986, 0.09896500408649445, 0.09791138023138046, 0.09784597903490067, 0.09768268465995789, 0.09766007959842682, 0.09757141023874283, 0.09744587540626526]}}
{"id": "WebQTest-1672", "question": "what language do the maasai speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["language.human_language.region"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["people.ethnicity.languages_spoken", "language.human_language.region", "language.human_language.region"], ["people.person.ethnicity", "people.person.nationality", "location.country.languages_spoken"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.region <SEP> language.human_language.region </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.0028832226525992155, -0.04924101009964943, -0.06398245692253113, -0.06742944568395615, -0.08627624064683914, -0.09277193248271942, -0.09615810960531235, -0.10062213242053986, -0.10387963801622391, -0.1076987162232399], "norm_scores": [0.10765187442302704, 0.10277527570724487, 0.10127133131027222, 0.10092286020517349, 0.09903860092163086, 0.09839735180139542, 0.09806473553180695, 0.09762794524431229, 0.09731043875217438, 0.09693950414657593]}}
{"id": "WebQTest-1673", "question": "what political party was hitler in", "prediction": [["base.activism.activist.area_of_activism", "government.political_party.ideology"], ["organization.organization_founder.organizations_founded"], ["base.activism.activism_issue.supporting_political_parties"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["government.politician.party", "government.political_party_tenure.party"], ["base.activism.activism_issue.supporting_political_parties", "government.political_party.ideology"], ["base.activism.activism_issue.supporting_political_parties", "government.political_party_tenure.party"], ["government.politician.party", "government.political_party.politicians_in_this_party"], ["base.activism.activism_issue.supporting_political_parties", "government.political_party.politicians_in_this_party"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["base.activism.activist.area_of_activism", "government.political_party.ideology"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party was hitler in [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism <SEP> government.political_party.ideology </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activism_issue.supporting_political_parties </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> base.activism.activism_issue.supporting_political_parties <SEP> government.political_party.ideology </PATH>", "<PATH> base.activism.activism_issue.supporting_political_parties <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.politician.party <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> base.activism.activism_issue.supporting_political_parties <SEP> government.political_party.politicians_in_this_party </PATH>"], "scores": [-0.017929822206497192, -0.020501963794231415, -0.030374279245734215, -0.030574925243854523, -0.03679199144244194, -0.051658689975738525, -0.05957268550992012, -0.07355187833309174, -0.07440479844808578, -0.08383561670780182], "norm_scores": [0.10301776230335236, 0.10275312513113022, 0.10174370557069778, 0.10172329843044281, 0.10109283775091171, 0.09960103780031204, 0.0988159030675888, 0.09744414687156677, 0.09736107289791107, 0.09644719213247299]}}
{"id": "WebQTest-1674", "question": "who is the leader of north korea today", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.political_appointer.appointees", "government.politician.government_positions_held"], ["government.government_position_held.office_holder", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the leader of north korea today [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.004339101258665323, -0.029347311705350876, -0.06742420792579651, -0.07367203384637833, -0.08145733922719955, -0.09175383299589157, -0.09427639096975327, -0.09532582759857178, -0.09540612250566483, -0.0996023565530777], "norm_scores": [0.10708510130643845, 0.10444029420614243, 0.10053829848766327, 0.09991210699081421, 0.09913728386163712, 0.0981217548251152, 0.09787454456090927, 0.09777189046144485, 0.0977640375494957, 0.09735465794801712]}}
{"id": "WebQTest-1675", "question": "where do the appalachian mountains run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.mountain_range.mountains"], ["geography.river.basin_countries"], ["location.location.containedby"], ["geography.mountain_pass.range"], ["base.aareas.schema.administrative_area.administrative_parent"], ["geography.mountain.mountain_range"], ["location.location.contains"], ["location.location.partly_containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.mountain_pass.range </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.partly_containedby </PATH>"], "scores": [-0.007678946480154991, -0.02017351984977722, -0.09576830267906189, -0.09585345536470413, -0.0980009138584137, -0.10860695689916611, -0.1175711378455162, -0.1299966722726822, -0.13242238759994507, -0.13424749672412872], "norm_scores": [0.10891901701688766, 0.10756658762693405, 0.09973486512899399, 0.0997263714671135, 0.09951244294643402, 0.09846258908510208, 0.09758388996124268, 0.0963788628578186, 0.09614535421133041, 0.0959700345993042]}}
{"id": "WebQTest-1676", "question": "what money does spain use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country"], ["location.country.currency_formerly_used"], ["location.country.form_of_government"], ["common.topic.notable_types"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does spain use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "</PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_used <PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.006133046466857195, -0.026066189631819725, -0.1382899284362793, -0.15267449617385864, -0.1625453531742096, -0.18526336550712585, -0.19654206931591034, -0.19964562356472015, -0.2030230611562729, -0.2040557861328125], "norm_scores": [0.11489292234182358, 0.11262542754411697, 0.10066958516836166, 0.09923185408115387, 0.0982571691274643, 0.09605012834072113, 0.0949728935956955, 0.0946786031126976, 0.0943593755364418, 0.09426197409629822]}}
{"id": "WebQTest-1677", "question": "where was terry waite born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality"], ["geography.river.basin_countries", "location.location.containedby"], ["base.biblioness.bibs_location.country"], ["common.topic.notable_types"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was terry waite born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> geography.river.basin_countries <SEP> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.009698639623820782, -0.017128201201558113, -0.12533888220787048, -0.12547722458839417, -0.1338757574558258, -0.14186814427375793, -0.14224351942539215, -0.14594610035419464, -0.148152157664299, -0.15121400356292725], "norm_scores": [0.11085592210292816, 0.11003536731004715, 0.09874997287988663, 0.09873631596565247, 0.09791054576635361, 0.09713113307952881, 0.09709467738866806, 0.09673583507537842, 0.09652267396450043, 0.09622758626937866]}}
{"id": "WebQTest-1678", "question": "what country did the battle of san juan hill take place", "prediction": [["location.location.events"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.combatant"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "location.location.containedby"], ["time.event.locations"], ["military.military_conflict.casualties", "location.location.containedby"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.culturalevent.event.entity_involved", "location.location.contains"], ["military.military_conflict.casualties", "location.location.contains"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did the battle of san juan hill take place [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.combatant </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.containedby </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> military.military_conflict.casualties <SEP> location.location.containedby </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.contains </PATH>", "<PATH> military.military_conflict.casualties <SEP> location.location.contains </PATH>"], "scores": [-0.0063990200869739056, -0.027118278667330742, -0.05506592616438866, -0.0596504881978035, -0.06329868733882904, -0.06381649523973465, -0.06727329641580582, -0.08121407777070999, -0.08506713062524796, -0.0862114429473877], "norm_scores": [0.10542389750480652, 0.10326206684112549, 0.10041608661413193, 0.09995678067207336, 0.09959277510643005, 0.09954122453927994, 0.09919772297143936, 0.09782442450523376, 0.09744822233915329, 0.0973367765545845]}}
{"id": "WebQTest-1679", "question": "what kind of government does the united states have currently", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.main_country", "location.country.form_of_government"], ["base.locations.countries.planet", "government.form_of_government.countries"], ["base.locations.countries.planet"], ["form.form_of_government.countries"], ["language.human_language.main_country", "government.form_of_government.countries"], ["base.locations.countries.form_of_government"], ["locations.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does the united states have currently [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.planet </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>", "<PATH> locations.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007673365529626608, -0.016539674252271652, -0.1509290188550949, -0.1565612107515335, -0.1571931540966034, -0.15815426409244537, -0.1585293859243393, -0.15998178720474243, -0.16033031046390533, -0.16536273062229156], "norm_scores": [0.11271389573812485, 0.11171896010637283, 0.09767025709152222, 0.0971217080950737, 0.09706034511327744, 0.09696710109710693, 0.09693074226379395, 0.09679006040096283, 0.09675632417201996, 0.09627063572406769]}}
{"id": "WebQTest-1680", "question": "when was the last time the orioles won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"], ["sports.sports_league_season.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_ event.champion"], ["sports.sports_league_season.championship"], ["time.event.instance_of_recurring_event"], ["sports.sports_teams.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the orioles won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_league_season.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_ event.champion </PATH>", "<PATH> sports.sports_league_season.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_teams.champion </PATH>", "<PATH> sports.sports_championship_event.champion <PATH>"], "scores": [-0.009175349026918411, -0.013419566676020622, -0.09894126653671265, -0.12776687741279602, -0.14961625635623932, -0.15615002810955048, -0.15787924826145172, -0.15795445442199707, -0.16059349477291107, -0.1648760885000229], "norm_scores": [0.11149194091558456, 0.11101973801851273, 0.10191980749368668, 0.09902385622262955, 0.09688370674848557, 0.09625275433063507, 0.09608645737171173, 0.09607923775911331, 0.09582601487636566, 0.09541650861501694]}}
{"id": "WebQTest-1682", "question": "who did ben stiller date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["common.topic.notable_types", "common.topic.notable_types"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["base.popstra.celebrity.friendship", "base.popstra.friendship.participant"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.parents", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.person.languages", "people.person.languages"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.ethnicity", "people.person.ethnicity"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben stiller date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.celebrity.sexual_relationships </PATH>", "<PATH> base.popstra.dated.participant <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> base.popstra.dated.participant <SEP> base.popstra.dated.participant </PATH>", "<PATH> base.popstra.celebrity.friendship <SEP> base.popstra.friendship.participant </PATH>"], "scores": [-0.015382395125925541, -0.030044453218579292, -0.03484940901398659, -0.04816334322094917, -0.0503980927169323, -0.05075212940573692, -0.05153283849358559, -0.05693842098116875, -0.06259197741746902, -0.06308342516422272], "norm_scores": [0.10313688218593597, 0.10163572430610657, 0.10114853084087372, 0.09981077164411545, 0.09958797693252563, 0.09955272078514099, 0.09947503358125687, 0.09893875569105148, 0.09838098287582397, 0.09833264350891113]}}
{"id": "WebQTest-1683", "question": "what is the most common language in norway", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.language_family"], ["language.human_language.region"], ["base.rosetta.languoid.languoid_class"], ["location.country.administrative_divisions"], ["language.human_language.country"], ["book.book_subject.works"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.015104272402822971, -0.022305237129330635, -0.0324426144361496, -0.03898357227444649, -0.12222406268119812, -0.12654006481170654, -0.1500622183084488, -0.16707222163677216, -0.16855134069919586, -0.17596662044525146], "norm_scores": [0.10885028541088104, 0.10806926339864731, 0.10697926580905914, 0.1062818095088005, 0.09779305756092072, 0.0973718911409378, 0.09510822594165802, 0.09350411593914032, 0.09336591511964798, 0.09267614036798477]}}
{"id": "WebQTest-1684", "question": "what influenced william shakespeare to start writing", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["people.person.profession"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["film.film_story_contributor.film_story_credits"], ["book.written_work.author"], ["fictional_universe.fictional_character.character_created_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced william shakespeare to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>"], "scores": [-0.009741481393575668, -0.013445364311337471, -0.1008109375834465, -0.10536500811576843, -0.10765054076910019, -0.1323217898607254, -0.13909058272838593, -0.15202005207538605, -0.15537111461162567, -0.16379527747631073], "norm_scores": [0.11016636341810226, 0.10975907742977142, 0.10057685524225235, 0.10011985898017883, 0.0998912900686264, 0.09745700657367706, 0.0967995747923851, 0.09555606544017792, 0.09523637592792511, 0.09443745762109756]}}
{"id": "WebQTest-1685", "question": "where did benjamin franklin went to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["book.written_work.subjects", "location.location.containedby"], ["book.written_work.author", "education.education.institution"], ["influence.influence_node.influenced_by", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.author <SEP> education.education.institution </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>"], "scores": [-0.013228636234998703, -0.023139404132962227, -0.024071095511317253, -0.031477633863687515, -0.0547419898211956, -0.07834043353796005, -0.10354374349117279, -0.10876470059156418, -0.10903105139732361, -0.1174197718501091], "norm_scores": [0.10537634789943695, 0.10433714836835861, 0.10423997789621353, 0.10347077995538712, 0.10109138488769531, 0.09873370826244354, 0.09627638757228851, 0.0957750454545021, 0.09574954211711884, 0.09494967758655548]}}
{"id": "WebQTest-1686", "question": "who is the governor of arizona 2009", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"]], "ground_paths": [["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of arizona 2009 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.appointed_by </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.02037392556667328, -0.021120255813002586, -0.026082145050168037, -0.026357633993029594, -0.028177965432405472, -0.0390797033905983, -0.03946896642446518, -0.039606645703315735, -0.04656395688652992, -0.04742306098341942], "norm_scores": [0.10130900144577026, 0.10123342275619507, 0.10073235630989075, 0.1007046103477478, 0.10052146017551422, 0.09943155199289322, 0.0993928536772728, 0.09937916696071625, 0.09869015216827393, 0.09860540926456451]}}
{"id": "WebQTest-1687", "question": "what movies has spike lee made", "prediction": [["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"], ["film.producer.film"], ["film.producer.films_executive_produced"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.film.written_by"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["film.writer.film"], ["film.producer.film"], ["film.film.directed_by"], ["film.director.film"], ["film.film.executive_produced_by"], ["film.film.produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has spike lee made [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.director.film </PATH>", "<PATH> film.producer.film </PATH>", "<PATH> film.producer.films_executive_produced </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.film.written_by </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.027287578210234642, -0.032225366681814194, -0.03384564816951752, -0.03795557841658592, -0.050931498408317566, -0.05693744122982025, -0.06051336228847504, -0.06159117445349693, -0.0690748542547226, -0.07418973743915558], "norm_scores": [0.10233112424612045, 0.10182708501815796, 0.10166222602128983, 0.10124526172876358, 0.0999399945139885, 0.09934156388044357, 0.09898696094751358, 0.0988803282380104, 0.09814310818910599, 0.09764238446950912]}}
{"id": "WebQTest-1688", "question": "what did bernie mac died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.cause_of_death"], ["deceased.deceased_person.cause_of_death"], ["deceased.cause_of_death.people"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["influence.influence_node.influenced", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bernie mac died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> deceased.deceased_person.cause_of_death </PATH>", "<PATH> deceased.cause_of_death.people </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0015589557588100433, -0.04409186169505119, -0.12182056903839111, -0.12653203308582306, -0.1269264668226242, -0.12703073024749756, -0.13050943613052368, -0.14613409340381622, -0.15049105882644653, -0.15357241034507751], "norm_scores": [0.1116466075181961, 0.1069975271821022, 0.09899575263261795, 0.09853043407201767, 0.09849157929420471, 0.09848131239414215, 0.09813932329416275, 0.09661784768104553, 0.09619779884815216, 0.09590183943510056]}}
{"id": "WebQTest-1690", "question": "where is electrolux based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization.place_founded"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization.place_founded", "location.country.capital"], ["organization.organization_founder.organizations_founded"], ["organization.organization_founder.organizations_founded", "people.person.place_of_birth"], ["organization.organization.place_founded", "location.location.contains"]], "ground_paths": [["organization.organization.founders", "people.deceased_person.place_of_death"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is electrolux based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.place_founded <SEP> location.country.capital </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> people.person.place_of_birth </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.contains </PATH>"], "scores": [-0.00470049399882555, -0.03471528738737106, -0.055033449083566666, -0.057759467512369156, -0.060431722551584244, -0.08822274953126907, -0.09745307266712189, -0.0999460518360138, -0.10134855657815933, -0.10395277291536331], "norm_scores": [0.10673166811466217, 0.10357574373483658, 0.1014925017952919, 0.10121621191501617, 0.1009460911154747, 0.09817931801080704, 0.09727726876735687, 0.09703506529331207, 0.09689906239509583, 0.09664705395698547]}}
{"id": "WebQTest-1691", "question": "what is houston timezone", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.time_Zones"], ["location.location.time_zonessymbol"], ["locations.location.time_zones"], ["location.location.time_zoneslanguage"], ["location.location.time_zonescommon"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is houston timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.location.time_zonessymbol </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> location.location.time_zoneslanguage </PATH>", "<PATH> location.location.time_zonescommon </PATH>"], "scores": [-9.540435712551698e-06, -0.19501566886901855, -0.19708415865898132, -0.20713074505329132, -0.20919111371040344, -0.20987476408481598, -0.23262420296669006, -0.2368244230747223, -0.24810048937797546, -0.24957586824893951], "norm_scores": [0.12165678292512894, 0.10010280460119247, 0.09989596158266068, 0.09889737516641617, 0.09869381785392761, 0.09862636774778366, 0.09640800207853317, 0.09600391238927841, 0.09492745250463486, 0.09478750079870224]}}
{"id": "WebQTest-1694", "question": "what was shakespeare 's wife called", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.nationality", "people.person.nationality"], ["book.book_subject.works", "book.book_subject.works"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.parents"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was shakespeare 's wife called [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.has_value <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.02563905529677868, -0.03669676557183266, -0.037888724356889725, -0.04058956727385521, -0.04365230351686478, -0.0440356582403183, -0.04653220623731613, -0.049186039716005325, -0.05367297679185867, -0.06511611491441727], "norm_scores": [0.1018785685300827, 0.10075823217630386, 0.10063820332288742, 0.10036676377058029, 0.10005983710289001, 0.10002148151397705, 0.09977208822965622, 0.0995076522231102, 0.09906218200922012, 0.09793505072593689]}}
{"id": "WebQTest-1695", "question": "when did truman become president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "location.location.events"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "location.location.containedby", "time.event.locations"], ["event.speech_or_presentation.speaker_s", "event.speech_or_presentation.event"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "location.location.containedby", "time.event.locations"], ["book.written_work.subjects", "base.culturalevent.event.entity_involved"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "location.location.events"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "location.location.containedby", "location.location.containedby"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did truman become president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location.events </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> location.location.containedby <SEP> time.event.locations </PATH>", "<PATH> event.speech_or_presentation.speaker_s <SEP> event.speech_or_presentation.event </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location.containedby <SEP> time.event.locations </PATH>", "<PATH> book.written_work.subjects <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> location.location.events </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.007197552360594273, -0.027993634343147278, -0.06147127225995064, -0.06360102444887161, -0.06662759929895401, -0.0735286995768547, -0.07457524538040161, -0.07469487190246582, -0.07597530633211136, -0.08070836216211319], "norm_scores": [0.10546182096004486, 0.10329127311706543, 0.0998905673623085, 0.09967805445194244, 0.09937682747840881, 0.0986933782696724, 0.09859014302492142, 0.09857834875583649, 0.09845220297574997, 0.0979873314499855]}}
{"id": "WebQTest-1696", "question": "what did ferdinand magellan do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"], ["base.culturalevent.event.entity_involved"], ["base.exhibitions.exhibition_subject.exhibitions_created_about_this_subject"], ["base.nobelprizes.nobel_prize_winner.nobel_prize_won"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["base.nobelprizes.nobel_prize_winner.nobel_prizes_won"], ["base.nobelprizes.nobel_prize_winner.nobel_positions"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did ferdinand magellan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.exhibitions.exhibition_subject.exhibitions_created_about_this_subject </PATH>", "<PATH> base.nobelprizes.nobel_prize_winner.nobel_prize_won </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> base.nobelprizes.nobel_prize_winner.nobel_prizes_won </PATH>", "<PATH> base.nobelprizes.nobel_prize_winner.nobel_positions </PATH>"], "scores": [-0.0016482763458043337, -0.061282552778720856, -0.06584417819976807, -0.10637754946947098, -0.11399710178375244, -0.1234658733010292, -0.1276921033859253, -0.13322284817695618, -0.13470503687858582, -0.136439248919487], "norm_scores": [0.11028830707073212, 0.10390361398458481, 0.10343071818351746, 0.09932215511798859, 0.09856823831796646, 0.09763932228088379, 0.0972275510430336, 0.09669129550457001, 0.09654808789491653, 0.0963807925581932]}}
{"id": "WebQTest-1698", "question": "where do burmese cats originate from", "prediction": [["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.breed_of"], ["biology.breed_origin.breeds_originating_here", "base.breed_origin.breeds_originating_here"], ["biology.breed_origin.breeds_originating_here", "base.breeds.breed_origin.breeds_originating_here"], ["biology.animal_breed.place_of_origin", "base.biblioness.bibs_location.country"], ["base.petbreeds.breed_of_origin.breeds_originating_here"], ["biology.animal_breed.place_of_origin", "base.breed_origin.breeds_originating_here"], ["biology.breed_origin.breeds_originating_here", "location.location.containedby"], ["biology.breed_origin.breeds_originating_here", "base.locations.continents.countries_within"]], "ground_paths": [["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.place_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do burmese cats originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.place_of_origin </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> base.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> base.breeds.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.place_of_origin <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> base.petbreeds.breed_of_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.place_of_origin <SEP> base.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> location.location.containedby </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> base.locations.continents.countries_within </PATH>"], "scores": [-0.003901412710547447, -0.027062593027949333, -0.06282642483711243, -0.13264578580856323, -0.13651862740516663, -0.14693735539913177, -0.1478784680366516, -0.14902782440185547, -0.15021513402462006, -0.1544501632452011], "norm_scores": [0.1111537516117096, 0.10860887914896011, 0.1047932505607605, 0.09772622585296631, 0.09734848141670227, 0.09633950144052505, 0.09624888002872467, 0.09613831341266632, 0.09602423757314682, 0.09561842679977417]}}
{"id": "WebQTest-1699", "question": "who played kirk", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["media_common.quotation.spoken_by_character", "media_common.quotation.author"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played kirk [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> media_common.quotation.spoken_by_character <SEP> media_common.quotation.author </PATH>", "<PATH> fictional_universe.fictional_character.quotations <SEP> media_common.quotation.author </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.023314565420150757, -0.031639132648706436, -0.03283937647938728, -0.03464579954743385, -0.03677348792552948, -0.041349753737449646, -0.04805607721209526, -0.05456801503896713, -0.09283731132745743, -0.09360934048891068], "norm_scores": [0.10256968438625336, 0.10171937197446823, 0.101597361266613, 0.10141400247812271, 0.10119844973087311, 0.10073639452457428, 0.10006308555603027, 0.09941359609365463, 0.09568098187446594, 0.09560714662075043]}}
{"id": "WebQTest-1700", "question": "where is mt st helens located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"], ["geography.mountain.mountain_range"], ["geography.mountain_range.mountains"], ["geography.mountain.mountain_region"], ["location.mountain.mountain_type"], ["location.hud_county_place.county"], ["location.location.containedby", "location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mt st helens located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> geography.mountain.mountain_region </PATH>", "<PATH> location.mountain.mountain_type </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.00804985873401165, -0.022120870649814606, -0.0912768617272377, -0.12001269310712814, -0.1606062948703766, -0.16500428318977356, -0.16505756974220276, -0.16544997692108154, -0.16621355712413788, -0.16878743469715118], "norm_scores": [0.11200985312461853, 0.1104447990655899, 0.10306500643491745, 0.10014549642801285, 0.09616163372993469, 0.09573964029550552, 0.09573454409837723, 0.09569698572158813, 0.0956239402294159, 0.09537812322378159]}}
{"id": "WebQTest-1701", "question": "what county is st paul minnesota in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"], ["base.biblioness.bibs_location.state"], ["base.biblioness.bibs_location.country"], ["location.citytown.postal_codes"], ["location.place_with_neighborhoods.neighborhoods"], ["base.biblioness.bibs_location.city"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul minnesota in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.place_with_neighborhoods.neighborhoods </PATH>", "<PATH> base.biblioness.bibs_location.city </PATH>"], "scores": [-0.01468360424041748, -0.018266653642058372, -0.03612319380044937, -0.0526277981698513, -0.0790582075715065, -0.08654775470495224, -0.0907277837395668, -0.090883769094944, -0.11166422814130783, -0.11286763101816177], "norm_scores": [0.10555481165647507, 0.10517727583646774, 0.10331583768129349, 0.10162465274333954, 0.09897385537624359, 0.0982353538274765, 0.09782558679580688, 0.09781033545732498, 0.09579876065254211, 0.09568354487419128]}}
{"id": "WebQTest-1702", "question": "who was king henry viii son", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"], ["fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["people.person.sibling_s"], ["people.person.parent"], ["people.person.place_of_birth"], ["people.person.religion"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king henry viii son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.sibling_s </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.religion </PATH>"], "scores": [-0.01369871012866497, -0.015882600098848343, -0.06370598822832108, -0.1044750064611435, -0.11758891493082047, -0.11763951927423477, -0.13615942001342773, -0.13749028742313385, -0.1485072374343872, -0.14884306490421295], "norm_scores": [0.10892482101917267, 0.10868720710277557, 0.10361174494028091, 0.0994725450873375, 0.09817657619714737, 0.09817161411046982, 0.09637021273374557, 0.09624204784631729, 0.09518758207559586, 0.09515561163425446]}}
{"id": "WebQTest-1705", "question": "where was franz ferdinand from", "prediction": [["people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["symbols.namesake.named_after"], ["people.person.place_of_birth", "location.location.containedby"], ["military.military_person.participated_in_conflicts"], ["symbols.name_source.namesakes"], ["people.deceased_person.place_of_burial"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was franz ferdinand from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>"], "scores": [-0.0009511832613497972, -0.07217923551797867, -0.07735654711723328, -0.0838804543018341, -0.11980338394641876, -0.12081927061080933, -0.1277237832546234, -0.1309923380613327, -0.13236363232135773, -0.1332406848669052], "norm_scores": [0.11031416058540344, 0.10272999852895737, 0.10219951719045639, 0.10153494030237198, 0.09795224666595459, 0.09785278886556625, 0.09717948734760284, 0.09686236828565598, 0.09672964364290237, 0.09664483368396759]}}
{"id": "WebQTest-1706", "question": "what was benedict arnold most famous for", "prediction": [["base.honouriam.dishonoured_person.dishonor_bestowed"], ["people.person.profession"], ["base.culturalevent.event.entity_involved"], ["base.honouriam.dishonoured_person.dishonor_bestowed", "base.honouri.dishonor.dishonor_bestowed"], ["base.activism.activist.area_of_activism"], ["base.honouriam.dishonoured_person.dishonor_bestowed", "people.person.profession"], ["base.honouriam.dishonored_person.dishonor_bestowed"], ["base.honouriam.dishonoured_person.dishonor_bestowed", "base.honouri.honouri.dishonor"], ["base.honouriam.dishonoured_person.dishonor_bestowed", "base.onephylogeny.type_of_thing.things_of_this_type"], ["base.honouriam.dishonoured_person.dishonor_bestowed", "base.honouri.dishonor.dishonor"]], "ground_paths": [["base.honouriam.dishonoured_person.dishonor_bestowed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was benedict arnold most famous for [/INST]", "raw_output": {"paths": ["<PATH> base.honouriam.dishonoured_person.dishonor_bestowed </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.honouriam.dishonoured_person.dishonor_bestowed <SEP> base.honouri.dishonor.dishonor_bestowed </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.honouriam.dishonoured_person.dishonor_bestowed <SEP> people.person.profession </PATH>", "<PATH> base.honouriam.dishonored_person.dishonor_bestowed </PATH>", "<PATH> base.honouriam.dishonoured_person.dishonor_bestowed <SEP> base.honouri.honouri.dishonor </PATH>", "<PATH> base.honouriam.dishonoured_person.dishonor_bestowed <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> base.honouriam.dishonoured_person.dishonor_bestowed <SEP> base.honouri.dishonor.dishonor </PATH>"], "scores": [-0.02160133421421051, -0.02534179762005806, -0.05456132814288139, -0.059955015778541565, -0.060252074152231216, -0.0700891837477684, -0.0716564953327179, -0.0717499852180481, -0.0727071464061737, -0.07875680923461914], "norm_scores": [0.10375744849443436, 0.10337007790803909, 0.10039334744215012, 0.09985332190990448, 0.09982366859912872, 0.09884649515151978, 0.09869169443845749, 0.0986824706196785, 0.09858806431293488, 0.09799344092607498]}}
{"id": "WebQTest-1707", "question": "who plays lex luthor on smallville", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.languages", "people.person.languages"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays lex luthor on smallville [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_winning_work.awards_won <SEP> award.award_winner.awards_won </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.022080596536397934, -0.02394232526421547, -0.02757716178894043, -0.0300282072275877, -0.030335823073983192, -0.04557926952838898, -0.048867885023355484, -0.04972318187355995, -0.0548693910241127, -0.05831177532672882], "norm_scores": [0.10171118378639221, 0.10152200609445572, 0.1011536568403244, 0.10090602189302444, 0.10087499022483826, 0.09934896975755692, 0.09902279078960419, 0.09893812984228134, 0.09843027591705322, 0.09809202700853348]}}
{"id": "WebQTest-1708", "question": "what are dallas cowboys colors", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.sports_team.color"], ["sports.sports_team_color.team"], ["sports.sports_team_colors.team"], ["sports.professional_sports_team.colors"], ["sports.sports_team_colors.sports_team"], ["sports.defunct_sports_team.later_known_as"], ["sports.sports_team.team_colors"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are dallas cowboys colors [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.color </PATH>", "<PATH> sports.sports_team_color.team </PATH>", "<PATH> sports.sports_team_colors.team </PATH>", "<PATH> sports.professional_sports_team.colors </PATH>", "<PATH> sports.sports_team_colors.sports_team </PATH>", "<PATH> sports.defunct_sports_team.later_known_as </PATH>", "<PATH> sports.sports_team.team_colors </PATH>", "<PATH> sports.sports_team.colors <PATH>"], "scores": [-8.315798913827166e-05, -0.11507085710763931, -0.12230496853590012, -0.1333835870027542, -0.1385713666677475, -0.14229387044906616, -0.15019236505031586, -0.1618434488773346, -0.16359718143939972, -0.1765877902507782], "norm_scores": [0.11378826946020126, 0.10142826288938522, 0.10069716721773148, 0.09958774596452713, 0.09907244145870209, 0.09870433062314987, 0.09792777895927429, 0.09679344296455383, 0.09662383794784546, 0.09537675231695175]}}
{"id": "WebQTest-1710", "question": "what is the currency of the czech republic", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country"], ["location.country.currency_used", "location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["common.topic.notable_types"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used", "finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of the czech republic [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.0023084611166268587, -0.03970364108681679, -0.12021074444055557, -0.13556386530399323, -0.15496161580085754, -0.1610216498374939, -0.16335603594779968, -0.17573024332523346, -0.1801701933145523, -0.18070779740810394], "norm_scores": [0.11357583850622177, 0.10940708965063095, 0.10094426572322845, 0.09940629452466965, 0.09749661386013031, 0.09690756350755692, 0.096681609749794, 0.09549262374639511, 0.09506958723068237, 0.09501849114894867]}}
{"id": "WebQTest-1711", "question": "when was the last time the la kings won a playoff series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["sports.professional_sports_team.owner_s", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event"], ["sports.sports_league_season.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the la kings won a playoff series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_league_season.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.006765213795006275, -0.017402762547135353, -0.06339826434850693, -0.08828437328338623, -0.11978514492511749, -0.12685510516166687, -0.1317167580127716, -0.14060579240322113, -0.14766791462898254, -0.15268786251544952], "norm_scores": [0.10957564413547516, 0.10841620713472366, 0.10354249179363251, 0.10099751502275467, 0.09786561131477356, 0.09717614948749542, 0.09670485556125641, 0.09584904462099075, 0.09517453610897064, 0.09469795972108841]}}
{"id": "WebQTest-1712", "question": "where did spencer pratt go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["common.topic.notable_types"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did spencer pratt go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.016187351197004318, -0.025585010647773743, -0.029352955520153046, -0.03051736205816269, -0.03055056743323803, -0.048805516213178635, -0.12565669417381287, -0.13626205921173096, -0.14225178956985474, -0.14400899410247803], "norm_scores": [0.10568903386592865, 0.10470045357942581, 0.10430669784545898, 0.1041853129863739, 0.10418184846639633, 0.10229726880788803, 0.09473010152578354, 0.09373076260089874, 0.09317101538181305, 0.09300743788480759]}}
{"id": "WebQTest-1713", "question": "what disease did abraham lincoln had", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["medicine.notable_person_with_medical_condition.condition", "medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.people"], ["medicine.symptom.notable_people_with_this_condition"], ["influence.influence_node.influenced_by", "medicine.disease.notable_people_with_this_condition"], ["people.person.nationality", "medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition", "medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did abraham lincoln had [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition <SEP> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.symptom.notable_people_with_this_condition </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.nationality <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.0030703148804605007, -0.0365193672478199, -0.04707005247473717, -0.10511662811040878, -0.10674544423818588, -0.10867378860712051, -0.11030542105436325, -0.11162649840116501, -0.11243733018636703, -0.11643136292695999], "norm_scores": [0.1085420623421669, 0.10497148334980011, 0.10386978834867477, 0.09801215678453445, 0.09785263240337372, 0.09766411781311035, 0.09750490635633469, 0.09737617522478104, 0.09729725867509842, 0.09690941870212555]}}
{"id": "WebQTest-1714", "question": "who is vancouver canucks coach", "prediction": [["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["ice_hockey.hockey_coach.current_team"], ["sports.sports_team.coaches"], ["ice_hockey.hockey_team.captain"], ["sports.sports_team.coaches", "ice_hockey.hockey_coach.current_team"], ["sports.sports_team.coaches", "sports.sports_team_coach.sports_coached"], ["sports.sports_team.coaches", "sports.sport.team_coaches"]], "ground_paths": [["ice_hockey.hockey_coach.current_team"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is vancouver canucks coach [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> ice_hockey.hockey_coach.current_team </PATH>", "<PATH> sports.sports_team.coaches </PATH>", "<PATH> ice_hockey.hockey_team.captain </PATH>", "<PATH> sports.sports_team.coaches <SEP> ice_hockey.hockey_coach.current_team </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sport.team_coaches </PATH>"], "scores": [-0.012141820043325424, -0.021205207332968712, -0.030372103676199913, -0.033850785344839096, -0.04283623397350311, -0.044184815138578415, -0.06335557997226715, -0.07230090349912643, -0.07744250446557999, -0.08508653193712234], "norm_scores": [0.1036505475640297, 0.1027153730392456, 0.10177808254957199, 0.10142464190721512, 0.1005173921585083, 0.10038192570209503, 0.09847584366798401, 0.09759888797998428, 0.09709835797548294, 0.09635896235704422]}}
{"id": "WebQTest-1715", "question": "where did cs lewis wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.person.nationality", "location.administrative_division.country"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did cs lewis wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>"], "scores": [-0.02125258930027485, -0.02840658463537693, -0.03625787794589996, -0.04177252575755119, -0.04720235615968704, -0.056019704788923264, -0.06384989619255066, -0.07074189931154251, -0.07082803547382355, -0.08385343104600906], "norm_scores": [0.10310503840446472, 0.10237006098031998, 0.10156947374343872, 0.10101088881492615, 0.10046390444040298, 0.09958197921514511, 0.098805271089077, 0.09812664240598679, 0.09811820089817047, 0.09684845805168152]}}
{"id": "WebQTest-1716", "question": "what is carlos pena jr nationality", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.ethnicity"], ["people.ethnicity.people"], ["location.location.people_born_here", "location.location.containedby"], ["people.ethnicity.people_with_this_ethnicity"], ["people.person.children", "people.person.nationality"], ["location.location.people_born_here"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is carlos pena jr nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>", "<PATH> people.person.children <SEP> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.00111870386172086, -0.06706442683935165, -0.07690799236297607, -0.08361253887414932, -0.09453025460243225, -0.10032140463590622, -0.10491181910037994, -0.10791822522878647, -0.11024868488311768, -0.115866519510746], "norm_scores": [0.10882920026779175, 0.1018839031457901, 0.10088592022657394, 0.10021179169416428, 0.09912365674972534, 0.09855128079652786, 0.09809992462396622, 0.0978054404258728, 0.0975777730345726, 0.0970311313867569]}}
{"id": "WebQTest-1719", "question": "who was married to jane wyman", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.parents"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was married to jane wyman [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.020239224657416344, -0.036685191094875336, -0.03955524042248726, -0.0431954450905323, -0.04375791922211647, -0.04689325392246246, -0.04956331476569176, -0.051174797117710114, -0.051316842436790466, -0.05697065591812134], "norm_scores": [0.1023930013179779, 0.10072281211614609, 0.10043414682149887, 0.10006922483444214, 0.10001295059919357, 0.09969986230134964, 0.0994340181350708, 0.09927390515804291, 0.09925980865955353, 0.09870019555091858]}}
{"id": "WebQTest-1721", "question": "what language does peru speak", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["location.country.main_country"], ["location.country.currency_formerly_used"], ["location.country.language_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does peru speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.main_country </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.language_spoken </PATH>", "</PATH> location.country.languages_spoken </PATH>"], "scores": [-0.010737081989645958, -0.029943102970719337, -0.03935842961072922, -0.04448939114809036, -0.1671695113182068, -0.17860303819179535, -0.18220193684101105, -0.18880513310432434, -0.2015969455242157, -0.20383213460445404], "norm_scores": [0.11172939091920853, 0.10960399359464645, 0.10857687145471573, 0.10802119970321655, 0.09554978460073471, 0.0944635346531868, 0.09412417560815811, 0.09350470453500748, 0.09231623262166977, 0.09211011230945587]}}
{"id": "WebQTest-1723", "question": "where is english spoken in the world map", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["languages.human_language.countries_spoken_in"], ["common.topic.notable_types"], ["base.rosetta.local_name.languoid"], ["base.rosetta.languoid.local_name"]], "ground_paths": [["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken in the world map [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> languages.human_language.countries_spoken_in </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.rosetta.local_name.languoid </PATH>", "<PATH> base.rosetta.languoid.local_name </PATH>"], "scores": [-0.013036083430051804, -0.023813901469111443, -0.028305981308221817, -0.05546097457408905, -0.14366158843040466, -0.15798646211624146, -0.16018716990947723, -0.1704138070344925, -0.17183516919612885, -0.17315785586833954], "norm_scores": [0.10991483926773071, 0.108736552298069, 0.10824919492006302, 0.10534925013780594, 0.09645535796880722, 0.09508350491523743, 0.09487447887659073, 0.09390918165445328, 0.09377579391002655, 0.09365184605121613]}}
{"id": "WebQTest-1724", "question": "when did arsenal won the league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["soccer.football_team.matches_played", "soccer.football_player_match_participation.season"], ["soccer.football_team.matches_played", "sports.sports_championship_event.champion"], ["soccer.football_team.matches_played", "soccer.football_team.player_statistics", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["soccer.football_team.matches_played", "soccer.football_team.matches_played"], ["soccer.football_team.matches_played", "sports.sports_team.matches_played"], ["soccer.football_team.matches_played", "soccer.football_match.teams"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did arsenal won the league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> soccer.football_team.matches_played <SEP> soccer.football_player_match_participation.season </PATH>", "<PATH> soccer.football_team.matches_played <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> soccer.football_team.matches_played <SEP> soccer.football_team.player_statistics <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> soccer.football_team.matches_played <SEP> soccer.football_team.matches_played </PATH>", "<PATH> soccer.football_team.matches_played <SEP> sports.sports_team.matches_played </PATH>", "<PATH> soccer.football_team.matches_played <SEP> soccer.football_match.teams </PATH>"], "scores": [-0.011065642349421978, -0.013764500617980957, -0.08791863918304443, -0.09692040085792542, -0.10248996317386627, -0.10301291942596436, -0.1052887961268425, -0.10759126394987106, -0.11556544899940491, -0.11575055867433548], "norm_scores": [0.10769721120595932, 0.10740694403648376, 0.09973041713237762, 0.09883669018745422, 0.09828774631023407, 0.09823635965585709, 0.09801303595304489, 0.09778762608766556, 0.09701094776391983, 0.09699299931526184]}}
{"id": "WebQTest-1725", "question": "what timezone is phoenix az in right now", "prediction": [["location.location.time_zones"], ["location.location.time_Zones"], ["location.location.time_zones", "location.location.time_zones"], ["locations.location.time_zones"], ["location.location.time_zonessymbol"], ["location.location.time_zoneslanguage"], ["location.location.time_zonescommon"], ["location.location.time_zonespeople"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is phoenix az in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> location.location.time_zonessymbol </PATH>", "<PATH> location.location.time_zoneslanguage </PATH>", "<PATH> location.location.time_zonescommon </PATH>", "<PATH> location.location.time_zonespeople </PATH>"], "scores": [-9.275566299038474e-06, -0.17728200554847717, -0.1857341229915619, -0.20114552974700928, -0.20283176004886627, -0.22182931005954742, -0.22408388555049896, -0.2283608615398407, -0.2292579561471939, -0.23448245227336884], "norm_scores": [0.12070653587579727, 0.10109792649745941, 0.10024703294038773, 0.09871392697095871, 0.09854761511087418, 0.09669312089681625, 0.09647536277770996, 0.09606362134218216, 0.09597748517990112, 0.09547735750675201]}}
{"id": "WebQTest-1726", "question": "where is south portland", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.location.contains"], ["location.citytown.postal_codes"], ["location.hud_county_place.county"], ["location.us_county.county_seat"], ["base.biblioness.bibs_location.country"], ["location.place_with_neighborhoods.neighborhoods"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.hud_county_place.county"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is south portland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.place_with_neighborhoods.neighborhoods </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.007517728488892317, -0.039928872138261795, -0.05002047121524811, -0.06855767220258713, -0.0717582106590271, -0.0723479837179184, -0.09415211528539658, -0.10935808718204498, -0.1135927364230156, -0.11826881021261215], "norm_scores": [0.10687194019556046, 0.10346364229917526, 0.10242477059364319, 0.10054359585046768, 0.10022231191396713, 0.10016322880983353, 0.09800288826227188, 0.09652393311262131, 0.09611604362726212, 0.09566766023635864]}}
{"id": "WebQTest-1727", "question": "what countries speak french as a first language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["base.rosetta.local_name.locale"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["location.country.first_languages"], ["location.country.currency_used"], ["location.country.administrative_divisions"], ["location.country.first_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["base.rosetta.local_name.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak french as a first language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.first_languages </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.country.first_language </PATH>"], "scores": [-0.011744518764317036, -0.02431466616690159, -0.027301771566271782, -0.1035279706120491, -0.12161651253700256, -0.12722048163414001, -0.1573891043663025, -0.15946657955646515, -0.16158001124858856, -0.17403657734394073], "norm_scores": [0.10977619141340256, 0.10840491950511932, 0.10808159410953522, 0.10014911741018295, 0.09835385531187057, 0.09780421108007431, 0.09489765763282776, 0.09470071643590927, 0.09450078010559082, 0.09333092719316483]}}
{"id": "WebQTest-1728", "question": "who was ptolemy and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["common.topic.image"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by"], ["film.film_story_contributor.film_story_credits"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was ptolemy and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.0017721273470669985, -0.05099629983305931, -0.07101118564605713, -0.07584328204393387, -0.10343477874994278, -0.10402055084705353, -0.1153189167380333, -0.12354215234518051, -0.12756550312042236, -0.129320427775383], "norm_scores": [0.10917174071073532, 0.10392796993255615, 0.10186854749917984, 0.10137749463319778, 0.09861857444047928, 0.09856082499027252, 0.09745350480079651, 0.09665541350841522, 0.09626731276512146, 0.09609851986169815]}}
{"id": "WebQTest-1729", "question": "what was the name of henry viii first wife", "prediction": [["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.religion", "people.person.religion"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["book.written_work.subjects", "theater.play.characters"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.religion", "people.person.religion"], ["people.person.children", "people.person.parents"], ["people.family_member.family", "people.family.members"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of henry viii first wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.034389372915029526, -0.03495750203728676, -0.035117559134960175, -0.03853846341371536, -0.03871043771505356, -0.03883599489927292, -0.03915145993232727, -0.04117899760603905, -0.04469364508986473, -0.05457846075296402], "norm_scores": [0.10056255012750626, 0.10050543397665024, 0.10048935562372208, 0.10014618188142776, 0.10012894868850708, 0.10011638700962067, 0.1000848039984703, 0.0998820811510086, 0.0995316430926323, 0.0985526442527771]}}
{"id": "WebQTest-1730", "question": "where does the parana river flow", "prediction": [["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.river.origin"], ["geography.river.cities"], ["geography.river.mouth"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["location.location.primarily_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the parana river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.origin </PATH>", "<PATH> geography.river.cities </PATH>", "<PATH> geography.river.mouth </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.primarily_containedby </PATH>"], "scores": [-0.01962057314813137, -0.022760355845093727, -0.027212118729948997, -0.05680682510137558, -0.07708138227462769, -0.08124009519815445, -0.08739302307367325, -0.13253574073314667, -0.13551194965839386, -0.14721286296844482], "norm_scores": [0.10598067939281464, 0.10564843565225601, 0.10517916828393936, 0.10211203247308731, 0.10006260126829147, 0.09964732825756073, 0.0990360900759697, 0.09466474503278732, 0.09438342601060867, 0.09328548610210419]}}
{"id": "WebQTest-1731", "question": "what is president nixon 's first name", "prediction": [["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.gender", "people.person.gender"], ["people.person.profession"], ["people.person.parents"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types"], ["people.person.children"], ["people.person.nationality", "location.location.containedby"], ["people.person.children", "people.person.children"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is president nixon 's first name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.02335975132882595, -0.03513411432504654, -0.0417834147810936, -0.05345652252435684, -0.055058803409338, -0.05761974677443504, -0.059611909091472626, -0.06169901788234711, -0.06301957368850708, -0.0633344054222107], "norm_scores": [0.10283597558736801, 0.1016322448849678, 0.10095870494842529, 0.09978705644607544, 0.0996273010969162, 0.0993724837899208, 0.09917472302913666, 0.09896794706583023, 0.09883734583854675, 0.0988062247633934]}}
{"id": "WebQTest-1732", "question": "where did theodor schwann get his education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann get his education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.016409240663051605, -0.020343342795968056, -0.02103475295007229, -0.047724757343530655, -0.04895889386534691, -0.06355035305023193, -0.07482942938804626, -0.07857980579137802, -0.0925547331571579, -0.09696058928966522], "norm_scores": [0.10400610417127609, 0.1035977303981781, 0.10352613031864166, 0.1007995679974556, 0.10067524015903473, 0.09921690821647644, 0.09810412675142288, 0.0977368876338005, 0.09638051688671112, 0.09595681726932526]}}
{"id": "WebQTest-1733", "question": "who or what influenced frank lloyd wright", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["people.person.profession"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["architecture.architect.structures_designed"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["architecture.structure.architect"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced frank lloyd wright [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.010004839859902859, -0.013444601558148861, -0.09346519410610199, -0.09392128139734268, -0.09429939091205597, -0.10939007252454758, -0.10975062847137451, -0.11103213578462601, -0.12774598598480225, -0.1319892853498459], "norm_scores": [0.10818272829055786, 0.10781124234199524, 0.09952027350664139, 0.09947489202022552, 0.09943728893995285, 0.09794797748327255, 0.09791266918182373, 0.0977872684597969, 0.0961664617061615, 0.09575925022363663]}}
{"id": "WebQTest-1734", "question": "what year did houston rockets win their first championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.defunct_sports_team.later_known_as"], ["sports.professional_sports_team.owner_s", "sports.sports_championship_event.champion"], ["sports.sports_team_championship_event.champion"], ["sports.professional_sports_team.owner_s"], ["sports.sports_teams.champion"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did houston rockets win their first championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.defunct_sports_team.later_known_as </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team_championship_event.champion </PATH>", "<PATH> sports.professional_sports_team.owner_s </PATH>", "<PATH> sports.sports_teams.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>"], "scores": [-0.006984193809330463, -0.017101677134633064, -0.12400192022323608, -0.12883754074573517, -0.1434633731842041, -0.14785908162593842, -0.1510738730430603, -0.15583530068397522, -0.15776635706424713, -0.16042126715183258], "norm_scores": [0.11171809583902359, 0.11059349030256271, 0.09938100725412369, 0.09890159219503403, 0.09746560454368591, 0.09703811258077621, 0.09672665596008301, 0.09626719355583191, 0.09608147293329239, 0.09582673013210297]}}
{"id": "WebQTest-1735", "question": "where do most of the people live in russia", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["location.country.administrative_divisions"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.languages_spoken"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.first_level_division_of"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do most of the people live in russia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.021937020123004913, -0.026530809700489044, -0.034651052206754684, -0.039261456578969955, -0.04153723642230034, -0.04902206361293793, -0.05879804864525795, -0.060167718678712845, -0.06908274441957474, -0.0875968486070633], "norm_scores": [0.10270973294973373, 0.10223899036645889, 0.10141214728355408, 0.10094566643238068, 0.10071620345115662, 0.0999651700258255, 0.09899267554283142, 0.09885717928409576, 0.09797978401184082, 0.09618246555328369]}}
{"id": "WebQTest-1736", "question": "where is holy roman empire located", "prediction": [["location.location.contains"], ["location.location.containedby"], ["fictional_universe.fictional_setting.languages", "language.human_language.main_country"], ["fictional_universe.fictional_setting.languages", "language.human_language.countries_spoken_in"], ["fictional_universe.fictional_setting.contains"], ["fictional_universe.fictional_setting.languages", "location.country.languages_spoken"], ["location.location.containedby", "location.location.containedby"], ["location.location.contains", "location.location.containedby"], ["fictional_universe.fictional_language.where_spoken", "language.human_language.countries_spoken_in"], ["fictional_universe.fictional_language.where_spoken", "language.human_language.region"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is holy roman empire located [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.main_country </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> fictional_universe.fictional_setting.contains </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> location.country.languages_spoken </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_language.where_spoken <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> fictional_universe.fictional_language.where_spoken <SEP> language.human_language.region </PATH>"], "scores": [-0.018333109095692635, -0.020800042897462845, -0.04296185448765755, -0.046173516660928726, -0.055754464119672775, -0.05983997508883476, -0.06706263870000839, -0.0719892680644989, -0.08357459306716919, -0.08359409868717194], "norm_scores": [0.10371050238609314, 0.10345496237277985, 0.10118743777275085, 0.10086297988891602, 0.0999012291431427, 0.09949391335248947, 0.0987778976559639, 0.09829244762659073, 0.09716026484966278, 0.09715837240219116]}}
{"id": "WebQTest-1737", "question": "what movies has liam hemsworth played in", "prediction": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["people.person.place_of_birth", "film.film.featured_film_locations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has liam hemsworth played in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.honored_for </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.026379283517599106, -0.030278319492936134, -0.03181193768978119, -0.03685545548796654, -0.03689446672797203, -0.036953918635845184, -0.0373782180249691, -0.03972850739955902, -0.04015156254172325, -0.041876617819070816], "norm_scores": [0.10094854980707169, 0.10055571794509888, 0.1004016250371933, 0.09989652037620544, 0.09989261627197266, 0.09988667815923691, 0.0998442992568016, 0.09960991889238358, 0.09956779330968857, 0.09939617663621902]}}
{"id": "WebQTest-1738", "question": "who plays nicholas newman on the young and the restless", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays nicholas newman on the young and the restless [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.021645648404955864, -0.026638492941856384, -0.027296656742691994, -0.036696337163448334, -0.037405747920274734, -0.03826601058244705, -0.038436729460954666, -0.04144049435853958, -0.050135836005210876, -0.05291823670268059], "norm_scores": [0.10155171900987625, 0.10104594379663467, 0.10097946226596832, 0.10003473609685898, 0.09996379911899567, 0.09987784177064896, 0.0998607948422432, 0.09956128895282745, 0.09869931638240814, 0.09842508286237717]}}
{"id": "WebQTest-1739", "question": "who is the voice of eric cartman on south park", "prediction": [["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.special_performance_type", "film.actor.film"], ["tv.regular_tv_appearance.special_performance_type", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["tv.regular_tv_appearance.special_performance_type", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of eric cartman on south park [/INST]", "raw_output": {"paths": ["<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.special_performance_type <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> tv.regular_tv_appearance.special_performance_type <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.01803242787718773, -0.028018081560730934, -0.03055993653833866, -0.04332980513572693, -0.04476713016629219, -0.04493463411927223, -0.04578329622745514, -0.04620197415351868, -0.05252458527684212, -0.05713914707303047], "norm_scores": [0.10232996940612793, 0.10131321847438812, 0.10105602443218231, 0.09977375715970993, 0.0996304526925087, 0.09961376339197159, 0.09952925890684128, 0.09948759526014328, 0.09886056929826736, 0.09840541332960129]}}
{"id": "WebQTest-1740", "question": "who is angelina jolies brother", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.place_of_birth"], ["people.person.children", "people.person.children"], ["people.person.profession", "people.person.profession"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.sibling_relationship.sibling", "people.person.sibling_s"]], "ground_paths": [["people.person.parents", "people.person.parents"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.nationality", "people.person.nationality"], ["film.producer.films_executive_produced", "film.producer.films_executive_produced"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolies brother [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.sibling_relationship.sibling <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.sibling_relationship.sibling <SEP> people.person.sibling_s </PATH>"], "scores": [-0.021403953433036804, -0.03386920690536499, -0.03984300419688225, -0.040899671614170074, -0.04180622845888138, -0.04793257266283035, -0.0482199564576149, -0.05167138949036598, -0.05511390417814255, -0.05601460859179497], "norm_scores": [0.10224718600511551, 0.10098056495189667, 0.10037912428379059, 0.10027310997247696, 0.10018224269151688, 0.0995703712105751, 0.09954176843166351, 0.09919878840446472, 0.09885788708925247, 0.09876888990402222]}}
{"id": "WebQTest-1741", "question": "where is the headquarters of bank of america", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.mailing_address.citytown"], ["organization.organization.headquarters"], ["location.mailing_address.citytown", "location.mailing_address.state_province_region"], ["location.mailing_address.citytown", "location.mailing_address.citytown"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["common.image.appears_in_topic_gallery", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the headquarters of bank of america [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters </PATH>", "<PATH> location.mailing_address.citytown <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> location.mailing_address.citytown <SEP> location.mailing_address.citytown </PATH>"], "scores": [-0.005139118060469627, -0.03632405400276184, -0.050884902477264404, -0.0536344051361084, -0.07402487844228745, -0.07696115970611572, -0.08476360887289047, -0.09050488471984863, -0.09198234230279922, -0.09305737167596817], "norm_scores": [0.10620594769716263, 0.10294502973556519, 0.10145692527294159, 0.10117834806442261, 0.09913616627454758, 0.09884550422430038, 0.09807726740837097, 0.09751579910516739, 0.09737181663513184, 0.09726720303297043]}}
{"id": "WebQTest-1742", "question": "what type of government japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent", "government.form_of_government.countries"], ["language.human_language.main_country", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.010510852560400963, -0.01427682489156723, -0.12947161495685577, -0.13963888585567474, -0.14855776727199554, -0.15133032202720642, -0.1523694396018982, -0.15552622079849243, -0.15599629282951355, -0.1595769226551056], "norm_scores": [0.1115882471203804, 0.11116879433393478, 0.09907280653715134, 0.09807060658931732, 0.09719981998205185, 0.09693069756031036, 0.09683002531528473, 0.09652483463287354, 0.09647946804761887, 0.09613463282585144]}}
{"id": "WebQTest-1744", "question": "what team does josh hamilton play for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does josh hamilton play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.0139518016949296, -0.016764696687459946, -0.029849711805582047, -0.033540770411491394, -0.050894059240818024, -0.05592699721455574, -0.07086566835641861, -0.07824698090553284, -0.0803247019648552, -0.0925976037979126], "norm_scores": [0.1038723811507225, 0.10358061641454697, 0.10223408043384552, 0.10185742378234863, 0.10010512173175812, 0.09960255771875381, 0.09812569618225098, 0.09740405529737473, 0.0972018837928772, 0.09601622819900513]}}
{"id": "WebQTest-1746", "question": "where is burundi country", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.river.basin_countries"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["geography.lake.basin_countries"], ["language.human_language.countries_spoken_in"], ["common.topic.notable_types"], ["book.written_work.subjects"], ["location.country.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is burundi country [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> location.country.containedby </PATH>"], "scores": [-0.003245021915063262, -0.051885273307561874, -0.06726013123989105, -0.07337794452905655, -0.09129765629768372, -0.09518136084079742, -0.112540103495121, -0.12658219039440155, -0.13373474776744843, -0.14181414246559143], "norm_scores": [0.10894004255533218, 0.10376797616481781, 0.10218475759029388, 0.1015615239739418, 0.09975778311491013, 0.09937109798192978, 0.09766102582216263, 0.09629924595355988, 0.09561292082071304, 0.0948435366153717]}}
{"id": "WebQTest-1748", "question": "through which countries of the sahel does the niger river flow", "prediction": [["location.location.partially_containedby"], ["geography.river.basin_countries"], ["location.location.partially_contains"], ["common.topic.notable_types"], ["location.location.containedby"], ["location.country.languages_spoken"], ["government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.location.partially_containedby"], ["geography.river.basin_countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: through which countries of the sahel does the niger river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.016998453065752983, -0.018577473238110542, -0.03132378309965134, -0.03940417990088463, -0.06537172198295593, -0.08621715754270554, -0.08662721514701843, -0.09481780976057053, -0.100218765437603, -0.1066744476556778], "norm_scores": [0.10481970757246017, 0.10465432703495026, 0.10332883894443512, 0.10249725729227066, 0.09986992180347443, 0.09780963510274887, 0.09776952862739563, 0.0969720184803009, 0.09644968807697296, 0.09582904726266861]}}
{"id": "WebQTest-1750", "question": "what city does paul ryan live in", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality", "location.country.capital"], ["people.person.nationality", "location.country.administrative_divisions"], ["people.person.nationality", "location.country.first_level_divisions"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does paul ryan live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.country.capital </PATH>", "<PATH> people.person.nationality <SEP> location.country.administrative_divisions </PATH>", "<PATH> people.person.nationality <SEP> location.country.first_level_divisions </PATH>"], "scores": [-0.020374413579702377, -0.02155866101384163, -0.03487998619675636, -0.03612848371267319, -0.038121532648801804, -0.05244443565607071, -0.06696926057338715, -0.08104705065488815, -0.08208209276199341, -0.08696617931127548], "norm_scores": [0.10318881273269653, 0.10306668281555176, 0.10170280188322067, 0.10157590359449387, 0.10137366503477097, 0.09993204474449158, 0.09849103540182114, 0.09711422026157379, 0.09701375663280487, 0.09654108434915543]}}
{"id": "WebQTest-1751", "question": "what city is acadia university in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["organization.organization.headquarters", "location.mailing_address.citytown"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.contains"], ["organization.organization.headquarters", "location.mailing_address.country"], ["education.educational_institution.newspaper", "book.newspaper.circulation_areas"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is acadia university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.contains </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> education.educational_institution.newspaper <SEP> book.newspaper.circulation_areas </PATH>"], "scores": [-0.006803735159337521, -0.044093161821365356, -0.046993520110845566, -0.05008852109313011, -0.05993151292204857, -0.06297305971384048, -0.0743086114525795, -0.07569761574268341, -0.08278157562017441, -0.08465487509965897], "norm_scores": [0.10531449317932129, 0.10145970433950424, 0.10116586089134216, 0.10085322707891464, 0.09986540675163269, 0.09956211596727371, 0.09843989461660385, 0.09830325841903687, 0.09760934859514236, 0.09742666035890579]}}
{"id": "WebQTest-1752", "question": "what did f. scott fitzgerald do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["book.written_work.author"], ["book.author.works_written"], ["film.film_story_contributor.film_story_credits"], ["freebase.valuenotation.is_reviewed"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["book.book_edition.author_editor"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did f. scott fitzgerald do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> freebase.valuenotation.is_reviewed </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.0005874744383618236, -0.07957325130701065, -0.08045154064893723, -0.09222065657377243, -0.10047102719545364, -0.12209663540124893, -0.12841612100601196, -0.13037410378456116, -0.13055548071861267, -0.1334116905927658], "norm_scores": [0.11034738272428513, 0.10196683555841446, 0.10187732428312302, 0.10068533569574356, 0.09985806792974472, 0.09772175550460815, 0.09710615873336792, 0.09691621363162994, 0.0968986302614212, 0.09662226587533951]}}
{"id": "WebQTest-1753", "question": "what countries does tomtom western europe include", "prediction": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["common.topic.notable_types"], ["location.location.containedby"], ["location.location.contains"], ["common.topic.notable_types", "common.topic.notable_types"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["government.form_of_government.countries", "location.country.form_of_government"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["geography.river.basin_countries", "location.location.partially_contains"]], "ground_paths": [["organization.organization.phone_number", "common.phone_number.service_location"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does tomtom western europe include [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> geography.river.basin_countries <SEP> location.location.partially_containedby </PATH>", "<PATH> geography.river.basin_countries <SEP> location.location.partially_contains </PATH>"], "scores": [-0.009193552657961845, -0.02156977914273739, -0.04449284076690674, -0.057965412735939026, -0.08994235098361969, -0.09205988794565201, -0.10476455092430115, -0.10578127950429916, -0.10628325492143631, -0.10667549818754196], "norm_scores": [0.10661330819129944, 0.10530196875333786, 0.10291559249162674, 0.10153834521770477, 0.09834281355142593, 0.09813479334115982, 0.09689591079950333, 0.09679744392633438, 0.09674886614084244, 0.0967109203338623]}}
{"id": "WebQTest-1754", "question": "what is the national language of iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.language_family"], ["language.human_language.writing_system"], ["language.human_language.country"], ["base.rosetta.languoid.languoid_class"], ["country.languages_spoken.countries_spoken_in"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national language of iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.writing_system </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> country.languages_spoken.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.017644528299570084, -0.021928410977125168, -0.027887186035513878, -0.04181123524904251, -0.15523721277713776, -0.17107725143432617, -0.1747310906648636, -0.17569446563720703, -0.18706540763378143, -0.1893848329782486], "norm_scores": [0.11006356030702591, 0.10959307104349136, 0.10894197225570679, 0.10743556916713715, 0.095915287733078, 0.09440796077251434, 0.09406362473964691, 0.09397304803133011, 0.09291054308414459, 0.09269530326128006]}}
{"id": "WebQTest-1755", "question": "where did the second battle of ypres occur", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["military.military_conflict.casualties"], ["military.military_conflict.casualties", "time.event.locations"], ["geography.river.basin_countries"], ["military.military_command.military_conflict"], ["military.military_conflict.casualties", "military.casualties.location"], ["geography.river.cities"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the second battle of ypres occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> military.military_conflict.casualties </PATH>", "<PATH> military.military_conflict.casualties <SEP> time.event.locations </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> military.military_command.military_conflict </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.location </PATH>", "<PATH> geography.river.cities </PATH>"], "scores": [-0.009449196048080921, -0.02003767155110836, -0.067743681371212, -0.10669819265604019, -0.11084503680467606, -0.11445622146129608, -0.12034643441438675, -0.12646298110485077, -0.12998871505260468, -0.13304302096366882], "norm_scores": [0.10870926827192307, 0.10756427049636841, 0.10255329310894012, 0.09863518923521042, 0.09822700917720795, 0.09787292778491974, 0.09729813784360886, 0.09670481830835342, 0.09636446833610535, 0.09607058763504028]}}
{"id": "WebQTest-1756", "question": "what languages do people speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["base.rosetta.languoid.languoid_class"], ["location.country.currency_formerly_used"], ["location.country.internet_tld"], ["location.country.currency_used"], ["location.country.language_spoken"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do people speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.internet_tld </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.language_spoken </PATH>"], "scores": [-0.0138884661719203, -0.02377927675843239, -0.02682720683515072, -0.06344565004110336, -0.140631765127182, -0.15021803975105286, -0.16564469039440155, -0.17332838475704193, -0.17830221354961395, -0.1854676902294159], "norm_scores": [0.11007140576839447, 0.10898807644844055, 0.1086563915014267, 0.10474953800439835, 0.09696848690509796, 0.09604336321353912, 0.09457310289144516, 0.09384921193122864, 0.09338358789682388, 0.09271684288978577]}}
{"id": "WebQTest-1757", "question": "what did michael crabtree do", "prediction": [["american_football.football_player.games", "american_football.football_player_game_statistics.position_s"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.position"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.role"], ["american_football.football_player.games", "american_football.football_player.position_s"], ["american_football.football_player.receiving", "american_football.football_player_receiving_statistics.position"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.position"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["american_football.football_player.receiving", "american_football.football_player_receiving_statistics.position_s"], ["sports.pro_athlete.teams", "sports.sports_position_held.office_position_or_title"]], "ground_paths": [["common.topic.notable_types"], ["american_football.football_player.position_s"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did michael crabtree do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.football_player_game_statistics.position_s </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.position </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.role </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.football_player.position_s </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.football_player_receiving_statistics.position </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.position </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.football_player_receiving_statistics.position_s </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_position_held.office_position_or_title </PATH>"], "scores": [-0.029700694605708122, -0.03222963958978653, -0.034032464027404785, -0.03700624778866768, -0.04024467244744301, -0.041063398122787476, -0.04691406339406967, -0.04736972227692604, -0.048779405653476715, -0.04890725016593933], "norm_scores": [0.10109604150056839, 0.10084069520235062, 0.10065905749797821, 0.10036016255617142, 0.10003568232059479, 0.09995381534099579, 0.09937072545289993, 0.09932546317577362, 0.0991855338215828, 0.09917286038398743]}}
{"id": "WebQTest-1758", "question": "where do they speak afrikaans", "prediction": [["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.region"], ["language.human_language.language_family"], ["people.ethnicity.languages_spoken"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.dialects"], ["location.country.currency_used"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak afrikaans [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.dialects </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.004594399593770504, -0.04441692307591438, -0.04919838160276413, -0.05428750813007355, -0.09209954738616943, -0.12196879088878632, -0.1298866719007492, -0.1443469077348709, -0.17882950603961945, -0.18354423344135284], "norm_scores": [0.1098613366484642, 0.10557234287261963, 0.10506875813007355, 0.10453540831804276, 0.10065650939941406, 0.09769443422555923, 0.09692395478487015, 0.09553249925374985, 0.09229443222284317, 0.0918603241443634]}}
{"id": "WebQTest-1759", "question": "what 3 states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.contains", "location.location.containedby"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.containedby", "location.location.contains"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.contains", "location.location.contains"], ["location.location.events", "time.event.locations"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.time_zones", "location.location.time_zones"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 3 states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> location.adjoining_relationship.adjoins <SEP> location.adjoining_relationship.adjoins </PATH>", "<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.026708345860242844, -0.03624408319592476, -0.03767077624797821, -0.03899616748094559, -0.039310235530138016, -0.04320128262042999, -0.04422285407781601, -0.045233819633722305, -0.04622935131192207, -0.048432957381010056], "norm_scores": [0.10139957070350647, 0.10043724626302719, 0.10029405355453491, 0.10016121715307236, 0.10012976080179214, 0.0997409075498581, 0.09963907301425934, 0.0995383933186531, 0.0994393453001976, 0.09922046214342117]}}
{"id": "WebQTest-1760", "question": "who plays ziva david on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.languages", "people.person.languages"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ziva david on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_program.languages <SEP> people.person.languages </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.021206848323345184, -0.030872365459799767, -0.03272411599755287, -0.03636213764548302, -0.043768733739852905, -0.04405886307358742, -0.04646385833621025, -0.04703620448708534, -0.04949440807104111, -0.051355089992284775], "norm_scores": [0.10192685574293137, 0.10094642639160156, 0.10075967013835907, 0.10039377212524414, 0.09965293854475021, 0.09962403029203415, 0.09938472509384155, 0.09932786226272583, 0.09908398985862732, 0.09889980405569077]}}
{"id": "WebQTest-1761", "question": "who ruled after king henry viii died", "prediction": [["people.person.parents"], ["people.person.children"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.religion", "people.person.religion"], ["people.person.gender", "people.person.gender"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.religion"], ["people.person.nationality"], ["people.deceased_person.cause_of_death"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who ruled after king henry viii died [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.deceased_person.place_of_burial <SEP> people.deceased_person.place_of_burial </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.religion </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.020894736051559448, -0.0374995581805706, -0.04143265634775162, -0.04516664147377014, -0.049695175141096115, -0.05406885966658592, -0.05870165303349495, -0.06417611241340637, -0.06427913159132004, -0.06495894491672516], "norm_scores": [0.10295288264751434, 0.10125748068094254, 0.10086001455783844, 0.10048409551382065, 0.1000300794839859, 0.09959353506565094, 0.0991332158446312, 0.09859199821949005, 0.09858183562755585, 0.09851484000682831]}}
{"id": "WebQTest-1763", "question": "who are the st louis cardinals coaches", "prediction": [["baseball.baseball_team.current_coaches"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"], ["sports.sports_team.coaches", "baseball.baseball_team.current_manager"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"], ["sports.sports_team.coaches", "baseball.baseball_coach.current_team_head_coached"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_manager"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_state"]], "ground_paths": [["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the st louis cardinals coaches [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> baseball.baseball_team.current_coaches <SEP> baseball.current_coaching_tenure.baseball_coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> baseball.baseball_team.current_manager </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team.coaches <SEP> baseball.baseball_coach.current_team_head_coached </PATH>", "<PATH> baseball.baseball_team.current_coaches <SEP> baseball.baseball_coach.current_team_manager </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> baseball.baseball_team.current_coaches <SEP> baseball.baseball_coach.current_team_state </PATH>"], "scores": [-0.01616899110376835, -0.03342745453119278, -0.03930928558111191, -0.0407402440905571, -0.04205659031867981, -0.0429820790886879, -0.047031570225954056, -0.04791577160358429, -0.04985921457409859, -0.052692119032144547], "norm_scores": [0.10253151506185532, 0.10077716410160065, 0.10018614679574966, 0.10004288703203201, 0.09991128742694855, 0.09981886297464371, 0.09941546618938446, 0.09932760149240494, 0.09913475811481476, 0.09885430335998535]}}
{"id": "WebQTest-1764", "question": "where is daud ibrahim", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["people.deceased_person.place_of_death"], ["location.location.people_born_here"], ["location.location.people_born_here", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.deceased_person.place_of_burial"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is daud ibrahim [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>"], "scores": [-0.016125641763210297, -0.022104980424046516, -0.04019217565655708, -0.04233197122812271, -0.08069231361150742, -0.08251580595970154, -0.08475939184427261, -0.09276299923658371, -0.09399291127920151, -0.10247503221035004], "norm_scores": [0.10504325479269028, 0.10441704094409943, 0.10254541039466858, 0.10232620686292648, 0.09847528487443924, 0.09829587489366531, 0.09807558357715607, 0.09729376435279846, 0.09717416763305664, 0.09635341167449951]}}
{"id": "WebQTest-1765", "question": "what region is turkey considered", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["base.locations.continents.countries_within"], ["location.location.contains_major_portion_of"], ["location.location.primarily_containedby"], ["language.human_language.countries_spoken_in"], ["food.diet_follower.follows_diet"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region is turkey considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.contains_major_portion_of </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> food.diet_follower.follows_diet </PATH>"], "scores": [-0.012447054497897625, -0.027991851791739464, -0.04776331037282944, -0.05520429089665413, -0.06717386096715927, -0.07237352430820465, -0.0978926420211792, -0.10505400598049164, -0.11920321732759476, -0.15300944447517395], "norm_scores": [0.10645245015621185, 0.1048104539513588, 0.10275855660438538, 0.1019967719912529, 0.10078319162130356, 0.10026051104068756, 0.09773432463407516, 0.0970369204878807, 0.0956735908985138, 0.09249328821897507]}}
{"id": "WebQTest-1766", "question": "what language do russians speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.language_family"], ["language.human_language.country"], ["people.ethnicity.languages_spoken"], ["language.human_language.region"], ["language.human_language.dialects"]], "ground_paths": [["people.ethnicity.geographic_distribution", "language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken"], ["people.ethnicity.geographic_distribution", "language.human_language.main_country"], ["people.ethnicity.geographic_distribution", "location.country.languages_spoken"], ["people.ethnicity.geographic_distribution", "location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["people.ethnicity.included_in_group", "people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do russians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> language.human_language.dialects </PATH>"], "scores": [-0.014701557345688343, -0.020232969895005226, -0.028481479734182358, -0.12608467042446136, -0.13319085538387299, -0.14110641181468964, -0.15858836472034454, -0.16366451978683472, -0.17176298797130585, -0.19137994945049286], "norm_scores": [0.11031195521354675, 0.10970345139503479, 0.10880228132009506, 0.0986846312880516, 0.09798584133386612, 0.09721329063177109, 0.09552858024835587, 0.09504489600658417, 0.09427827596664429, 0.09244685620069504]}}
{"id": "WebQTest-1767", "question": "where was martin luther king jr buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"], ["symbols.name_source.namesakes"], ["people.person.place_of_burial"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["influence.influence_node.influenced_by"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.person.place_of_burial </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0032138715032488108, -0.031629692763090134, -0.08661863952875137, -0.09477382898330688, -0.09592136740684509, -0.10260330885648727, -0.11161191016435623, -0.12103362381458282, -0.12461204826831818, -0.1330563724040985], "norm_scores": [0.10903507471084595, 0.10598035901784897, 0.10030993819236755, 0.09949522465467453, 0.09938111901283264, 0.0987192690372467, 0.09783393889665604, 0.09691650420427322, 0.09657031297683716, 0.09575827419757843]}}
{"id": "WebQTest-1768", "question": "what are the official languages of argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.language_family.geographic_distribution"], ["location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["language.language_family.languages"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> language.language_family.languages </PATH>"], "scores": [-0.00826356466859579, -0.025014135986566544, -0.04711952805519104, -0.05853720381855965, -0.1321425586938858, -0.14689409732818604, -0.17106057703495026, -0.17352715134620667, -0.17460501194000244, -0.17897891998291016], "norm_scores": [0.11064962297677994, 0.10881161689758301, 0.10643268376588821, 0.1052243784070015, 0.09775747358798981, 0.09632598608732224, 0.09402602910995483, 0.09379439055919647, 0.09369335323572159, 0.09328443557024002]}}
{"id": "WebQTest-1769", "question": "where is the city wales", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.first_level_division_of"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.contains"], ["location.country.first_level_divisions"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["geography.river.basin_countries"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the city wales [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.018491044640541077, -0.021126627922058105, -0.036413416266441345, -0.04808441549539566, -0.06653957813978195, -0.06724057346582413, -0.07395928353071213, -0.07858774811029434, -0.08150574564933777, -0.11646401137113571], "norm_scores": [0.10428320616483688, 0.10400871932506561, 0.10243085771799088, 0.10124233365058899, 0.09939102828502655, 0.09932137280702591, 0.0986563041806221, 0.0982007309794426, 0.09791459888219833, 0.09455081075429916]}}
{"id": "WebQTest-1770", "question": "what country does rafael nadal play for", "prediction": [["people.person.nationality"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_affiliation.salary"], ["people.person.nationality", "sports.sports_team_location.teams"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does rafael nadal play for [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_affiliation.salary </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>"], "scores": [-0.017631402239203453, -0.021305231377482414, -0.029583042487502098, -0.03904453665018082, -0.04293498769402504, -0.0470687597990036, -0.048494793474674225, -0.05010663717985153, -0.05297242850065231, -0.054324910044670105], "norm_scores": [0.10228949040174484, 0.10191438347101212, 0.10107424110174179, 0.10012243688106537, 0.09973367303609848, 0.09932225197553635, 0.09918071329593658, 0.09902098029851913, 0.09873760491609573, 0.09860415756702423]}}
{"id": "WebQTest-1771", "question": "what is the islamic belief", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["belief.belief.belief_of"], ["base.argumentmaps.intellectual_dispute.positions"], ["symbols.namesake.named_after"], ["religion.deity.deity_of"], ["common.topic.notable_types"], ["people.person.religion"], ["symbols.name_source.namesakes"]], "ground_paths": [["religion.religion.beliefs"], ["religion.belief.belief_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the islamic belief [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> belief.belief.belief_of </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.religion </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.008319783955812454, -0.018836503848433495, -0.10406068712472916, -0.1136886477470398, -0.11787984520196915, -0.12246867269277573, -0.13052406907081604, -0.1352560818195343, -0.14017584919929504, -0.15819382667541504], "norm_scores": [0.11001554876565933, 0.10886460542678833, 0.09997107088565826, 0.09901316463947296, 0.09859905391931534, 0.09814763814210892, 0.09736019372940063, 0.0969005674123764, 0.09642501175403595, 0.09470319002866745]}}
{"id": "WebQTest-1772", "question": "who is the prime minister of spain 2011", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of spain 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.008884268812835217, -0.019329680129885674, -0.029763169586658478, -0.03352167457342148, -0.05281763896346092, -0.07139752060174942, -0.07504376024007797, -0.07646198570728302, -0.08022081106901169, -0.08053058385848999], "norm_scores": [0.10445311665534973, 0.10336773842573166, 0.10229486227035522, 0.10191110521554947, 0.0999634861946106, 0.09812332689762115, 0.09776619076728821, 0.09762763231992722, 0.09726135432720184, 0.09723123162984848]}}
{"id": "WebQTest-1773", "question": "who was the son of king leonidas", "prediction": [["people.person.gender"], ["people.person.children"], ["people.person.parents"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["people.person.gender", "people.person.gender"], ["people.person.gender", "people.person.children", "people.person.children"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.sibling_s", "people.person.sibling_s"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the son of king leonidas [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.gender <SEP> people.person.children <SEP> people.person.children </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> people.sibling_relationship.sibling <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.person.sibling_s <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> people.person.sibling_s <SEP> people.person.sibling_s </PATH>"], "scores": [-0.008928311057388783, -0.030342282727360725, -0.04243089631199837, -0.0664897933602333, -0.0860748291015625, -0.09301043301820755, -0.09968017041683197, -0.10187379270792007, -0.10318829119205475, -0.10965891927480698], "norm_scores": [0.10668088495731354, 0.10442071408033371, 0.10316601395606995, 0.1007135659456253, 0.09876027703285217, 0.0980776846408844, 0.09742570668458939, 0.09721223264932632, 0.09708452224731445, 0.09645836055278778]}}
{"id": "WebQTest-1774", "question": "who was judi dench married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.children", "people.person.parents"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was judi dench married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.021085359156131744, -0.027161791920661926, -0.040155015885829926, -0.04500737413764, -0.04654781147837639, -0.04700425639748573, -0.047165900468826294, -0.04925094544887543, -0.05304853990674019, -0.05368082970380783], "norm_scores": [0.10221140831708908, 0.10159221291542053, 0.10028073936700821, 0.09979531913995743, 0.09964171051979065, 0.09959623962640762, 0.09958013892173767, 0.09937272220849991, 0.09899607300758362, 0.09893348813056946]}}
{"id": "WebQTest-1775", "question": "how many languages do they speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.form_of_government"], ["location.country.currency_formerly_used"], ["location.country.internet_tld"], ["location.country.currency_used"], ["location.country.language_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.internet_tld </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.language_spoken </PATH>"], "scores": [-0.016563037410378456, -0.017514921724796295, -0.032783810049295425, -0.048333991318941116, -0.1440058797597885, -0.14660786092281342, -0.17693111300468445, -0.17921459674835205, -0.18272146582603455, -0.18602333962917328], "norm_scores": [0.10985586792230606, 0.10975135117769241, 0.10808829218149185, 0.10642050206661224, 0.0967109203338623, 0.09645960479974747, 0.09357853978872299, 0.09336510300636292, 0.09303826093673706, 0.09273156523704529]}}
{"id": "WebQTest-1776", "question": "where are the sahara desert located", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"], ["location.location.partially_contains"], ["geography.river.basin_countries"], ["location.location.primarily_containedby"], ["base.locations.continents.countries_within"], ["common.topic.notable_types"], ["location.location.containedby", "location.location.containedby"], ["base.locations.countries.continent"]], "ground_paths": [["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the sahara desert located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.002408320317044854, -0.05372575297951698, -0.07172467559576035, -0.08163284510374069, -0.08915957063436508, -0.10500175505876541, -0.11613011360168457, -0.14215169847011566, -0.14299166202545166, -0.15251243114471436], "norm_scores": [0.10967561602592468, 0.10418932139873505, 0.10233079642057419, 0.10132189095020294, 0.10056213289499283, 0.09898156672716141, 0.09788616746664047, 0.09537187963724136, 0.09529180079698563, 0.09438885003328323]}}
{"id": "WebQTest-1777", "question": "what is rihanna mum called", "prediction": [["people.person.parents"], ["people.person.children"], ["music.artist.origin"], ["people.person.parent"], ["music.artist.genre", "music.artist.genre"], ["music.artist.origin", "people.person.place_of_birth"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["common.topic.notable_types"], ["people.person.profession"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is rihanna mum called [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.origin <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.012645519338548183, -0.015087268315255642, -0.12670449912548065, -0.1447845995426178, -0.15202179551124573, -0.15638454258441925, -0.15855155885219574, -0.16278253495693207, -0.1652868092060089, -0.17074620723724365], "norm_scores": [0.1118689626455307, 0.11159613728523254, 0.09981009364128113, 0.0980217307806015, 0.09731489419937134, 0.09689125418663025, 0.09668151289224625, 0.09627331793308258, 0.0960325226187706, 0.09550967812538147]}}
{"id": "WebQTest-1779", "question": "when did president theodore roosevelt take office", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.us_president.vice_president"], ["business.employment_tenure.person", "business.employment_tenure.start_date"], ["base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.government_position_held.basic_title"], ["book.written_work.subjects", "base.culturalevent.event.entity_involved"], ["government.politician.election_campaigns", "government.election_campaign.election"], ["government.government_position_held.office_holder", "government.political_district.representatives", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did president theodore roosevelt take office [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.start_date </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.basic_title </PATH>", "<PATH> book.written_work.subjects <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.politician.election_campaigns <SEP> government.election_campaign.election </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.political_district.representatives <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>"], "scores": [-0.027888977900147438, -0.03429271653294563, -0.03474651277065277, -0.0353672057390213, -0.04416356608271599, -0.045862793922424316, -0.04927099496126175, -0.04986250773072243, -0.050093188881874084, -0.05050024017691612], "norm_scores": [0.10143870115280151, 0.10079118609428406, 0.10074545443058014, 0.10068294405937195, 0.09980118274688721, 0.09963174909353256, 0.09929275512695312, 0.09923405200242996, 0.09921115636825562, 0.09917078167200089]}}
{"id": "WebQTest-1780", "question": "who does michael oher play for in 2010", "prediction": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael oher play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.016906531527638435, -0.018773281946778297, -0.018944034352898598, -0.028844643384218216, -0.037605542689561844, -0.03906981274485588, -0.04468867555260658, -0.05239972472190857, -0.06141209602355957, -0.08890014886856079], "norm_scores": [0.10239016264677048, 0.10219920426607132, 0.10218175500631332, 0.10117507725954056, 0.10029256343841553, 0.10014582425355911, 0.09958469122648239, 0.09881974756717682, 0.09793314337730408, 0.0952778160572052]}}
{"id": "WebQTest-1781", "question": "who or what influenced albert einstein", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["people.person.profession"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["people.person.gender", "people.person.gender"], ["people.person.nationality"], ["people.person.religion"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced albert einstein [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.religion </PATH>"], "scores": [-0.011334611102938652, -0.01240423135459423, -0.1026102751493454, -0.10501275956630707, -0.11960005015134811, -0.12538939714431763, -0.12661543488502502, -0.14051733911037445, -0.15209200978279114, -0.1642821878194809], "norm_scores": [0.10978545248508453, 0.10966809093952179, 0.10020843893289566, 0.09996797889471054, 0.09852029383182526, 0.09795158356428146, 0.09783156216144562, 0.09648092091083527, 0.09537062793970108, 0.09421510249376297]}}
{"id": "WebQTest-1782", "question": "where does egypt export to", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.statistical_region.places_imported_from", "location.imports_and_exports.imported_from"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["location.location.partially_contains", "location.location.containedby"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.contains", "location.location.contains"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["location.location.partially_contains", "location.location.partially_containedby"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.events", "location.location.events"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does egypt export to [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.imports_and_exports.imported_from <SEP> location.statistical_region.places_imported_from </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.statistical_region.places_imported_from <SEP> location.imports_and_exports.imported_from </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.026350652799010277, -0.029038749635219574, -0.03222323954105377, -0.03663215413689613, -0.03878973424434662, -0.04180213436484337, -0.042607419192790985, -0.04605169594287872, -0.04723555967211723, -0.04732755199074745], "norm_scores": [0.10125068575143814, 0.10097888112068176, 0.10065782815217972, 0.10021501034498215, 0.09999901801347733, 0.09969823807477951, 0.09961798042058945, 0.09927546232938766, 0.09915800392627716, 0.09914888441562653]}}
{"id": "WebQTest-1784", "question": "where is the seat of the african union", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded", "location.country.capital"], ["organization.organization_founder.organizations_founded", "base.locations.continents.countries_within"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization_founder.organizations_founded", "base.locations.countries.continent"], ["organization.organization.geographic_scope", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seat of the african union [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.country.capital </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.locations.continents.countries_within </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.locations.countries.continent </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.location.containedby </PATH>"], "scores": [-0.00976035837084055, -0.03541609272360802, -0.039077453315258026, -0.04357609897851944, -0.049065470695495605, -0.054432738572359085, -0.06492756307125092, -0.06581909954547882, -0.06766664981842041, -0.07698789238929749], "norm_scores": [0.10415767878293991, 0.10151942819356918, 0.10114840418100357, 0.10069440305233002, 0.10014316439628601, 0.09960711747407913, 0.09856721758842468, 0.09847937524318695, 0.0982976034283638, 0.09738560020923615]}}
{"id": "WebQTest-1785", "question": "what was kim richards in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["people.person.languages", "film.film.language"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"]], "ground_paths": [["film.actor.film", "film.performance.film"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was kim richards in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>"], "scores": [-0.01778327114880085, -0.03672497346997261, -0.037614911794662476, -0.03922466188669205, -0.0430070124566555, -0.044364042580127716, -0.0576571561396122, -0.05792121961712837, -0.06876872479915619, -0.06944704055786133], "norm_scores": [0.10297858715057373, 0.1010463535785675, 0.10095646977424622, 0.10079409182071686, 0.10041356831789017, 0.10027739405632019, 0.09895322471857071, 0.0989270955324173, 0.09785977751016617, 0.09779342263936996]}}
{"id": "WebQTest-1788", "question": "what artistic style did henri rousseau work in", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"], ["visual_art.artist.associated_periods_or_movements"], ["film.film_genre.films_in_this_genre"], ["art.art_period_movement.associated_artists"], ["art.visual_artist.associated_periods_or_movements"], ["influence.influence_node.influenced_by"], ["visual_art.visual_artists.associated_periods_or_movements"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic style did henri rousseau work in [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.artist.associated_periods_or_movements </PATH>", "<PATH> film.film_genre.films_in_this_genre </PATH>", "<PATH> art.art_period_movement.associated_artists </PATH>", "<PATH> art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.visual_artists.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.0020567141473293304, -0.03961788862943649, -0.05124252289533615, -0.09425415843725204, -0.10692021995782852, -0.11836180090904236, -0.11845628172159195, -0.12060439586639404, -0.12119156122207642, -0.12191267311573029], "norm_scores": [0.10904195159673691, 0.10502218455076218, 0.10380839556455612, 0.09943810105323792, 0.09818654507398605, 0.09706953912973404, 0.09706037491559982, 0.09685210138559341, 0.09679524600505829, 0.0967254713177681]}}
{"id": "WebQTest-1789", "question": "what is john edwards indicted for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.election_campaign.candidate", "government.election_campaign.offense"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["people.person.profession", "people.profession.specializations"], ["government.election_campaign.candidate", "government.election_campaign.cause"], ["people.person.profession", "people.profession.part_of_professional_field"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["base.activism.activist.area_of_activism", "base.activism.activism_issue.activists"]], "ground_paths": [["base.crime.acquitted_person.acquittal", "base.crime.acquittal.crime_type"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is john edwards indicted for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.offense </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.cause </PATH>", "<PATH> people.person.profession <SEP> people.profession.part_of_professional_field </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activism_issue.activists </PATH>"], "scores": [-0.022487621754407883, -0.025690238922834396, -0.042831942439079285, -0.04584699496626854, -0.051781389862298965, -0.05330866575241089, -0.05339960381388664, -0.058741819113492966, -0.060987818986177444, -0.06137147545814514], "norm_scores": [0.1025388091802597, 0.10221093893051147, 0.1004738062620163, 0.10017132759094238, 0.0995786264538765, 0.09942665696144104, 0.09941761940717697, 0.09888792783021927, 0.09866607934236526, 0.0986282229423523]}}
{"id": "WebQTest-1790", "question": "where was huey newton killed", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death", "base.biblioness.bibs_location.city"], ["military.military_person.participated_in_conflicts", "people.deceased_person.place_of_death"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.place_lived.person", "people.deceased_place.place_of_death"], ["people.deceased_person.place_of_death", "base.biblioness.bibs_location.state"], ["people.deceased_person.place_of_death", "location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was huey newton killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> base.biblioness.bibs_location.city </PATH>", "<PATH> military.military_person.participated_in_conflicts <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.deceased_place.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> base.biblioness.bibs_location.state </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.people_born_here </PATH>"], "scores": [-0.00035490834852680564, -0.07682885974645615, -0.10526323318481445, -0.11347943544387817, -0.11713609099388123, -0.11819879710674286, -0.12340810149908066, -0.12440068274736404, -0.1266598105430603, -0.13145342469215393], "norm_scores": [0.1108098179101944, 0.10265166312456131, 0.09977394342422485, 0.09895753860473633, 0.09859634190797806, 0.0984916239976883, 0.09797988086938858, 0.09788267314434052, 0.09766179323196411, 0.09719476848840714]}}
{"id": "WebQTest-1791", "question": "when was the printing press invented by gutenberg", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["book.book_subject.works"], ["symbols.namesake.named_after"], ["book.inventor.inventions"], ["book.author.book_editions_published"], ["book.written_work.subjects"], ["book.invention.inventor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the printing press invented by gutenberg [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> book.inventor.inventions </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.invention.inventor </PATH>"], "scores": [-0.014841721393167973, -0.02653149701654911, -0.03041253052651882, -0.05246179923415184, -0.06401943415403366, -0.06516437232494354, -0.07734869420528412, -0.08084166795015335, -0.08445398509502411, -0.09351570904254913], "norm_scores": [0.10447601228952408, 0.10326182097196579, 0.10286183655261993, 0.1006186306476593, 0.09946241229772568, 0.09934859722852707, 0.09814545512199402, 0.09780322015285492, 0.09745056182146072, 0.09657148271799088]}}
{"id": "WebQTest-1792", "question": "where is midlothian scotland", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.country.first_level_divisions"], ["location.hud_county_place.county"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["location.country.administrative_divisions"]], "ground_paths": [["location.administrative_division.second_level_division_of"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is midlothian scotland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.00274586770683527, -0.0485042929649353, -0.06255146861076355, -0.07409045100212097, -0.08293066173791885, -0.08985589444637299, -0.09528113901615143, -0.10781494528055191, -0.11150630563497543, -0.11390151083469391], "norm_scores": [0.10785733163356781, 0.1030331626534462, 0.10159595310688019, 0.10043037682771683, 0.09954646229743958, 0.09885946661233902, 0.09832457453012466, 0.09709988534450531, 0.09674212336540222, 0.09651068598031998]}}
{"id": "WebQTest-1793", "question": "where does marta play soccer", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality", "sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does marta play soccer [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.026362931355834007, -0.03318305313587189, -0.03346571326255798, -0.03853306919336319, -0.03932689502835274, -0.03934737294912338, -0.04205856844782829, -0.04206366837024689, -0.044189490377902985, -0.05235956236720085], "norm_scores": [0.10127847641706467, 0.10059009492397308, 0.10056167095899582, 0.10005337744951248, 0.09997397661209106, 0.09997193515300751, 0.09970126301050186, 0.09970074892044067, 0.09948903322219849, 0.09867950528860092]}}
{"id": "WebQTest-1794", "question": "where is mitt romney 's family from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["people.person.parents"], ["people.person.children"], ["people.person.religion"], ["people.family.members"], ["people.person.ethnicity"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.parents", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mitt romney 's family from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.religion </PATH>", "<PATH> people.family.members </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.013536845333874226, -0.023449482396245003, -0.04462707042694092, -0.06833905726671219, -0.06997302174568176, -0.07128791511058807, -0.07336179912090302, -0.09251054376363754, -0.09542287141084671, -0.09727875888347626], "norm_scores": [0.10523825883865356, 0.10420022159814835, 0.10201671719551086, 0.09962614625692368, 0.09946350008249283, 0.09933280199766159, 0.09912700951099396, 0.09724690765142441, 0.09696410596370697, 0.0967843160033226]}}
{"id": "WebQTest-1795", "question": "what are john terry 's children called", "prediction": [["people.person.children"], ["people.person.parents"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.parent"], ["influence.influence_node.influenced_by", "people.person.children"], ["influence.influence_node.influenced", "people.person.children"], ["influence.influence_node.influenced_by", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are john terry 's children called [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.deceased_person.place_of_burial <SEP> people.deceased_person.place_of_burial </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.children </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.children </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.009207806549966335, -0.018476391211152077, -0.1269262284040451, -0.12786653637886047, -0.1384201943874359, -0.14141401648521423, -0.14933837950229645, -0.15280885994434357, -0.15328830480575562, -0.1536683738231659], "norm_scores": [0.11124010384082794, 0.11021382361650467, 0.09888648241758347, 0.09879354387521744, 0.09775638580322266, 0.09746415913105011, 0.09669487178325653, 0.09635987877845764, 0.09631368517875671, 0.09627708792686462]}}
{"id": "WebQTest-1796", "question": "what city was michael jackson born in", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["music.artist.origin"], ["symbols.namesake.named_after"], ["music.artist.origin", "location.location.containedby"], ["symbols.namesake.named_after", "location.location.containedby"], ["symbols.namesake.named_after", "music.artist.origin"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city was michael jackson born in [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> music.artist.origin <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> music.artist.origin </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.01168186217546463, -0.020204205065965652, -0.039288230240345, -0.10078737884759903, -0.10413739830255508, -0.10929720103740692, -0.10985121130943298, -0.1125573068857193, -0.11868151277303696, -0.11927799880504608], "norm_scores": [0.10747165232896805, 0.1065596416592598, 0.10454533249139786, 0.09830959886312485, 0.0979808047413826, 0.0974765419960022, 0.09742255508899689, 0.09715928137302399, 0.09656607359647751, 0.09650848805904388]}}
{"id": "WebQTest-1797", "question": "who fought in the battle of vicksburg", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"], ["military.military_conflict.casualties", "military.military_person.casualties"], ["time.event.locations", "people.person.place_of_birth"], ["military.military_conflict.casualties", "military.casualties.combatant"], ["military.military_command.military_conflict", "military.military_command.military_combatant"], ["military.military_conflict.combatants", "military.military_combatant.military_conflicts"], ["military.military_command.military_conflict", "military.military_command.military_commander"], ["military.military_conflict.force_strengths", "military.military_person.force_deployments"]], "ground_paths": [["base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the battle of vicksburg [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_person.casualties </PATH>", "<PATH> time.event.locations <SEP> people.person.place_of_birth </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.combatant </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_command.military_combatant </PATH>", "<PATH> military.military_conflict.combatants <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_command.military_commander </PATH>", "<PATH> military.military_conflict.force_strengths <SEP> military.military_person.force_deployments </PATH>"], "scores": [-0.016434818506240845, -0.030989332124590874, -0.04525541141629219, -0.045881155878305435, -0.046349190175533295, -0.04940931499004364, -0.05217667669057846, -0.05716468766331673, -0.05837243050336838, -0.05933518707752228], "norm_scores": [0.10300643742084503, 0.10151808708906174, 0.10008010268211365, 0.1000175029039383, 0.09997069835662842, 0.09966524690389633, 0.09938982129096985, 0.09889529645442963, 0.09877592325210571, 0.09868087619543076]}}
{"id": "WebQTest-1799", "question": "what kind of writing is nikolai gogol famous for", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.author.school_or_movement"], ["book.book_edition.author_editor"], ["book.book_subject.works"], ["book.author.book_editions_published"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"], ["book.school_or_movement.associated_authors"], ["influence.influence_node.influenced_by", "book.written_work.author"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of writing is nikolai gogol famous for [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.school_or_movement </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>", "<PATH> book.school_or_movement.associated_authors </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.written_work.author </PATH>"], "scores": [-0.015622472390532494, -0.02405928261578083, -0.030074842274188995, -0.0438450388610363, -0.04880043491721153, -0.06662216037511826, -0.06726373732089996, -0.07743789255619049, -0.08277309685945511, -0.08630620688199997], "norm_scores": [0.10391117632389069, 0.10303818434476852, 0.10242021083831787, 0.10101953893899918, 0.10052017122507095, 0.09874460101127625, 0.09868127107620239, 0.09768236428499222, 0.09716258943080902, 0.0968199223279953]}}
{"id": "WebQTest-1800", "question": "what year was first world series", "prediction": [["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"], ["time.event.locations"], ["sports.sports_team.championships"], ["time.recurring_event.instances"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["sports.sports_championship_event.champion", "time.event.instance_of_recurring_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was first world series [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> time.event.instance_of_recurring_event </PATH>"], "scores": [-0.011846504174172878, -0.018140627071261406, -0.053577911108732224, -0.06032983213663101, -0.08340553194284439, -0.09086170047521591, -0.1026020497083664, -0.10681004077196121, -0.10862426459789276, -0.12336210161447525], "norm_scores": [0.1065487414598465, 0.10588021576404572, 0.10219381749629974, 0.10150613635778427, 0.09919063001871109, 0.09845379739999771, 0.09730467945337296, 0.09689607471227646, 0.09672044962644577, 0.0953054428100586]}}
{"id": "WebQTest-1801", "question": "who plays simon birch", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["people.person.gender", "people.person.gender"], ["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.film", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays simon birch [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.021485839039087296, -0.02813858911395073, -0.04283424839377403, -0.04402163624763489, -0.04573921486735344, -0.045865464955568314, -0.04927647113800049, -0.05190465226769447, -0.05597406253218651, -0.06714337319135666], "norm_scores": [0.10239589214324951, 0.10171693563461304, 0.10023307055234909, 0.1001141220331192, 0.09994231164455414, 0.09992970526218414, 0.09958942234516144, 0.09932802617549896, 0.09892464429140091, 0.09782586991786957]}}
{"id": "WebQTest-1802", "question": "where are the timezones in the usa", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.time_zones.locations_in_this_time_zone"], ["base.locations.countries.continent"], ["base.locations.continents.countries_within"], ["location.location.time_zones", "location.location.time_zones"], ["base.locations.countries.states_provinces_within"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the timezones in the usa [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.time_zones.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> location.location.containedby </PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-2.5080787963815965e-05, -0.1215580478310585, -0.1386374831199646, -0.15180154144763947, -0.17453502118587494, -0.1753782331943512, -0.18148814141750336, -0.18522457778453827, -0.18845273554325104, -0.19434335827827454], "norm_scores": [0.1161307841539383, 0.10284098982810974, 0.10109944641590118, 0.09977728873491287, 0.09753458946943283, 0.09745237976312637, 0.09685876965522766, 0.096497543156147, 0.09618653357028961, 0.09562160074710846]}}
{"id": "WebQTest-1803", "question": "what does gm make", "prediction": [["automotive.company.make_s"], ["automotive.make.parent_company"], ["common.topic.subjects"], ["common.topic.subject_of"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["base.cars_refactor.company.primary_make"], ["business.industry.companies"], ["film.film.production_companies"], ["organization.organization.parent"]], "ground_paths": [["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does gm make [/INST]", "raw_output": {"paths": ["<PATH> automotive.company.make_s </PATH>", "<PATH> automotive.make.parent_company </PATH>", "<PATH> common.topic.subjects </PATH>", "<PATH> common.topic.subject_of </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> base.cars_refactor.company.primary_make </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> film.film.production_companies </PATH>", "<PATH> organization.organization.parent </PATH>"], "scores": [-0.012355117127299309, -0.018145296722650528, -0.07265027612447739, -0.09925246983766556, -0.12065359205007553, -0.1240357756614685, -0.12448029965162277, -0.12656810879707336, -0.1296582818031311, -0.14175331592559814], "norm_scores": [0.10871794819831848, 0.10809027403593063, 0.10235649347305298, 0.09966947883367538, 0.09755910933017731, 0.09722970426082611, 0.09718649089336395, 0.09698379784822464, 0.09668456017971039, 0.0955222025513649]}}
{"id": "WebQTest-1804", "question": "what do people speak in uk", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["base.rosetta.languoid.languoid_class"], ["location.country.first_languages"], ["location.country.main_languages"], ["location.country.internet_tld"], ["location.country.language_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.first_languages </PATH>", "<PATH> location.country.main_languages </PATH>", "<PATH> location.country.internet_tld </PATH>", "<PATH> location.country.language_spoken </PATH>"], "scores": [-0.013304159976541996, -0.02254416234791279, -0.03593043237924576, -0.055828630924224854, -0.15459874272346497, -0.1707005351781845, -0.17755447328090668, -0.19194094836711884, -0.19525329768657684, -0.1958240270614624], "norm_scores": [0.11109696328639984, 0.11007514595985413, 0.10861147195100784, 0.10647166520357132, 0.09645809978246689, 0.09491739422082901, 0.09426906704902649, 0.09292256832122803, 0.0926152840256691, 0.09256244450807571]}}
{"id": "WebQTest-1805", "question": "what do people in jamaica speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["people.ethnicity.languages_spoken"], ["film.film.featured_film_locations"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in jamaica speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011150682345032692, -0.01934291049838066, -0.04496998339891434, -0.06302065402269363, -0.14149634540081024, -0.15622317790985107, -0.17034101486206055, -0.19158872961997986, -0.19323837757110596, -0.19793027639389038], "norm_scores": [0.11109032481908798, 0.1101839616894722, 0.10739614814519882, 0.10547496378421783, 0.09751418977975845, 0.09608864039182663, 0.09474160522222519, 0.09274980425834656, 0.09259692579507828, 0.09216348081827164]}}
{"id": "WebQTest-1806", "question": "what position did lebron james play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position_s"], ["sports.sports_position.sport", "sports.sports_position.sport", "basketball.basketball_player.position_s"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_position.sport", "sports.sports_team.sport", "basketball.basketball_player.position_s"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.position_s"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.position"], ["basketball.basketball_position.players"], ["sports.pro_athlete.teams", "sports.sports_team_roster.position"], ["basketball.basketball_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did lebron james play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_position.sport <SEP> basketball.basketball_player.position_s </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport <SEP> basketball.basketball_player.position_s </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.position_s </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.008236388675868511, -0.03368840739130974, -0.04646414518356323, -0.04958058148622513, -0.052451252937316895, -0.059925831854343414, -0.061542119830846786, -0.061932213604450226, -0.06334326416254044, -0.06391599029302597], "norm_scores": [0.10426147282123566, 0.10164128988981247, 0.10035102069377899, 0.10003875941038132, 0.09975199401378632, 0.09900917112827301, 0.09884927421808243, 0.09881071746349335, 0.09867138415575027, 0.0986148938536644]}}
{"id": "WebQTest-1807", "question": "what was wayne gretzky 's first team", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["ice_hockey.hockey_player.hockey_position", "ice.hockey.hockey_team.captain"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["ice_hockey.hockey_player.hockey_team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was wayne gretzky 's first team [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> ice_hockey.hockey_player.hockey_position <SEP> ice.hockey.hockey_team.captain </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> ice_hockey.hockey_player.hockey_team </PATH>"], "scores": [-0.008184237405657768, -0.020421992987394333, -0.03507060557603836, -0.04561669006943703, -0.05412257835268974, -0.0574214830994606, -0.06300218403339386, -0.06994973123073578, -0.07337730377912521, -0.07437475770711899], "norm_scores": [0.10426197201013565, 0.10299381613731384, 0.10149609297513962, 0.10043133795261383, 0.09958070516586304, 0.09925273060798645, 0.0987003743648529, 0.09801702946424484, 0.09768164157867432, 0.09758425503969193]}}
{"id": "WebQTest-1808", "question": "who is the president of peru now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of peru now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.00399754149839282, -0.03678315505385399, -0.04323975741863251, -0.044406745582818985, -0.05135514959692955, -0.06155122071504593, -0.06349588930606842, -0.06783607602119446, -0.06950952857732773, -0.07087007164955139], "norm_scores": [0.10482428967952728, 0.10144328325986862, 0.10079041123390198, 0.10067286342382431, 0.09997577220201492, 0.09896158427000046, 0.09876933693885803, 0.09834158420562744, 0.09817714244127274, 0.09804366528987885]}}
{"id": "WebQTest-1809", "question": "what kind of guitar did jerry garcia play", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"], ["music.instrument.family"], ["music.musical_group.member", "music.group_membership.role"], ["common.topic.image", "music.group_member.instruments_played"], ["music.music_group.member", "music.group_membership.role"], ["music.group_member.instruments_played", "music.group_member.instruments_played"], ["music.group_membership.group", "music.group_membership.role"], ["common.topic.notable_types"]], "ground_paths": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did jerry garcia play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> common.topic.image <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.music_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.012790520675480366, -0.026478687301278114, -0.033176299184560776, -0.03490130603313446, -0.07976392656564713, -0.10772612690925598, -0.11181657016277313, -0.11592979729175568, -0.1217823475599289, -0.12179730087518692], "norm_scores": [0.10649409145116806, 0.10504632443189621, 0.10434510558843613, 0.10416527092456818, 0.09959541261196136, 0.09684908390045166, 0.09645373374223709, 0.09605781733989716, 0.09549727290868759, 0.09549584984779358]}}
{"id": "WebQTest-1810", "question": "what type of art leonardo da vinci do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"], ["interests.hobby.people_with_this_hobby"], ["influence.influence_node.influenced_by"], ["visual_art.visual_artists.art_forms"], ["visual_art.art_form.artists"], ["film.film.subjects"], ["interests.hobbyist.hobbies"], ["visual_art.visual_artist.visual_art_forms"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art leonardo da vinci do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> interests.hobby.people_with_this_hobby </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.visual_artists.art_forms </PATH>", "<PATH> visual_art.art_form.artists </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> interests.hobbyist.hobbies </PATH>", "<PATH> visual_art.visual_artist.visual_art_forms </PATH>"], "scores": [-0.00022524296946357936, -0.08882173895835876, -0.09940733760595322, -0.11583082377910614, -0.12124492973089218, -0.126270592212677, -0.12697051465511322, -0.13370844721794128, -0.13474991917610168, -0.1385602504014969], "norm_scores": [0.111356720328331, 0.10191532224416733, 0.1008421778678894, 0.09919952601194382, 0.09866389632225037, 0.09816928207874298, 0.09810060262680054, 0.09744182974100113, 0.0973404049873352, 0.09697020798921585]}}
{"id": "WebQTest-1811", "question": "who are the senators of new jersey now", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["base.biblioness.bibs_location.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"]], "ground_paths": [["location.location.containedby", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of new jersey now [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.017927439883351326, -0.018094124272465706, -0.021561644971370697, -0.036693982779979706, -0.03902507945895195, -0.043205372989177704, -0.04499766603112221, -0.050659164786338806, -0.05184045061469078, -0.05351746827363968], "norm_scores": [0.1019933745265007, 0.10197637975215912, 0.10162337869405746, 0.10009714961051941, 0.09986408799886703, 0.09944750368595123, 0.09926941990852356, 0.09870899468660355, 0.09859246760606766, 0.09842725843191147]}}
{"id": "WebQTest-1812", "question": "where was rihanna raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"], ["people.person.place_of_birth"], ["music.album.artist"], ["people.person.nationality", "location.location.containedby"], ["influence.influence_node.influenced_by"], ["location.location.people_born_here", "location.location.containedby"], ["symbols.namesake.named_after"], ["location.location.people_ born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> location.location.people_ born_here </PATH>"], "scores": [-0.019596947357058525, -0.021051183342933655, -0.026088852435350418, -0.09919096529483795, -0.11135276407003403, -0.15660016238689423, -0.17521584033966064, -0.18128128349781036, -0.18454258143901825, -0.1878667026758194], "norm_scores": [0.10989586263895035, 0.10973616689443588, 0.10918473452329636, 0.10148786008358002, 0.10026106238365173, 0.09582561254501343, 0.09405825287103653, 0.09348947554826736, 0.09318507462739944, 0.09287583827972412]}}
{"id": "WebQTest-1813", "question": "what team does messi play for 2011", "prediction": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does messi play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.02513246051967144, -0.025725368410348892, -0.02613564394414425, -0.032695114612579346, -0.03561869263648987, -0.036406759172677994, -0.03781168535351753, -0.038016755133867264, -0.038579825311899185, -0.04209384694695473], "norm_scores": [0.1008710041642189, 0.10081121325492859, 0.10076986253261566, 0.10011103004217148, 0.09981877356767654, 0.09974014014005661, 0.09960011392831802, 0.09957968443632126, 0.0995236411690712, 0.09917452186346054]}}
{"id": "WebQTest-1815", "question": "where does lil wayne go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.educational_institution.subsidiary_or_constituent_schools"], ["music.artist.origin", "location.location.containedby"], ["music.artist.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does lil wayne go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> music.artist.origin <SEP> location.location.containedby </PATH>", "<PATH> music.artist.education <SEP> education.education.institution </PATH>"], "scores": [-0.016337737441062927, -0.016338441520929337, -0.028166066855192184, -0.035865385085344315, -0.04464653506875038, -0.06521981209516525, -0.09869212657213211, -0.11499468237161636, -0.11755714565515518, -0.12185099720954895], "norm_scores": [0.10499922186136246, 0.10499914735555649, 0.10376457870006561, 0.1029687225818634, 0.10206849873065948, 0.09999006986618042, 0.0966985672712326, 0.0951349139213562, 0.09489144384860992, 0.09448486566543579]}}
{"id": "WebQTest-1817", "question": "where did jrr tolkien go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.place_of_birth", "location.location.containedby"], ["business.employment_tenure.person", "business.employer.employees"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jrr tolkien go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.011543632484972477, -0.01954044960439205, -0.03358247131109238, -0.044563524425029755, -0.04494135081768036, -0.05317721515893936, -0.07915971428155899, -0.08261805772781372, -0.08490340411663055, -0.08797711879014969], "norm_scores": [0.1043207049369812, 0.10348980128765106, 0.10204674303531647, 0.1009322926402092, 0.10089416801929474, 0.10006663203239441, 0.09750013053417206, 0.09716352820396423, 0.09694173187017441, 0.09664421528577805]}}
{"id": "WebQTest-1818", "question": "what state is saint louis university in", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.contains"], ["organization.organization.headquarters", "location.mailing_address.country"], ["education.educational_institution.newspaper", "book.newspaper.circulation_areas"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["education.school_newspaper.school", "book.newspaper.circulation_areas"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is saint louis university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> education.educational_institution.newspaper <SEP> book.newspaper.circulation_areas </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> education.school_newspaper.school <SEP> book.newspaper.circulation_areas </PATH>"], "scores": [-0.006018918007612228, -0.034031063318252563, -0.040532488375902176, -0.0557948499917984, -0.05974167585372925, -0.06965388357639313, -0.07036083936691284, -0.08227872103452682, -0.08367827534675598, -0.08855364471673965], "norm_scores": [0.10541567206382751, 0.10250372439622879, 0.10183947533369064, 0.10029695183038712, 0.09990188479423523, 0.09891653060913086, 0.09884662181138992, 0.09767556935548782, 0.09753896296024323, 0.09706458449363708]}}
{"id": "WebQTest-1819", "question": "what religion does canada follow", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical.subjects"], ["book.newspaper.circulation_areas", "book.periodical_subject.periodicals"], ["organization.organization.geographic_scope", "religion.religion.organizations"], ["religion.religion.countries"], ["royalty.kingdom.rulers", "people.person.religion"], ["organization.organization.place_founded", "religion.religion.organizations"], ["religion.religion.statistics"]], "ground_paths": [["royalty.monarch.kingdom", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical_subject.periodicals"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does canada follow [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.subjects </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical_subject.periodicals </PATH>", "<PATH> organization.organization.geographic_scope <SEP> religion.religion.organizations </PATH>", "<PATH> religion.religion.countries </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>", "<PATH> organization.organization.place_founded <SEP> religion.religion.organizations </PATH>", "<PATH> religion.religion.statistics </PATH>"], "scores": [-0.012745077721774578, -0.021889805793762207, -0.02696777507662773, -0.04788123071193695, -0.0796029195189476, -0.08597864210605621, -0.09185024350881577, -0.09721674025058746, -0.103676937520504, -0.11861003190279007], "norm_scores": [0.10568000376224518, 0.10471799224615097, 0.10418758541345596, 0.10203129798173904, 0.0988454818725586, 0.09821727126836777, 0.09764227271080017, 0.09711968153715134, 0.09649429470300674, 0.09506403654813766]}}
{"id": "WebQTest-1821", "question": "which countries have spanish as their main language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["base.rosetta.local_name.locale"], ["location.country.form_of_government"], ["location.country.currency_used"], ["location.country.main_language"], ["location.country.administrative_divisions"], ["base.rosetta.local_name.languoid"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries have spanish as their main language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.main_language </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> base.rosetta.local_name.languoid </PATH>"], "scores": [-0.008827644400298595, -0.02770073339343071, -0.03211970627307892, -0.09496655315160751, -0.11523254960775375, -0.11870049685239792, -0.14253881573677063, -0.14907163381576538, -0.16113731265068054, -0.16509319841861725], "norm_scores": [0.10954342782497406, 0.10749538987874985, 0.10702142864465714, 0.10050245374441147, 0.09848617762327194, 0.09814522415399551, 0.09583327919244766, 0.09520924836397171, 0.09406738728284836, 0.09369600564241409]}}
{"id": "WebQTest-1822", "question": "who is in paul ryan 's family", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.religion"], ["people.person.religion", "people.person.religion"]], "ground_paths": [["people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"], ["people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in paul ryan 's family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.religion </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>"], "scores": [-0.020205853506922722, -0.025419699028134346, -0.034203559160232544, -0.04418191313743591, -0.05779877305030823, -0.05971232429146767, -0.0620432049036026, -0.06777781993150711, -0.07530625909566879, -0.07801738381385803], "norm_scores": [0.10325924307107925, 0.10272226482629776, 0.10182391852140427, 0.10081293433904648, 0.09944948554039001, 0.09925936162471771, 0.09902827441692352, 0.09846200048923492, 0.09772352129220963, 0.09745895117521286]}}
{"id": "WebQTest-1823", "question": "what government does iraq use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["people.person.nationality", "government.form_of_government.countries"], ["base.locations.countries.continent"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["language.human_language.main_country", "government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["government.form_of_government.countries", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does iraq use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>"], "scores": [-0.005956504959613085, -0.02247701585292816, -0.1305995136499405, -0.1579378843307495, -0.16414529085159302, -0.1681206375360489, -0.16929304599761963, -0.17426200211048126, -0.1749439537525177, -0.18023838102817535], "norm_scores": [0.11352796852588654, 0.11166783422231674, 0.1002238541841507, 0.09752100706100464, 0.0969175323843956, 0.09653301537036896, 0.09641990810632706, 0.09594198316335678, 0.09587658196687698, 0.09537031501531601]}}
{"id": "WebQTest-1824", "question": "what type of currency do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"], ["location.country.form_of_government"], ["base.coinsdaily.coin_type.country"], ["finance.currency.countries_used", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> finance.currency.countries_used <PATH>", "</PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>"], "scores": [-0.0035166468005627394, -0.032004207372665405, -0.1309870183467865, -0.14981742203235626, -0.1701224446296692, -0.1785060465335846, -0.1789117306470871, -0.18459923565387726, -0.18537184596061707, -0.18776217103004456], "norm_scores": [0.11440238356590271, 0.1111893281340599, 0.10071064531803131, 0.09883197396993637, 0.09684542566537857, 0.09603690356016159, 0.09599795192480087, 0.09545350074768066, 0.0953797921538353, 0.09515207260847092]}}
{"id": "WebQTest-1825", "question": "what did lincoln steffens wrote", "prediction": [["book.author.works_written"], ["book.written_work.author"], ["book.book_edition.author_editor"], ["influence.influence_node.influenced_by", "book.written_work.author"], ["book.book_subject.works"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["media_common.quotation.author"], ["influence.influence_node.influenced_by", "book.book_edition.author_editor"], ["influence.influence_node.influenced_by", "book.written_work.subjects"]], "ground_paths": [["book.written_work.author"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did lincoln steffens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.written_work.author </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> media_common.quotation.author </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.book_edition.author_editor </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.written_work.subjects </PATH>"], "scores": [-0.012574701569974422, -0.016188744455575943, -0.07288899272680283, -0.10009394586086273, -0.10023016482591629, -0.10797170549631119, -0.11069251596927643, -0.11442821472883224, -0.11558206379413605, -0.11857236176729202], "norm_scores": [0.10763797163963318, 0.1072496697306633, 0.10133777558803558, 0.0986180454492569, 0.09860461950302124, 0.0978442132472992, 0.09757835417985916, 0.09721451252698898, 0.09710241109132767, 0.09681247919797897]}}
{"id": "WebQTest-1826", "question": "what schools did anders celsius attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did anders celsius attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.01849411614239216, -0.019395211711525917, -0.022708909586071968, -0.032296981662511826, -0.04900047555565834, -0.06485482305288315, -0.06706136465072632, -0.08067820966243744, -0.11613326519727707, -0.12318349629640579], "norm_scores": [0.10410463064908981, 0.10401087254285812, 0.1036667749285698, 0.10267756879329681, 0.10097673535346985, 0.09938843548297882, 0.09916937351226807, 0.09782815724611282, 0.0944204106926918, 0.09375707060098648]}}
{"id": "WebQTest-1827", "question": "what can do in new york", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["metropolitan_transit.transit_system.area_served"], ["film.film.featured_film_locations"], ["organization.organization.geographic_scope"], ["base.usnationalparks.us_national_park.state"], ["location.neighborhood.neighborhood_of"], ["transportation.bridge.locale"], ["metropolitan_transit.transit.transit_system.area_served"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.neighborhood.neighborhood_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what can do in new york [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>", "<PATH> location.neighborhood.neighborhood_of </PATH>", "<PATH> transportation.bridge.locale </PATH>", "<PATH> metropolitan_transit.transit.transit_system.area_served </PATH>"], "scores": [-0.013925520703196526, -0.01654495671391487, -0.0342741422355175, -0.09963821619749069, -0.11404664814472198, -0.11758933961391449, -0.11824691295623779, -0.12667129933834076, -0.14825040102005005, -0.15286941826343536], "norm_scores": [0.10822216421365738, 0.10793905705213547, 0.10604224354028702, 0.09933256357908249, 0.09791160374879837, 0.09756533801555634, 0.09750121086835861, 0.09668327122926712, 0.09461928904056549, 0.09418325126171112]}}
{"id": "WebQTest-1828", "question": "what currency does france accept", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["base.coinsdaily.coin_type.country"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does france accept [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>"], "scores": [-0.008372992277145386, -0.021459097042679787, -0.09626970440149307, -0.11537163704633713, -0.12225047498941422, -0.12752211093902588, -0.1288938969373703, -0.12920179963111877, -0.1298360824584961, -0.13152286410331726], "norm_scores": [0.10960317403078079, 0.10817824304103851, 0.10038066655397415, 0.09848139435052872, 0.09780628979206085, 0.09729204326868057, 0.09715867042541504, 0.09712876379489899, 0.09706717729568481, 0.09690357744693756]}}
{"id": "WebQTest-1829", "question": "what type of religion did massachusetts have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["government.governmental_jurisdiction.governing_officials", "government.governmental_jurisdiction.governing_officials"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"], ["religion.religion.states"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.jurisdiction_of_office"], ["religion.religion.organizations"], ["religion.religion.countries"], ["religion.religion.statistical_region"], ["book.written_work.subjects"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion did massachusetts have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.district_represented </PATH>", "<PATH> religion.religion.states </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> religion.religion.organizations </PATH>", "<PATH> religion.religion.countries </PATH>", "<PATH> religion.religion.statistical_region </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.003433697624132037, -0.03851577267050743, -0.0753776952624321, -0.07946662604808807, -0.07976845651865005, -0.09369547665119171, -0.09584935009479523, -0.10038946568965912, -0.10106279700994492, -0.10471543669700623], "norm_scores": [0.10760696977376938, 0.1038973405957222, 0.10013721883296967, 0.09972859919071198, 0.0996984988451004, 0.09831961989402771, 0.09810808300971985, 0.09766366332769394, 0.09759793430566788, 0.0972420945763588]}}
{"id": "WebQTest-1830", "question": "what language do they speak in colombia south america", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.location.containedby"], ["location.location.languages_spoken"], ["base.rosetta.languoid.languoid_class"], ["film.film.featured_film_locations"], ["location.location.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in colombia south america [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.languages_spoken </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> location.location.main_country </PATH>"], "scores": [-0.00827194657176733, -0.018610823899507523, -0.057211171835660934, -0.06299863755702972, -0.14323845505714417, -0.14637869596481323, -0.1580648571252823, -0.1741446852684021, -0.1827363222837448, -0.1828601211309433], "norm_scores": [0.1108500212430954, 0.10970986634492874, 0.1055557057261467, 0.1049465760588646, 0.09685467183589935, 0.09655100107192993, 0.09542925655841827, 0.09390703588724136, 0.09310367703437805, 0.09309215843677521]}}
{"id": "WebQTest-1833", "question": "what kind of government does brazil has", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["people.person.nationality", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["political_form.countries.form_of_government"], ["common.image.appears_in_topic_gallery"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does brazil has [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "</PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> political_form.countries.form_of_government </PATH>", "<PATH> common.image.appears_in_topic_gallery </PATH>"], "scores": [-0.00367571203969419, -0.02926463820040226, -0.16123975813388824, -0.16411350667476654, -0.17210792005062103, -0.17315678298473358, -0.17686326801776886, -0.17973628640174866, -0.1945987343788147, -0.19889116287231445], "norm_scores": [0.11496603488922119, 0.11206149309873581, 0.098206527531147, 0.09792471677064896, 0.0971449762582779, 0.09704313427209854, 0.09668412804603577, 0.09640674293041229, 0.0949845090508461, 0.09457766264677048]}}
{"id": "WebQTest-1835", "question": "what country did gregor mendel live in", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_burial"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["book.written_work.subjects", "book.written_work.subjects"]], "ground_paths": [["people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did gregor mendel live in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.subjects </PATH>"], "scores": [-0.01602308079600334, -0.016157876700162888, -0.04719877615571022, -0.05527679994702339, -0.06085212901234627, -0.06471653282642365, -0.07045979052782059, -0.07726605236530304, -0.09158655256032944, -0.09216733276844025], "norm_scores": [0.10437511652708054, 0.10436104238033295, 0.1011713445186615, 0.10035737603902817, 0.09979940950870514, 0.09941449016332626, 0.09884516149759293, 0.09817468374967575, 0.09677878767251968, 0.09672260284423828]}}
{"id": "WebQTest-1836", "question": "where laura ingalls wilder lived", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.parents", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced", "location.location.people_born_here"], ["people.person.parents", "people.person.place_of_birth"], ["book.written_work.author", "book.book_subject.works"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["book.written_work.author", "book.written_work.subjects"]], "ground_paths": [["people.person.parents", "people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["influence.influence_node.influenced", "location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"], ["book.author.works_written", "fictional_universe.work_of_fiction.setting"], ["people.person.nationality", "location.administrative_division.first_level_division_of"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where laura ingalls wilder lived [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.parents <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> location.location.people_born_here </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>", "<PATH> book.written_work.author <SEP> book.book_subject.works </PATH>", "<PATH> book.book_edition.author_editor <SEP> book.written_work.subjects </PATH>", "<PATH> book.written_work.author <SEP> book.written_work.subjects </PATH>"], "scores": [-0.02526632882654667, -0.028653601184487343, -0.029922740533947945, -0.031515371054410934, -0.033331748098134995, -0.049306489527225494, -0.05636061728000641, -0.06316692382097244, -0.068488210439682, -0.07252665609121323], "norm_scores": [0.10206490010023117, 0.10171975940465927, 0.10159075260162354, 0.10142908245325089, 0.10124500840902328, 0.09964050352573395, 0.09894009679555893, 0.09826897084712982, 0.09774743765592575, 0.0973534882068634]}}
{"id": "WebQTest-1838", "question": "what sports are popular in germany", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["time.event.locations"], ["cricket.cricket_team.country"], ["sports.sports_facility.teams"], ["tv.tv_program.country_of_origin"], ["film.film_subject.films", "film.film.country", "sports.sports_team_location.teams"], ["film.film_subject.films", "film.film.country", "sports.sports_team.location"], ["location.location.events_tournaments_participated_in"], ["sports.sports_teamlocation.teams"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["cricket.cricket_team.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports are popular in germany [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> cricket.cricket_team.country </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> tv.tv_program.country_of_origin </PATH>", "<PATH> film.film_subject.films <SEP> film.film.country <SEP> sports.sports_team_location.teams </PATH>", "<PATH> film.film_subject.films <SEP> film.film.country <SEP> sports.sports_team.location </PATH>", "<PATH> location.location.events_tournaments_participated_in </PATH>", "<PATH> sports.sports_teamlocation.teams </PATH>"], "scores": [-0.01042774599045515, -0.015447466634213924, -0.13666458427906036, -0.14082090556621552, -0.1506647765636444, -0.16047468781471252, -0.16295400261878967, -0.1630219668149948, -0.1678101271390915, -0.17930595576763153], "norm_scores": [0.11235988885164261, 0.1117972880601883, 0.09903468936681747, 0.09862392395734787, 0.09765785187482834, 0.09670452028512955, 0.09646505117416382, 0.09645849466323853, 0.09599773585796356, 0.09490048885345459]}}
{"id": "WebQTest-1839", "question": "what 's my timezone in oklahoma", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.time_Zones"], ["locations.location.time_zones"], ["location.location.time_zonessymbol"], ["location.location.time_zonescommon_in_this_time_zone"], ["location.location.time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's my timezone in oklahoma [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> location.location.time_zonessymbol </PATH>", "<PATH> location.location.time_zonescommon_in_this_time_zone </PATH>", "<PATH> location.location.time_zone </PATH>"], "scores": [-9.432377737539355e-06, -0.16709297895431519, -0.18913088738918304, -0.19022028148174286, -0.19149024784564972, -0.21197889745235443, -0.23048155009746552, -0.23509451746940613, -0.23561924695968628, -0.23619593679904938], "norm_scores": [0.12048614770174026, 0.10194681584835052, 0.09972469508647919, 0.09961611032485962, 0.09948968142271042, 0.09747201949357986, 0.09568510949611664, 0.09524472802877426, 0.09519476443529129, 0.09513988345861435]}}
{"id": "WebQTest-1840", "question": "what do they speak in norway", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["location.country.language_spoken"], ["languages.human_language.countries_spoken_in"], ["language.human_language.language_family"], ["location.country.currency_used"], ["book.written_work.subjects"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.language_spoken </PATH>", "<PATH> languages.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.017171483486890793, -0.018705714493989944, -0.03282656893134117, -0.0501876138150692, -0.15873195230960846, -0.17368179559707642, -0.17432622611522675, -0.1856568306684494, -0.1868051141500473, -0.19293394684791565], "norm_scores": [0.11042539030313492, 0.11025610566139221, 0.1087101399898529, 0.10683910548686981, 0.09584953635931015, 0.09442725777626038, 0.09436643123626709, 0.09330323338508606, 0.09319615364074707, 0.09262672066688538]}}
{"id": "WebQTest-1842", "question": "what currency does singapore use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["location.location.containedby"], ["location.country.currency_used", "location.country.currency_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does singapore use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.currency_used <PATH>"], "scores": [-0.004023393150418997, -0.0318758450448513, -0.13789111375808716, -0.16239573061466217, -0.17856861650943756, -0.18811330199241638, -0.19021038711071014, -0.19050365686416626, -0.1932412087917328, -0.19772793352603912], "norm_scores": [0.1151546984910965, 0.11199161410331726, 0.10072647780179977, 0.09828821569681168, 0.09671138972043991, 0.09579269587993622, 0.09559202939271927, 0.09556400775909424, 0.09530274569988251, 0.09487610310316086]}}
{"id": "WebQTest-1843", "question": "what did the blackfoot indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken", "language.human_language.language_family", "language.human_language.language_family"], ["people.ethnicity.languages_spoken", "language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken", "common.topic.notable_types", "common.topic.notable_types"], ["people.ethnicity.languages_spoken", "language.human_language.region", "language.human_language.region"], ["people.ethnicity.included_in_group", "people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "location.country.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the blackfoot indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.language_family <SEP> language.human_language.language_family </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.region <SEP> language.human_language.region </PATH>", "<PATH> people.ethnicity.included_in_group <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>"], "scores": [-0.0011639727745205164, -0.06714747101068497, -0.06954718381166458, -0.0760476216673851, -0.0779816210269928, -0.07918183505535126, -0.07937571406364441, -0.08356239646673203, -0.08610781282186508, -0.08750224858522415], "norm_scores": [0.10717621445655823, 0.10033262521028519, 0.10009213536977768, 0.09944360703229904, 0.09925147145986557, 0.09913241863250732, 0.09911319613456726, 0.09869910776615143, 0.09844820201396942, 0.09831102192401886]}}
{"id": "WebQTest-1844", "question": "who plays robin hood in prince of thieves", "prediction": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["people.person.gender", "people.person.gender"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film.starring", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["film.performance.film", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film.produced_by"], ["film.performance.film", "film.actor.film"]], "ground_paths": [["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays robin hood in prince of thieves [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>", "<PATH> film.film.language <SEP> people.person.languages </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>"], "scores": [-0.012274456210434437, -0.037192367017269135, -0.04346941038966179, -0.04695911332964897, -0.04964236915111542, -0.054392945021390915, -0.05478333309292793, -0.0643998235464096, -0.06678616255521774, -0.0676761195063591], "norm_scores": [0.1038064956665039, 0.10125181823968887, 0.10061824321746826, 0.10026773065328598, 0.09999904781579971, 0.09952511638402939, 0.09948627650737762, 0.09853415191173553, 0.0982992947101593, 0.0982118546962738]}}
{"id": "WebQTest-1845", "question": "what standard time is texas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"], ["location.location.time_zones", "location.location.time_zones"], ["language.human_language.countries_spoken_in"], ["time.location.time_zones"], ["base.biblioness.bibs_location.state"], ["location.location.time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is texas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> time.location.time_zones </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.location.time_zone </PATH>"], "scores": [-1.6932601283770055e-05, -0.13673460483551025, -0.18598510324954987, -0.18907953798770905, -0.19709372520446777, -0.1976272314786911, -0.21828746795654297, -0.22557729482650757, -0.2256002426147461, -0.22658416628837585], "norm_scores": [0.11948341876268387, 0.10421540588140488, 0.0992070883512497, 0.09890057891607285, 0.09811113774776459, 0.0980588048696518, 0.09605366736650467, 0.09535600244998932, 0.09535381197929382, 0.09526003897190094]}}
{"id": "WebQTest-1847", "question": "where does kurdish people live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"], ["location.country.official_language"], ["people.ethnicity.languages_spoken"], ["location.location.contains"], ["people.ethnicity.included_in_group"], ["language.human_language.countries_spoken_in"], ["location.administrative_division.country"], ["location.country.languages_spoken"], ["location.location.people_ethnicity"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kurdish people live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.location.people_ethnicity </PATH>"], "scores": [-0.0011907569132745266, -0.06103871017694473, -0.11442761868238449, -0.11718110740184784, -0.11744222044944763, -0.1180640235543251, -0.12134091556072235, -0.1236373707652092, -0.12368132174015045, -0.13752047717571259], "norm_scores": [0.11069183796644211, 0.10426149517297745, 0.09884107112884521, 0.09856928884983063, 0.09854355454444885, 0.09848230332136154, 0.09816010296344757, 0.09793494641780853, 0.09793063998222351, 0.09658470004796982]}}
{"id": "WebQTest-1848", "question": "what instrument does louis armstrong", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.musical_group.member", "music.group_membership.role"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_member.instruments_played", "music.group_member.instruments_played"], ["common.topic.image", "music.group_member.instruments_played"], ["music.artist.instruments_played"], ["music.instrument.family"], ["music.track_contribution.role", "music.track_contribution.role"], ["film.film.subjects", "music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument does louis armstrong [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>", "<PATH> common.topic.image <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.artist.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.track_contribution.role <SEP> music.track_contribution.role </PATH>", "<PATH> film.film.subjects <SEP> music.group_member.instruments_played </PATH>"], "scores": [-0.0002590103540569544, -0.07950915396213531, -0.1110512986779213, -0.12291314452886581, -0.12387662380933762, -0.1293894499540329, -0.135065957903862, -0.1355707198381424, -0.13833555579185486, -0.14852948486804962], "norm_scores": [0.11177382618188858, 0.10325764119625092, 0.10005151480436325, 0.09887173026800156, 0.09877650439739227, 0.09823347628116608, 0.09767743945121765, 0.0976281464099884, 0.0973585844039917, 0.09637115895748138]}}
{"id": "WebQTest-1849", "question": "who does alyson stoner play in camp rock", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["film.film.starring", "film.performance.character"], ["music.recording.artist", "music.recording.artist"]], "ground_paths": [["film.film.starring", "film.performance.character"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does alyson stoner play in camp rock [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.film.starring <SEP> film.performance.character </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>"], "scores": [-0.02493826299905777, -0.025065049529075623, -0.025436842814087868, -0.03877986967563629, -0.04269902780652046, -0.04371323809027672, -0.0452035516500473, -0.058857277035713196, -0.0606243796646595, -0.07802599668502808], "norm_scores": [0.10194455832242966, 0.101931631565094, 0.10189374536275864, 0.10054320096969604, 0.10014992207288742, 0.10004840046167374, 0.09989941120147705, 0.09854468703269958, 0.09837069362401962, 0.09667369723320007]}}
{"id": "WebQTest-1850", "question": "what religion does madonna practice", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"], ["religion.religion.practices"], ["symbols.namesake.named_after"], ["religion.religion.founding_figures"], ["influence.influence_node.influenced_by", "people.person.religion"], ["music.artist.origin", "people.person.religion"], ["religion.religious_text.religious_text_of"], ["religion.notable_person.religion"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does madonna practice [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.practices </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> religion.religion.founding_figures </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.religion </PATH>", "<PATH> music.artist.origin <SEP> people.person.religion </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> religion.notable_person.religion </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0026481077075004578, -0.04042113199830055, -0.10304983705282211, -0.11321496218442917, -0.12164479494094849, -0.12327040731906891, -0.12349085509777069, -0.13073697686195374, -0.1361633688211441, -0.13759948313236237], "norm_scores": [0.11047731339931488, 0.1063820868730545, 0.09992386400699615, 0.0989132672548294, 0.09808295220136642, 0.0979236289858818, 0.09790204465389252, 0.09719520062208176, 0.09666921198368073, 0.0965304896235466]}}
{"id": "WebQTest-1851", "question": "what countries are located near egypt", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.events", "time.event.locations"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["location.location.partially_contains", "location.location.partially_contains"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_containedby"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["location.location.partially_contains", "location.location.containedby"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["government.form_of_government.countries", "location.country.form_of_government"], ["time.event.locations", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.contains", "location.location.containedby"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["location.location.partially_containedby", "location.location.contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.contains_major_portion_of", "location.location.containedby"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.contains", "location.location.contains"], ["location.location.time_zones", "location.location.time_zones"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.events", "location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are located near egypt [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.events <SEP> time.event.locations </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.location.events <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03747996315360069, -0.0375525988638401, -0.03803011029958725, -0.042873315513134, -0.0440501943230629, -0.047856803983449936, -0.04801904037594795, -0.05131831020116806, -0.0522770993411541, -0.052665334194898605], "norm_scores": [0.10077454149723053, 0.10076722502708435, 0.10071911662817001, 0.10023248940706253, 0.10011459141969681, 0.09973422437906265, 0.09971804916858673, 0.09938959032297134, 0.09929434210062027, 0.09925580024719238]}}
{"id": "WebQTest-1853", "question": "what made richard hammond famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["tv.tv_personality.tv_regular_appearances", "tv.tv_regular_personal_appearance.appearance_type"], ["film.film_crewmember.films_crewed", "film.film_job.films_with_this_crew_job"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["influence.influence_node.influenced_by", "people.person.profession"], ["common.topic.notable_types", "rdf-schema#domain"], ["common.topic.notable_types", "type.type.properties"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what made richard hammond famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> tv.tv_personality.tv_regular_appearances <SEP> tv.tv_regular_personal_appearance.appearance_type </PATH>", "<PATH> film.film_crewmember.films_crewed <SEP> film.film_job.films_with_this_crew_job </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> common.topic.notable_types <SEP> rdf-schema#domain </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.properties </PATH>"], "scores": [-0.0017573003424331546, -0.04842730611562729, -0.10381974279880524, -0.11362039297819138, -0.11591854691505432, -0.11850665509700775, -0.11928872019052505, -0.129197895526886, -0.13192467391490936, -0.13434235751628876], "norm_scores": [0.1104154959321022, 0.10538080334663391, 0.09970223158597946, 0.09872986376285553, 0.0985032245516777, 0.09824860841035843, 0.09817180782556534, 0.0972038060426712, 0.09693911671638489, 0.09670503437519073]}}
{"id": "WebQTest-1854", "question": "who originally voiced lois griffin", "prediction": [["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["people.person.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced lois griffin [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>"], "scores": [-0.027503984048962593, -0.028127728030085564, -0.032266609370708466, -0.0336565226316452, -0.0385274775326252, -0.04072904586791992, -0.040811117738485336, -0.04451229050755501, -0.049744341522455215, -0.05076741799712181], "norm_scores": [0.10111921280622482, 0.10105616599321365, 0.10063876211643219, 0.1004989817738533, 0.10001064836978912, 0.09979070723056793, 0.09978251904249191, 0.09941388666629791, 0.0988951101899147, 0.09879398345947266]}}
{"id": "WebQTest-1855", "question": "what airport do you fly into in paris", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"], ["common.topic.notable_types"], ["film.film_location.featured_in_films", "film.film.featured_film_locations"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["travel.transportation.mode_of_transportation"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into in paris [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> film.film_location.featured_in_films <SEP> film.film.featured_film_locations </PATH>", "<PATH> film.film.featured_film_locations <SEP> film.film.featured_film_locations </PATH>", "<PATH> travel.transportation.mode_of_transportation </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.006812980864197016, -0.024972636252641678, -0.05920620635151863, -0.07077471911907196, -0.09076429158449173, -0.09873224049806595, -0.11312500387430191, -0.11358941346406937, -0.11588665097951889, -0.11798600852489471], "norm_scores": [0.10764292627573013, 0.10570580512285233, 0.10214835405349731, 0.1009734645485878, 0.09897509217262268, 0.0981895923614502, 0.0967864990234375, 0.09674155712127686, 0.0965195745229721, 0.09631715714931488]}}
{"id": "WebQTest-1857", "question": "what rihanna new album called 2012", "prediction": [["music.album.artist"], ["music.artist.album"], ["music.recording.artist"], ["music.composition.composer"], ["music.composition.lyricist"], ["music.composer.compositions"], ["music.featured_artist.albums"], ["music.producer.releases_produced"], ["music.lyricist.album"], ["music.music_video.artist"]], "ground_paths": [["music.artist.album"], ["music.album.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rihanna new album called 2012 [/INST]", "raw_output": {"paths": ["<PATH> music.album.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.featured_artist.albums </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> music.lyricist.album </PATH>", "<PATH> music.music_video.artist </PATH>"], "scores": [-0.009758242405951023, -0.02178080379962921, -0.056125473231077194, -0.0679296925663948, -0.07852344214916229, -0.08448164910078049, -0.0971028208732605, -0.10090859979391098, -0.105924591422081, -0.11602726578712463], "norm_scores": [0.10655855387449265, 0.10528510808944702, 0.10173051804304123, 0.10053673386573792, 0.09947729110717773, 0.09888635575771332, 0.09764613211154938, 0.09727521985769272, 0.09678851068019867, 0.09581560641527176]}}
{"id": "WebQTest-1858", "question": "where does dave ramsey live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.place_of_birth"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.children", "people.person.place_of_birth"], ["people.person.children", "people.deceased_person.place_of_death"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does dave ramsey live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.children <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.02536369487643242, -0.026747556403279305, -0.027563007548451424, -0.030682599171996117, -0.04010362550616264, -0.04374737665057182, -0.0794701799750328, -0.08376745134592056, -0.09630928933620453, -0.09720584005117416], "norm_scores": [0.10297539830207825, 0.10283299535512924, 0.10274916887283325, 0.10242912918329239, 0.10146867483854294, 0.10109962522983551, 0.09755180776119232, 0.09713350236415863, 0.09592287987470627, 0.09583690762519836]}}
{"id": "WebQTest-1860", "question": "what discovery did galileo make", "prediction": [["astronomy.astronomer.astronomical_objects_discovered"], ["law.invention.inventor"], ["symbols.namesake.named_after"], ["astronomy.astronomical_discovery.discoverer"], ["base.argumentmaps.innovator.original_ideas"], ["law.inventor.inventions"], ["symbols.name_source.namesakes"], ["base.argumentmaps.original_idea.innovator"], ["astronomy.astronomical_discovery.astronomer"], ["astronomy.astronomer.astronomical_objects_observed"]], "ground_paths": [["astronomy.astronomical_discovery.discoverer"], ["astronomy.astronomer.astronomical_objects_discovered"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what discovery did galileo make [/INST]", "raw_output": {"paths": ["<PATH> astronomy.astronomer.astronomical_objects_discovered </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> astronomy.astronomical_discovery.discoverer </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> astronomy.astronomical_discovery.astronomer </PATH>", "<PATH> astronomy.astronomer.astronomical_objects_observed </PATH>"], "scores": [-0.012625230476260185, -0.035775184631347656, -0.03963956609368324, -0.048885367810726166, -0.05727234110236168, -0.061924759298563004, -0.06380479037761688, -0.07588152587413788, -0.07825582474470139, -0.0863066241145134], "norm_scores": [0.10441313683986664, 0.10202373564243317, 0.10163024067878723, 0.1006949171423912, 0.09985392540693283, 0.0993904396891594, 0.09920375049114227, 0.09801290929317474, 0.09778046607971191, 0.09699642658233643]}}
{"id": "WebQTest-1861", "question": "what is the government system of saudi arabia", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["location.location.contains", "location.country.form_of_government"], ["government.governmental_jurisdiction.government"], ["location.location.contains", "government.form_of_government.countries"], ["language.human_language.main_country", "government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["government.government.government_for"], ["government.form_of_government.countries", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of saudi arabia [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> location.location.contains <SEP> location.country.form_of_government </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>", "<PATH> location.location.contains <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> government.government.government_for </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.010758791118860245, -0.012742164544761181, -0.0960782989859581, -0.11314903199672699, -0.12092036753892899, -0.13556179404258728, -0.14016427099704742, -0.14295372366905212, -0.1479804962873459, -0.15027013421058655], "norm_scores": [0.1099671795964241, 0.10974929481744766, 0.1009739488363266, 0.09926486760377884, 0.0984964445233345, 0.09706480801105499, 0.09661910682916641, 0.09634996950626373, 0.0958668515086174, 0.09564760327339172]}}
{"id": "WebQTest-1864", "question": "who plays the role of tony dinozzo on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the role of tony dinozzo on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.018048090860247612, -0.030509863048791885, -0.031224507838487625, -0.03309663012623787, -0.041651345789432526, -0.041956860572099686, -0.043992869555950165, -0.04856831580400467, -0.0486864298582077, -0.04873867705464363], "norm_scores": [0.10207654535770416, 0.1008123829960823, 0.10074035823345184, 0.10055194050073624, 0.09969541430473328, 0.09966496378183365, 0.09946224838495255, 0.09900821000337601, 0.09899651259183884, 0.09899134188890457]}}
{"id": "WebQTest-1865", "question": "what did jesus do for living", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["base.activism.activist.area_of_activism"], ["fictional_universe.fictional_character.occupation"], ["symbols.namesake.named_after"], ["common.topic.image"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["symbols.name_source.namesakes"], ["base.activism.activism_issue.activists"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesus do for living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>"], "scores": [-0.0010066676186397672, -0.07644874602556229, -0.07820788770914078, -0.09634434431791306, -0.10098583996295929, -0.1105337142944336, -0.1147056594491005, -0.11988678574562073, -0.12071987241506577, -0.12282630801200867], "norm_scores": [0.10969554632902145, 0.10172434896230698, 0.1015455573797226, 0.09972048550844193, 0.09925870597362518, 0.09831550717353821, 0.097906194627285, 0.0974002406001091, 0.09731912612915039, 0.09711434692144394]}}
{"id": "WebQTest-1866", "question": "what is the state name of new york city", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children", "location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby", "location.location.containedby"], ["base.biblioness.bibs_location.state"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_children"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.containedby", "base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.country", "location.location.containedby"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state name of new york city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.containedby <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.country <SEP> location.location.containedby </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.newspaper.circulation_areas </PATH>"], "scores": [-0.011250984854996204, -0.04773187264800072, -0.05067943409085274, -0.050727199763059616, -0.05275837704539299, -0.060605376958847046, -0.060626495629549026, -0.0610632449388504, -0.06125260889530182, -0.0672057643532753], "norm_scores": [0.1041879802942276, 0.10045561194419861, 0.10015995055437088, 0.10015516728162766, 0.09995193034410477, 0.09917067736387253, 0.09916858375072479, 0.09912528097629547, 0.09910652041435242, 0.09851827472448349]}}
{"id": "WebQTest-1868", "question": "who plays knox overstreet", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays knox overstreet [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.02716117911040783, -0.03349672257900238, -0.03691872954368591, -0.03767441585659981, -0.03882856294512749, -0.03915870562195778, -0.04024671018123627, -0.04069366306066513, -0.04638451710343361, -0.04723886772990227], "norm_scores": [0.10116714239120483, 0.10052821785211563, 0.10018480569124222, 0.10010912269353867, 0.09999365359544754, 0.09996064007282257, 0.09985193610191345, 0.09980732202529907, 0.09924094378948212, 0.09915619343519211]}}
{"id": "WebQTest-1870", "question": "what movies did david carradine play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["people.person.languages", "film.film.language"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["people.person.languages", "film.film.language"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["film.film.edited_by"], ["people.person.nationality", "film.film.country"], ["film.director.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.producer.films_executive_produced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did david carradine play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.016202161088585854, -0.0345379002392292, -0.034775618463754654, -0.036419421434402466, -0.03697296604514122, -0.0386766716837883, -0.06324473023414612, -0.06506200134754181, -0.06544134020805359, -0.06759466230869293], "norm_scores": [0.1029987633228302, 0.10112740844488144, 0.10110337287187576, 0.10093731433153152, 0.10088145732879639, 0.10070973634719849, 0.0982656329870224, 0.09808722883462906, 0.09805002063512802, 0.0978391170501709]}}
{"id": "WebQTest-1871", "question": "what is the stanley cup named after", "prediction": [["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team_owner.teams_owned"], ["sports.sports_championship_event.championship", "sports.sports_team_owner.teams_owned"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship", "sports.sports_team.previously_known_as"], ["sports.sports_team.championships", "sports.sports_team_owner.teams_owned"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.previously_known_as"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the stanley cup named after [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team_owner.teams_owned </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team_owner.teams_owned </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.previously_known_as </PATH>", "<PATH> sports.sports_team.championships <SEP> sports.sports_team_owner.teams_owned </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.014783692546188831, -0.017044447362422943, -0.047545842826366425, -0.05571719631552696, -0.06066417694091797, -0.06167565658688545, -0.06322844326496124, -0.0637969970703125, -0.06688843667507172, -0.07106773555278778], "norm_scores": [0.10379765927791595, 0.10356326401233673, 0.10045213252305984, 0.09963463991880417, 0.09914297610521317, 0.09904274344444275, 0.0988890752196312, 0.0988328605890274, 0.09852779656648636, 0.09811687469482422]}}
{"id": "WebQTest-1873", "question": "where is traditional chinese used", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.administrative_divisions"], ["language.human_language.region"], ["food.beer_country_region.beers_from_here"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.language_family"], ["language.human_language.dialects"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is traditional chinese used [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> food.beer_country_region.beers_from_here </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.dialects </PATH>"], "scores": [-0.018319271504878998, -0.025419801473617554, -0.027959387749433517, -0.039725665003061295, -0.12293020635843277, -0.1288173794746399, -0.154840350151062, -0.16257616877555847, -0.16683417558670044, -0.1753969043493271], "norm_scores": [0.10854354500770569, 0.10777555406093597, 0.10750219970941544, 0.10624471306800842, 0.09776244312524796, 0.09718858450651169, 0.09469208121299744, 0.09396238625049591, 0.09356313943862915, 0.09276540577411652]}}
{"id": "WebQTest-1874", "question": "who does portugal trade with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["people.person.nationality", "location.location.people_born_here"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["government.form_of_government.countries", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.location.time_zones", "location.location.time_zones"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.country.form_of_government", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["location.country.form_of_government", "location.country.form_of_government"], ["book.written_work.subjects", "book.written_work.subjects"], ["organization.organization.founders", "organization.organization.founders"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["military.military_combatant.belongs_to_combatant_group", "military.military_combatant.includes_allies"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does portugal trade with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> location.imports_and_exports.imported_from <SEP> location.statistical_region.places_imported_from </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.statistical_region.places_imported_from </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>"], "scores": [-0.029103491455316544, -0.030780835077166557, -0.03389960527420044, -0.034557562321424484, -0.03798632323741913, -0.03972269594669342, -0.04307123273611069, -0.043646570295095444, -0.044783130288124084, -0.04912573844194412], "norm_scores": [0.10095906257629395, 0.1007898598909378, 0.10047601163387299, 0.10040991753339767, 0.10006623715162277, 0.09989263117313385, 0.09955869615077972, 0.09950143098831177, 0.09938841313123703, 0.09895773231983185]}}
{"id": "WebQTest-1875", "question": "what type of political system is brazil", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["people.person.nationality", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["geography.river.basin_countries", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of political system is brazil [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <PATH>", "</PATH> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> geography.river.basin_countries <SEP> location.country.form_of_government </PATH>"], "scores": [-0.0047148000448942184, -0.02533579058945179, -0.15883544087409973, -0.16525311768054962, -0.17144644260406494, -0.17215389013290405, -0.17658619582653046, -0.17714737355709076, -0.17776130139827728, -0.17798544466495514], "norm_scores": [0.11433261632919312, 0.11199910193681717, 0.09800232946872711, 0.09737539291381836, 0.09677419066429138, 0.09670574963092804, 0.09627806395292282, 0.09622405469417572, 0.0961650013923645, 0.0961434468626976]}}
{"id": "WebQTest-1876", "question": "what language did the ancient babylonians speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.person.nationality", "people.ethnicity.people", "people.ethnicity.languages_spoken"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"], ["base.culturalevent.event.entity_involved", "location.location.events", "language.human_language.countries_spoken_in"], ["language.language_family.languages"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the ancient babylonians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> people.ethnicity.people <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> language.language_family.languages </PATH>"], "scores": [-0.007837282493710518, -0.029910791665315628, -0.03569315746426582, -0.057177044451236725, -0.07417866587638855, -0.09939989447593689, -0.1005222424864769, -0.10169322788715363, -0.10189608484506607, -0.10525387525558472], "norm_scores": [0.10649432986974716, 0.10416939109563828, 0.10356876999139786, 0.10136745125055313, 0.09965860098600388, 0.09717652201652527, 0.09706752747297287, 0.09695392102003098, 0.0969342589378357, 0.09660931676626205]}}
{"id": "WebQTest-1877", "question": "what kind of painter was wassily kandinsky", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["book.author.school_or_movement"], ["visual_art.art_period_movement.associated_artists", "visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"], ["book.school_or_movement.associated_authors"], ["book.school_or_movement.associated_authors", "book.school_or_movement.associated_authors"], ["people.person.profession"], ["book.author.school_or_movement", "book.school_or_movement.associated_authors"], ["influence.influence_node.influenced_by", "visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of painter was wassily kandinsky [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> book.author.school_or_movement </PATH>", "<PATH> visual_art.art_period_movement.associated_artists <SEP> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> book.school_or_movement.associated_authors </PATH>", "<PATH> book.school_or_movement.associated_authors <SEP> book.school_or_movement.associated_authors </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> book.author.school_or_movement <SEP> book.school_or_movement.associated_authors </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.003317688126116991, -0.03773517161607742, -0.059504032135009766, -0.06864103674888611, -0.07099223136901855, -0.07619635760784149, -0.08658067882061005, -0.08728360384702682, -0.0918404683470726, -0.09214454889297485], "norm_scores": [0.10658218711614609, 0.10297629982233047, 0.10075884312391281, 0.09984240680932999, 0.0996079370379448, 0.09909090399742126, 0.09806723892688751, 0.09799832850694656, 0.09755278378725052, 0.09752313047647476]}}
{"id": "WebQTest-1878", "question": "what form of government does the united states use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["form.form_of_government.countries"], ["base.locations.countries.form_of_government"], ["locations.country.form_of_government"], ["politicals.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["book.written_work.subjects"], ["organization.organization.founders"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>", "<PATH> locations.country.form_of_government </PATH>", "<PATH> politicals.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.00705904234200716, -0.01820000447332859, -0.14866720139980316, -0.14869388937950134, -0.16111385822296143, -0.16445355117321014, -0.1675354689359665, -0.16996565461158752, -0.17034590244293213, -0.17080208659172058], "norm_scores": [0.11317174136638641, 0.1119178980588913, 0.09822868555784225, 0.09822607040405273, 0.09701365232467651, 0.09669018536806107, 0.09639265388250351, 0.0961586982011795, 0.0961221382021904, 0.0960782989859581]}}
{"id": "WebQTest-1879", "question": "what money does cuba use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_used", "location.country.currency_used"], ["location.country.currency_used", "location.country.currency_used"], ["location.country.currency_formerly_used"], ["common.topic.notable_types"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does cuba use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.012113319709897041, -0.01524705532938242, -0.1414434015750885, -0.144215926527977, -0.1611807644367218, -0.16386233270168304, -0.17029307782649994, -0.1718088537454605, -0.1755674034357071, -0.17575062811374664], "norm_scores": [0.1126522421836853, 0.11229977756738663, 0.0989857167005539, 0.09871165454387665, 0.09705115109682083, 0.09679124504327774, 0.0961708053946495, 0.09602514654397964, 0.09566490352153778, 0.09564737975597382]}}
{"id": "WebQTest-1880", "question": "who is my representative in illinois house", "prediction": [["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.governmental_body.members", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_bodies", "people.person.nationality"]], "ground_paths": [["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.governmental_body.members", "government.government_position_held.office_holder"], ["government.governmental_body.members", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my representative in illinois house [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_body.members <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_bodies <SEP> people.person.nationality </PATH>"], "scores": [-0.02257058583199978, -0.03028690069913864, -0.0309144277125597, -0.03130970150232315, -0.03261406347155571, -0.03593180701136589, -0.037459783256053925, -0.03849976137280464, -0.042233359068632126, -0.044691577553749084], "norm_scores": [0.1012134775519371, 0.10043548792600632, 0.10037248581647873, 0.10033281147480011, 0.1002020314335823, 0.09987014532089233, 0.09971766173839569, 0.09961400181055069, 0.09924277663230896, 0.09899912029504776]}}
{"id": "WebQTest-1881", "question": "what timezone is sweden gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["language.human_language.countries_spoken_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["base.locations.countries.continent"], ["common.topic.notable_types"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0029867766425013542, -0.032546233385801315, -0.1175323873758316, -0.1309211105108261, -0.13326826691627502, -0.1335102915763855, -0.14030681550502777, -0.1563289612531662, -0.16062676906585693, -0.16272534430027008], "norm_scores": [0.11193080991506577, 0.10867061465978622, 0.09981667995452881, 0.09848916530609131, 0.09825827181339264, 0.09823449701070786, 0.09756910800933838, 0.09601829200983047, 0.09560651332139969, 0.09540607780218124]}}
{"id": "WebQTest-1882", "question": "what was franklin d roosevelt 's job before president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "base.aareas.schema.administrative_area.administrative_parent", "government.governmental_jurisdiction.government_positions"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "government.governmental_jurisdiction.government_positions"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "base.onephylogeny.type_of_thing.things_of_this_type"], ["common.topic.subject_of", "people.person.nationality", "government.governmental_jurisdiction.government_positions"], ["common.topic.subjects", "people.person.nationality", "government.governmental_jurisdiction.government_positions"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "base.aareas.schema.administrative_area.administrative_parent", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "base.aareas.schema.administrative_area.administrative_parent", "government.government_position_held.office_position_or_title"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "base.aareas.schema.administrative_area.administrative_parent", "government.governmental_jurisdiction.government_positions"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "government.governmental_jurisdiction.government_positions"]], "ground_paths": [["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.appointed_by", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin d roosevelt 's job before president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> base.aareas.schema.administrative_area.administrative_parent <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> common.topic.subject_of <SEP> people.person.nationality <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> common.topic.subjects <SEP> people.person.nationality <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> base.aareas.schema.administrative_area.administrative_parent <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> base.aareas.schema.administrative_area.administrative_parent <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> base.aareas.schema.administrative_area.administrative_parent <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> government.governmental_jurisdiction.government_positions </PATH>"], "scores": [-0.01896410807967186, -0.02525273896753788, -0.02765030600130558, -0.029930826276540756, -0.03476176783442497, -0.03533898666501045, -0.03603431209921837, -0.036921530961990356, -0.03708908334374428, -0.037678610533475876], "norm_scores": [0.1013064831495285, 0.10067140311002731, 0.10043032467365265, 0.1002015545964241, 0.09971864521503448, 0.09966111183166504, 0.09959183633327484, 0.0995035171508789, 0.09948685020208359, 0.0994282141327858]}}
{"id": "WebQTest-1883", "question": "where did robert downey jr go to jail", "prediction": [["people.person.nationality", "location.location.containedby"], ["base.prison.imprisonment.prisoner", "base.prison.prison.prisoners"], ["people.person.languages", "location.country.official_language"], ["base.prison.prisoner.imprisoned", "base.prison.prison.prisoners"], ["people.person.languages", "location.country.languages_spoken"], ["people.person.languages", "location.location.containedby"], ["people.person.languages", "language.human_language.countries_spoken_in"], ["base.prison.imprisonment.prisoner", "base.prison.imprison.prisoners"], ["base.prison.imprisonment.prisoner", "base.prison.imprison.prison"], ["people.person.nationality", "base.prison.prison.prisoners"]], "ground_paths": [["celebrities.rehab.celebrity", "celebrities.rehab.rehab_facility"], ["base.prison.imprisonment.prisoner", "base.prison.imprisonment.prison"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert downey jr go to jail [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> base.prison.imprisonment.prisoner <SEP> base.prison.prison.prisoners </PATH>", "<PATH> people.person.languages <SEP> location.country.official_language </PATH>", "<PATH> base.prison.prisoner.imprisoned <SEP> base.prison.prison.prisoners </PATH>", "<PATH> people.person.languages <SEP> location.country.languages_spoken </PATH>", "<PATH> people.person.languages <SEP> location.location.containedby </PATH>", "<PATH> people.person.languages <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> base.prison.imprisonment.prisoner <SEP> base.prison.imprison.prisoners </PATH>", "<PATH> base.prison.imprisonment.prisoner <SEP> base.prison.imprison.prison </PATH>", "<PATH> people.person.nationality <SEP> base.prison.prison.prisoners </PATH>"], "scores": [-0.00694669596850872, -0.027264771983027458, -0.0485314205288887, -0.05671684816479683, -0.06702127307653427, -0.07258216291666031, -0.07374772429466248, -0.07565277814865112, -0.07695005089044571, -0.08488555252552032], "norm_scores": [0.10531678050756454, 0.10319853574037552, 0.10102702677249908, 0.10020344704389572, 0.09917621314525604, 0.09862623363733292, 0.09851134568452835, 0.09832385927438736, 0.09819638729095459, 0.09742023050785065]}}
{"id": "WebQTest-1884", "question": "what degrees did barack obama get", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["book.author.book_editions_published", "book.book_edition.place_of_publication"], ["education.education.student", "education.education.major_field_of_study"], ["common.topic.notable_types"], ["people.person.education", "education.education.major_field_of_study"], ["book.written_work.author", "education.education.degree"], ["book.author.book_editions_published", "book.written_work.subjects"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did barack obama get [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> book.author.book_editions_published <SEP> book.book_edition.place_of_publication </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> book.written_work.author <SEP> education.education.degree </PATH>", "<PATH> book.author.book_editions_published <SEP> book.written_work.subjects </PATH>"], "scores": [-0.011101104319095612, -0.019734028726816177, -0.027149273082613945, -0.0947972908616066, -0.103118896484375, -0.1041337326169014, -0.10559295862913132, -0.10792580991983414, -0.10928808897733688, -0.10968535393476486], "norm_scores": [0.10696792602539062, 0.10604844987392426, 0.10526499152183533, 0.09837953746318817, 0.09756427258253098, 0.09746530652046204, 0.09732317924499512, 0.09709640592336655, 0.09696422517299652, 0.09692570567131042]}}
{"id": "WebQTest-1885", "question": "what does god shiva represent", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["common.topic.notable_types"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["common.topic.notable_types", "common.topic.notable_types"], ["common.topic.image"], [], ["religion.religion.deities", "religion.religion.deities"], ["base.saints.saint.venerated_in"]], "ground_paths": [["religion.deity.deity_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does god shiva represent [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> </PATH>", "<PATH> religion.religion.deities <SEP> religion.religion.deities </PATH>", "<PATH> base.saints.saint.venerated_in </PATH>"], "scores": [-0.010786419734358788, -0.025616871193051338, -0.05326477810740471, -0.05427328497171402, -0.07839660346508026, -0.09125053882598877, -0.10236120223999023, -0.11061708629131317, -0.11421862244606018, -0.12639330327510834], "norm_scores": [0.10674074292182922, 0.10516941547393799, 0.10230152308940887, 0.10219840705394745, 0.09976254403591156, 0.09848840534687042, 0.09740018844604492, 0.09659937769174576, 0.09625209122896194, 0.09508736431598663]}}
{"id": "WebQTest-1886", "question": "what inventions were made by ben franklin", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["law.inventor.us_patents"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by"], ["law.us_patent.inventor"], ["base.argumentmaps.original_idea.innovator"], ["symbols.name_source.namesakes"], ["book.written_work.author"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions were made by ben franklin [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> law.us_patent.inventor </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.007684469688683748, -0.02096523530781269, -0.07281907647848129, -0.08594999462366104, -0.08835174888372421, -0.12280569970607758, -0.1331709623336792, -0.13499213755130768, -0.13791100680828094, -0.14310118556022644], "norm_scores": [0.10897989571094513, 0.10754212737083435, 0.10210777074098587, 0.10077576339244843, 0.10053401440382004, 0.0971292182803154, 0.09612764418125153, 0.09595274180173874, 0.0956730768084526, 0.0951777994632721]}}
{"id": "WebQTest-1887", "question": "what is the name of the currency used in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country"], ["language.human_language.countries_spoken_in"], ["common.topic.notable_types"], ["government.form_of_government.countries", "location.country.currency_used"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.currency_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.004797094035893679, -0.025733333081007004, -0.14462310075759888, -0.1489456444978714, -0.15746888518333435, -0.15756437182426453, -0.16229130327701569, -0.16297084093093872, -0.16453799605369568, -0.16564828157424927], "norm_scores": [0.11308305710554123, 0.11074013262987137, 0.09832679480314255, 0.09790270030498505, 0.09707178920507431, 0.09706252068281174, 0.09660479426383972, 0.09653916954994202, 0.09638799726963043, 0.096281036734581]}}
{"id": "WebQTest-1888", "question": "what year did the milwaukee brewers go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.championship"], ["sports.sports_league_season.champion"], ["sports.mascot.team"], ["sports.mports_championship_event.champion"], ["time.event.locations"], ["sports.mports_team.championships"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the milwaukee brewers go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_league_season.champion </PATH>", "<PATH> sports.mascot.team </PATH>", "<PATH> sports.mports_championship_event.champion </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> sports.mports_team.championships </PATH>"], "scores": [-0.005492842756211758, -0.021689163520932198, -0.04782703518867493, -0.0851379781961441, -0.09799753874540329, -0.11178445816040039, -0.11859195679426193, -0.13049188256263733, -0.1441868394613266, -0.15757496654987335], "norm_scores": [0.10891220718622208, 0.10716243833303452, 0.1043977364897728, 0.1005743220448494, 0.09928926080465317, 0.09792976081371307, 0.09726537019014359, 0.09611477702856064, 0.0948074609041214, 0.09354662895202637]}}
{"id": "WebQTest-1889", "question": "who was vice president with bill clinton", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.gender", "people.person.gender"], ["people.person.profession", "common.topic.notable_types"], ["people.person.religion", "people.person.religion"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.ethnicity", "people.ethnicity.people"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president with bill clinton [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.ethnicity <SEP> people.ethnicity.people </PATH>"], "scores": [-0.01847093179821968, -0.02179691009223461, -0.03230268508195877, -0.037386443465948105, -0.053507253527641296, -0.058973971754312515, -0.0704994946718216, -0.07395028322935104, -0.07526500523090363, -0.08089020848274231], "norm_scores": [0.10341589897871017, 0.10307250916957855, 0.1019953191280365, 0.10147811472415924, 0.09985532611608505, 0.09931094199419022, 0.09817289561033249, 0.09783469885587692, 0.09770616143941879, 0.09715808928012848]}}
{"id": "WebQTest-1890", "question": "what songs did franz liszt compose", "prediction": [["music.composition.composer"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["music.composer.compositions"], ["music.recording.artist"], ["music.artist.genre"], ["book.author.works_written"], ["music.album.artist"], ["film.film.music"], ["common.topic.notable_types"]], "ground_paths": [["music.composition.composer"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> film.film.music </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0030893755611032248, -0.061421215534210205, -0.06149109825491905, -0.06760424375534058, -0.07505293190479279, -0.10727488249540329, -0.10829278081655502, -0.1192517951130867, -0.12168220430612564, -0.1234765574336052], "norm_scores": [0.10844843834638596, 0.1023034155368805, 0.10229625552892685, 0.10167282074689865, 0.10091829299926758, 0.09771834313869476, 0.09761893004179001, 0.09655497223138809, 0.09632057696580887, 0.09614790230989456]}}
{"id": "WebQTest-1891", "question": "where did the russian japanese war happen", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["book.written_work.subjects"], ["military.military_conflict.casualties"], ["military.military_command.military_conflict", "time.event.locations"], ["military.military_conflict.combatants", "time.event.locations"], ["military.military_conflict.casualties", "time.event.locations"], ["event.disaster.areas_affected"], ["language.human_language.main_country", "language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the russian japanese war happen [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> military.military_conflict.casualties </PATH>", "<PATH> military.military_command.military_conflict <SEP> time.event.locations </PATH>", "<PATH> military.military_conflict.combatants <SEP> time.event.locations </PATH>", "<PATH> military.military_conflict.casualties <SEP> time.event.locations </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> language.human_language.main_country <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011344288475811481, -0.01843063160777092, -0.060862861573696136, -0.07450427860021591, -0.09859580546617508, -0.10242613404989243, -0.10405567288398743, -0.10514045506715775, -0.10669918358325958, -0.10915444046258926], "norm_scores": [0.10694494843482971, 0.1061897724866867, 0.10177817195653915, 0.1003992035984993, 0.09800933301448822, 0.09763464331626892, 0.09747567772865295, 0.09736999869346619, 0.0972183346748352, 0.09697993099689484]}}
{"id": "WebQTest-1892", "question": "what year did the la kings win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_club.championships"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_teams.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["sports.sports_honor.champion"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the la kings win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_club.championships </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_teams.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_honor.champion </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.008206157945096493, -0.016044560819864273, -0.11809023469686508, -0.1287553459405899, -0.15893618762493134, -0.1590564250946045, -0.16158141195774078, -0.16343912482261658, -0.17351457476615906, -0.17658473551273346], "norm_scores": [0.11234301328659058, 0.11146587133407593, 0.10065237432718277, 0.09958460927009583, 0.09662395715713501, 0.096612349152565, 0.096368707716465, 0.09618985652923584, 0.09522556513547897, 0.0949336513876915]}}
{"id": "WebQTest-1893", "question": "what kind of money does egypt have", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["finance.currency.countries_used", "location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["language.human_language.countries_spoken_in", "location.country.currency_used"], ["finance.currency.countries_used", "common.topic.notable_types", "common.topic.notable_types"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money does egypt have [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.002619110280647874, -0.04152689874172211, -0.08463731408119202, -0.09756285697221756, -0.09915165603160858, -0.11919816583395004, -0.12183777242898941, -0.13079951703548431, -0.13132746517658234, -0.13282881677150726], "norm_scores": [0.10971037298440933, 0.10552376508712769, 0.10107125341892242, 0.09977325797080994, 0.09961485862731934, 0.09763781726360321, 0.09738042950630188, 0.09651163220405579, 0.09646068513393402, 0.09631597995758057]}}
{"id": "WebQTest-1894", "question": "what baseball team was jackie robinson on", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team.sport"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["baseball.baseball_player.lifetime_batting_statistics", "baseball.lifetime_batting_statistics.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_team.championships"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what baseball team was jackie robinson on [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team.sport </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> baseball.baseball_player.lifetime_batting_statistics <SEP> baseball.lifetime_batting_statistics.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.014599000103771687, -0.014612296596169472, -0.03208453953266144, -0.035837553441524506, -0.06182215362787247, -0.06413090229034424, -0.06818375736474991, -0.07059089839458466, -0.07091683149337769, -0.0768822506070137], "norm_scores": [0.1036761999130249, 0.10367482155561447, 0.10187911987304688, 0.10149747878313065, 0.09889408946037292, 0.09866602718830109, 0.09826695919036865, 0.0980306938290596, 0.09799875319004059, 0.09741588681936264]}}
{"id": "WebQTest-1895", "question": "what body of water does st lawrence river flow into", "prediction": [["geography.river.mouth"], ["geography.body_of_water.mouth"], ["location.location.containedby", "geography.lake.basin_countries"], ["location.location.containedby", "location.location.containedby"], ["geography.river.mouth", "geography.river.mouth"], ["location.location.containedby"], ["geography.lake.basin_countries"], ["location.location.partially_containedby"], ["geography.body_of_water.bridges", "geography.river.mouth"], ["location.location.containedby", "location.location.contains"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what body of water does st lawrence river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> geography.body_of_water.mouth </PATH>", "<PATH> location.location.containedby <SEP> geography.lake.basin_countries </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> geography.river.mouth <SEP> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> geography.body_of_water.bridges <SEP> geography.river.mouth </PATH>", "<PATH> location.location.containedby <SEP> location.location.contains </PATH>"], "scores": [-0.0027853045612573624, -0.05134489759802818, -0.0709260031580925, -0.07880918681621552, -0.08148680627346039, -0.08193683624267578, -0.08245296776294708, -0.08838412165641785, -0.0912407636642456, -0.09496571868658066], "norm_scores": [0.10717642307281494, 0.102096326649189, 0.10011661052703857, 0.09933047741651535, 0.09906485676765442, 0.09902029484510422, 0.09896919876337051, 0.09838392585515976, 0.09810328483581543, 0.09773854166269302]}}
{"id": "WebQTest-1896", "question": "what form of government does the united states have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.form_of_government"], ["form.form_of_government.countries"], ["locations.country.form_of_government"], ["politicals.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["book.written_work.subjects"], ["organization.organization.founders"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> locations.country.form_of_government </PATH>", "<PATH> politicals.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.00723967095836997, -0.017851607874035835, -0.14721782505512238, -0.14837612211704254, -0.1615571677684784, -0.1636163592338562, -0.1679082214832306, -0.1687707155942917, -0.16982495784759521, -0.17028763890266418], "norm_scores": [0.11310506612062454, 0.11191114783287048, 0.09833096712827682, 0.09821713715791702, 0.09693102538585663, 0.09673162549734116, 0.096317358314991, 0.09623432159423828, 0.09613292664289474, 0.0960884541273117]}}
{"id": "WebQTest-1897", "question": "what are the names of ariel 's six sisters", "prediction": [["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.sibling_relationship.sibling", "fictional_universe.sibling_relationship.sibling"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of ariel 's six sisters [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> fictional_universe.sibling_relationship_of_fictional_characters.siblings <SEP> fictional_universe.sibling_relationship_of_fictional_characters.siblings </PATH>", "<PATH> fictional_universe.sibling_relationship_of_fictional_characters.siblings <SEP> fictional_universe.fictional_character.siblings </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.sibling_relationship.sibling <SEP> fictional_universe.sibling_relationship.sibling </PATH>", "<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.fictional_character.siblings </PATH>", "<PATH> fictional_universe.fictional_character.children <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.sibling_relationship_of_fictional_characters.siblings </PATH>"], "scores": [-0.016569219529628754, -0.021437563002109528, -0.027912259101867676, -0.03297317773103714, -0.034656744450330734, -0.03610626235604286, -0.04277092218399048, -0.04312654957175255, -0.044476304203271866, -0.04531983286142349], "norm_scores": [0.10180821269750595, 0.10131377726793289, 0.10065992176532745, 0.10015177726745605, 0.09998330473899841, 0.09983848035335541, 0.09917530417442322, 0.09914004802703857, 0.09900631755590439, 0.09892284125089645]}}
{"id": "WebQTest-1898", "question": "what voices does seth macfarlane play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["fictional_universe.fictional_universe.works_set_here", "fictional_universe.fictional_universe.characters"], ["fictional_universe.fictional_universe.works_set_here", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["fictional_universe.fictional_character.character_created_by", "fictional_universe.fictional_character.character_created_by"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what voices does seth macfarlane play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_universe.characters </PATH>", "<PATH> tv.tv_program_creator.programs_created <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here <SEP> fictional_universe.fictional_universe.characters </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> fictional_universe.fictional_character.character_created_by </PATH>"], "scores": [-0.01904407888650894, -0.021600261330604553, -0.02311788871884346, -0.024644244462251663, -0.026516515761613846, -0.03065396659076214, -0.04059261456131935, -0.045500386506319046, -0.05244791507720947, -0.05551100894808769], "norm_scores": [0.10149481147527695, 0.10123570263385773, 0.10108218342065811, 0.10092800855636597, 0.10073922574520111, 0.10032328218221664, 0.09933114796876907, 0.09884484112262726, 0.09816049784421921, 0.09786029160022736]}}
{"id": "WebQTest-1899", "question": "who was william mckinley married to", "prediction": [["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["people.person.parents", "people.person.parents"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["people.person.parents", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_ownership.owner"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.person.profession", "common.topic.notable_types"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.marriage.spouse"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was william mckinley married to [/INST]", "raw_output": {"paths": ["<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_owner.pets_owned </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_ownership.owner </PATH>", "<PATH> base.famouspets.pet_owner.pets_owned <SEP> base.famouspets.pet_owner.pets_owned </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> base.famouspets.pet_owner.pets_owned <SEP> base.famouspets.pet_ownership.owner </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.03758969530463219, -0.03826960548758507, -0.03837195783853531, -0.03943878412246704, -0.03944826498627663, -0.04046075418591499, -0.04193166643381119, -0.04305482655763626, -0.04324594885110855, -0.043927207589149475], "norm_scores": [0.10029861330986023, 0.10023044794797897, 0.1002201959490776, 0.10011333227157593, 0.10011237859725952, 0.10001106560230255, 0.09986406564712524, 0.09975197166204453, 0.099732905626297, 0.09966497868299484]}}
{"id": "WebQTest-1900", "question": "who played darth vader in the first movie", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader in the first movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.019611475989222527, -0.01973159983754158, -0.024079428985714912, -0.02819686383008957, -0.05264132842421532, -0.08337210863828659, -0.08866971731185913, -0.0892106145620346, -0.09331079572439194, -0.09467429667711258], "norm_scores": [0.10400103032588959, 0.10398853570222855, 0.10353739559650421, 0.10311195999383926, 0.10062199831008911, 0.09757684171199799, 0.09706127643585205, 0.09700879454612732, 0.0966118574142456, 0.09648021310567856]}}
{"id": "WebQTest-1901", "question": "where did kevin james go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.parent_institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin james go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>"], "scores": [-0.01323768775910139, -0.017517656087875366, -0.02708701230585575, -0.04407019168138504, -0.06554529815912247, -0.0792926549911499, -0.11601260304450989, -0.12108137458562851, -0.1302340030670166, -0.13367317616939545], "norm_scores": [0.10623661428689957, 0.1057829037308693, 0.10477545112371445, 0.10301105678081512, 0.10082246363162994, 0.0994459018111229, 0.09586049616336823, 0.09537582844495773, 0.0945068746805191, 0.09418240189552307]}}
{"id": "WebQTest-1902", "question": "what year was hitler was born", "prediction": [["people.person.place_of_birth"], ["military.military_person.participated_in_conflicts"], ["base.activism.activist.area_of_activism", "time.event.locations"], ["base.activism.activist.area_of_activism"], ["military.military_person.participated_in_conflicts", "time.event.locations"], ["organization.organization_founder.organizations_founded"], ["base.activism.activism_issue.activists"], ["military.military_commander.military_commands", "military.military_command.military_commander"], ["base.activism.activist.area_of_activism", "location.location.people_born_here"], ["military.military_person.participated_in_conflicts", "time.event.included_in_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was hitler was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> time.event.locations </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> military.military_person.participated_in_conflicts <SEP> time.event.locations </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_command.military_commander </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> location.location.people_born_here </PATH>", "<PATH> military.military_person.participated_in_conflicts <SEP> time.event.included_in_event </PATH>"], "scores": [-0.011347664520144463, -0.03727281838655472, -0.0441402830183506, -0.055061545222997665, -0.058311544358730316, -0.05883639678359032, -0.06081608310341835, -0.06508073955774307, -0.06543572247028351, -0.06672612577676773], "norm_scores": [0.10416651517152786, 0.10150068253278732, 0.10080602765083313, 0.09971108287572861, 0.09938754886388779, 0.09933540225028992, 0.09913893789052963, 0.09871704876422882, 0.09868200868368149, 0.09855475276708603]}}
{"id": "WebQTest-1904", "question": "what 's there to see in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"], ["geography.lake.cities"], ["transportation.bridge.locale"], ["olympics.olympic_host_city.olympics_hosted"], ["geography.lake.lake_type"], ["film.film_festival.location"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's there to see in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> geography.lake.cities </PATH>", "<PATH> transportation.bridge.locale </PATH>", "<PATH> olympics.olympic_host_city.olympics_hosted </PATH>", "<PATH> geography.lake.lake_type </PATH>", "<PATH> film.film_festival.location </PATH>"], "scores": [-0.01591126061975956, -0.017175178974866867, -0.023656295612454414, -0.10881424695253372, -0.11869402974843979, -0.12896879017353058, -0.13175667822360992, -0.13470569252967834, -0.13870887458324432, -0.13915960490703583], "norm_scores": [0.10816872864961624, 0.10803210735321045, 0.1073341965675354, 0.09857220947742462, 0.09760313481092453, 0.09660542011260986, 0.0963364690542221, 0.09605278074741364, 0.09566903859376907, 0.09562592208385468]}}
{"id": "WebQTest-1905", "question": "where was auschwitz concentration camp located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["time.event.locations"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.locations.continents.countries_within"], ["book.written_work.subjects"], ["location.location.events"], ["base.biblioness.bibs_location.country"], ["film.film.featured_film_locations"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was auschwitz concentration camp located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.0019881015177816153, -0.06518581509590149, -0.0883680209517479, -0.09062423557043076, -0.09473743289709091, -0.09999953210353851, -0.10084466636180878, -0.10172020643949509, -0.10569119453430176, -0.10982545465230942], "norm_scores": [0.10870201885700226, 0.10204487293958664, 0.09970645606517792, 0.0994817465543747, 0.09907340258359909, 0.09855343401432037, 0.09847018122673035, 0.09838401526212692, 0.09799410402774811, 0.09758980572223663]}}
{"id": "WebQTest-1907", "question": "what kind of currency does dominican republic use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"], ["location.location.containedby"], ["location.country.currency_formerly_used"], ["travel.tourist_attraction.near_travel_destination"], ["location.country. currency_used"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does dominican republic use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.country. currency_used </PATH>", "<PATH> location.country.currency_used <PATH>", "</PATH> location.country.currency_used </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-8.065791917033494e-05, -0.10574605315923691, -0.1313871592283249, -0.17786623537540436, -0.17864730954170227, -0.1834903508424759, -0.18463291227817535, -0.18522219359874725, -0.19333983957767487, -0.1970636546611786], "norm_scores": [0.1164059266448021, 0.10473340004682541, 0.10208205878734589, 0.09744595736265182, 0.09736987203359604, 0.09689944237470627, 0.09678879380226135, 0.0967317670583725, 0.09594972431659698, 0.09559309482574463]}}
{"id": "WebQTest-1909", "question": "what is spoken in czech republic", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["film.film.country"], ["location.country.currency_used"], ["film.film.featured_film_locations"], ["location.country.form_of_government"], ["location.country.language_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is spoken in czech republic [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> film.film.country </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.language_spoken </PATH>"], "scores": [-0.012289569713175297, -0.018311405554413795, -0.044204406440258026, -0.07158935815095901, -0.15587861835956573, -0.17462469637393951, -0.17548377811908722, -0.18255119025707245, -0.18328092992305756, -0.1872788369655609], "norm_scores": [0.11115340888500214, 0.11048606783151627, 0.10766197741031647, 0.10475366562604904, 0.09628593176603317, 0.09449776262044907, 0.09441661834716797, 0.0937516838312149, 0.09368330240249634, 0.09330950677394867]}}
{"id": "WebQTest-1910", "question": "where was shakespeare born at", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was shakespeare born at [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.012588424608111382, -0.014162595383822918, -0.0858415961265564, -0.10168792307376862, -0.1023283302783966, -0.11203707009553909, -0.11725401133298874, -0.11975683271884918, -0.12450958043336868, -0.13138143718242645], "norm_scores": [0.10818811506032944, 0.1080179437994957, 0.10054630786180496, 0.09896557778120041, 0.09890221804380417, 0.0979466438293457, 0.09743700176477432, 0.09719343483448029, 0.09673259407281876, 0.09607014805078506]}}
{"id": "WebQTest-1911", "question": "where did scott fitzgerald go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.educational_institution.subsidiary_or_constituent_schools"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.education", "education.educational_institution.parent_institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did scott fitzgerald go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.subsidiary_or_constituent_schools </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>"], "scores": [-0.013828684575855732, -0.014893182553350925, -0.02650550752878189, -0.047630563378334045, -0.08023829758167267, -0.08865399658679962, -0.10314546525478363, -0.11321793496608734, -0.12301913648843765, -0.12402087450027466], "norm_scores": [0.10605665296316147, 0.10594381392002106, 0.10472067445516586, 0.10253165662288666, 0.09924224764108658, 0.09841056913137436, 0.09699473530054092, 0.09602265805006027, 0.09508612006902695, 0.09499091655015945]}}
{"id": "WebQTest-1912", "question": "what did joan crawford died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.people.deceased_person.cause_of_death"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["influence.influence_node.influenced", "people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joan crawford died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.002564498223364353, -0.03539838641881943, -0.12071126699447632, -0.1285402476787567, -0.13235105574131012, -0.1375182569026947, -0.14613403379917145, -0.15512458980083466, -0.15559276938438416, -0.15703728795051575], "norm_scores": [0.11198608577251434, 0.10836885124444962, 0.09950698912143707, 0.09873099625110626, 0.09835547208786011, 0.0978485494852066, 0.09700912982225418, 0.09614087641239166, 0.09609587490558624, 0.09595715999603271]}}
{"id": "WebQTest-1913", "question": "where did rob kardashian go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["common.topic.notable_types"], ["people.person.place_of_birth", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rob kardashian go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.013989263214170933, -0.01850680448114872, -0.027606960386037827, -0.03221529349684715, -0.057288866490125656, -0.09166115522384644, -0.09665823727846146, -0.10351038724184036, -0.1156667098402977, -0.12219059467315674], "norm_scores": [0.1054564043879509, 0.10498107969760895, 0.1040300652384758, 0.10355176031589508, 0.10098762810230255, 0.09757543355226517, 0.09708905220031738, 0.09642606228590012, 0.09526097029447556, 0.09464152157306671]}}
{"id": "WebQTest-1914", "question": "what year was allen iverson mvp", "prediction": [["award.award_honor.award_winner", "award.award_honor.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["award.award_nomination.award_nominee", "award.award_nomination.season"], ["award.award_winner.awards", "award.award_winning_work.awards_won"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["award.award_winner.awards", "award.award_honor.honored_for"], ["award.award_winner.awards", "award.award_honor.season"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["award.award_honor.award_winner", "award.award_honor.honored_for"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was allen iverson mvp [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.season </PATH>", "<PATH> award.award_winner.awards <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> award.award_winner.awards <SEP> award.award_honor.honored_for </PATH>", "<PATH> award.award_winner.awards <SEP> award.award_honor.season </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>"], "scores": [-0.028342586010694504, -0.029994739219546318, -0.03096805326640606, -0.03374005854129791, -0.040099114179611206, -0.04130909591913223, -0.04240155220031738, -0.04350940138101578, -0.04849228262901306, -0.04865157976746559], "norm_scores": [0.1010436937212944, 0.10087689757347107, 0.10077875852584839, 0.10049977898597717, 0.0998627245426178, 0.09974196553230286, 0.09963306784629822, 0.09952274709939957, 0.09902806580066681, 0.09901229292154312]}}
{"id": "WebQTest-1915", "question": "where did harry s truman go to school", "prediction": [["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.place_of_birth", "location.location.containedby"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harry s truman go to school [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.011006291024386883, -0.026462962850928307, -0.03289952129125595, -0.0331515334546566, -0.04452527314424515, -0.04708496853709221, -0.058185070753097534, -0.0844949334859848, -0.08959256857633591, -0.1001426950097084], "norm_scores": [0.1042218878865242, 0.102623350918293, 0.10196492820978165, 0.10193924605846405, 0.10078637301921844, 0.10052872449159622, 0.0994190201163292, 0.09683741629123688, 0.09634503722190857, 0.09533393383026123]}}
{"id": "WebQTest-1916", "question": "what county is utica ny in", "prediction": [["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.location.contains"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.citytown.postal_codes"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is utica ny in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.021954307332634926, -0.024010855704545975, -0.024205783382058144, -0.04430670663714409, -0.06494124233722687, -0.0776841789484024, -0.08410762995481491, -0.08776447921991348, -0.0970059186220169, -0.09956537187099457], "norm_scores": [0.10409750044345856, 0.10388363897800446, 0.10386338829994202, 0.1017964780330658, 0.09971748292446136, 0.09845485538244247, 0.09782446920871735, 0.09746738523244858, 0.09657079726457596, 0.09632394462823868]}}
{"id": "WebQTest-1917", "question": "what did charles babbage discover", "prediction": [["law.invention.inventor"], ["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"], ["law.inventor.inventions"], ["base.argumentmaps.original_idea.innovator"], ["symbols.name_source.namesakes"], ["influence.influence_node.influenced_by"], ["common.topic.notable_types"], ["astronomy.astronomer.astronomical_objects_discovered"], ["people.person.profession"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did charles babbage discover [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> astronomy.astronomer.astronomical_objects_discovered </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.0030604375060647726, -0.05352534353733063, -0.06056306138634682, -0.062046948820352554, -0.0797441154718399, -0.09171827882528305, -0.11152493953704834, -0.11721117794513702, -0.11721904575824738, -0.12723568081855774], "norm_scores": [0.10818246006965637, 0.10285849869251251, 0.10213715583086014, 0.10198571532964706, 0.1001967266201973, 0.0990041196346283, 0.09706246107816696, 0.09651210904121399, 0.0965113565325737, 0.09554946422576904]}}
{"id": "WebQTest-1919", "question": "who plays king julian madagascar", "prediction": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film.starring", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.film", "film.actor.film"], ["film.performance.film", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["film.performance.film", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays king julian madagascar [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.029613014310598373, -0.03503403812646866, -0.03802356496453285, -0.0404745377600193, -0.040570054203271866, -0.0430024228990078, -0.04300541430711746, -0.04684186354279518, -0.04792432859539986, -0.052190396934747696], "norm_scores": [0.1012108325958252, 0.10066364705562592, 0.10036316514015198, 0.10011747479438782, 0.10010790824890137, 0.09986470639705658, 0.0998644083738327, 0.09948202222585678, 0.09937439858913422, 0.09895136207342148]}}
{"id": "WebQTest-1920", "question": "who is rick scott", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_holder"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held"], ["people.person.nationality"], ["government.politician.government_positions_held", "government.politician.government_positions_held"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.politician.government_positions_held"]], "ground_paths": [["people.profession.people_with_this_profession"], ["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is rick scott [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.001771134091541171, -0.05751525238156319, -0.06533638387918472, -0.07398387044668198, -0.07807738333940506, -0.07853502035140991, -0.07971100509166718, -0.08138531446456909, -0.08364778012037277, -0.08434642106294632], "norm_scores": [0.1068628579378128, 0.101068876683712, 0.1002814844250679, 0.099418044090271, 0.09901190549135208, 0.09896660596132278, 0.0988502949476242, 0.09868492186069489, 0.09846189618110657, 0.09839313477277756]}}
{"id": "WebQTest-1921", "question": "where did the crimean war take place", "prediction": [["time.event.locations"], ["location.location.events"], ["book.written_work.subjects"], ["base.culturalevent.event.entity_involved"], ["book.book_subject.works"], ["event.disaster.areas_affected"], ["military.military_conflict.casualties"], ["geography.body_of_water.bridges"], ["geography.body_of_water.bridges", "location.location.events"], ["film.film_genre.films_in_this_genre"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the crimean war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> military.military_conflict.casualties </PATH>", "<PATH> geography.body_of_water.bridges </PATH>", "<PATH> geography.body_of_water.bridges <SEP> location.location.events </PATH>", "<PATH> film.film_genre.films_in_this_genre </PATH>"], "scores": [-0.009368023835122585, -0.020180968567728996, -0.09313811361789703, -0.10461024940013885, -0.11869145929813385, -0.12188860028982162, -0.15137778222560883, -0.1556123048067093, -0.16037580370903015, -0.16311830282211304], "norm_scores": [0.11041248589754105, 0.10922503471374512, 0.10154002904891968, 0.10038180649280548, 0.09897821396589279, 0.09866227954626083, 0.09579528868198395, 0.09539049863815308, 0.09493718296289444, 0.09467718005180359]}}
{"id": "WebQTest-1922", "question": "what offices did theodore roosevelt hold", "prediction": [["business.employment_tenure.person", "business.employment_tenure.title"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["common.topic.notable_types", "type.type.expected_by"], ["government.government_position_held.office_holder", "government.government_position_held.basic_title"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["base.onephylogeny.type_of_thing.things_of_this_type", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"], ["base.onephylogeny.type_of_thing.things_of_this_type", "freebase.type_hints.included_types", "type.type.expected_by"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_office_category.officeholders"], ["business.employment_tenure.person", "business.employment_tenure.title"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["common.topic.notable_types", "type.type.expected_by"], ["government.government_position_held.appointed_by", "government.government_position_held.basic_title"], ["government.government_position_held.office_holder", "government.government_position_held.basic_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what offices did theodore roosevelt hold [/INST]", "raw_output": {"paths": ["<PATH> business.employment_tenure.person <SEP> business.employment_tenure.title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.basic_title </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.basic_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> people.person.nationality <SEP> government.government_office_or_title.jurisdiction </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> freebase.type_hints.included_types <SEP> type.type.expected_by </PATH>"], "scores": [-0.016359493136405945, -0.01816881075501442, -0.02743704617023468, -0.030637601390480995, -0.04353363811969757, -0.04668538272380829, -0.0475689060986042, -0.054439909756183624, -0.06006387993693352, -0.07023299485445023], "norm_scores": [0.10253237187862396, 0.10234702378511429, 0.10140283405780792, 0.10107880085706711, 0.09978365898132324, 0.09946966171264648, 0.09938181191682816, 0.09870130568742752, 0.09814776480197906, 0.09715475142002106]}}
{"id": "WebQTest-1923", "question": "what movies has miley cyrus been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["music.artist.genre", "music.album.genre"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["people.person.children", "film.producer.film"], ["music.artist.concert_tours", "music.album.supporting_tours"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["people.person.parents", "film.film.produced_by"], ["music.artist.origin", "film.film.featured_film_locations"], ["music.concert_tour.artist", "music.album.supporting_tours"], ["people.person.children", "film.film.executive_produced_by"], ["people.person.children", "film.film.produced_by"], ["music.lyricist.lyrics_written", "film.film.featured_song"], ["film.film.music"], ["music.recording_contribution.contributor", "music.recording_contribution.album"], ["music.artist.genre", "music.album.genre"], ["music.artist.genre", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["film.film_featured_song.performed_by", "film.film.featured_song"], ["people.person.languages", "film.film.language"], ["music.artist.album", "media_common.adaptation.adapted_from"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "tv.tv_program.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has miley cyrus been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> music.artist.genre <SEP> music.album.genre </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.02762923389673233, -0.029325585812330246, -0.03747160732746124, -0.04315029829740524, -0.04376772418618202, -0.049729492515325546, -0.050558529794216156, -0.05279529094696045, -0.05600334703922272, -0.05850602313876152], "norm_scores": [0.10173621028661728, 0.10156377404928207, 0.10073979198932648, 0.1001693531870842, 0.10010751336812973, 0.09951247274875641, 0.09943000972270966, 0.09920785576105118, 0.09889010339975357, 0.09864293038845062]}}
{"id": "WebQTest-1925", "question": "who did james franco play in milk", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.performance.character"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.performance.character"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.film.starring", "film.film_character.portrayed_in_films"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did james franco play in milk [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.film.starring <SEP> film.performance.character </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.01911924034357071, -0.027075115591287613, -0.0329979844391346, -0.03823420777916908, -0.04165958613157272, -0.05652180314064026, -0.057330384850502014, -0.05856819823384285, -0.0590406097471714, -0.062054917216300964], "norm_scores": [0.10263752192258835, 0.10182418674230576, 0.10122287273406982, 0.10069423913955688, 0.10034991055727005, 0.09886951744556427, 0.09878960996866226, 0.09866739809513092, 0.09862079471349716, 0.09832397103309631]}}
{"id": "WebQTest-1926", "question": "when did the libyan conflict start", "prediction": [["time.event.locations"], ["time.event.includes_event"], ["time.event.included_in_event"], ["base.culturalevent.event.entity_involved"], ["time.event.instance_of_recurring_event"], ["location.location.events"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.military_conflict.casualties"], ["military.military_combatant_group.conflict", "military.military_combatant_group.conflict"], ["time.event.locations", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the libyan conflict start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.event.locations <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.conflict </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>"], "scores": [-0.01063245628029108, -0.025992920622229576, -0.037090033292770386, -0.0867161899805069, -0.09196271747350693, -0.10193508863449097, -0.11241474747657776, -0.11677176505327225, -0.11725698411464691, -0.12042504549026489], "norm_scores": [0.10732580721378326, 0.10568982362747192, 0.10452345758676529, 0.09946296364068985, 0.09894248843193054, 0.0979607105255127, 0.09693947434425354, 0.09651802480220795, 0.09647120535373688, 0.09616606682538986]}}
{"id": "WebQTest-1928", "question": "what wnba team is diana taurasi on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team.sport"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.sports_team.location"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wnba team is diana taurasi on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team.sport </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.019135907292366028, -0.025309748947620392, -0.02883932925760746, -0.03192716836929321, -0.03309822455048561, -0.034331630915403366, -0.051761694252491, -0.054823435842990875, -0.05695527791976929, -0.058975499123334885], "norm_scores": [0.1020490899682045, 0.10142099857330322, 0.10106365382671356, 0.10075206309556961, 0.1006341427564621, 0.10051009804010391, 0.09877338260412216, 0.09847143292427063, 0.09826172888278961, 0.09806341677904129]}}
{"id": "WebQTest-1929", "question": "what do see in paris", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["film.film_location.featured_in_films"], ["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"], ["film.film.featured_film_locations"], ["transportation.bridge.locale"], ["tv.tv_program.filming_locations"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do see in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> film.film_location.featured_in_films </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> transportation.bridge.locale </PATH>", "<PATH> tv.tv_program.filming_locations </PATH>"], "scores": [-0.012622755952179432, -0.01664654165506363, -0.04063313081860542, -0.1104755625128746, -0.11388516426086426, -0.11824217438697815, -0.12791891396045685, -0.1341180056333542, -0.13727664947509766, -0.16007457673549652], "norm_scores": [0.10868369787931442, 0.10824726521968842, 0.1056816577911377, 0.09855245798826218, 0.09821701049804688, 0.09779000282287598, 0.09684828668832779, 0.0962497740983963, 0.09594622999429703, 0.09378360211849213]}}
{"id": "WebQTest-1932", "question": "who made the laws in canada", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.government"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.governmental_body.jurisdiction"], ["government.government.government_for"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["location.country.form_of_government"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.government", "people.person.nationality"]], "ground_paths": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_body.jurisdiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who made the laws in canada [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_body.jurisdiction </PATH>", "<PATH> government.government.government_for </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.government <SEP> people.person.nationality </PATH>"], "scores": [-0.015376650728285313, -0.030663542449474335, -0.04062764346599579, -0.04343416541814804, -0.04769087955355644, -0.050223205238580704, -0.05037518963217735, -0.05293801426887512, -0.05455484613776207, -0.06130628287792206], "norm_scores": [0.10296949744224548, 0.10140738636255264, 0.10040196776390076, 0.10012058168649673, 0.09969530254602432, 0.09944316744804382, 0.09942805022001266, 0.09917356073856354, 0.09901334345340729, 0.09834711253643036]}}
{"id": "WebQTest-1933", "question": "what year did adam morrison get drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"], ["sports.sports_league_draft.picks", "sports.sports_league_draft_pick.draft"], ["sports.pro_athlete.teams", "sports.sports_league_draft_pick.draft"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft.picks"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did adam morrison get drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_league_draft.picks <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.01960841193795204, -0.022043390199542046, -0.02224981226027012, -0.023039832711219788, -0.03569453954696655, -0.04623361676931381, -0.058428116142749786, -0.060339607298374176, -0.061497002840042114, -0.06380413472652435], "norm_scores": [0.10217606276273727, 0.10192757099866867, 0.10190653055906296, 0.10182604938745499, 0.10054559260606766, 0.09949149936437607, 0.09828561544418335, 0.09809793531894684, 0.09798446297645569, 0.09775865077972412]}}
{"id": "WebQTest-1934", "question": "where michael jackson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["symbols.namesake.named_after"], ["tv.tv_actor.starring_roles", "tv.tv_character.place_of_birth"], ["people.person.nationality"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["location.person.place_of_birth"], ["tv.tv_actor.starring_roles", "tv.tv_program.country_of_origin"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where michael jackson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>", "<PATH> location.person.place_of_birth </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_program.country_of_origin </PATH>"], "scores": [-0.005926710087805986, -0.03520992025732994, -0.04761742055416107, -0.11554694175720215, -0.11616422235965729, -0.12008090317249298, -0.12055730819702148, -0.1285383403301239, -0.1342809498310089, -0.13918715715408325], "norm_scores": [0.10934571921825409, 0.10619014501571655, 0.10488073527812958, 0.09799282997846603, 0.097932368516922, 0.09754955023527145, 0.09750308096408844, 0.09672800451517105, 0.09617412090301514, 0.09570343792438507]}}
{"id": "WebQTest-1935", "question": "what college did maya moore go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["common.topic.notable_types"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did maya moore go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.012919798493385315, -0.02099268138408661, -0.02480083517730236, -0.05473227798938751, -0.059928860515356064, -0.06077028810977936, -0.07358819246292114, -0.10451110452413559, -0.1051177829504013, -0.11615955084562302], "norm_scores": [0.10510823875665665, 0.10426312685012817, 0.10386683791875839, 0.10080402344465256, 0.10028154402971268, 0.1001971960067749, 0.09892108291387558, 0.09590896219015121, 0.09585078805685043, 0.0947982519865036]}}
{"id": "WebQTest-1936", "question": "when was the last time the dodgers won a championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.mascot.team"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["sports.sports_league_season.champion"], ["sports.moment_team.championships"], ["sports.mports_team.championships"], ["sports.mports_championship_event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dodgers won a championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.mascot.team </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_league_season.champion </PATH>", "<PATH> sports.moment_team.championships </PATH>", "<PATH> sports.mports_team.championships </PATH>", "<PATH> sports.mports_championship_event.champion </PATH>"], "scores": [-0.009641718119382858, -0.013752193190157413, -0.09979983419179916, -0.11669345945119858, -0.12216771394014359, -0.1267671436071396, -0.13720306754112244, -0.14345349371433258, -0.1446131020784378, -0.14495596289634705], "norm_scores": [0.10997001826763153, 0.10951891541481018, 0.10048913955688477, 0.09880577772855759, 0.0982663631439209, 0.09781543165445328, 0.09679993987083435, 0.09619680047035217, 0.0960853099822998, 0.09605236351490021]}}
{"id": "WebQTest-1937", "question": "who is captain james cook facts", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["fictional_universe.fictional_character.occupation"], ["common.topic.notable_types"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["influence.influence_node.influenced_by", "people.person.profession"], ["people.profession.people_with_this_profession"], ["book.written_work.subjects"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is captain james cook facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.0017674746923148632, -0.06406152248382568, -0.07642272859811783, -0.08459840714931488, -0.09221922606229782, -0.1002172976732254, -0.10558196157217026, -0.10770118236541748, -0.10801157355308533, -0.11603295058012009], "norm_scores": [0.10869519412517548, 0.1021307185292244, 0.10087602585554123, 0.10005465894937515, 0.09929506480693817, 0.09850405901670456, 0.09797703474760056, 0.0977696105837822, 0.09773927927017212, 0.09695841372013092]}}
{"id": "WebQTest-1938", "question": "where did robin williams get married", "prediction": [["people.person.children", "people.person.place_of_birth"], ["people.marriage.spouse", "people.marriage.location_of_ceremony"], ["people.person.nationality", "location.location.containedby"], ["people.person.spouse_s", "people.marriage.location_of_ceremony"], ["people.person.parents", "people.person.place_of_birth"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.places_lived", "people.place_lived.location"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.children", "people.deceased_person.place_of_death"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.spouse_s", "people.marriage.location_of_ceremony"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin williams get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.location_of_ceremony </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.location_of_ceremony </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.children <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.023440465331077576, -0.026341566815972328, -0.028306782245635986, -0.03610525280237198, -0.04737796261906624, -0.0473935604095459, -0.051746245473623276, -0.05703406035900116, -0.059536486864089966, -0.05975285544991493], "norm_scores": [0.10203786194324493, 0.10174226760864258, 0.10154252499341965, 0.10075372457504272, 0.09962433576583862, 0.09962278604507446, 0.09919009357690811, 0.09866698086261749, 0.0984203889966011, 0.0983990952372551]}}
{"id": "WebQTest-1939", "question": "what form of government does canada", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["form_of_government.countries"], ["form.form_of_government.countries"], ["locations.country.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does canada [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> form_of_government.countries </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> locations.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.country.form_of_government <PATH>"], "scores": [-0.00833243690431118, -0.01732950657606125, -0.15260763466358185, -0.1663254052400589, -0.16823650896549225, -0.16848593950271606, -0.16912738978862762, -0.178111270070076, -0.18013808131217957, -0.1826932430267334], "norm_scores": [0.11373662948608398, 0.11271792650222778, 0.09845606237649918, 0.0971146821975708, 0.09692925959825516, 0.09690508246421814, 0.09684295207262039, 0.095976822078228, 0.09578248858451843, 0.09553806483745575]}}
{"id": "WebQTest-1940", "question": "who do islamic people follow", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["base.argumentmaps.intellectual_dispute.positions", "book.book_subject.works"], ["religion.belief.belief_of"], ["people.person.religion"], ["symbols.namesake.named_after"], ["religion.religion.notable_figures"], ["base.argumentmaps.intellectual_dispute.positions"], ["religion.belief.belief_of", "religion.religion.deities"], ["base.argumentmaps.intellectual_dispute.positions", "book.author.works_written"]], "ground_paths": [["religion.religion.deities"], ["religion.deity.deity_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people follow [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions <SEP> book.book_subject.works </PATH>", "<PATH> religion.belief.belief_of </PATH>", "<PATH> people.person.religion </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions </PATH>", "<PATH> religion.belief.belief_of <SEP> religion.religion.deities </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions <SEP> book.author.works_written </PATH>"], "scores": [-0.005647012032568455, -0.03156139329075813, -0.0668770894408226, -0.07128044217824936, -0.0728234127163887, -0.10050874203443527, -0.10101328045129776, -0.11029748618602753, -0.11763715744018555, -0.11861450225114822], "norm_scores": [0.10760857164859772, 0.10485578328371048, 0.1012173593044281, 0.10077263414859772, 0.10061726719141006, 0.09786985069513321, 0.09782049059867859, 0.09691649675369263, 0.09620777517557144, 0.09611379355192184]}}
{"id": "WebQTest-1941", "question": "what kind of government does peru", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "government.form_of_government.countries"], ["people.person.nationality", "government.form_of_government.countries"], ["base.locations.countries.continent"], ["common.topic.notable_types"], ["geography.river.basin_countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does peru [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.00962036196142435, -0.0155435586348176, -0.13427934050559998, -0.1426032930612564, -0.1490352749824524, -0.15622255206108093, -0.16503533720970154, -0.16626977920532227, -0.174480140209198, -0.17643362283706665], "norm_scores": [0.11246936023235321, 0.11180514842271805, 0.09928770363330841, 0.09846468269824982, 0.09783338755369186, 0.09713274985551834, 0.09628050029277802, 0.09616172313690186, 0.0953754335641861, 0.09518930315971375]}}
{"id": "WebQTest-1942", "question": "who was in the israeli palestinian conflict", "prediction": [["time.event.people_involved"], ["people.person.nationality"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["people.person.ethnicity"], ["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["time.event.locations", "people.person.place_of_birth"], ["government.government_position_held.basic_title", "government.political_appointer.appointees"]], "ground_paths": [["time.event.people_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was in the israeli palestinian conflict [/INST]", "raw_output": {"paths": ["<PATH> time.event.people_involved </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> time.event.included_in_event <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> time.event.locations <SEP> people.person.place_of_birth </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.political_appointer.appointees </PATH>"], "scores": [-0.0005754093290306628, -0.07251613587141037, -0.09773190319538116, -0.09886249899864197, -0.10315532237291336, -0.11157339811325073, -0.11201459169387817, -0.11467394977807999, -0.11683834344148636, -0.11766611784696579], "norm_scores": [0.10979000478982925, 0.10216904431581497, 0.09962499141693115, 0.09951241314411163, 0.09908614307641983, 0.09825552999973297, 0.09821218252182007, 0.09795135259628296, 0.097739577293396, 0.09765870124101639]}}
{"id": "WebQTest-1943", "question": "who founded the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.spun_off_from"], ["organization.organization.geographic_scope", "people.person.nationality"], ["government.governmental_jurisdiction.government_bodies", "people.person.nationality"], ["organization.organization.geographic_scope", "people.person.place_of_birth"], ["symbols.namesake.named_after"], ["government.governmental_body.members", "government.politician.government_positions_held"], ["organization.organization.spun_off_from", "organization.organization.founders"], ["government.governmental_body.members", "government.government_position_held.office_holder"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.spun_off_from </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.government_bodies <SEP> people.person.nationality </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> government.governmental_body.members <SEP> government.politician.government_positions_held </PATH>", "<PATH> organization.organization.spun_off_from <SEP> organization.organization.founders </PATH>", "<PATH> government.governmental_body.members <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.0014365966198965907, -0.051807936280965805, -0.10603378713130951, -0.11521971225738525, -0.1353151947259903, -0.13627126812934875, -0.1416846662759781, -0.14408792555332184, -0.14479535818099976, -0.14510808885097504], "norm_scores": [0.11158988624811172, 0.1061081737279892, 0.10050757974386215, 0.0995885580778122, 0.09760726243257523, 0.09751398116350174, 0.0969875231385231, 0.09675471484661102, 0.09668629616498947, 0.09665606170892715]}}
{"id": "WebQTest-1944", "question": "where did diego velazquez die", "prediction": [["people.deceased_person.place_of_death"], ["symbols.namesake.named_after", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did diego velazquez die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> symbols.namesake.named_after <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people_born_here </PATH>"], "scores": [-0.00024549910449422896, -0.09920260310173035, -0.10401258617639542, -0.10972911864519119, -0.11211425065994263, -0.11226044595241547, -0.11322013288736343, -0.11716082692146301, -0.11840534955263138, -0.1256260722875595], "norm_scores": [0.11055519431829453, 0.10013885796070099, 0.09965834021568298, 0.09909027069807053, 0.09885420650243759, 0.09883975982666016, 0.0987449511885643, 0.09835659712553024, 0.09823426604270935, 0.09752749651670456]}}
{"id": "WebQTest-1946", "question": "what is the country code for mexico", "prediction": [["location.country.internet_tld"], ["location.location.containedby"], ["location.country.languages_spoken"], ["location.location.contains"], ["language.human_language.countries_spoken_in"], ["base.locations.countries.continent"], ["base.aareas.schema.administrative_area.administrative_parent"], ["film.film.featured_film_locations", "film.film.country"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["film.film_location.featured_in_films"]], "ground_paths": [["location.country.internet_tld"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the country code for mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.internet_tld </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> film.film.featured_film_locations <SEP> film.film.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> film.film_location.featured_in_films </PATH>"], "scores": [-0.016655294224619865, -0.030009793117642403, -0.034059781581163406, -0.046351734548807144, -0.06061811000108719, -0.07540453225374222, -0.07657989114522934, -0.07692315429449081, -0.08140966296195984, -0.08258480578660965], "norm_scores": [0.10419928282499313, 0.10281699895858765, 0.10240143537521362, 0.10115042328834534, 0.0997176244854927, 0.0982540100812912, 0.09813858568668365, 0.09810490906238556, 0.09766574203968048, 0.09755103290081024]}}
{"id": "WebQTest-1947", "question": "what currency do they use in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency do they use in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.00448094867169857, -0.0291964802891016, -0.13790100812911987, -0.1383054405450821, -0.15688148140907288, -0.1573547124862671, -0.16094793379306793, -0.16616761684417725, -0.16946467757225037, -0.17157450318336487], "norm_scores": [0.11309393495321274, 0.11033301800489426, 0.09896821528673172, 0.09892820566892624, 0.09710745513439178, 0.09706152230501175, 0.09671337902545929, 0.09620988368988037, 0.09589319676160812, 0.09569109976291656]}}
{"id": "WebQTest-1950", "question": "where is president kennedy buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["base.onephylogeny.type_of_thing.things_of_this_type", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.place_of_burial"], ["base.americancomedy.impersonated_celebrity.impersonated_by"], ["symbols.name_source.namesakes"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.deceased_person.place_of_burial"], ["base.onephylogeny.type_of_thing.things_of_this_type"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is president kennedy buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.place_of_burial </PATH>", "<PATH> base.americancomedy.impersonated_celebrity.impersonated_by </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.deceased_person.place_of_burial </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type </PATH>"], "scores": [-0.004924258217215538, -0.028884440660476685, -0.07529982179403305, -0.08375105261802673, -0.09352988749742508, -0.09404053539037704, -0.09505970776081085, -0.10283651202917099, -0.10397325456142426, -0.10424071550369263], "norm_scores": [0.10759393125772476, 0.1050466001033783, 0.10028223693370819, 0.09943830966949463, 0.09847065806388855, 0.09842038154602051, 0.0983201265335083, 0.09755847603082657, 0.09744764119386673, 0.09742157906293869]}}
{"id": "WebQTest-1952", "question": "where was the prophet mohammed from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"], ["people.person.nationality"], ["people.deceased_person.place_of_death"], ["religion.religion.notable_figures"], ["location.location.people_born_here", "location.location.containedby"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the prophet mohammed from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0044190832413733006, -0.02884722873568535, -0.09463619440793991, -0.10846082121133804, -0.11057833582162857, -0.1185397282242775, -0.12934617698192596, -0.13081245124340057, -0.13937532901763916, -0.14075736701488495], "norm_scores": [0.10998255759477615, 0.10732842981815338, 0.10049467533826828, 0.09911492466926575, 0.09890527278184891, 0.09812097996473312, 0.09706634283065796, 0.09692412614822388, 0.09609772264957428, 0.09596499800682068]}}
{"id": "WebQTest-1954", "question": "what language do ukrainian people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"], ["base.rosetta.languoid.languoid_class"], ["language.human_language.region"], ["language.human_language.language_family"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do ukrainian people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "</PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.009105952456593513, -0.01800837181508541, -0.07259385287761688, -0.08558985590934753, -0.14822040498256683, -0.1709136962890625, -0.17902709543704987, -0.18300142884254456, -0.18302595615386963, -0.18512767553329468], "norm_scores": [0.1118592843413353, 0.11086788773536682, 0.10497831553220749, 0.10362284630537033, 0.0973319411277771, 0.09514804184436798, 0.09437919408082962, 0.09400483220815659, 0.09400253742933273, 0.09380517154932022]}}
{"id": "WebQTest-1955", "question": "who are the judges appointed in the supreme court", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"], ["book.written_work.subjects", "book.written_work.author"], ["government.governmental_jurisdiction.government_bodies", "people.person.nationality"], ["book.book_subject.works", "book.author.works_written"], ["people.appointment.appointed_role", "people.appointee.position"], ["people.appointed_role.appointment", "people.appointee.position"], ["book.book_subject.works", "book.written_work.author"], ["organization.organization.geographic_scope", "people.person.nationality"], ["book.book_subject.works", "book.written_work.subjects"]], "ground_paths": [["book.written_work.subjects", "book.author.works_written"], ["law.court.jurisdiction", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the judges appointed in the supreme court [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.author.works_written </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>", "<PATH> government.governmental_jurisdiction.government_bodies <SEP> people.person.nationality </PATH>", "<PATH> book.book_subject.works <SEP> book.author.works_written </PATH>", "<PATH> people.appointment.appointed_role <SEP> people.appointee.position </PATH>", "<PATH> people.appointed_role.appointment <SEP> people.appointee.position </PATH>", "<PATH> book.book_subject.works <SEP> book.written_work.author </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>", "<PATH> book.book_subject.works <SEP> book.written_work.subjects </PATH>"], "scores": [-0.01337466575205326, -0.02186419814825058, -0.029673472046852112, -0.04923434928059578, -0.0535542294383049, -0.0607868917286396, -0.06872677057981491, -0.07018359005451202, -0.07537280768156052, -0.07748281955718994], "norm_scores": [0.103915736079216, 0.1030372753739357, 0.10223575681447983, 0.10025538504123688, 0.09982321411371231, 0.09910383075475693, 0.09832007437944412, 0.09817695617675781, 0.09766881167888641, 0.09746294468641281]}}
{"id": "WebQTest-1956", "question": "which country in europe has the largest land area", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["base.locations.countries.continent"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["geography.river.basin_countries"], ["location.location.primarily_containedby"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in europe has the largest land area [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.primarily_containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.014029898680746555, -0.02765069715678692, -0.028763243928551674, -0.04487740620970726, -0.06813094764947891, -0.07931532710790634, -0.09462235122919083, -0.09559373557567596, -0.09988430142402649, -0.10031253844499588], "norm_scores": [0.10520875453948975, 0.103785440325737, 0.10367004573345184, 0.10201287269592285, 0.09966807812452316, 0.09855956584215164, 0.09706240147352219, 0.0969681665301323, 0.0965530052781105, 0.09651166945695877]}}
{"id": "WebQTest-1957", "question": "when did kaley cuoco join charmed", "prediction": [["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.seasons"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.seasons"], ["tv.regular_tv_appearance.series", "tv.tv_series_season.regular_cast"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.seasons"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.seasons"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kaley cuoco join charmed [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_series_season.regular_cast </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> tv.tv_guest_role.actor <SEP> tv.tv_guest_role.episodes_appeared_in </PATH>"], "scores": [-0.025819387286901474, -0.026603227481245995, -0.028846608474850655, -0.03114594891667366, -0.038274914026260376, -0.038993146270513535, -0.04896654561161995, -0.05291086435317993, -0.05459553375840187, -0.05517840012907982], "norm_scores": [0.10143522918224335, 0.10135575383901596, 0.10112863034009933, 0.10089636594057083, 0.10017962753772736, 0.10010770708322525, 0.09911425411701202, 0.09872408956289291, 0.09855790436267853, 0.09850048273801804]}}
{"id": "WebQTest-1958", "question": "where is chris rock from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["music.artist.origin"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.place_of_birth", "base.biblioness.bibs_location.country"], ["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris rock from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.state </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.008108709938824177, -0.021819012239575386, -0.08018466085195541, -0.11565256118774414, -0.12287092953920364, -0.12306763976812363, -0.12778611481189728, -0.1279391050338745, -0.12957264482975006, -0.13241109251976013], "norm_scores": [0.1093987226486206, 0.10790906846523285, 0.10179115831851959, 0.09824411571025848, 0.09753750264644623, 0.09751831740140915, 0.09705926477909088, 0.09704441577196121, 0.09688602387905121, 0.0966114029288292]}}
{"id": "WebQTest-1960", "question": "when was the last time the toronto maple leafs win the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team.championships"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_ event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs win the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team.championships </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.champion <PATH>", "<PATH> sports.sports_team.championships <PATH>", "<PATH> sports.sports_championship_ event.champion </PATH>"], "scores": [-0.009305362589657307, -0.011996065266430378, -0.07306168228387833, -0.08055397868156433, -0.12431227415800095, -0.13079825043678284, -0.14753563702106476, -0.1557525098323822, -0.15912507474422455, -0.16038501262664795], "norm_scores": [0.10990162193775177, 0.1096063032746315, 0.10311339795589447, 0.10234373062849045, 0.0979619175195694, 0.09732858836650848, 0.09571311622858047, 0.09492988139390945, 0.09461025893688202, 0.09449112415313721]}}
{"id": "WebQTest-1961", "question": "what kind of language do egyptian speak", "prediction": [["people.ethnicity.languages_spoken", "base.rosetta.languoid.languoid_class", "base.rosetta.languoid.languoid_class"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.main_country"], ["people.ethnicity.languages_spoken", "common.topic.notable_types", "common.topic.notable_types"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"], ["people.ethnicity.languages_spoken", "language.human_language.language_family", "language.human_language.language_family"], ["people.ethnicity.people", "people.person.nationality", "location.country.languages_spoken"], ["people.ethnicity.languages_spoken", "language.human_language.region", "language.human_language.region"], ["people.ethnicity.languages_spoken", "language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do egyptian speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken <SEP> base.rosetta.languoid.languoid_class <SEP> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.language_family <SEP> language.human_language.language_family </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.region <SEP> language.human_language.region </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.032567139714956284, -0.03471657261252403, -0.03667242452502251, -0.03672601282596588, -0.03888458013534546, -0.04063501954078674, -0.04140031337738037, -0.04260074719786644, -0.0440995991230011, -0.0479123517870903], "norm_scores": [0.10070694983005524, 0.10049072653055191, 0.10029436647891998, 0.1002889946103096, 0.10007274895906448, 0.09989772737026215, 0.0998213142156601, 0.09970155358314514, 0.0995522290468216, 0.09917337447404861]}}
{"id": "WebQTest-1962", "question": "what language do brazilians use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["people.ethnicity.languages_spoken"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "location.country.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"], ["people.person.nationality", "people.ethnicity.people", "people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do brazilians use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.person.nationality <SEP> people.ethnicity.people <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0013971439329907298, -0.061446838080883026, -0.07236207276582718, -0.07836903631687164, -0.09404896944761276, -0.10072030872106552, -0.1036912202835083, -0.10978768765926361, -0.11216868460178375, -0.11631087958812714], "norm_scores": [0.10866356641054153, 0.10233040899038315, 0.10121951997280121, 0.1006133183836937, 0.09904801845550537, 0.09838943183422089, 0.0980975553393364, 0.09750132262706757, 0.0972694605588913, 0.09686737507581711]}}
{"id": "WebQTest-1963", "question": "what position did vince lombardi play", "prediction": [["american_football.football_player.position_s"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_coach.sports_coached"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_member.position_s"], ["sports.sports_position.sport", "sports.sports_team_coach.sports_coached", "american_football.football_player.position_s"], ["sports.pro_athlete.teams", "sports.sports_team_roster.position"], ["sports.sports_team_coach.sports_coached", "sports.sports_position.sport", "american_football.football_player.position_s"], ["sports.sports_team_coach.sports_coached", "sports.sports_position.sport"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_coach_tenure.position"], ["sports.sports_team_coach.teams_coached", "sports.sports_team_coach.sports_coached"], ["sports.sports_position.sport", "sports.sports_position.sport", "american_football.football_player.position_s"]], "ground_paths": [["american_football.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did vince lombardi play [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_member.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team_coach.sports_coached <SEP> american_football.football_player.position_s </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.position </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_position.sport <SEP> american_football.football_player.position_s </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_position.sport </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_coach_tenure.position </PATH>", "<PATH> sports.sports_team_coach.teams_coached <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_position.sport <SEP> american_football.football_player.position_s </PATH>"], "scores": [-0.010128861293196678, -0.029341688379645348, -0.045483022928237915, -0.04869959503412247, -0.058774590492248535, -0.05928753688931465, -0.060011524707078934, -0.0632445439696312, -0.06412851810455322, -0.06419524550437927], "norm_scores": [0.10408687591552734, 0.10210615396499634, 0.10047126561403275, 0.10014861822128296, 0.09914468228816986, 0.09909383207559586, 0.09902212768793106, 0.09870249778032303, 0.09861528873443604, 0.09860870242118835]}}
{"id": "WebQTest-1964", "question": "who played stanley kowalski in a streetcar named desire movie", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["film.performance.film", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["film.performance.character", "film.actor.film"], ["theater.play.country_of_origin", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played stanley kowalski in a streetcar named desire movie [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>"], "scores": [-0.02806880697607994, -0.03360563889145851, -0.03521430864930153, -0.035901106894016266, -0.037043362855911255, -0.037953272461891174, -0.038447242230176926, -0.04123099893331528, -0.042397502809762955, -0.04774753004312515], "norm_scores": [0.10097263753414154, 0.10041511058807373, 0.10025370121002197, 0.10018488019704819, 0.10007050633430481, 0.09997949004173279, 0.09993012249469757, 0.09965232759714127, 0.09953614324331284, 0.09900505095720291]}}
{"id": "WebQTest-1965", "question": "who did the voice for lola bunny", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.dubbing_performance.character", "film.actor.dubbing_performances"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice for lola bunny [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.dubbing_performance.character <SEP> film.actor.dubbing_performances </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.015479322522878647, -0.027646709233522415, -0.03084757551550865, -0.03304256498813629, -0.043140288442373276, -0.0454496368765831, -0.060343511402606964, -0.06183888763189316, -0.06760182231664658, -0.07125789672136307], "norm_scores": [0.10304789245128632, 0.10180167108774185, 0.10147633403539658, 0.10125383734703064, 0.10023654997348785, 0.10000533610582352, 0.09852690994739532, 0.09837967902421951, 0.09781435877084732, 0.0974573940038681]}}
{"id": "WebQTest-1966", "question": "what book did charles darwin wrote in 1859", "prediction": [["book.written_work.author"], ["book.written_work.subjects"], ["common.topic.notable_types"], ["book.book_edition.author_editor"], ["book.author.works_written"], ["influence.influence_node.influenced_by", "book.book_edition.author_editor"], ["influence.influence_node.influenced_by", "book.written_work.author"], [], ["base.argumentmaps.innovator.original_ideas"], ["media_common.literary_genre.books_in_this_genre"]], "ground_paths": [["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"], [], ["base.argumentmaps.innovator.original_ideas"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did charles darwin wrote in 1859 [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.book_edition.author_editor </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> book.written_work.author </PATH>", "<PATH> </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> media_common.literary_genre.books_in_this_genre </PATH>"], "scores": [-0.0015256402548402548, -0.06559938192367554, -0.06746694445610046, -0.06809556484222412, -0.09058325737714767, -0.09708622843027115, -0.0971185714006424, -0.09730761498212814, -0.10202201455831528, -0.1039026528596878], "norm_scores": [0.1080152839422226, 0.10131140798330307, 0.10112237930297852, 0.10105883330106735, 0.09881161153316498, 0.09817112982273102, 0.09816795587539673, 0.09814940392971039, 0.09768776595592499, 0.09750422835350037]}}
{"id": "WebQTest-1967", "question": "what language do serbs speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.person.ethnicity", "people.person.nationality", "location.country.languages_spoken"], ["location.country.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"], ["people.person.nationality", "people.person.ethnicity", "people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "location.country.official_language"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do serbs speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.official_language </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.008629661984741688, -0.030973106622695923, -0.047776754945516586, -0.05162516236305237, -0.06569617986679077, -0.06922462582588196, -0.07051705569028854, -0.07663086801767349, -0.0802045464515686, -0.08604694157838821], "norm_scores": [0.10510999709367752, 0.10278752446174622, 0.10107475519180298, 0.10068651288747787, 0.09927967935800552, 0.0989299938082695, 0.09880221635103226, 0.0981999933719635, 0.09784968942403793, 0.09727967530488968]}}
{"id": "WebQTest-1968", "question": "what was the cause of death for laci peterson", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"], ["people.person.cause_of_death"], ["people.cause_of_death.people", "people.deceased_person.cause_of_death"], ["people.person.deceased_people", "people.deceased_person.cause_of_death"], ["people.marriage.spouse", "people.deceased_person.cause_of_death"], ["deceased_person.cause_of_death"], ["medicine.disease.parent_disease"], ["people.people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for laci peterson [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_people <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.marriage.spouse <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.parent_disease </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>"], "scores": [-0.002472654217854142, -0.03420962393283844, -0.09963953495025635, -0.10124362260103226, -0.10794728249311447, -0.11014202982187271, -0.11437323689460754, -0.12024641036987305, -0.1260562688112259, -0.12667511403560638], "norm_scores": [0.1095300167798996, 0.10610844939947128, 0.09938804060220718, 0.09922873228788376, 0.09856576472520828, 0.0983496829867363, 0.09793441742658615, 0.09736092388629913, 0.0967969074845314, 0.09673701971769333]}}
{"id": "WebQTest-1969", "question": "what team is kris humphries play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["people.person.nationality", "sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is kris humphries play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>"], "scores": [-0.02083934284746647, -0.020911797881126404, -0.021771220490336418, -0.02974371425807476, -0.03583143278956413, -0.036706626415252686, -0.04852639511227608, -0.05323871597647667, -0.06895236670970917, -0.07539880275726318], "norm_scores": [0.10203821957111359, 0.10203083604574203, 0.10194317996501923, 0.10113366693258286, 0.10051986575126648, 0.10043193399906158, 0.09925182908773422, 0.09878522902727127, 0.09724508225917816, 0.09662020951509476]}}
{"id": "WebQTest-1971", "question": "what movies gerard butler has been in", "prediction": [["film.performance.actor", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["film.film.produced_by"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.actor.film", "film.performance.film"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["music.recording.artist"], ["film.producer.film"], ["film.actor.film", "film.film.starring"], ["film.producer.films_executive_produced"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.film.produced_by", "film.film.sequel"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies gerard butler has been in [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> people.person.nationality <SEP> media_common.netflix_title.netflix_genres </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.02990533784031868, -0.03128800541162491, -0.032535020262002945, -0.03262289986014366, -0.040769584476947784, -0.04089033976197243, -0.041316717863082886, -0.04169609025120735, -0.04724128171801567, -0.049543507397174835], "norm_scores": [0.10088936984539032, 0.10074996203184128, 0.10062441229820251, 0.10061556845903397, 0.09979920834302902, 0.0997871682047844, 0.09974462538957596, 0.09970679134130478, 0.09915543347597122, 0.09892741590738297]}}
{"id": "WebQTest-1972", "question": "where did john franklin live", "prediction": [["people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["location.location.people_born_here", "location.location.containedby"], ["people.deceased_person.place_of_death", "base.biblioness.bibs_location.city"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.parents", "people.deceased_person.place_of_death"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john franklin live [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> base.biblioness.bibs_location.city </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.parents <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.023584380745887756, -0.034202586859464645, -0.03761545568704605, -0.041549697518348694, -0.041560910642147064, -0.05141735076904297, -0.05170314759016037, -0.054631322622299194, -0.055164635181427, -0.056217946112155914], "norm_scores": [0.10213519632816315, 0.10105644166469574, 0.10071214288473129, 0.10031668841838837, 0.10031556338071823, 0.09933166205883026, 0.09930328279733658, 0.09901292622089386, 0.09896013885736465, 0.09885595738887787]}}
{"id": "WebQTest-1975", "question": "what was the last movie sean connery was in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.languages", "film.film.language"]], "ground_paths": [["film.film.produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the last movie sean connery was in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>"], "scores": [-0.0158167015761137, -0.03129880502820015, -0.03458346053957939, -0.034757427871227264, -0.03580082952976227, -0.058504361659288406, -0.059939730912446976, -0.06330606341362, -0.0637662410736084, -0.0707676038146019], "norm_scores": [0.10313659906387329, 0.10155212134122849, 0.10121911019086838, 0.10120149701833725, 0.10109596699476242, 0.0988265872001648, 0.09868483245372772, 0.09835318475961685, 0.09830794483423233, 0.09762205928564072]}}
{"id": "WebQTest-1976", "question": "what team did drogba play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["people.person.place_of_birth", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did drogba play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>"], "scores": [-0.0263435710221529, -0.030453450977802277, -0.030901355668902397, -0.0320892296731472, -0.03448567911982536, -0.038032595068216324, -0.038262978196144104, -0.041071098297834396, -0.04503731057047844, -0.04673491790890694], "norm_scores": [0.1010027676820755, 0.10058850795030594, 0.10054347664117813, 0.10042411088943481, 0.10018372535705566, 0.09982901811599731, 0.09980602562427521, 0.09952614456415176, 0.09913218766450882, 0.09896404296159744]}}
{"id": "WebQTest-1977", "question": "where did crabtree go to college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["sports.school_sports_team.school"], ["common.topic.notable_types"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did crabtree go to college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> sports.school_sports_team.school </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.017000196501612663, -0.017405441030859947, -0.028415508568286896, -0.033993352204561234, -0.04725251346826553, -0.058768950402736664, -0.06884601712226868, -0.07288321107625961, -0.11520170420408249, -0.11781899631023407], "norm_scores": [0.10409754514694214, 0.10405535995960236, 0.10291598737239838, 0.10234353691339493, 0.1009955108165741, 0.09983906894922256, 0.09883803874254227, 0.09843981266021729, 0.09436090290546417, 0.0941142588853836]}}
{"id": "WebQTest-1978", "question": "where does freddie ljungberg come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["people.ethnicity.people"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth", "location.location.containedby"], ["people.ethnicity.people", "people.ethnicity.geographic_distribution"], ["location.location.people_born_here", "location.location.containedby"], ["people.ethnicity.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does freddie ljungberg come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.ethnicity.people <SEP> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.ethnicity.people_born_here </PATH>"], "scores": [-0.00471473066136241, -0.03864104300737381, -0.03907228633761406, -0.09285750985145569, -0.11091260612010956, -0.11192435026168823, -0.12671636044979095, -0.12785300612449646, -0.13038329780101776, -0.13058510422706604], "norm_scores": [0.10894423723220825, 0.10531016439199448, 0.10526475310325623, 0.09975263476371765, 0.09796774387359619, 0.09786868095397949, 0.0964316576719284, 0.09632211178541183, 0.09607869386672974, 0.09605930745601654]}}
{"id": "WebQTest-1979", "question": "who is number 22 on miami dolphins", "prediction": [["american_football.football_player.position_s", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["american_football.football_player.position_s"], ["american_football.football_player.position_s", "people.person.gender", "people.person.gender"], ["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_team.roster", "sports.pro_athlete.teams"], ["sports.sports_team_roster.team", "sports.pro_athlete.teams"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.player"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is number 22 on miami dolphins [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.player </PATH>", "<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.football_player.position_s <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_team.roster <SEP> sports.pro_athlete.teams </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.pro_athlete.teams </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.drafted_athlete.drafted </PATH>"], "scores": [-0.02049849182367325, -0.026192311197519302, -0.027089770883321762, -0.032527487725019455, -0.03541376069188118, -0.03564248979091644, -0.037326913326978683, -0.04195500165224075, -0.042192935943603516, -0.046025730669498444], "norm_scores": [0.10140568017959595, 0.10082993656396866, 0.100739486515522, 0.10019317269325256, 0.09990441054105759, 0.09988155961036682, 0.09971345961093903, 0.09925305098295212, 0.09922943264245987, 0.0988498330116272]}}
{"id": "WebQTest-1981", "question": "what state did john adams live in", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["location.location.people_born_here", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["people.person.nationality", "location.administrative_division.first_level_division_of"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["people.person.places_lived", "people.place_lived.location"], ["government.government_position_held.office_holder", "government.political_district.representatives"], ["people.person.nationality", "location.administrative_division.country"]], "ground_paths": [["tv.tv_program.country_of_origin", "base.locations.countries.states_provinces_within"], ["symbols.name_source.namesakes", "base.usnationalparks.us_national_park.state"], ["people.deceased_person.place_of_burial", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["symbols.name_source.namesakes", "location.location.containedby"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state did john adams live in [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.first_level_division_of </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.political_district.representatives </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>"], "scores": [-0.024878004565835, -0.0337064191699028, -0.04088814929127693, -0.04165863245725632, -0.042444996535778046, -0.044730640947818756, -0.04709184914827347, -0.04821286350488663, -0.048722922801971436, -0.05108608305454254], "norm_scores": [0.10175888240337372, 0.1008644625544548, 0.10014268010854721, 0.10006554424762726, 0.09998688846826553, 0.09975861757993698, 0.09952334314584732, 0.09941184520721436, 0.09936115145683289, 0.09912661463022232]}}
{"id": "WebQTest-1982", "question": "what type of music did vivaldi compose", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["music.composer.genre"], ["music.album.genre"], ["book.written_work.author"], ["music.composition.composer"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["book.author.school_or_movement"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did vivaldi compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.composer.genre </PATH>", "<PATH> music.album.genre </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> book.author.school_or_movement </PATH>"], "scores": [-0.0026683371979743242, -0.04017407447099686, -0.10868276655673981, -0.11190450191497803, -0.1262979805469513, -0.13174310326576233, -0.13204391300678253, -0.13743406534194946, -0.1417660266160965, -0.14448288083076477], "norm_scores": [0.11096040159463882, 0.10687582939863205, 0.09979908168315887, 0.09947806596755981, 0.09805649518966675, 0.0975240170955658, 0.0974946841597557, 0.0969705879688263, 0.09655141830444336, 0.09628945589065552]}}
{"id": "WebQTest-1985", "question": "what team does mike fisher play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sport.team_coaches", "sports.sports_team.sport"], ["ice_hockey.hockey_player.hockey_position", "ice_hockey.hockey_team.captain"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["ice_hockey.hockey_player.hockey_position", "sports.sports_team.captain"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does mike fisher play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sport.team_coaches <SEP> sports.sports_team.sport </PATH>", "<PATH> ice_hockey.hockey_player.hockey_position <SEP> ice_hockey.hockey_team.captain </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> ice_hockey.hockey_player.hockey_position <SEP> sports.sports_team.captain </PATH>"], "scores": [-0.014786554500460625, -0.017144987359642982, -0.0236024372279644, -0.04850315302610397, -0.04971329867839813, -0.05049620196223259, -0.0598464161157608, -0.06671668589115143, -0.07177279889583588, -0.08226367831230164], "norm_scores": [0.10340190678834915, 0.10315833985805511, 0.10249433666467667, 0.09997367113828659, 0.09985275566577911, 0.09977461397647858, 0.09884604811668396, 0.09816927462816238, 0.0976741835474968, 0.09665483981370926]}}
{"id": "WebQTest-1986", "question": "what films has daniel craig been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"]], "ground_paths": [["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["film.film.executive_produced_by"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has daniel craig been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>"], "scores": [-0.020506026223301888, -0.03426564857363701, -0.03647954761981964, -0.038440193980932236, -0.03882007300853729, -0.041712112724781036, -0.04478627070784569, -0.044880565255880356, -0.04555196687579155, -0.053880155086517334], "norm_scores": [0.1019580289721489, 0.10056473314762115, 0.10034233331680298, 0.10014579445123672, 0.10010775178670883, 0.099818654358387, 0.09951227158308029, 0.09950288385152817, 0.09943611174821854, 0.09861142188310623]}}
{"id": "WebQTest-1987", "question": "what materials did eduardo paolozzi use in his work", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.art_forms", "visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists", "visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artist", "visual_art.art_period_movement.associated_artists"], ["visual_art.art_period_movement.associated_artists", "visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists", "visual_art.period_movement.associated_artists"], ["visual_art.art_form.artists", "visual_art.visual_artist.art_forms"], ["visual_art.art_form.artists"], ["visual_art.visual_artist.art_forms", "visual_art.art_form.visual_artist"]], "ground_paths": [["visual_art.visual_artist.artworks", "visual_art.artwork.media"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what materials did eduardo paolozzi use in his work [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.art_forms <SEP> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists <SEP> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artist <SEP> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.art_period_movement.associated_artists <SEP> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists <SEP> visual_art.period_movement.associated_artists </PATH>", "<PATH> visual_art.art_form.artists <SEP> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_form.artists </PATH>", "<PATH> visual_art.visual_artist.art_forms <SEP> visual_art.art_form.visual_artist </PATH>"], "scores": [-0.002482348121702671, -0.033897049725055695, -0.04616639018058777, -0.07299506664276123, -0.07511955499649048, -0.08303304761648178, -0.08418795466423035, -0.08802194148302078, -0.09269904345273972, -0.09309042990207672], "norm_scores": [0.10663867741823196, 0.10334073752164841, 0.10208055377006531, 0.09937828779220581, 0.09916738420724869, 0.09838571399450302, 0.09827214479446411, 0.09789609909057617, 0.09743929654359818, 0.09740117192268372]}}
{"id": "WebQTest-1988", "question": "what did joe biden study in college", "prediction": [["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["education.education.student", "education.education.field_of_study"], ["people.person.education", "education.education.field_of_study"], ["people.person.education", "education.field_of_study.students_major_field"], ["people.person.education", "education.field_of_study.students_majoring"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.subjects", "book.written_work.school_or_movement"], ["education.education.student", "education.education.major_field_of_study"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joe biden study in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> education.education.student <SEP> education.education.field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.field_of_study </PATH>", "<PATH> people.person.education <SEP> education.field_of_study.students_major_field </PATH>", "<PATH> people.person.education <SEP> education.field_of_study.students_majoring </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.015517381951212883, -0.01680387184023857, -0.028316909447312355, -0.04243677482008934, -0.05899867042899132, -0.06339213252067566, -0.07336360216140747, -0.07582417875528336, -0.08326131105422974, -0.08485956490039825], "norm_scores": [0.10391858965158463, 0.10378498584032059, 0.1025969609618187, 0.10115848481655121, 0.09949690103530884, 0.09906072914600372, 0.09807785600423813, 0.09783682972192764, 0.09711189568042755, 0.09695681929588318]}}
{"id": "WebQTest-1990", "question": "who plays timon 's voice in the lion king", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.film", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.starring", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["music.recording.releases", "music.featured_artist.recordings"], ["music.recording.releases", "music.artist.track"], ["film.film.starring", "film.actor.film"], ["film.film.language", "people.person.languages"], ["film.film.featured_song", "music.recording.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays timon 's voice in the lion king [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>"], "scores": [-0.02889314293861389, -0.02912607043981552, -0.030908221378922462, -0.03640228882431984, -0.03859585151076317, -0.04136877879500389, -0.04152114689350128, -0.043615590780973434, -0.043801650404930115, -0.060900501906871796], "norm_scores": [0.10106362402439117, 0.10104008764028549, 0.10086017847061157, 0.10030756890773773, 0.10008777678012848, 0.09981062263250351, 0.09979540854692459, 0.099586620926857, 0.09956809133291245, 0.097880057990551]}}
{"id": "WebQTest-1991", "question": "what part of the country is ohio in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"], ["base.locations.states_and_provences.country"], ["base.locations.countries.states_provinces_within"], ["location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.first_level_division_of"], ["location.location.partially_contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.locations.states_and_provences.country </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.012059614062309265, -0.018375007435679436, -0.06663785129785538, -0.07712824642658234, -0.07752576470375061, -0.0785222053527832, -0.08069119602441788, -0.08442801237106323, -0.08517951518297195, -0.09033486247062683], "norm_scores": [0.1056191548705101, 0.10495423525571823, 0.10000912845134735, 0.09896548092365265, 0.09892614930868149, 0.09882762283086777, 0.09861350059509277, 0.09824568778276443, 0.09817188233137131, 0.09766708314418793]}}
{"id": "WebQTest-1992", "question": "which island is kailua on", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.island.island_group"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.biblioness.bibs_location.country"], ["base.schemastaging.disputed_location_claimant.disputed_territories"], ["base.island.island.island_group"], ["geography.island.body_of_water"], ["base.island.island_group.islands_in_group"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which island is kailua on [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories </PATH>", "<PATH> base.island.island.island_group </PATH>", "<PATH> geography.island.body_of_water </PATH>", "<PATH> base.island.island_group.islands_in_group </PATH>"], "scores": [-0.008765008300542831, -0.03268910199403763, -0.04471449926495552, -0.06708507984876633, -0.08052714914083481, -0.0826459601521492, -0.09280996024608612, -0.10435744374990463, -0.10584856569766998, -0.10720328986644745], "norm_scores": [0.10654293745756149, 0.1040242463350296, 0.10278081148862839, 0.10050705820322037, 0.09916508197784424, 0.0989551842212677, 0.09795451164245605, 0.09682989120483398, 0.09668560326099396, 0.0965547189116478]}}
{"id": "WebQTest-1993", "question": "what language do speak in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.language_family.geographic_distribution"], ["location.country.currency_formerly_used"], ["base.rosetta.languoid.languoid_class"], ["location.country.currency_used"], ["location.country.language_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do speak in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.language_spoken </PATH>", "</PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0070969657972455025, -0.028338538482785225, -0.050229012966156006, -0.0601184107363224, -0.16658678650856018, -0.16738569736480713, -0.18267133831977844, -0.19389311969280243, -0.19789424538612366, -0.19871731102466583], "norm_scores": [0.1122320294380188, 0.10987319052219391, 0.10749414563179016, 0.10643632709980011, 0.09568662941455841, 0.09561021625995636, 0.09415987133979797, 0.093109130859375, 0.0927373394370079, 0.09266103804111481]}}
{"id": "WebQTest-1994", "question": "what did whoopi goldberg won a grammy for", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_winner.awards_won", "award.award_category.winners"], ["people.person.profession", "award.award_discipline.awards_in_this_discipline"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["people.person.nationality", "award.award_discipline.awards_in_this_discipline"], ["award.award_winner.awards_won", "award.award_honor.honored_for"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_category.nominees"], ["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did whoopi goldberg won a grammy for [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_category.winners </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_category.winners </PATH>", "<PATH> people.person.profession <SEP> award.award_discipline.awards_in_this_discipline </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> people.person.nationality <SEP> award.award_discipline.awards_in_this_discipline </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.honored_for </PATH>"], "scores": [-0.016681665554642677, -0.021800853312015533, -0.025061435997486115, -0.030315319076180458, -0.03551798313856125, -0.036522869020700455, -0.05300028249621391, -0.05767729505896568, -0.0606498159468174, -0.06260266155004501], "norm_scores": [0.10234398394823074, 0.1018214002251625, 0.10148994624614716, 0.10095813125371933, 0.10043424367904663, 0.10033336281776428, 0.09869367629289627, 0.098233163356781, 0.09794159978628159, 0.09775052219629288]}}
{"id": "WebQTest-1996", "question": "who played maggie in himym", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["tv.tv_program_writer_relationship.tv_program", "tv.tv_writer.tv_programs"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_program.languages", "people.person.languages"], ["tv.tv_series_season.regular_cast", "tv.tv_actor.starring_roles"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.place_of_birth", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played maggie in himym [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program_writer_relationship.tv_program <SEP> tv.tv_writer.tv_programs </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.languages <SEP> people.person.languages </PATH>", "<PATH> tv.tv_series_season.regular_cast <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.017523856833577156, -0.035837672650814056, -0.03635571151971817, -0.03925488516688347, -0.048785340040922165, -0.04880603402853012, -0.05089867487549782, -0.05640457198023796, -0.057441238313913345, -0.06418879330158234], "norm_scores": [0.10283366590738297, 0.10096752643585205, 0.10091523826122284, 0.10062308609485626, 0.09966866672039032, 0.09966660290956497, 0.0994582548737526, 0.09891214966773987, 0.09880967438220978, 0.09814518690109253]}}
{"id": "WebQTest-1997", "question": "who does owen schmitt play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["people.person.nationality", "sports.sports_team_location.teams"]], "ground_paths": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does owen schmitt play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>"], "scores": [-0.01966642029583454, -0.021682681515812874, -0.027780825272202492, -0.028074370697140694, -0.03385733440518379, -0.04495372250676155, -0.04917408153414726, -0.056027039885520935, -0.06156470626592636, -0.06379461288452148], "norm_scores": [0.10210869461297989, 0.1019030287861824, 0.1012834906578064, 0.10125376284122467, 0.10066990554332733, 0.09955901652574539, 0.09913972020149231, 0.09846264868974686, 0.09791889786720276, 0.09770078957080841]}}
{"id": "WebQTest-1998", "question": "what party was thomas jefferson affiliated with", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.politician.party", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["influence.influence_node.influenced_by", "organization.organization.founders"], ["organization.organization.founders"], ["base.culturalevent.event.entity_involved", "base.politicalconventions.political_convention.party"], ["government.politician.party", "government.political_party.politicians_in_this_party"], ["symbols.namesake.named_after", "organization.organization.founders"]], "ground_paths": [["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was thomas jefferson affiliated with [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> organization.organization.founders </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.politician.party <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> symbols.namesake.named_after <SEP> organization.organization.founders </PATH>"], "scores": [-0.018712183460593224, -0.01901419647037983, -0.02653547003865242, -0.029968274757266045, -0.04421035945415497, -0.0603640154004097, -0.06354419142007828, -0.0652783215045929, -0.06710992008447647, -0.0712902843952179], "norm_scores": [0.10280731320381165, 0.10277626663446426, 0.10200615972280502, 0.10165659338235855, 0.10021905601024628, 0.09861315786838531, 0.09830005466938019, 0.09812972694635391, 0.09795016050338745, 0.09754154831171036]}}
{"id": "WebQTest-1999", "question": "what religions are popular in france", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["royalty.kingdom.rulers", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical.subjects"], ["religion.religion.countries"], ["people.person.quotations", "people.person.religion"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["people.person.nationality", "people.deceased_person.religion"], ["symbols.namesake.named_after"]], "ground_paths": [["periodicals.newspaper_circulation_area.newspapers", "book.periodical.subjects"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religions are popular in france [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.subjects </PATH>", "<PATH> religion.religion.countries </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> people.person.nationality <SEP> people.deceased_person.religion </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.009583382867276669, -0.013020599260926247, -0.07301906496286392, -0.09128838032484055, -0.10290177166461945, -0.10625731945037842, -0.11337316036224365, -0.11904395371675491, -0.1266990602016449, -0.1303955465555191], "norm_scores": [0.10812193155288696, 0.10775092989206314, 0.10147616267204285, 0.09963908791542053, 0.09848864376544952, 0.09815870970487595, 0.09746270626783371, 0.09691158682107925, 0.09617254883050919, 0.0958176925778389]}}
{"id": "WebQTest-2000", "question": "who played berger in sex and the city", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["film.film.country", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.film", "film.performance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["film.film.starring", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["film.performance.film", "film.actor.film"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played berger in sex and the city [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>"], "scores": [-0.02318604849278927, -0.02362663671374321, -0.04702651500701904, -0.04955055192112923, -0.052836962044239044, -0.053402528166770935, -0.05496156960725784, -0.05757995322346687, -0.0590820349752903, -0.0626361146569252], "norm_scores": [0.10254333913326263, 0.10249818116426468, 0.10012757033109665, 0.09987516701221466, 0.09954746812582016, 0.099491186439991, 0.09933619946241379, 0.09907644242048264, 0.09892772883176804, 0.09857675433158875]}}
{"id": "WebQTest-2001", "question": "who won the 2000 fa cup final", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["sports.sports_award.award", "sports.sports_award.award_winner"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.runner_up"], ["sports.sports_award_type.winners", "sports.sports_award_winner.awards"], ["time.event.locations", "people.person.place_of_birth"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award.award_winner"], ["time.event.instance_of_recurring_event", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the 2000 fa cup final [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_award.award <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_award.award <SEP> sports.sports_award_winner.awards </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_award_type.winners <SEP> sports.sports_award_winner.awards </PATH>", "<PATH> time.event.locations <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.019913358613848686, -0.022438257932662964, -0.024372989311814308, -0.02979693002998829, -0.03294059634208679, -0.03395794332027435, -0.036407578736543655, -0.0373397134244442, -0.0544605627655983, -0.06258629262447357], "norm_scores": [0.10155442357063293, 0.10129833221435547, 0.10110253095626831, 0.10055564343929291, 0.10024002194404602, 0.1001380980014801, 0.09989309310913086, 0.09980002045631409, 0.09810590744018555, 0.09731195867061615]}}
{"id": "WebQTest-2002", "question": "what are the names of michael jackson 's kids", "prediction": [["people.person.parents"], ["people.person.children"], ["music.recording.artist", "music.recording.artist"], ["music.composition.composer", "people.person.parents"], ["music.composition.composer"], ["music.artist.genre", "music.artist.genre"], ["music.artist.origin", "people.person.place_of_birth"], ["music.composition.composer", "people.person.children"], ["music.recording.artist", "people.person.parents"], ["people.person.profession"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>", "<PATH> music.composition.composer <SEP> people.person.parents </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.origin <SEP> people.person.place_of_birth </PATH>", "<PATH> music.composition.composer <SEP> people.person.children </PATH>", "<PATH> music.recording.artist <SEP> people.person.parents </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.008133594878017902, -0.019361557438969612, -0.09253387898206711, -0.09417448937892914, -0.10162980109453201, -0.10519516468048096, -0.11213503777980804, -0.11551063507795334, -0.12392837554216385, -0.12630607187747955], "norm_scores": [0.10843272507190704, 0.1072220578789711, 0.09965653717517853, 0.09949317574501038, 0.09875418245792389, 0.0984027162194252, 0.09772217273712158, 0.09739286452531815, 0.09657647460699081, 0.09634711593389511]}}
{"id": "WebQTest-2003", "question": "what movies have robert pattinson in them", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["music.artist.track", "music.recording.releases"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["music.recording.artist", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies have robert pattinson in them [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.honored_for </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.028856826946139336, -0.028993451967835426, -0.030880676582455635, -0.033113930374383926, -0.03450903296470642, -0.03522729501128197, -0.03563641011714935, -0.040622029453516006, -0.042001619935035706, -0.04371748864650726], "norm_scores": [0.10065075755119324, 0.10063701122999191, 0.10044726729393005, 0.10022319108247757, 0.10008347034454346, 0.10001160949468613, 0.09997069835662842, 0.09947352111339569, 0.09933638572692871, 0.09916608035564423]}}
{"id": "WebQTest-2004", "question": "what the time zone in japan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.location.time_zones"], ["base.locations.countries.continent"], ["language.human_language.countries_spoken_in", "time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["location.country.form_of_government"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in japan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.location.time_zones </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.0004690545902121812, -0.06369433552026749, -0.10545291751623154, -0.13655507564544678, -0.1588040590286255, -0.1601622849702835, -0.16076043248176575, -0.16404138505458832, -0.1722085028886795, -0.18275174498558044], "norm_scores": [0.11370907723903656, 0.10674235224723816, 0.10237672924995422, 0.09924159944057465, 0.09705794602632523, 0.09692621231079102, 0.09686825424432755, 0.09655095636844635, 0.0957656279206276, 0.09476125240325928]}}
{"id": "WebQTest-2005", "question": "who plays bob kelso in scrubs", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.tv_program_writer_relationship.tv_program", "tv.tv_writer.tv_programs"], ["tv.tv_program_creator.programs_created"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays bob kelso in scrubs [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_program_writer_relationship.tv_program <SEP> tv.tv_writer.tv_programs </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.015752241015434265, -0.027461543679237366, -0.0334184393286705, -0.040263354778289795, -0.04814359173178673, -0.049081116914749146, -0.05186004191637039, -0.05210240185260773, -0.05281204357743263, -0.05475734919309616], "norm_scores": [0.10270962119102478, 0.10151397436857224, 0.10091106593608856, 0.10022269189357758, 0.09943602979183197, 0.09934284538030624, 0.09906716644763947, 0.09904315322637558, 0.09897289425134659, 0.0987805500626564]}}
{"id": "WebQTest-2006", "question": "what race is vanessa carlton", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality"], ["people.person.nationality", "people.person.nationality"], ["people.person.nationality", "people.ethnicity.people"], ["music.artist.genre", "music.artist.genre"], ["music.artist.origin", "people.person.nationality"], ["people.person.nationality", "people.person.ethnicity"], ["people.ethnicity.people", "people.person.ethnicity"], ["people.person.ethnicity", "people.person.ethnicity"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what race is vanessa carlton [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> people.ethnicity.people </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.origin <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.ethnicity </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.ethnicity </PATH>"], "scores": [-0.0064956131391227245, -0.04184044152498245, -0.05249885097146034, -0.06432802230119705, -0.0713331326842308, -0.07752985507249832, -0.07877751439809799, -0.08515610545873642, -0.092379130423069, -0.09771011769771576], "norm_scores": [0.10618062317371368, 0.10249323397874832, 0.10140661895275116, 0.10021413117647171, 0.09951457381248474, 0.09889981150627136, 0.09877649694681168, 0.09814844280481339, 0.09744207561016083, 0.09692399203777313]}}
{"id": "WebQTest-2007", "question": "what do people in czech republic speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"], ["people.ethnicity.languages_spoken"], ["location.country.language_spoken"], ["film.film.country"], ["location.country.currency_used"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in czech republic speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>", "<PATH> location.country.language_spoken </PATH>", "<PATH> film.film.country </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.01267238799482584, -0.014119290746748447, -0.06686196476221085, -0.06916598230600357, -0.15289217233657837, -0.18176999688148499, -0.18670254945755005, -0.18983235955238342, -0.1936097890138626, -0.19673572480678558], "norm_scores": [0.11174731701612473, 0.11158574372529984, 0.10585291683673859, 0.10560932010412216, 0.09712710976600647, 0.09436238557100296, 0.09389808773994446, 0.0936046689748764, 0.09325174987316132, 0.09296070784330368]}}
{"id": "WebQTest-2008", "question": "which countries speak arabic language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.form_of_government"], ["base.rosetta.local_name.locale"], ["base.rosetta.languoid.languoid_class"], ["base.rosetta.local_name.languoid"], ["language.human_language.region"], ["location.country.currency_used"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak arabic language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>", "<PATH> base.rosetta.local_name.languoid </PATH>", "<PATH> language.human_language.region </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.016856420785188675, -0.017301414161920547, -0.033058300614356995, -0.07962972670793533, -0.15727080404758453, -0.16474805772304535, -0.1656130850315094, -0.1683662235736847, -0.1818784922361374, -0.1842089146375656], "norm_scores": [0.11026418209075928, 0.11021512746810913, 0.10849208384752274, 0.10355531424283981, 0.09581936150789261, 0.09510556608438492, 0.09502333402633667, 0.09476207941770554, 0.09349024295806885, 0.0932726263999939]}}
{"id": "WebQTest-2009", "question": "where does tim cook work", "prediction": [["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["organization.organization.founders"], ["people.person.place_of_birth"], ["organization.organization_founder.organizations_founded"], ["people.person.employer", "business.employer.employees"], ["people.person.nationality"]], "ground_paths": [["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.employment_history", "business.employment_tenure.company"], ["organization.leadership.person", "organization.organization.leadership"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim cook work [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> people.person.employer <SEP> business.employer.employees </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.01964816078543663, -0.03490166366100311, -0.041326407343149185, -0.042048752307891846, -0.04429430142045021, -0.06120502948760986, -0.06343341618776321, -0.06399435549974442, -0.06456919759511948, -0.06780466437339783], "norm_scores": [0.10310283303260803, 0.10154208540916443, 0.10089179128408432, 0.1008189395070076, 0.10059279948472977, 0.098906010389328, 0.0986858531832695, 0.09863051027059555, 0.09857383370399475, 0.09825541079044342]}}
{"id": "WebQTest-2010", "question": "what degrees does romney have", "prediction": [["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.degree"], ["common.topic.notable_types", "common.topic.notable_types"], ["common.topic.notable_types"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.religion", "religion.religion.degrees"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.country.first_level_divisions"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees does romney have [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.religion <SEP> religion.religion.degrees </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.country.first_level_divisions </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.018284272402524948, -0.018482942134141922, -0.0195157527923584, -0.09284504503011703, -0.0933377593755722, -0.09903449565172195, -0.10251887142658234, -0.10555139183998108, -0.1055992916226387, -0.10849600285291672], "norm_scores": [0.10590288043022156, 0.10588184744119644, 0.10577254742383957, 0.09829387068748474, 0.09824544936418533, 0.09768736362457275, 0.09734758734703064, 0.09705281257629395, 0.09704817086458206, 0.09676745533943176]}}
{"id": "WebQTest-2011", "question": "where is bob marley buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"], ["people.person.place_of_burial"], ["symbols.name_source.namesakes"], ["people.deceased_person.place_of_death"], ["location.location.people_buried_here"], ["location.location.people_born_here"], ["music.artist.place_of_interment"], ["symbols.namesake.named_after"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is bob marley buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>", "<PATH> people.person.place_of_burial </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_buried_here </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.place_of_interment </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.003599157789722085, -0.032158590853214264, -0.08377327769994736, -0.10460487753152847, -0.11104205250740051, -0.11189967393875122, -0.12927940487861633, -0.13325288891792297, -0.13498075306415558, -0.13703010976314545], "norm_scores": [0.10981189459562302, 0.10672008991241455, 0.10135150700807571, 0.09926202893257141, 0.09862511605024338, 0.09854056686162949, 0.09684275835752487, 0.09645871818065643, 0.09629219770431519, 0.09609506279230118]}}
{"id": "WebQTest-2012", "question": "what is london uk time zone", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.biblioness.bibs_location.country"], ["location.location.time_zones", "location.location.time_zones"], ["locations.location.time_zones"], ["location.location.time_Zones"], ["location.location.time_zonessymbol"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is london uk time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> locations.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.location.time_zonessymbol </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-4.430609988048673e-05, -0.10302247852087021, -0.18894465267658234, -0.19087298214435577, -0.19755259156227112, -0.19799195230007172, -0.20838826894760132, -0.2117188274860382, -0.22722813487052917, -0.22896865010261536], "norm_scores": [0.11889483779668808, 0.10726058483123779, 0.0984293520450592, 0.0982397198677063, 0.09758570790290833, 0.09754284471273422, 0.09653401374816895, 0.09621303528547287, 0.09473235160112381, 0.09456761181354523]}}
{"id": "WebQTest-2013", "question": "where to stay in chicago tourist", "prediction": [["location.location.containedby"], ["travel.travel_destination.accommodation"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["travel.travel_destination.tourist_accommodations"], ["organization.organization.geographic_scope", "travel.travel_destination.accommodation"], ["metropolitan_transit.transit_system.area_served", "travel.travel_destination.accommodation"], ["travel.travel_destination.tourist_destinations"], ["film.film.featured_film_locations"], ["base.arthist.helynevek.intezmeny"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where to stay in chicago tourist [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.travel_destination.tourist_accommodations </PATH>", "<PATH> organization.organization.geographic_scope <SEP> travel.travel_destination.accommodation </PATH>", "<PATH> metropolitan_transit.transit_system.area_served <SEP> travel.travel_destination.accommodation </PATH>", "<PATH> travel.travel_destination.tourist_destinations </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>"], "scores": [-0.021108176559209824, -0.022184530273079872, -0.025188175961375237, -0.030039440840482712, -0.11374685913324356, -0.14314639568328857, -0.150456041097641, -0.15920628607273102, -0.1652601659297943, -0.1654047816991806], "norm_scores": [0.10794860869646072, 0.10783247649669647, 0.107509084045887, 0.10698878765106201, 0.0983976274728775, 0.09554687887430191, 0.09485102444887161, 0.0940246656537056, 0.0934571772813797, 0.09344366192817688]}}
{"id": "WebQTest-2014", "question": "what type of breast cancer did sheryl crow have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.parent_disease"], ["music.artist.genre", "music.artist.genre"], ["people.person.medical_condition", "medicine.disease.notable_people_with_this_condition"], ["medicine.breast_cancer.breast_cancer"], ["common.topic.notable_types"], ["medicine.cancer_patient.notable_people_with_this_condition"], ["medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of breast cancer did sheryl crow have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.parent_disease </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.medical_condition <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.breast_cancer.breast_cancer </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> medicine.cancer_patient.notable_people_with_this_condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.0027247504331171513, -0.034534331411123276, -0.05238403379917145, -0.09696213155984879, -0.11135268956422806, -0.12002158910036087, -0.12179140001535416, -0.12380117177963257, -0.1245003342628479, -0.12469488382339478], "norm_scores": [0.10915917903184891, 0.10574152320623398, 0.10387080907821655, 0.09934213757514954, 0.09792277961969376, 0.09707757830619812, 0.096905916929245, 0.09671135246753693, 0.09664375334978104, 0.0966249480843544]}}
{"id": "WebQTest-2015", "question": "what type of government does argentina have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["people.person.nationality", "government.form_of_government.countries"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["government.form_of_government.countries", "location.country.form_of_government"], ["base.locations.countries.continent"], ["geography.river.basin_countries", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does argentina have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> geography.river.basin_countries <SEP> location.country.form_of_government </PATH>"], "scores": [-0.008846433833241463, -0.015618126839399338, -0.13651809096336365, -0.14259681105613708, -0.1573300063610077, -0.16521510481834412, -0.1656862497329712, -0.1701466292142868, -0.1717851608991623, -0.17230601608753204], "norm_scores": [0.1127372607588768, 0.11197640746831894, 0.09922482818365097, 0.09862350672483444, 0.09718111157417297, 0.09641784429550171, 0.09637242555618286, 0.09594352543354034, 0.0957864448428154, 0.09573657810688019]}}
{"id": "WebQTest-2016", "question": "where does fabio capello come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"], ["sports.sports_team_coach_tenure.coach", "sports.sports_team_coach_tenure.place_of_birth"], ["sports.sports_team_manager.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["sports.sports_team_coach_tenure.coach", "sports.sports_team_coach_tenure.place_coached"], ["sports.sports_team_coach_tenure.coach", "sports.sports_team_coach_tenure.location"], ["people.person.place_of_birth", "location.location.containedby"], ["sports.sports_team_manager.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does fabio capello come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> sports.sports_team_coach_tenure.coach <SEP> sports.sports_team_coach_tenure.place_of_birth </PATH>", "<PATH> sports.sports_team_manager.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> sports.sports_team_coach_tenure.coach <SEP> sports.sports_team_coach_tenure.place_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.coach <SEP> sports.sports_team_coach_tenure.location </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_team_manager.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.005606860388070345, -0.034168411046266556, -0.04837595671415329, -0.08616933971643448, -0.0880376547574997, -0.09358388930559158, -0.09805901348590851, -0.10130947828292847, -0.10178087651729584, -0.10330967605113983], "norm_scores": [0.1072399765253067, 0.10422036796808243, 0.10275011509656906, 0.09893930703401566, 0.0987546369433403, 0.09820843487977982, 0.09776992350816727, 0.09745263308286667, 0.09740670770406723, 0.09725790470838547]}}
{"id": "WebQTest-2017", "question": "what party was abe lincoln part of", "prediction": [["organization.organization_founder.organizations_founded"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["government.politician.party", "government.political_party_tenure.party"], ["influence.influence_node.influenced_by", "organization.organization.founders"], ["people.deceased_person.place_of_death", "location.location.containedby", "organization.organization.geographic_scope"], ["base.culturalevent.event.entity_involved", "base.politicalconventions.political_convention.party"], ["organization.organization.founders"], ["government.politician.party", "government.political_party.politicians_in_this_party"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["government.politician.party", "government.political_party_tenure.party"], ["influence.influence_node.influenced_by", "organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was abe lincoln part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> organization.organization.founders </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby <SEP> organization.organization.geographic_scope </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> government.politician.party <SEP> government.political_party.politicians_in_this_party </PATH>"], "scores": [-0.013783120550215244, -0.020445656031370163, -0.02583124116063118, -0.041355010122060776, -0.04920794069766998, -0.0592166893184185, -0.07678421586751938, -0.07712419331073761, -0.07814855873584747, -0.08110120892524719], "norm_scores": [0.10389510542154312, 0.10320519655942917, 0.10265087336301804, 0.10106965154409409, 0.1002790704369545, 0.09928040206432343, 0.09755153208971024, 0.09751836210489273, 0.09741852432489395, 0.09713130444288254]}}
{"id": "WebQTest-2018", "question": "what did jack london do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["influence.influence_node.influenced_by", "people.person.profession"], ["book.written_work.author"], ["film.film_story_contributor.film_story_credits"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jack london do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>"], "scores": [-0.001284765894524753, -0.055621616542339325, -0.0670432597398758, -0.09595020115375519, -0.11031117290258408, -0.12480894476175308, -0.1293753683567047, -0.13728709518909454, -0.1394217312335968, -0.14116530120372772], "norm_scores": [0.11029359698295593, 0.10446049273014069, 0.10327417403459549, 0.10033156722784042, 0.09890101104974747, 0.0974775105714798, 0.09703340381383896, 0.09626873582601547, 0.09606344997882843, 0.09589610248804092]}}
{"id": "WebQTest-2019", "question": "who did morgan freeman marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did morgan freeman marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.018894977867603302, -0.03596644848585129, -0.03996258229017258, -0.04267933592200279, -0.044875774532556534, -0.04628352075815201, -0.05011612921953201, -0.05134131386876106, -0.05224690958857536, -0.05408218875527382], "norm_scores": [0.10250084102153778, 0.10076584666967392, 0.10036397725343704, 0.10009168833494186, 0.09987208247184753, 0.09973158687353134, 0.09935007989406586, 0.09922844171524048, 0.09913861751556396, 0.09895683825016022]}}
{"id": "WebQTest-2020", "question": "what are the mountains in northern italy called", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.contains"], ["geography.mountain_range.mountains"], ["base.aareas.schema.administrative_area.administrative_parent"], ["geography.mountain.mountain_range"], ["base.aareas.schema.administrative_area.administrative_children"], ["language.human_language.countries_spoken_in"], ["location.location.primarily_containedby"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the mountains in northern italy called [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.primarily_containedby </PATH>"], "scores": [-0.013009794056415558, -0.028834044933319092, -0.03366321697831154, -0.04324312135577202, -0.08996856212615967, -0.09082953631877899, -0.10368746519088745, -0.11654599010944366, -0.11712248623371124, -0.11745864897966385], "norm_scores": [0.10635993629693985, 0.10469011217355728, 0.10418576747179031, 0.10319244116544724, 0.09848164767026901, 0.09839688986539841, 0.09713981300592422, 0.09589873254299164, 0.09584346413612366, 0.09581125527620316]}}
{"id": "WebQTest-2021", "question": "what influenced john steinbeck to start writing", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["people.person.profession"], ["book.written_work.author"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced john steinbeck to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> influence.influence_node.influenced <PATH>"], "scores": [-0.009609296917915344, -0.013462821952998638, -0.10399474948644638, -0.10448038578033447, -0.1314772367477417, -0.14111249148845673, -0.14356078207492828, -0.14425207674503326, -0.16035059094429016, -0.16292960941791534], "norm_scores": [0.11056805402040482, 0.11014280468225479, 0.10060941427946091, 0.10056056082248688, 0.09788205474615097, 0.09694346785545349, 0.09670642018318176, 0.09663958102464676, 0.09509628266096115, 0.09485134482383728]}}
{"id": "WebQTest-2022", "question": "where is dwayne johnson ethnicity", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality"], ["people.ethnicity.people_with_this_ethnicity"], ["people.ethnicity.people", "people.ethnicity.people"], ["people.ethnicity.people", "people.ethnicity.included_in_group"], ["people.ethnicity.people", "people.person.ethnicity"], ["people.ethnicity.included_in_group"], ["people.ethnicity.people", "people.ethnicity.languages_spoken"], ["people.ethnicity.people", "people.ethnicity.includes_groups"]], "ground_paths": [["people.person.ethnicity"], ["people.ethnicity.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwayne johnson ethnicity [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>", "<PATH> people.ethnicity.people <SEP> people.ethnicity.people </PATH>", "<PATH> people.ethnicity.people <SEP> people.ethnicity.included_in_group </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.ethnicity.people <SEP> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.people <SEP> people.ethnicity.includes_groups </PATH>"], "scores": [-0.016237175092101097, -0.01629951223731041, -0.04628673940896988, -0.04938846454024315, -0.05621766299009323, -0.07580424845218658, -0.0759245976805687, -0.08636283129453659, -0.09087404608726501, -0.10625284165143967], "norm_scores": [0.10463489592075348, 0.10462836921215057, 0.10153742879629135, 0.10122298449277878, 0.10053406655788422, 0.09858410805463791, 0.09857223927974701, 0.09754867106676102, 0.0971096009016037, 0.09562759846448898]}}
{"id": "WebQTest-2023", "question": "where are the nato headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.country"], ["book.book_subject.works", "book.written_work.subjects"], ["location.location.containedby"], ["military.military_combatant.includes_allies"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.founders", "base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nato headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> book.book_subject.works <SEP> book.written_work.subjects </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> military.military_combatant.includes_allies </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.007355715613812208, -0.04740627482533455, -0.05359450727701187, -0.05512425675988197, -0.06131120026111603, -0.062030963599681854, -0.06649314612150192, -0.06898747384548187, -0.07379743456840515, -0.07431601732969284], "norm_scores": [0.10507587343454361, 0.1009506955742836, 0.10032790899276733, 0.10017454624176025, 0.09955669194459915, 0.09948505461215973, 0.0990421324968338, 0.09879539906978607, 0.098321333527565, 0.09827035665512085]}}
{"id": "WebQTest-2024", "question": "in which state was the battle of antietam fought", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["common.image.appears_in_topic_gallery", "time.event.locations"], ["common.image.appears_in_topic_gallery", "location.location.events"], ["event.disaster.areas_affected"], ["book.written_work.subjects"], ["common.image.appears_in_topic_gallery"], ["meteorology.tropical_cyclone.affected_areas"], ["military.military_conflict.casualties", "location.location.events"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which state was the battle of antietam fought [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> common.image.appears_in_topic_gallery <SEP> time.event.locations </PATH>", "<PATH> common.image.appears_in_topic_gallery <SEP> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> common.image.appears_in_topic_gallery </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> military.military_conflict.casualties <SEP> location.location.events </PATH>"], "scores": [-0.0013138381764292717, -0.051083873957395554, -0.11551150679588318, -0.12774477899074554, -0.12896692752838135, -0.13380064070224762, -0.1391267031431198, -0.14719262719154358, -0.147298663854599, -0.14764592051506042], "norm_scores": [0.11180174350738525, 0.10637356340885162, 0.0997362807393074, 0.09852360934019089, 0.09840326756238937, 0.09792877733707428, 0.09740857779979706, 0.09662605077028275, 0.09661580622196198, 0.09658225625753403]}}
{"id": "WebQTest-2025", "question": "which ocean does the zambezi river flow into", "prediction": [["geography.river.mouth"], ["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.containedby", "location.location.containedby"], ["geography.geographical_feature_category.features"], ["location.location.partially_contains"], ["location.location.containedby", "geography.river.mouth"], ["geography.river.basin_countries"], ["geography.body_of_water.river"], ["location.location.containedby", "location.location.partially_containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which ocean does the zambezi river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> geography.geographical_feature_category.features </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby <SEP> geography.river.mouth </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> geography.body_of_water.river </PATH>", "<PATH> location.location.containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.0024122910108417273, -0.05087866261601448, -0.06768982112407684, -0.07598081976175308, -0.0898522362112999, -0.09061039984226227, -0.09113075584173203, -0.11377942562103271, -0.11934308707714081, -0.12152783572673798], "norm_scores": [0.10825454443693161, 0.1031329557299614, 0.10141365975141525, 0.10057631134986877, 0.09919080883264542, 0.09911563247442245, 0.09906407445669174, 0.09684562683105469, 0.09630830585956573, 0.09609811753034592]}}
{"id": "WebQTest-2027", "question": "what team did david beckham play for before la galaxy", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.loans", "soccer.football_player_loan.lending_team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for before la galaxy [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player_loan.player <SEP> soccer.football_player_loan.lending_team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>"], "scores": [-0.02350223809480667, -0.02430199272930622, -0.02613668702542782, -0.030958086252212524, -0.03192166984081268, -0.035827428102493286, -0.039982233196496964, -0.04468570277094841, -0.04508552700281143, -0.04549532011151314], "norm_scores": [0.10113168507814407, 0.10105083882808685, 0.10086560994386673, 0.10038046538829803, 0.10028378665447235, 0.09989286959171295, 0.09947869926691055, 0.09901189804077148, 0.09897232055664062, 0.09893176704645157]}}
{"id": "WebQTest-2028", "question": "who is the current leader of france 2010", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["fictional_universe.character_occupation.characters_with_this_occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation", "people.person.profession"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["government.government_position_held.jurisdiction_of_office", "government.political_appointer.appointees"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current leader of france 2010 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.political_appointer.appointees </PATH>"], "scores": [-0.0004492521402426064, -0.07205647975206375, -0.07777225226163864, -0.07825763523578644, -0.0792747288942337, -0.0839647725224495, -0.08940019458532333, -0.10018236935138702, -0.1016974225640297, -0.10304470360279083], "norm_scores": [0.10808617621660233, 0.10061704367399216, 0.10004357248544693, 0.09999503195285797, 0.09989338368177414, 0.09942597150802612, 0.09888700395822525, 0.0978265255689621, 0.09767842292785645, 0.09754691272974014]}}
{"id": "WebQTest-2029", "question": "where was the palace of knossos located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["location.location.contains"], ["architecture.structure.architect"], ["architecture.building.building_function"], ["architecture.architect.structures_designed"], ["symbols.name_source.namesakes"], ["architecture.location.containedby"], ["architecture.building.location"], ["architecture.structure.location"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the palace of knossos located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.building.building_function </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> architecture.location.containedby </PATH>", "<PATH> architecture.building.location </PATH>", "<PATH> architecture.structure.location </PATH>"], "scores": [-0.0009290318703278899, -0.0759071558713913, -0.09961360692977905, -0.10668984800577164, -0.11217554658651352, -0.11524297297000885, -0.12115813046693802, -0.12297292053699493, -0.1321147084236145, -0.1343102604150772], "norm_scores": [0.1105688139796257, 0.10258173197507858, 0.10017848759889603, 0.09947209805250168, 0.09892792254686356, 0.09862493723630905, 0.09804327040910721, 0.0978655070066452, 0.0969749242067337, 0.09676223993301392]}}
{"id": "WebQTest-2030", "question": "where is roswell area 51", "prediction": [["aviation.airport.serves"], ["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.nearby_airports"], ["aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.hud_county_place.county"], ["base.locations.countries.hud_county_place"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is roswell area 51 [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> base.locations.countries.hud_county_place </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.004442959558218718, -0.03144506737589836, -0.08015980571508408, -0.10914702713489532, -0.11540349572896957, -0.1262291520833969, -0.13078954815864563, -0.1381852775812149, -0.14457616209983826, -0.15161550045013428], "norm_scores": [0.11025537550449371, 0.10731808096170425, 0.10221540182828903, 0.0992949977517128, 0.09867570549249649, 0.09761323034763336, 0.09716908633708954, 0.09645310789346695, 0.09583865106105804, 0.09516637772321701]}}
{"id": "WebQTest-2031", "question": "when was father chris riley born", "prediction": [["people.person.place_of_birth"], ["people.person.children"], ["people.person.parents"], ["people.person.place_of_birth", "location.location.people_born_here"], ["people.person.nationality"], ["people.person.place_of_birth", "location.location.containedby"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["people.person.gender", "people.person.gender"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was father chris riley born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.001417043386027217, -0.0671037957072258, -0.0896795466542244, -0.09165757894515991, -0.09875024110078812, -0.10325627028942108, -0.10717583447694778, -0.11002986878156662, -0.1122545450925827, -0.11311473697423935], "norm_scores": [0.10914406180381775, 0.10220512747764587, 0.09992362558841705, 0.09972617030143738, 0.09902134537696838, 0.09857615828514099, 0.09819053113460541, 0.09791070222854614, 0.09769312292337418, 0.09760913252830505]}}
