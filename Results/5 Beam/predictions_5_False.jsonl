{"id": "WebQTest-0", "question": "what does jamaican people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does jamaican people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.008198565803468227, -0.023380694910883904, -0.051023658365011215, -0.0764419212937355, -0.15585646033287048], "norm_scores": [0.21098066866397858, 0.2078017294406891, 0.20213614404201508, 0.19706293940544128, 0.18201854825019836]}}
{"id": "WebQTest-1", "question": "what did james k polk do before he was president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "government.governmental_jurisdiction.government_positions"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did james k polk do before he was president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.nationality <SEP> government.government_office_or_title.jurisdiction </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> government.governmental_jurisdiction.government_positions </PATH>"], "scores": [-0.019876781851053238, -0.02230488881468773, -0.0279074739664793, -0.02823815867304802, -0.03507762774825096], "norm_scores": [0.20136266946792603, 0.20087432861328125, 0.199752077460289, 0.19968602061271667, 0.19832494854927063]}}
{"id": "WebQTest-3", "question": "who plays ken barlow in coronation street", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ken barlow in coronation street [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.017484398558735847, -0.026588493958115578, -0.03165874257683754, -0.03347013518214226, -0.03468649461865425], "norm_scores": [0.20226745307445526, 0.2004343420267105, 0.19942066073417664, 0.19905975461006165, 0.19881777465343475]}}
{"id": "WebQTest-6", "question": "where is jamarcus russell from", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.nationality"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.location"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jamarcus russell from [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.007794094737619162, -0.020432105287909508, -0.09204212576150894, -0.09605205804109573, -0.1026192158460617], "norm_scores": [0.21134045720100403, 0.20868633687496185, 0.19426482915878296, 0.19348739087581635, 0.19222091138362885]}}
{"id": "WebQTest-7", "question": "where was george washington carver from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["symbols.namesake.named_after"], ["people.person.ethnicity"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was george washington carver from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.005067558027803898, -0.03712364286184311, -0.03987021744251251, -0.11079514771699905, -0.11676410585641861], "norm_scores": [0.21149548888206482, 0.20482327044010162, 0.20426149666309357, 0.19027608633041382, 0.18914371728897095]}}
{"id": "WebQTest-8", "question": "what else did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0068788169883191586, -0.02558249793946743, -0.06954198330640793, -0.07523977011442184, -0.08946090936660767], "norm_scores": [0.2094070166349411, 0.20552672445774078, 0.19668757915496826, 0.19557011127471924, 0.19280852377414703]}}
{"id": "WebQTest-9", "question": "who was richard nixon married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["people.person.profession", "common.topic.notable_types"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_ownership.owner"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was richard nixon married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.022462034597992897, -0.033730532974004745, -0.04651813209056854, -0.04675724729895592, -0.04801511764526367], "norm_scores": [0.20342595875263214, 0.20114652812480927, 0.1985907107591629, 0.1985432356595993, 0.19829364120960236]}}
{"id": "WebQTest-12", "question": "who is governor of ohio 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["base.locations.states_and_provences.country", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is governor of ohio 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.01670314371585846, -0.021682146936655045, -0.032637082040309906, -0.03322640433907509, -0.03456251695752144], "norm_scores": [0.20221886038780212, 0.2012145221233368, 0.19902224838733673, 0.19890500605106354, 0.1986394226551056]}}
{"id": "WebQTest-13", "question": "who was vice president after kennedy died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president after kennedy died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>"], "scores": [-0.012038864195346832, -0.013505383394658566, -0.06316497176885605, -0.06659898161888123, -0.07510852813720703], "norm_scores": [0.20684777200222015, 0.2065446674823761, 0.19653823971748352, 0.1958644837141037, 0.19420483708381653]}}
{"id": "WebQTest-14", "question": "where is the fukushima daiichi nuclear plant located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.street_address", "location.mailing_address.citytown"], ["location.location.contains"], ["location.location.street_address", "location.mailing_address.state_province_region"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear plant located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.009365313686430454, -0.026427067816257477, -0.0394461452960968, -0.0509718582034111, -0.05974072217941284], "norm_scores": [0.20561031997203827, 0.202131986618042, 0.19951747357845306, 0.19723111391067505, 0.1955091655254364]}}
{"id": "WebQTest-16", "question": "what countries are part of the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.022299092262983322, -0.024499857798218727, -0.031083492562174797, -0.033969249576330185, -0.0427381657063961], "norm_scores": [0.20172588527202606, 0.20128244161605835, 0.1999616026878357, 0.1993853896856308, 0.1976446509361267]}}
{"id": "WebQTest-19", "question": "what is my timezone in louisiana", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.administrative_division.country", "location.location.time_zones"], ["location.administrative_division.country"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in louisiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.administrative_division.country <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-1.2789466381946113e-05, -0.15863995254039764, -0.17053784430027008, -0.18302783370018005, -0.1883160024881363], "norm_scores": [0.22948119044303894, 0.19581961631774902, 0.19350358843803406, 0.19110174477100372, 0.19009386003017426]}}
{"id": "WebQTest-20", "question": "what did st augustine do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st augustine do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.001891873893328011, -0.05469192937016487, -0.06710515916347504, -0.13363352417945862, -0.1342410147190094], "norm_scores": [0.21560831367969513, 0.20451951026916504, 0.20199644565582275, 0.18899524211883545, 0.18888045847415924]}}
{"id": "WebQTest-21", "question": "who is keyshia cole dad", "prediction": [["people.person.parents"], ["people.person.children"], ["fictional_universe.fictional_character.parents"], ["people.person.gender"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is keyshia cole dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.006424006074666977, -0.025616493076086044, -0.15417803823947906, -0.16749870777130127, -0.17100748419761658], "norm_scores": [0.2201135903596878, 0.2159293293952942, 0.18987950682640076, 0.18736694753170013, 0.1867106705904007]}}
{"id": "WebQTest-22", "question": "what kind government does egypt have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind government does egypt have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005580144934356213, -0.02349884808063507, -0.12492801249027252, -0.1276427060365677, -0.1393595039844513], "norm_scores": [0.21600064635276794, 0.21216465532779694, 0.19170036911964417, 0.19118066132068634, 0.1889537125825882]}}
{"id": "WebQTest-23", "question": "what town was martin luther king assassinated in", "prediction": [["people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.country"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town was martin luther king assassinated in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0016481397906318307, -0.06923016160726547, -0.07014200091362, -0.07187144458293915, -0.07628392428159714], "norm_scores": [0.21147383749485016, 0.19765423238277435, 0.19747410714626312, 0.197132870554924, 0.19626493752002716]}}
{"id": "WebQTest-24", "question": "where did edgar allan poe died", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["people.place_of_death.people", "people.place_of_death.location"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did edgar allan poe died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>"], "scores": [-0.00021357950754463673, -0.0911717638373375, -0.11157216131687164, -0.11233983933925629, -0.11921544373035431], "norm_scores": [0.2178928554058075, 0.19894835352897644, 0.19493086636066437, 0.19478125870227814, 0.19344662129878998]}}
{"id": "WebQTest-26", "question": "what to do today in atlanta with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["organization.organization.geographic_scope"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do today in atlanta with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.geographic_scope </PATH>"], "scores": [-0.015394269488751888, -0.01637439802289009, -0.024270929396152496, -0.12661093473434448, -0.12873327732086182], "norm_scores": [0.2093024104833603, 0.20909737050533295, 0.20745272934436798, 0.18727226555347443, 0.18687523901462555]}}
{"id": "WebQTest-28", "question": "what electorate does anna bligh represent", "prediction": [["people.person.nationality", "base.biblioness.bibs_location.country"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["people.person.nationality", "location.administrative_division.country"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["government.politician.government_positions_held", "government.political_district.representatives"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what electorate does anna bligh represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.014770488254725933, -0.019715730100870132, -0.025687307119369507, -0.04234358295798302, -0.04254825785756111], "norm_scores": [0.20285548269748688, 0.20185478031635284, 0.20065298676490784, 0.1973385363817215, 0.19729815423488617]}}
{"id": "WebQTest-31", "question": "where did andy murray started playing tennis", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_facility.teams"], ["people.person.place_of_birth"], ["sports.sports_team_location.teams"], ["sports.sports_facility.athlete", "sports.sports_facility.location"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did andy murray started playing tennis [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_facility.athlete <SEP> sports.sports_facility.location </PATH>"], "scores": [-0.022971341386437416, -0.033882275223731995, -0.0405946746468544, -0.04376387968659401, -0.05857834964990616], "norm_scores": [0.20341239869594574, 0.2012050449848175, 0.19985900819301605, 0.19922660291194916, 0.19629693031311035]}}
{"id": "WebQTest-32", "question": "what is the australian dollar called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_used", "common.topic.notable_types", "common.topic.notable_types"], ["location.country.currency_used", "location.location.containedby", "location.location.containedby"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the australian dollar called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.country.currency_used <SEP> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.007541714236140251, -0.029208030551671982, -0.06065724417567253, -0.06631316989660263, -0.07491306960582733], "norm_scores": [0.2081335484981537, 0.20367255806922913, 0.1973668932914734, 0.19625376164913177, 0.19457323849201202]}}
{"id": "WebQTest-33", "question": "what timezone is sweden", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.0005943930009379983, -0.06141196936368942, -0.15360409021377563, -0.1559600830078125, -0.19659656286239624], "norm_scores": [0.22335243225097656, 0.21017350256443024, 0.19166350364685059, 0.1912124902009964, 0.18359804153442383]}}
{"id": "WebQTest-34", "question": "who did cam newton sign with", "prediction": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cam newton sign with [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_passing_statistics.player <SEP> american_football.player_passing_statistics.team </PATH>"], "scores": [-0.0232354998588562, -0.02496255375444889, -0.025353122502565384, -0.028924720361828804, -0.03199504315853119], "norm_scores": [0.20073208212852478, 0.20038571953773499, 0.2003074586391449, 0.19959332048892975, 0.19898143410682678]}}
{"id": "WebQTest-35", "question": "what county is frederick md in", "prediction": [["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.location.contains"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is frederick md in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.019593793898820877, -0.021097684279084206, -0.02988540008664131, -0.03264179453253746, -0.08920472115278244], "norm_scores": [0.20374684035778046, 0.20344066619873047, 0.20166072249412537, 0.20110562443733215, 0.19004622101783752]}}
{"id": "WebQTest-36", "question": "what highschool did harper lee go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did harper lee go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.012917008250951767, -0.019593609496951103, -0.026807863265275955, -0.029773840680718422, -0.055305395275354385], "norm_scores": [0.20319700241088867, 0.20184485614299774, 0.200393944978714, 0.19980044662952423, 0.19476380944252014]}}
{"id": "WebQTest-37", "question": "what timezone is utah in", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is utah in [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-1.3368665349844377e-05, -0.14416950941085815, -0.18849097192287445, -0.19601520895957947, -0.1985430121421814], "norm_scores": [0.23063166439533234, 0.1996699571609497, 0.19101355969905853, 0.1895817071199417, 0.18910309672355652]}}
{"id": "WebQTest-38", "question": "who did george w. bush run against for the second term", "prediction": [["time.event.locations", "people.person.nationality"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.election.campaigns", "government.election_campaign.candidate"], ["government.election_campaign.election", "government.politician.election_campaigns"], ["government.election_campaign.election", "government.election_campaign.candidate"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did george w. bush run against for the second term [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations <SEP> people.person.nationality </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> government.election.campaigns <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election_campaign.election <SEP> government.politician.election_campaigns </PATH>", "<PATH> government.election_campaign.election <SEP> government.election_campaign.candidate </PATH>"], "scores": [-0.012816812843084335, -0.022768180817365646, -0.023361215367913246, -0.03744764253497124, -0.04625925421714783], "norm_scores": [0.20315337181091309, 0.20114172995090485, 0.20102249085903168, 0.19821064174175262, 0.19647178053855896]}}
{"id": "WebQTest-39", "question": "who is niall ferguson 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is niall ferguson 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02745852991938591, -0.02902214601635933, -0.031552113592624664, -0.03915245831012726, -0.04241051524877548], "norm_scores": [0.20129285752773285, 0.2009783685207367, 0.20047053694725037, 0.19895265996456146, 0.19830553233623505]}}
{"id": "WebQTest-41", "question": "where is the galapagos islands located on a world map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.island.island_group"], ["base.locations.continents.countries_within"], ["book.written_work.subjects"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the galapagos islands located on a world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.00022925972007215023, -0.08745642006397247, -0.10675983130931854, -0.12047289311885834, -0.1421787589788437], "norm_scores": [0.21882861852645874, 0.20054961740970612, 0.19671545922756195, 0.1940363049507141, 0.1898699700832367]}}
{"id": "WebQTest-42", "question": "what are the songs that justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"], ["music.composition.composer"], ["music.lyricist.lyrics_written"], ["music.artist.album"]], "ground_paths": [["music.featured_artist.recordings"], ["music.featured_artist.albums"], ["music.artist.album"], ["music.recording.artist"], ["music.album.artist"], ["music.recording.featured_artists"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["music.composer.compositions"], ["music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the songs that justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>", "<PATH> music.artist.album </PATH>"], "scores": [-0.018416479229927063, -0.021427761763334274, -0.033112116158008575, -0.05248163640499115, -0.07015491276979446], "norm_scores": [0.20414450764656067, 0.2035306990146637, 0.20116640627384186, 0.19730740785598755, 0.19385096430778503]}}
{"id": "WebQTest-43", "question": "what did george clemenceau do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george clemenceau do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.0034787158947438, -0.04795292392373085, -0.05247221514582634, -0.05460941419005394, -0.058190327137708664], "norm_scores": [0.20809055864810944, 0.19903866946697235, 0.1981411874294281, 0.19771817326545715, 0.19701142609119415]}}
{"id": "WebQTest-44", "question": "what is the state flower of arizona", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of arizona [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.012949727475643158, -0.016393637284636497, -0.022354749962687492, -0.024692794308066368, -0.049159254878759384], "norm_scores": [0.2024305909872055, 0.20173463225364685, 0.2005356401205063, 0.20006735622882843, 0.19523181021213531]}}
{"id": "WebQTest-45", "question": "where was rihanna born and raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"], ["people.person.place_of_birth"], ["music.album.artist"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna born and raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.007295887917280197, -0.02666286565363407, -0.055176991969347, -0.09078067541122437, -0.11336679011583328], "norm_scores": [0.2103787362575531, 0.20634354650974274, 0.20054292678833008, 0.1935284584760666, 0.18920640647411346]}}
{"id": "WebQTest-46", "question": "what year did president william henry harrison take office", "prediction": [["government.election_campaign.candidate", "government.election_campaign.election"], ["government.election_campaign.candidate", "time.event.instance_of_recurring_event"], ["people.person.nationality", "location.location.events"], ["government.us_president.vice_president"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did president william henry harrison take office [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.candidate <SEP> government.election_campaign.election </PATH>", "<PATH> government.election_campaign.candidate <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> people.person.nationality <SEP> location.location.events </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.019227851182222366, -0.03601621091365814, -0.03717632219195366, -0.03887300565838814, -0.039873190224170685], "norm_scores": [0.20301780104637146, 0.19963793456554413, 0.1994064599275589, 0.19906841218471527, 0.19886940717697144]}}
{"id": "WebQTest-47", "question": "where george lopez was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where george lopez was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.008128474466502666, -0.02050725929439068, -0.0822841227054596, -0.09304847568273544, -0.13802659511566162], "norm_scores": [0.2121792733669281, 0.20956894755363464, 0.1970142275094986, 0.1949048638343811, 0.18633264303207397]}}
{"id": "WebQTest-48", "question": "where are samsung based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.place_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are samsung based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.008792991749942303, -0.03241724148392677, -0.03926851972937584, -0.053424935787916183, -0.0559336356818676], "norm_scores": [0.2058909833431244, 0.2010839730501175, 0.19971099495887756, 0.19690372049808502, 0.19641035795211792]}}
{"id": "WebQTest-49", "question": "what did the islamic people believe in", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"], ["religion.deity.deity_of"], ["religion.religion.deities"], ["symbols.namesake.named_after"]], "ground_paths": [["religion.belief.belief_of"], ["religion.religion.beliefs"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the islamic people believe in [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> religion.religion.deities </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.00848609022796154, -0.017315184697508812, -0.10753759741783142, -0.12113732099533081, -0.12135707587003708], "norm_scores": [0.21350941061973572, 0.2116326093673706, 0.1933746337890625, 0.19076260924339294, 0.19072067737579346]}}
{"id": "WebQTest-51", "question": "who will play mr gray in the film", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will play mr gray in the film [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02220212295651436, -0.022337960079312325, -0.029427524656057358, -0.03206673264503479, -0.03920420631766319], "norm_scores": [0.20136971771717072, 0.20134237408638, 0.19991996884346008, 0.19939304888248444, 0.19797495007514954]}}
{"id": "WebQTest-52", "question": "what did george orwell died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["influence.influence_node.influenced_by", "people.deceased_person.cause_of_death"], ["deceased_person.cause_of_death"], ["influence.influence_node.influenced", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george orwell died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0015874766977503896, -0.04356297850608826, -0.1176808625459671, -0.12542854249477386, -0.13408119976520538], "norm_scores": [0.21698026359081268, 0.20806092023849487, 0.19319750368595123, 0.19170644879341125, 0.19005484879016876]}}
{"id": "WebQTest-54", "question": "what country did adolf hitler control", "prediction": [["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_commander.military_commands", "military.military_command.military_combatant"]], "ground_paths": [["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["people.deceased_person.place_of_death", "location.country.capital"], ["military.military_commander.military_commands", "military.military_command.military_combatant"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.political_appointer.appointees", "government.governmental_jurisdiction.governing_officials"], ["military.military_command.military_commander", "military.military_combatant.military_commanders"], ["organization.organization.founders", "organization.organization.place_founded"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.crime.crime.victim_s", "time.event.locations"], ["organization.organization_founder.organizations_founded", "organization.organization.place_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did adolf hitler control [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_command.military_combatant </PATH>"], "scores": [-0.026697462424635887, -0.02749873325228691, -0.028036735951900482, -0.030164064839482307, -0.04147065430879593], "norm_scores": [0.200813889503479, 0.2006530463695526, 0.20054513216018677, 0.20011894404888153, 0.19786903262138367]}}
{"id": "WebQTest-55", "question": "who is michael buble", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced_by", "people.person.profession"], ["common.topic.image"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael buble [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> common.topic.image </PATH>"], "scores": [-0.0003142687783110887, -0.08193723857402802, -0.136261984705925, -0.15830187499523163, -0.18688899278640747], "norm_scores": [0.22330576181411743, 0.20580291748046875, 0.19492100179195404, 0.19067195057868958, 0.1852983683347702]}}
{"id": "WebQTest-56", "question": "what county is kansas city kansas", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.hud_county_place.county"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is kansas city kansas [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.017870476469397545, -0.022520044818520546, -0.0378895029425621, -0.04269877076148987, -0.058597102761268616], "norm_scores": [0.20361994206905365, 0.20267540216445923, 0.19958418607711792, 0.19862665235996246, 0.19549378752708435]}}
{"id": "WebQTest-58", "question": "who did jackie robinson first play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.lifetime_batting_statistics", "baseball.lifetime_batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jackie robinson first play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.baseball_player.lifetime_batting_statistics <SEP> baseball.lifetime_batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.013897929340600967, -0.0170788262039423, -0.033279936760663986, -0.034692905843257904, -0.03633524477481842], "norm_scores": [0.20263995230197906, 0.20199640095233917, 0.19875019788742065, 0.19846956431865692, 0.1981438845396042]}}
{"id": "WebQTest-59", "question": "when did annie open", "prediction": [["theater.theater_character.plays_appears_in"], ["theater.play.characters"], ["film.film.directed_by"], ["film.film.country"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did annie open [/INST]", "raw_output": {"paths": ["<PATH> theater.theater_character.plays_appears_in </PATH>", "<PATH> theater.play.characters </PATH>", "<PATH> film.film.directed_by </PATH>", "<PATH> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.030787494033575058, -0.04275864362716675, -0.04378097504377365, -0.055958036333322525, -0.06236032024025917], "norm_scores": [0.20328281819820404, 0.2008637934923172, 0.20065855979919434, 0.19822995364665985, 0.19696487486362457]}}
{"id": "WebQTest-60", "question": "where did eleanor roosevelt die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.person.place_of_birth"], ["symbols.name_source.namesakes"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did eleanor roosevelt die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.0015145593788474798, -0.06722194701433182, -0.07930757105350494, -0.08446228504180908, -0.08644726127386093], "norm_scores": [0.21274179220199585, 0.199212446808815, 0.19681933522224426, 0.19580739736557007, 0.1954191029071808]}}
{"id": "WebQTest-61", "question": "what are the religions practiced in indonesia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.kingdom.rulers", "people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["religion.religion.countries"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["book.book_subject.works", "book.written_work.subjects"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the religions practiced in indonesia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> religion.religion.countries </PATH>"], "scores": [-0.006135350558906794, -0.01719113625586033, -0.07303917407989502, -0.09222913533449173, -0.09601482003927231], "norm_scores": [0.2102678269147873, 0.2079559564590454, 0.1966603696346283, 0.1929224580526352, 0.19219347834587097]}}
{"id": "WebQTest-62", "question": "what was jesse james killed with", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.deceased_person.weapons", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse james killed with [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.weapons <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.001480209524743259, -0.06016581878066063, -0.07126670330762863, -0.07580886036157608, -0.08323631435632706], "norm_scores": [0.21161964535713196, 0.19955801963806152, 0.1973549723625183, 0.19646058976650238, 0.19500680267810822]}}
{"id": "WebQTest-63", "question": "who was vp for lincoln", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.political_party_tenure.politician", "government.political_party_tenure.politician"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for lincoln [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.politician </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.007774852681905031, -0.018630653619766235, -0.09710129350423813, -0.1026405468583107, -0.10555537790060043], "norm_scores": [0.21186023950576782, 0.20957277715206146, 0.19375614821910858, 0.19268585741519928, 0.19212500751018524]}}
{"id": "WebQTest-64", "question": "what other books did charles dickens write", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"], ["fictional_universe.fictional_character.character_created_by"], ["film.film_story_contributor.film_story_credits"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["book.written_work.author"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did charles dickens write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>"], "scores": [-0.005766851361840963, -0.029115011915564537, -0.048150770366191864, -0.05315623804926872, -0.06532443314790726], "norm_scores": [0.20698270201683044, 0.20220601558685303, 0.19839328527450562, 0.1974027007818222, 0.19501523673534393]}}
{"id": "WebQTest-65", "question": "who was the first dictator of the soviet union", "prediction": [["people.person.nationality"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["organization.organization_founder.organizations_founded"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first dictator of the soviet union [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>"], "scores": [-0.015901822596788406, -0.01953849010169506, -0.03267011418938637, -0.03511454164981842, -0.04305999353528023], "norm_scores": [0.2026786208152771, 0.20194289088249207, 0.1993083655834198, 0.1988217830657959, 0.19724830985069275]}}
{"id": "WebQTest-66", "question": "what is the currency of puerto rico called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"], ["location.country.currency_used", "location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of puerto rico called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0005839678342454135, -0.061679571866989136, -0.13329510390758514, -0.13768696784973145, -0.14576518535614014], "norm_scores": [0.21962589025497437, 0.20660938322544098, 0.19233034551143646, 0.19148750603199005, 0.18994686007499695]}}
{"id": "WebQTest-67", "question": "what kind of cancer did carl wilson have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.cancer_patient.cancer_type"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did carl wilson have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.cancer_patient.cancer_type </PATH>"], "scores": [-0.012385585345327854, -0.015975898131728172, -0.0354125089943409, -0.05253196507692337, -0.11529979854822159], "norm_scores": [0.20676085352897644, 0.20601986348628998, 0.2020541876554489, 0.19862458109855652, 0.18654055893421173]}}
{"id": "WebQTest-68", "question": "who plays the voice of kitt in knight rider", "prediction": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.languages", "people.person.languages"], ["tv.tv_program_creator.programs_created"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of kitt in knight rider [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.languages <SEP> people.person.languages </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>"], "scores": [-0.016017744317650795, -0.020926332101225853, -0.021741768345236778, -0.056423455476760864, -0.05844695121049881], "norm_scores": [0.20373845100402832, 0.2027408331632614, 0.20257556438446045, 0.19567035138607025, 0.19527482986450195]}}
{"id": "WebQTest-69", "question": "what county is brentwood tennessee in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is brentwood tennessee in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01453614141792059, -0.023292971774935722, -0.03540460765361786, -0.03540878742933273, -0.07904618978500366], "norm_scores": [0.20460358262062073, 0.20281973481178284, 0.20037806034088135, 0.20037724077701569, 0.19182132184505463]}}
{"id": "WebQTest-71", "question": "who did michael j fox marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did michael j fox marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.025646446272730827, -0.027381209656596184, -0.040427058935165405, -0.04672067612409592, -0.04684428870677948], "norm_scores": [0.20235680043697357, 0.20200607180595398, 0.1993878334760666, 0.19813689589500427, 0.19811241328716278]}}
{"id": "WebQTest-72", "question": "what battles did stonewall jackson fight in", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts"], ["military.military_conflict.military_personnel_involved"], ["military.military_command.military_commander", "military.military_conflict.commanders"], ["book.written_work.subjects"]], "ground_paths": [["people.person.nationality", "book.written_work.subjects"], ["base.culturalevent.event.entity_involved"], ["common.topic.image"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["military.military_conflict.military_personnel_involved"], ["military.military_person.participated_in_conflicts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what battles did stonewall jackson fight in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_command.military_commander <SEP> military.military_conflict.commanders </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.011255338788032532, -0.029525654390454292, -0.030198609456419945, -0.03214335814118385, -0.055585313588380814], "norm_scores": [0.2041192352771759, 0.20042376220226288, 0.20028892159461975, 0.1998998075723648, 0.19526825845241547]}}
{"id": "WebQTest-73", "question": "what language do the maasai tribe speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken", "language.human_language.region", "language.human_language.region"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai tribe speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.region <SEP> language.human_language.region </PATH>"], "scores": [-0.0006479345029219985, -0.08105751127004623, -0.09000980108976364, -0.090569868683815, -0.09200732409954071], "norm_scores": [0.21441075205802917, 0.19784502685070038, 0.1960817575454712, 0.19597196578979492, 0.19569048285484314]}}
{"id": "WebQTest-74", "question": "who did benjamin franklin get married to", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.children"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did benjamin franklin get married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.028391852974891663, -0.030767422169446945, -0.031349584460258484, -0.03695536404848099, -0.04197376221418381], "norm_scores": [0.20109973847866058, 0.2006225883960724, 0.2005058228969574, 0.19938497245311737, 0.19838689267635345]}}
{"id": "WebQTest-75", "question": "what disease did patrick swayze died from", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.people"], ["people.person.medical_condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did patrick swayze died from [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.005158272106200457, -0.027568839490413666, -0.03892406076192856, -0.08018025010824203, -0.1169319599866867], "norm_scores": [0.2097928673028946, 0.20514358580112457, 0.2028273046016693, 0.19462968409061432, 0.1876065582036972]}}
{"id": "WebQTest-76", "question": "what works of art did leonardo da vinci produce", "prediction": [["visual_art.visual_artist.art_forms"], ["film.film.subjects"], ["visual_art.art_form.artists"], ["visual_art.visual_art_form.artists"], ["visual_art.art_form.artist"]], "ground_paths": [["visual_art.artwork.artist"], ["visual_art.visual_artist.artworks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what works of art did leonardo da vinci produce [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> visual_art.art_form.artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.art_form.artist </PATH>"], "scores": [-0.0008198689902201295, -0.07521290332078934, -0.08424278348684311, -0.09553032368421555, -0.10649086534976959], "norm_scores": [0.21470166742801666, 0.19930900633335114, 0.19751738011837006, 0.19530043005943298, 0.19317151606082916]}}
{"id": "WebQTest-77", "question": "what capital of austria", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.first_level_division_of"], ["location.country.first_level_divisions"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital of austria [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.country.first_level_divisions </PATH>"], "scores": [-0.006843313574790955, -0.0388750359416008, -0.06436818093061447, -0.06844503432512283, -0.06881383806467056], "norm_scores": [0.20864881575107574, 0.20207133889198303, 0.19698502123355865, 0.19618357717990875, 0.19611123204231262]}}
{"id": "WebQTest-78", "question": "what type of cancer did eva peron have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.people"], ["medicine.cancer_patient.cancer_type"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.cancer_patient.cancer_type </PATH>"], "scores": [-0.008040118962526321, -0.027215782552957535, -0.036165010184049606, -0.038503702729940414, -0.11914582550525665], "norm_scores": [0.20755009353160858, 0.20360809564590454, 0.2017941027879715, 0.20132270455360413, 0.18572500348091125]}}
{"id": "WebQTest-79", "question": "what country did buddha come from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"], ["people.person.ethnicity"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.0067318156361579895, -0.03340662643313408, -0.048447467386722565, -0.06352154910564423, -0.09087875485420227], "norm_scores": [0.20846755802631378, 0.20298023521900177, 0.19995008409023285, 0.19695864617824554, 0.19164343178272247]}}
{"id": "WebQTest-80", "question": "what county is greeley colorado in", "prediction": [["location.us_county.county_seat"], ["location.us_county.hud_county_place"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.location.contains"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is greeley colorado in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.017801322042942047, -0.024760473519563675, -0.026572056114673615, -0.032804474234580994, -0.08367900550365448], "norm_scores": [0.2038450539112091, 0.20243138074874878, 0.20206500589847565, 0.2008095681667328, 0.19084899127483368]}}
{"id": "WebQTest-82", "question": "what did peter tchaikovsky do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did peter tchaikovsky do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.0008741500787436962, -0.06996553391218185, -0.07292211800813675, -0.08268898725509644, -0.11774683743715286], "norm_scores": [0.21390976011753082, 0.19962944090366364, 0.19904008507728577, 0.1971055567264557, 0.19031518697738647]}}
{"id": "WebQTest-83", "question": "which countries speak german officially", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.local_name.locale"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak german officially [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>"], "scores": [-0.01800289936363697, -0.021006107330322266, -0.02396588772535324, -0.12208102643489838, -0.13888464868068695], "norm_scores": [0.20927727222442627, 0.20864970982074738, 0.20803305506706238, 0.18859122693538666, 0.18544869124889374]}}
{"id": "WebQTest-84", "question": "what type of music did john lennon sing", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["music.artist.album", "music.album.genre"], ["music.artist.label"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did john lennon sing [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.artist.album <SEP> music.album.genre </PATH>", "<PATH> music.artist.label </PATH>"], "scores": [-0.0011991525534540415, -0.05504981428384781, -0.1138552576303482, -0.11603392660617828, -0.12050482630729675], "norm_scores": [0.2164463996887207, 0.20509889721870422, 0.1933857500553131, 0.19296488165855408, 0.1921040564775467]}}
{"id": "WebQTest-85", "question": "who are the colorado representatives", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.political_district.elections", "common.image.appears_in_topic_gallery"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the colorado representatives [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.elections <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.025101711973547935, -0.027027826756238937, -0.02835155464708805, -0.03942829743027687, -0.04128500074148178], "norm_scores": [0.20142796635627747, 0.2010403722524643, 0.2007744163274765, 0.19856277108192444, 0.19819442927837372]}}
{"id": "WebQTest-86", "question": "which country does greenland belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["location.country.administrative_divisions"], ["location.location.contains"], ["common.topic.notable_types"]], "ground_paths": [["location.country.administrative_divisions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country does greenland belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.01740480586886406, -0.03391647711396217, -0.038326866924762726, -0.045874543488025665, -0.0549929179251194], "norm_scores": [0.20416660606861115, 0.20082314312458038, 0.19993939995765686, 0.19843599200248718, 0.19663481414318085]}}
{"id": "WebQTest-87", "question": "what the zip code for seattle washington", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the zip code for seattle washington [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_codes <PATH>"], "scores": [-0.00016003964992705733, -0.09008311480283737, -0.15424023568630219, -0.16956152021884918, -0.17594127357006073], "norm_scores": [0.22450841963291168, 0.20520102977752686, 0.19244936108589172, 0.18952324986457825, 0.18831799924373627]}}
{"id": "WebQTest-88", "question": "who is willow smith mom name", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"], ["music.artist.genre", "music.artist.genre"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is willow smith mom name [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> base.popstra.celebrity.breakup <SEP> base.popstra.celebrity.breakup </PATH>"], "scores": [-0.007839732803404331, -0.022685816511511803, -0.12791533768177032, -0.13284710049629211, -0.14252330362796783], "norm_scores": [0.21604806184768677, 0.21286427974700928, 0.1916029453277588, 0.19066034257411957, 0.1888243705034256]}}
{"id": "WebQTest-89", "question": "what do you call the chinese writing system", "prediction": [["language.human_language.writing_system"], ["language.language_writing_system.writing_systems"], ["location.country.writing_system"], ["language.language_writing_system.writing_system"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.writing_system"], ["language.language_writing_system.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call the chinese writing system [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.writing_system </PATH>", "<PATH> language.language_writing_system.writing_systems </PATH>", "<PATH> location.country.writing_system </PATH>", "<PATH> language.language_writing_system.writing_system </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.008052196353673935, -0.04435911774635315, -0.052800919860601425, -0.053480975329875946, -0.05587051436305046], "norm_scores": [0.2070617824792862, 0.19967883825302124, 0.19800031185150146, 0.19786567986011505, 0.19739344716072083]}}
{"id": "WebQTest-90", "question": "who played on the jeffersons", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played on the jeffersons [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.023991812020540237, -0.024889009073376656, -0.02863135188817978, -0.030799629166722298, -0.03213658928871155], "norm_scores": [0.2008202224969864, 0.20064014196395874, 0.19989067316055298, 0.19945771992206573, 0.19919122755527496]}}
{"id": "WebQTest-91", "question": "what is the name of the san francisco newspaper", "prediction": [["book.newspaper.circulation_areas"], ["periodicals.newspaper_circulation_area.newspapers"], ["location.location.containedby"], ["book.newspaper.headquarters"], ["book.periodical.language"]], "ground_paths": [["periodicals.newspaper_circulation_area.newspapers"], ["book.newspaper.circulation_areas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the san francisco newspaper [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> book.newspaper.headquarters </PATH>", "<PATH> book.periodical.language </PATH>"], "scores": [-0.003122257301583886, -0.029983438551425934, -0.09309271723031998, -0.09721030294895172, -0.10738911479711533], "norm_scores": [0.21282745897769928, 0.2071867436170578, 0.1945154070854187, 0.1937161087989807, 0.1917543113231659]}}
{"id": "WebQTest-93", "question": "what continent does armenia belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does armenia belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.014887981116771698, -0.02459562197327614, -0.028707440942525864, -0.05693603307008743, -0.06786642968654633], "norm_scores": [0.2047569453716278, 0.2027788609266281, 0.20194678008556366, 0.19632582366466522, 0.194191575050354]}}
{"id": "WebQTest-94", "question": "what did randy savage died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["people.person.deceased_cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did randy savage died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.person.deceased_cause_of_death </PATH>"], "scores": [-0.0008843456744216383, -0.05453190580010414, -0.12343238294124603, -0.13181772828102112, -0.134139284491539], "norm_scores": [0.2181050032377243, 0.2067125141620636, 0.19294951856136322, 0.19133831560611725, 0.19089464843273163]}}
{"id": "WebQTest-95", "question": "what was the title of the book charles darwin wrote", "prediction": [["book.written_work.author"], ["common.topic.notable_types"], ["book.book_edition.author_editor"], [], ["book.written_work.subjects"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["book.written_work.author"], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the title of the book charles darwin wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.0014558013062924147, -0.07544206082820892, -0.07550685852766037, -0.07782676070928574, -0.08014030754566193], "norm_scores": [0.21239933371543884, 0.19725196063518524, 0.19723917543888092, 0.19678214192390442, 0.19632738828659058]}}
{"id": "WebQTest-96", "question": "where did richard nixon die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people"], ["deceased_person.place_of_death"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard nixon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.0005533869261853397, -0.07791638374328613, -0.09886692464351654, -0.10648947954177856, -0.11098465323448181], "norm_scores": [0.2161298245191574, 0.20003977417945862, 0.19589242339134216, 0.19440491497516632, 0.1935330033302307]}}
{"id": "WebQTest-97", "question": "what songs does smokey robinson sing", "prediction": [["music.recording.artist"], ["music.artist.track"], ["music.composition.composer"], ["music.composer.compositions"], ["music.album.artist"]], "ground_paths": [["music.featured_artist.recordings"], ["music.artist.album"], ["music.producer.releases_produced"], ["music.recording.artist"], ["music.album.artist"], ["music.recording.producer"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["music.recording.featured_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs does smokey robinson sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.014374496415257454, -0.03723517060279846, -0.04707207903265953, -0.048819344490766525, -0.051833268254995346], "norm_scores": [0.20514479279518127, 0.200508251786232, 0.19854553043842316, 0.19819892942905426, 0.19760246574878693]}}
{"id": "WebQTest-98", "question": "who was vice president when jfk was president", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president when jfk was president [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.010204569436609745, -0.014708216302096844, -0.07142660021781921, -0.08045706152915955, -0.08833645284175873], "norm_scores": [0.20863229036331177, 0.20769481360912323, 0.1962425261735916, 0.1944783478975296, 0.1929520070552826]}}
{"id": "WebQTest-99", "question": "what is serbian language called", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.language_family"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is serbian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.017783496528863907, -0.025761602446436882, -0.03597822040319443, -0.03822685778141022, -0.06992032378911972], "norm_scores": [0.20395731925964355, 0.20233659446239471, 0.20027993619441986, 0.1998300701379776, 0.19359607994556427]}}
{"id": "WebQTest-100", "question": "what language is spoken in haiti today", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in haiti today [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.011659243144094944, -0.016996579244732857, -0.05151597410440445, -0.06749901920557022, -0.16662749648094177], "norm_scores": [0.21018394827842712, 0.20906512439250946, 0.20197145640850067, 0.1987689882516861, 0.18001045286655426]}}
{"id": "WebQTest-102", "question": "who played barbara gordon batgirl", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played barbara gordon batgirl [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.011415372602641582, -0.032407596707344055, -0.03937609866261482, -0.04394613206386566, -0.04733380675315857], "norm_scores": [0.2047349065542221, 0.2004818618297577, 0.1990896463394165, 0.1981818825006485, 0.19751165807247162]}}
{"id": "WebQTest-103", "question": "what basketball teams has shaq played for", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball teams has shaq played for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.021226663142442703, -0.021570198237895966, -0.02483324334025383, -0.026104334741830826, -0.03955959901213646], "norm_scores": [0.20108485221862793, 0.20101578533649445, 0.20036092400550842, 0.2001064121723175, 0.19743198156356812]}}
{"id": "WebQTest-104", "question": "where is jay leno from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jay leno from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.004563184920698404, -0.03420838341116905, -0.05960696190595627, -0.10090991109609604, -0.1302746832370758], "norm_scores": [0.21243816614151, 0.20623283088207245, 0.20106077194213867, 0.19292552769184113, 0.18734268844127655]}}
{"id": "WebQTest-105", "question": "what language do people from thailand speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from thailand speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.015738392248749733, -0.019109096378087997, -0.029345247894525528, -0.06310205906629562, -0.13638752698898315], "norm_scores": [0.20733097195625305, 0.2066333144903183, 0.20452895760536194, 0.19773994386196136, 0.18376673758029938]}}
{"id": "WebQTest-106", "question": "who did armie hammer play in the social network", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did armie hammer play in the social network [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.018967488780617714, -0.026983138173818588, -0.027963608503341675, -0.03343658894300461, -0.04191124066710472], "norm_scores": [0.2021830528974533, 0.20056889951229095, 0.2003723531961441, 0.19927871227264404, 0.19759702682495117]}}
{"id": "WebQTest-107", "question": "what was thomas jefferson role in the declaration of independence", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.governmental_body", "government.government_office_or_title.office_holders"], ["book.author.book_editions_published"], ["book.written_work.subjects"], ["book.written_work.author"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was thomas jefferson role in the declaration of independence [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.029783766716718674, -0.03138388693332672, -0.032761137932538986, -0.03372528403997421, -0.0399165116250515], "norm_scores": [0.20074625313282013, 0.20042529702186584, 0.20014944672584534, 0.19995658099651337, 0.19872242212295532]}}
{"id": "WebQTest-108", "question": "what was the book written by charles darwin", "prediction": [["book.written_work.subjects"], ["book.written_work.author"], ["book.book_subject.works"], ["book.book_edition.author_editor"], ["common.topic.notable_types"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["book.written_work.author"], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the book written by charles darwin [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.013645205646753311, -0.01765778474509716, -0.056723855435848236, -0.061841752380132675, -0.06753368675708771], "norm_scores": [0.20600225031375885, 0.20517730712890625, 0.19731637835502625, 0.19630911946296692, 0.19519491493701935]}}
{"id": "WebQTest-109", "question": "who did tim tebow play college football for", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality", "location.location.containedby"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["people.person.education", "education.education.institution"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tim tebow play college football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.016059787943959236, -0.022531526163220406, -0.02806081995368004, -0.0363459549844265, -0.044613391160964966], "norm_scores": [0.2027004510164261, 0.20139284431934357, 0.20028237998485565, 0.19862985610961914, 0.19699446856975555]}}
{"id": "WebQTest-110", "question": "who does peyton manning play football for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peyton manning play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.009654486551880836, -0.021168209612369537, -0.031120188534259796, -0.03605367988348007, -0.04879079759120941], "norm_scores": [0.20396170020103455, 0.20162680745124817, 0.19963018596172333, 0.19864772260189056, 0.1961335837841034]}}
{"id": "WebQTest-111", "question": "where is the carpathian mountain range located", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.contains"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the carpathian mountain range located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.016322394832968712, -0.016768338158726692, -0.03735468164086342, -0.0663575679063797, -0.08587861806154251], "norm_scores": [0.20564515888690948, 0.20555347204208374, 0.2013651430606842, 0.19560883939266205, 0.19182738661766052]}}
{"id": "WebQTest-112", "question": "what was robert burns", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"], ["influence.influence_node.influenced", "people.person.profession"], ["influence.influence_node.influenced"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.00378662277944386, -0.043738435953855515, -0.06073978170752525, -0.07096658647060394, -0.0872761532664299], "norm_scores": [0.2100663036108017, 0.20183922350406647, 0.1984366774559021, 0.1964176446199417, 0.19324015080928802]}}
{"id": "WebQTest-114", "question": "who did the voice of darth vader in episode 3", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice of darth vader in episode 3 [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.025564230978488922, -0.027205022051930428, -0.03158476576209068, -0.03378146514296532, -0.03693723678588867], "norm_scores": [0.20109127461910248, 0.2007616013288498, 0.19988423585891724, 0.19944563508033752, 0.19881722331047058]}}
{"id": "WebQTest-115", "question": "what super bowl did peyton manning win", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_championship.events"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what super bowl did peyton manning win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.005561273079365492, -0.037737030535936356, -0.04431597888469696, -0.05532906949520111, -0.061440978199243546], "norm_scores": [0.20714972913265228, 0.2005906105041504, 0.19927528500556946, 0.1970926821231842, 0.19589175283908844]}}
{"id": "WebQTest-116", "question": "which country was justin bieber born in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["music.artist.origin"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was justin bieber born in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.007229674607515335, -0.026592573150992393, -0.05423498898744583, -0.0562882162630558, -0.1058463379740715], "norm_scores": [0.20863233506679535, 0.20463146269321442, 0.19905243813991547, 0.19864414632320404, 0.1890396624803543]}}
{"id": "WebQTest-118", "question": "who fought in the gulf war 1991", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["time.event.locations"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"]], "ground_paths": [["film.film_genre.films_in_this_genre", "film.film.country"], ["time.event.locations"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "tv.tv_program.country_of_origin"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["film.film.genre", "film.film.country"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["film.film.subjects", "film.film.country"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "film.film.country"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the gulf war 1991 [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>", "<PATH> time.event.includes_event <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> time.event.included_in_event <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.020263854414224625, -0.02920619770884514, -0.03257247433066368, -0.03356653451919556, -0.0337115153670311], "norm_scores": [0.20192670822143555, 0.20012906193733215, 0.19945649802684784, 0.19925832748413086, 0.199229434132576]}}
{"id": "WebQTest-119", "question": "who plays london tipton in suite life on deck", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.genre", "music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays london tipton in suite life on deck [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.015268526040017605, -0.029820958152413368, -0.032304417341947556, -0.03621968626976013, -0.043120380491018295], "norm_scores": [0.203233003616333, 0.20029687881469727, 0.19980007410049438, 0.19901934266090393, 0.19765068590641022]}}
{"id": "WebQTest-121", "question": "what team did david beckham play for in 2011", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["book.written_work.subjects", "book.written_work.subjects"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for in 2011 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.022366046905517578, -0.024906981736421585, -0.028720224276185036, -0.028875533491373062, -0.030859094113111496], "norm_scores": [0.20095723867416382, 0.20044727623462677, 0.19968436658382416, 0.19965337216854095, 0.1992577165365219]}}
{"id": "WebQTest-122", "question": "where did francisco coronado come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did francisco coronado come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.007554952520877123, -0.022445660084486008, -0.06633611768484116, -0.08066911995410919, -0.11562703549861908], "norm_scores": [0.21029652655124664, 0.20718824863433838, 0.1982913464307785, 0.19546949863433838, 0.1887543648481369]}}
{"id": "WebQTest-123", "question": "what all does google now do", "prediction": [["organization.organization.sectors"], ["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.sectors"], ["business.brand.products"]], "ground_paths": [["projects.project_participation.participant", "projects.project_participation.project"], ["internet.website.owner"], ["business.consumer_company.products"], ["computer.computer_manufacturer_brand.computer_models"], ["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"], ["computer.software_developer.software"], ["business.consumer_company.products", "business.consumer_product.company"], ["computer.software.developer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all does google now do [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.sectors </PATH>", "<PATH> business.brand.products </PATH>"], "scores": [-0.04303588718175888, -0.04585406929254532, -0.046794965863227844, -0.05332082882523537, -0.05560487136244774], "norm_scores": [0.2011784464120865, 0.20061230659484863, 0.20042362809181213, 0.1991199404001236, 0.19866566359996796]}}
{"id": "WebQTest-124", "question": "what language does egyptian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does egyptian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.03664849326014519, -0.03691074624657631, -0.03783171623945236, -0.040427543222904205, -0.0406033955514431], "norm_scores": [0.20036722719669342, 0.2003146857023239, 0.20013029873371124, 0.19961145520210266, 0.19957636296749115]}}
{"id": "WebQTest-125", "question": "what are abraham sons names", "prediction": [["people.person.children"], ["people.person.parents"], ["fictional_universe.fictional_character.parents"], ["people.person.sibling_s"], ["fictional_universe.fictional_character.children"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are abraham sons names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.person.sibling_s </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>"], "scores": [-0.005784861743450165, -0.030546262860298157, -0.09311256557703018, -0.09755471348762512, -0.10594411194324493], "norm_scores": [0.21236255764961243, 0.20716871321201324, 0.19460409879684448, 0.1937415599822998, 0.19212298095226288]}}
{"id": "WebQTest-126", "question": "who wrote the jana gana mana", "prediction": [["music.composition.composer"], ["music.composition.lyricist"], ["music.composer.compositions"], ["music.lyricist.lyrics_written"], ["people.person.national_anthem"]], "ground_paths": [["music.composition.composer"], ["music.lyricist.lyrics_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the jana gana mana [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>", "<PATH> people.person.national_anthem </PATH>"], "scores": [-0.008211682550609112, -0.02889719046652317, -0.043204132467508316, -0.07993479818105698, -0.09855271130800247], "norm_scores": [0.2087879776954651, 0.20451346039772034, 0.2016083151102066, 0.19433745741844177, 0.190752774477005]}}
{"id": "WebQTest-127", "question": "what type of cancer did gilda radner die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.cancer_patient.cancer_type"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did gilda radner die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.cancer_patient.cancer_type </PATH>"], "scores": [-0.00829212088137865, -0.026275916025042534, -0.02833271585404873, -0.055642034858465195, -0.12418866902589798], "norm_scores": [0.20804481208324432, 0.20433680713176727, 0.20391696691513062, 0.19842348992824554, 0.18527790904045105]}}
{"id": "WebQTest-128", "question": "who plays juni cortez", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays juni cortez [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.020571697503328323, -0.029332976788282394, -0.032530542463064194, -0.03581620380282402, -0.039311591535806656], "norm_scores": [0.20219606161117554, 0.20043230056762695, 0.19979241490364075, 0.19913706183433533, 0.19844220578670502]}}
{"id": "WebQTest-129", "question": "what was lucille ball", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lucille ball [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.0015132855623960495, -0.06353341788053513, -0.07349666208028793, -0.09302246570587158, -0.13062579929828644], "norm_scores": [0.21450763940811157, 0.20160801708698273, 0.19960930943489075, 0.19574958086013794, 0.18852542340755463]}}
{"id": "WebQTest-131", "question": "who inspired obama", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who inspired obama [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.01007079891860485, -0.01599695160984993, -0.11110568791627884, -0.11563490331172943, -0.11707753688097], "norm_scores": [0.2129322588443756, 0.21167412400245667, 0.19246980547904968, 0.191600039601326, 0.19132381677627563]}}
{"id": "WebQTest-132", "question": "who is michael j fox wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.022271446883678436, -0.03634810820221901, -0.0388694666326046, -0.04488663747906685, -0.045179061591625214], "norm_scores": [0.20306415855884552, 0.20022571086883545, 0.19972150027751923, 0.1985233575105667, 0.19846531748771667]}}
{"id": "WebQTest-133", "question": "where did dolly parton grow up", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin", "location.location.people_born_here"], ["music.artist.origin", "location.location.containedby"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did dolly parton grow up [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin <SEP> location.location.people_born_here </PATH>", "<PATH> music.artist.origin <SEP> location.location.containedby </PATH>"], "scores": [-0.00867469608783722, -0.03153165802359581, -0.03289606049656868, -0.0991547629237175, -0.12529060244560242], "norm_scores": [0.2102213352918625, 0.20547080039978027, 0.20519065856933594, 0.19203563034534454, 0.18708162009716034]}}
{"id": "WebQTest-134", "question": "what type of books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.010760513134300709, -0.021795814856886864, -0.04109925776720047, -0.05730415880680084, -0.07581380754709244], "norm_scores": [0.20615655183792114, 0.20389404892921448, 0.19999593496322632, 0.19678114354610443, 0.19317230582237244]}}
{"id": "WebQTest-138", "question": "what are the four main languages spoken in spain", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four main languages spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.01753162406384945, -0.01841837354004383, -0.024768292903900146, -0.0608561746776104, -0.12604400515556335], "norm_scores": [0.2063271850347519, 0.206144317984581, 0.20483945310115814, 0.197579026222229, 0.18511010706424713]}}
{"id": "WebQTest-139", "question": "where is the nra headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.place_founded"], ["organization.organization.place_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the nra headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.003603600664064288, -0.03819592297077179, -0.05104748159646988, -0.06732701510190964, -0.06979639828205109], "norm_scores": [0.20859937369823456, 0.2015068233013153, 0.1989337056875229, 0.19572138786315918, 0.19523867964744568]}}
{"id": "WebQTest-141", "question": "who is gimli 's father in the hobbit", "prediction": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.parents"], ["people.person.gender"], ["fictional_universe.fictional_character.children"], ["people.person.parents"]], "ground_paths": [["fictional_universe.fictional_character.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is gimli 's father in the hobbit [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.029574673622846603, -0.03038782812654972, -0.03939561918377876, -0.03979460150003433, -0.045903462916612625], "norm_scores": [0.20148898661136627, 0.2013252228498459, 0.19951987266540527, 0.19944027066230774, 0.19822561740875244]}}
{"id": "WebQTest-142", "question": "who developed the tcp ip reference model", "prediction": [["law.invention.inventor"], ["base.biblioness.bibs_location.country", "people.person.nationality"], ["common.topic.notable_types"], ["internet.website.owner", "organization.organization_founder.organizations_founded"], ["internet.website.owner"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who developed the tcp ip reference model [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> internet.website.owner <SEP> organization.organization_founder.organizations_founded </PATH>", "<PATH> internet.website.owner </PATH>"], "scores": [-0.010567519813776016, -0.04855065047740936, -0.06088171899318695, -0.06783321499824524, -0.07591290771961212], "norm_scores": [0.20856159925460815, 0.20078834891319275, 0.19832761585712433, 0.19695372879505157, 0.19536879658699036]}}
{"id": "WebQTest-145", "question": "what college did albert einstein go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.employment_history", "business.employer.employees"]], "ground_paths": [["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "organization.organization.founders"], ["people.person.education", "education.education.institution"], ["business.employment_tenure.person", "business.employer.employees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did albert einstein go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>"], "scores": [-0.02078746072947979, -0.022353529930114746, -0.023281851783394814, -0.036355700343847275, -0.04766093194484711], "norm_scores": [0.20185786485671997, 0.2015419900417328, 0.20135498046875, 0.19873963296413422, 0.19650548696517944]}}
{"id": "WebQTest-146", "question": "what team does jordan own", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team_owner.teams_owned"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.professional_sports_team.owner_s"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jordan own [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team_owner.teams_owned </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.professional_sports_team.owner_s </PATH>"], "scores": [-0.0035203995648771524, -0.0440678708255291, -0.04940316826105118, -0.05328967794775963, -0.07594870030879974], "norm_scores": [0.2084636688232422, 0.2001800686120987, 0.19911488890647888, 0.19834253191947937, 0.19389882683753967]}}
{"id": "WebQTest-147", "question": "who is the state governor of tennessee", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.administrative_division.country", "people.person.nationality"]], "ground_paths": [["base.locations.countries.states_provinces_within", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of tennessee [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>"], "scores": [-0.020837612450122833, -0.025245731696486473, -0.028033310547471046, -0.0282964538782835, -0.03399728611111641], "norm_scores": [0.20129120349884033, 0.20040583610534668, 0.1998479664325714, 0.19979538023471832, 0.19865962862968445]}}
{"id": "WebQTest-149", "question": "what do portuguese people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do portuguese people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.012878783978521824, -0.0225230660289526, -0.033658791333436966, -0.07233191281557083, -0.13401517271995544], "norm_scores": [0.20841889083385468, 0.20641851425170898, 0.20413264632225037, 0.19638890027999878, 0.184641033411026]}}
{"id": "WebQTest-150", "question": "what language does cuba speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does cuba speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.007869028486311436, -0.021818310022354126, -0.07542552053928375, -0.07608012855052948, -0.17067696154117584], "norm_scores": [0.21255609393119812, 0.2096116840839386, 0.19867084920406342, 0.1985408365726471, 0.180620476603508]}}
{"id": "WebQTest-153", "question": "what are the sights to see in madrid", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["organization.organization.geographic_scope"]], "ground_paths": [["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["common.topic.image"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sights to see in madrid [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> organization.organization.geographic_scope </PATH>"], "scores": [-0.01695026084780693, -0.01923961378633976, -0.02021550014615059, -0.0796159952878952, -0.12428189814090729], "norm_scores": [0.20695626735687256, 0.20648302137851715, 0.2062816172838211, 0.19438520073890686, 0.18589386343955994]}}
{"id": "WebQTest-154", "question": "what instruments did louis armstrong play", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.musical_group.member", "music.group_membership.role"], ["music.instrument.family"], ["music.artist.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.artist.instruments_played </PATH>"], "scores": [-0.000265244219917804, -0.08427099138498306, -0.10385657101869583, -0.12056712806224823, -0.12229606509208679], "norm_scores": [0.21773241460323334, 0.20018883049488068, 0.19630615413188934, 0.19305303692817688, 0.19271954894065857]}}
{"id": "WebQTest-155", "question": "what year was george w bush elected", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["government.election_campaign.candidate"], ["time.event.locations", "location.location.events"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was george w bush elected [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> government.election_campaign.candidate </PATH>", "<PATH> time.event.locations <SEP> location.location.events </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.014890993013978004, -0.03383675590157509, -0.0420696847140789, -0.046444062143564224, -0.050573788583278656], "norm_scores": [0.20456986129283905, 0.2007305920124054, 0.1990847885608673, 0.19821582734584808, 0.19739893078804016]}}
{"id": "WebQTest-156", "question": "what time zone am i in california", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.states_provinces_within", "location.location.time_zones"], ["base.locations.countries.states_provinces_within"], ["base.biblioness.bibs_location.state", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone am i in california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.states_provinces_within <SEP> location.location.time_zones </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> location.location.time_zones </PATH>"], "scores": [-1.7279982785112225e-05, -0.13606585562229156, -0.15201862156391144, -0.1581665277481079, -0.16324809193611145], "norm_scores": [0.2254839688539505, 0.19680246710777283, 0.1936878263950348, 0.19250071048736572, 0.19152498245239258]}}
{"id": "WebQTest-157", "question": "where is mission san buenaventura located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mission san buenaventura located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.0008678458398208022, -0.06249850615859032, -0.09101993590593338, -0.12959131598472595, -0.13813936710357666], "norm_scores": [0.2171565443277359, 0.204177126288414, 0.198435977101326, 0.19092772901058197, 0.18930265307426453]}}
{"id": "WebQTest-159", "question": "what time in hilo hawaii", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["time.location.time_zones"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in hilo hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> time.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-7.666787132620811e-05, -0.09431150555610657, -0.15255163609981537, -0.15674233436584473, -0.17842893302440643], "norm_scores": [0.22420081496238708, 0.20403821766376495, 0.19249442219734192, 0.19168943166732788, 0.18757708370685577]}}
{"id": "WebQTest-160", "question": "where did rudolf virchow conduct his research", "prediction": [["people.deceased_person.place_of_death", "location.location.containedby"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.employment_history", "business.employer.employees"], ["people.deceased_person.place_of_death"]], "ground_paths": [["business.employment_tenure.person", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rudolf virchow conduct his research [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.012302053160965443, -0.02481241151690483, -0.033617351204156876, -0.03575330227613449, -0.0402318574488163], "norm_scores": [0.2034274935722351, 0.2008984088897705, 0.19913728535175323, 0.19871237874031067, 0.1978244185447693]}}
{"id": "WebQTest-161", "question": "what university did gordon brown attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["common.topic.notable_types"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did gordon brown attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.014102017506957054, -0.01962924934923649, -0.02293539047241211, -0.07240433245897293, -0.09115681797266006], "norm_scores": [0.20597757399082184, 0.2048422247171402, 0.20416609942913055, 0.1943119615316391, 0.19070209562778473]}}
{"id": "WebQTest-162", "question": "what countries does greece share borders with", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.containedby", "base.locations.continents.countries_within"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.events", "location.location.events"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.location.containedby", "location.location.containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["media_common.netflix_title.netflix_genres", "film.film.country"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["media_common.netflix_genre.titles", "film.film.country"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["organization.organization.founders", "organization.organization.founders"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.containedby", "base.locations.countries.continent"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["time.event.locations", "time.event.locations"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.country.official_language", "location.country.languages_spoken"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does greece share borders with [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.028807638213038445, -0.03569624945521355, -0.042506247758865356, -0.047631971538066864, -0.0478065088391304], "norm_scores": [0.20234470069408417, 0.2009555995464325, 0.1995917409658432, 0.1985713094472885, 0.19853666424751282]}}
{"id": "WebQTest-163", "question": "what year did the seahawks win the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the seahawks win the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>"], "scores": [-0.011123147793114185, -0.014434272423386574, -0.04619327187538147, -0.06764206290245056, -0.07046280056238174], "norm_scores": [0.20619970560073853, 0.20551808178424835, 0.19909359514713287, 0.19486872851848602, 0.19431984424591064]}}
{"id": "WebQTest-164", "question": "what did stephen hawking become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["people.profession.people_with_this_profession"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0021326416172087193, -0.05001330375671387, -0.0736100897192955, -0.08009766787290573, -0.10076833516359329], "norm_scores": [0.21207404136657715, 0.20215904712677002, 0.1974445879459381, 0.19616779685020447, 0.19215451180934906]}}
{"id": "WebQTest-165", "question": "who plays donna noble", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.ethnicity", "people.person.ethnicity"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays donna noble [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.022375216707587242, -0.028007589280605316, -0.03498852252960205, -0.038695186376571655, -0.04349662363529205], "norm_scores": [0.2022341936826706, 0.20109833776950836, 0.19969937205314636, 0.19896051287651062, 0.1980075240135193]}}
{"id": "WebQTest-166", "question": "what was dr seuss education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.012479369528591633, -0.013555381447076797, -0.04445388540625572, -0.05720324069261551, -0.09668128192424774], "norm_scores": [0.20648515224456787, 0.20626309514045715, 0.19998733699321747, 0.1974537968635559, 0.18981057405471802]}}
{"id": "WebQTest-167", "question": "where did queensland get its name from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.armorial_grant.coat_of_arms", "symbols.armorial_grant.armiger"], ["symbols.name_source.namesakes", "symbols.namesake.named_after"], ["symbols.armorial_grant.armiger", "symbols.armorial_grant.armiger"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did queensland get its name from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.armorial_grant.coat_of_arms <SEP> symbols.armorial_grant.armiger </PATH>", "<PATH> symbols.name_source.namesakes <SEP> symbols.namesake.named_after </PATH>", "<PATH> symbols.armorial_grant.armiger <SEP> symbols.armorial_grant.armiger </PATH>"], "scores": [-0.006737159565091133, -0.026937460526823997, -0.07672299444675446, -0.07936791330575943, -0.08077029138803482], "norm_scores": [0.20959989726543427, 0.20540840923786163, 0.19543242454528809, 0.19491620361804962, 0.1946430504322052]}}
{"id": "WebQTest-168", "question": "what document did james madison write", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"]], "ground_paths": [["book.book_subject.works"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did james madison write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.basic_title </PATH>"], "scores": [-0.013004635460674763, -0.01687689498066902, -0.056627802550792694, -0.0674048364162445, -0.07671918720006943], "norm_scores": [0.2066638469696045, 0.20586514472961426, 0.1978423297405243, 0.1957216113805771, 0.19390706717967987]}}
{"id": "WebQTest-169", "question": "what are the three official languages of belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["book.newspaper.circulation_areas", "book.periodical.language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the three official languages of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.language </PATH>"], "scores": [-0.009514189325273037, -0.021016722545027733, -0.038949597626924515, -0.08194869756698608, -0.1307191401720047], "norm_scores": [0.20940057933330536, 0.20700575411319733, 0.20332659780979156, 0.19476905465126038, 0.18549799919128418]}}
{"id": "WebQTest-170", "question": "who played bilbo in lord of the rings", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["people.person.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.031266696751117706, -0.032335858792066574, -0.03439399600028992, -0.035231947898864746, -0.04042897745966911], "norm_scores": [0.20069314539432526, 0.2004786878824234, 0.2000664919614792, 0.19989891350269318, 0.1988627314567566]}}
{"id": "WebQTest-171", "question": "what airport do you fly into to get to destin fl", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["location.location.nearby_airports", "location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into to get to destin fl [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> location.location.nearby_airports <SEP> location.location.containedby </PATH>"], "scores": [-0.010933653451502323, -0.017981357872486115, -0.04676869884133339, -0.07065217196941376, -0.07885467261075974], "norm_scores": [0.20686201751232147, 0.2054092437028885, 0.1995803564786911, 0.19487015902996063, 0.1932782679796219]}}
{"id": "WebQTest-172", "question": "what do you call members of the senate", "prediction": [["government.government_office_or_title.governmental_body_if_any", "common.topic.notable_types", "common.topic.notable_types"], ["government.governmental_body.offices_positions", "common.topic.notable_types", "common.topic.notable_types"], ["government.government_position_held.governmental_body", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.governmental_body_if_any", "government.government_position_held.office_holder", "government.government_position_held.basic_title"], ["government.government_position_held.governmental_body", "government.government_position_held.basic_title"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call members of the senate [/INST]", "raw_output": {"paths": ["<PATH> government.government_office_or_title.governmental_body_if_any <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.governmental_body.offices_positions <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.governmental_body_if_any <SEP> government.government_position_held.office_holder <SEP> government.government_position_held.basic_title </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.basic_title </PATH>"], "scores": [-0.012304753996431828, -0.020132597535848618, -0.02097931317985058, -0.032950226217508316, -0.03817654401063919], "norm_scores": [0.20252789556980133, 0.20094873011112213, 0.20077864825725555, 0.19838947057724, 0.19735532999038696]}}
{"id": "WebQTest-173", "question": "what countries are part of the baltic", "prediction": [["location.location.containedby"], ["location.location.contains"], ["common.topic.notable_types"], ["geography.body_of_water.islands"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the baltic [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> geography.body_of_water.islands </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.011247726157307625, -0.019348781555891037, -0.052024535834789276, -0.09696042537689209, -0.10240047425031662], "norm_scores": [0.20908643305301666, 0.20739945769309998, 0.2007320523262024, 0.19191163778305054, 0.190870463848114]}}
{"id": "WebQTest-174", "question": "what team does colin kaepernick play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does colin kaepernick play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>"], "scores": [-0.01309303566813469, -0.02536660060286522, -0.026004893705248833, -0.03068009950220585, -0.04058333486318588], "norm_scores": [0.20282232761383057, 0.20034819841384888, 0.20022034645080566, 0.19928646087646484, 0.19732262194156647]}}
{"id": "WebQTest-175", "question": "where did joe flacco attend college", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did joe flacco attend college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.016487304121255875, -0.02027233876287937, -0.02246108464896679, -0.03419167175889015, -0.052663370966911316], "norm_scores": [0.2025444358587265, 0.20177924633026123, 0.20133808255195618, 0.1989900767803192, 0.19534814357757568]}}
{"id": "WebQTest-176", "question": "who is emma stone father", "prediction": [["people.person.parents"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.gender"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is emma stone father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.007974510081112385, -0.024091368541121483, -0.10836880654096603, -0.11111174523830414, -0.1287325769662857], "norm_scores": [0.21382395923137665, 0.2104054093360901, 0.19339965283870697, 0.1928698867559433, 0.18950112164020538]}}
{"id": "WebQTest-177", "question": "where was theodore roosevelt buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["base.usnris.nris_listing.significant_person", "base.usnris.nris_listing.significant_level"], ["base.usnris.nris_listing.significant_person", "base.usnris.nris_listing.significant_person"], ["people.person.place_of_interment"]], "ground_paths": [["people.place_of_interment.interred_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was theodore roosevelt buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> base.usnris.nris_listing.significant_person <SEP> base.usnris.nris_listing.significant_level </PATH>", "<PATH> base.usnris.nris_listing.significant_person <SEP> base.usnris.nris_listing.significant_person </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.010264943353831768, -0.023693710565567017, -0.05890435352921486, -0.06002681702375412, -0.06152234598994255], "norm_scores": [0.20658272504806519, 0.2038271278142929, 0.19677510857582092, 0.19655434787273407, 0.19626063108444214]}}
{"id": "WebQTest-178", "question": "who was the leader of the us during wwii", "prediction": [["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speaker_s"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["military.military_conflict.commanders", "military.military_command.military_commander"], ["military.military_conflict.commanders", "military.military_commander.military_commands"]], "ground_paths": [["event.speech_topic.speeches_or_presentations_on_this_topic", "event.speech_or_presentation.speaker_s"], ["military.military_command.military_conflict", "military.military_commander.military_commands"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the us during wwii [/INST]", "raw_output": {"paths": ["<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speaker_s </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> military.military_conflict.commanders <SEP> military.military_command.military_commander </PATH>", "<PATH> military.military_conflict.commanders <SEP> military.military_commander.military_commands </PATH>"], "scores": [-0.016041399911046028, -0.025617452338337898, -0.02802199311554432, -0.02812035009264946, -0.034414585679769516], "norm_scores": [0.2020876109600067, 0.2001616358757019, 0.19968092441558838, 0.1996612846851349, 0.19840851426124573]}}
{"id": "WebQTest-179", "question": "what artistic movement did henri matisse belong to", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"], ["visual_art.artist.associated_periods_or_movements"], ["influence.influence_node.influenced_by"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic movement did henri matisse belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.artist.associated_periods_or_movements </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.0013147778809070587, -0.04161987453699112, -0.1011665090918541, -0.10587818175554276, -0.11632528901100159], "norm_scores": [0.21470551192760468, 0.20622386038303375, 0.19430239498615265, 0.19338905811309814, 0.19137921929359436]}}
{"id": "WebQTest-180", "question": "where the missouri river ends", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"], ["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.partially_contains"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where the missouri river ends [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.003613412380218506, -0.05738334357738495, -0.05907474830746651, -0.06988857686519623, -0.07221212238073349], "norm_scores": [0.20993967354297638, 0.19894933700561523, 0.19861312210559845, 0.19647692143917084, 0.1960209310054779]}}
{"id": "WebQTest-181", "question": "what type of currency do they use in thailand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["royalty.kingdom.rulers", "location.country.currency_used"], ["royalty.monarch.kingdom", "location.country.currency_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in thailand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> royalty.kingdom.rulers <SEP> location.country.currency_used </PATH>", "<PATH> royalty.monarch.kingdom <SEP> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009797261096537113, -0.016250548884272575, -0.10446509718894958, -0.10598050057888031, -0.12168049812316895], "norm_scores": [0.21250873804092407, 0.21114178001880646, 0.19331389665603638, 0.19302117824554443, 0.19001439213752747]}}
{"id": "WebQTest-182", "question": "who is khloe kardashian 's husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is khloe kardashian 's husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.019344691187143326, -0.020388063043355942, -0.03484608978033066, -0.03578724339604378, -0.04656309261918068], "norm_scores": [0.20241211354732513, 0.20220103859901428, 0.19929863512516022, 0.19911116361618042, 0.19697706401348114]}}
{"id": "WebQTest-184", "question": "what state does romney live in", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.country"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.administrative_division.country"], ["people.person.places_lived", "people.place_lived.location"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state does romney live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.029055936262011528, -0.030480707064270973, -0.03813527897000313, -0.041218049824237823, -0.04568367078900337], "norm_scores": [0.20157390832901, 0.2012869268655777, 0.1997520476579666, 0.19913719594478607, 0.1982499063014984]}}
{"id": "WebQTest-185", "question": "what type of government does france use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["film.film.country", "government.form_of_government.countries"], ["base.locations.countries.continent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does france use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> film.film.country <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.010945587418973446, -0.013301472179591656, -0.13498829305171967, -0.15680915117263794, -0.16675086319446564], "norm_scores": [0.2173445075750351, 0.21683306992053986, 0.19198957085609436, 0.18784557282924652, 0.18598730862140656]}}
{"id": "WebQTest-186", "question": "who rules denmark right now", "prediction": [["royalty.monarch.kingdom"], ["people.person.nationality"], ["royalty.kingdom.rulers"], ["royalty.monarch.kingdom", "people.person.nationality"], ["common.topic.notable_types"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who rules denmark right now [/INST]", "raw_output": {"paths": ["<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.010597756132483482, -0.01962975226342678, -0.06546453386545181, -0.09486255049705505, -0.12905006110668182], "norm_scores": [0.21074363589286804, 0.208848774433136, 0.19949229061603546, 0.19371297955513, 0.1872023493051529]}}
{"id": "WebQTest-187", "question": "what did stephen hawking study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"], ["people.person.profession", "people.profession.part_of_professional_field"], ["education.education.student", "education.education.degree"]], "ground_paths": [["education.education.student", "education.education.major_field_of_study"], ["people.person.profession", "people.professional_field.professions_in_this_field"], ["book.author.works_written", "book.written_work.subjects"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["book.written_work.author", "book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.profession <SEP> people.profession.part_of_professional_field </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>"], "scores": [-0.019155293703079224, -0.020334146916866302, -0.02530663274228573, -0.051456622779369354, -0.05922883376479149], "norm_scores": [0.20318499207496643, 0.20294561982154846, 0.20193897187709808, 0.1967267394065857, 0.19520364701747894]}}
{"id": "WebQTest-188", "question": "where did bristol palin go to school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did bristol palin go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.008781262673437595, -0.023614907637238503, -0.027803093194961548, -0.033207397907972336, -0.10570738464593887], "norm_scores": [0.20618867874145508, 0.20315271615982056, 0.2023036628961563, 0.2012133002281189, 0.18714159727096558]}}
{"id": "WebQTest-189", "question": "what region of the world is egypt associated with", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"], ["location.location.events"], ["time.event.locations"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region of the world is egypt associated with [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.018491551280021667, -0.018941951915621758, -0.03651376813650131, -0.0591210201382637, -0.06760319322347641], "norm_scores": [0.20433397591114044, 0.20424197614192963, 0.20068441331386566, 0.19619840383529663, 0.19454124569892883]}}
{"id": "WebQTest-190", "question": "what did dr. seuss do", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by"], ["fictional_universe.fictional_character.occupation"], ["book.written_work.author"], ["book.book_edition.author_editor"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr. seuss do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.008195281028747559, -0.02997354045510292, -0.05771353095769882, -0.06599431484937668, -0.08122285455465317], "norm_scores": [0.20817893743515015, 0.2036941796541214, 0.19812136888504028, 0.19648753106594086, 0.19351798295974731]}}
{"id": "WebQTest-191", "question": "where did drew brees go to college wikianswers", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did drew brees go to college wikianswers [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>"], "scores": [-0.017404615879058838, -0.018479490652680397, -0.024161482229828835, -0.02698199637234211, -0.040764760226011276], "norm_scores": [0.2016303688287735, 0.20141375064849854, 0.2002725750207901, 0.199708491563797, 0.19697485864162445]}}
{"id": "WebQTest-193", "question": "what school did ben roethlisberger go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did ben roethlisberger go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.018492890521883965, -0.02242124266922474, -0.022516809403896332, -0.0263209268450737, -0.04065999388694763], "norm_scores": [0.2015177458524704, 0.2007276564836502, 0.20070847868919373, 0.19994640350341797, 0.19709981977939606]}}
{"id": "WebQTest-194", "question": "where did the iroquois indians come from", "prediction": [["people.ethnicity.geographic_distribution"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["people.ethnicity.geographic_distribution", "base.locations.states_and_provences.country"], ["people.ethnicity.geographic_distribution", "people.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "base.locations.countries.continent"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the iroquois indians come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.states_and_provences.country </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.countries.continent </PATH>"], "scores": [-0.00020316895097494125, -0.09997354447841644, -0.10008550435304642, -0.1081913635134697, -0.11401261389255524], "norm_scores": [0.21738868951797485, 0.1967466026544571, 0.1967245638370514, 0.1951363980770111, 0.19400376081466675]}}
{"id": "WebQTest-195", "question": "what kind of guitar did george harrison use", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"], ["music.instrument.family"], ["music.musical_group.member", "music.group_membership.role"]], "ground_paths": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did george harrison use [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>"], "scores": [-0.009193383157253265, -0.034338340163230896, -0.0365581251680851, -0.04056993126869202, -0.08039236813783646], "norm_scores": [0.2062467336654663, 0.20112532377243042, 0.20067936182022095, 0.1998758763074875, 0.19207274913787842]}}
{"id": "WebQTest-196", "question": "what language do navajo people speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do navajo people speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.000619101629126817, -0.08114559948444366, -0.08392735570669174, -0.08715467900037766, -0.09453009814023972], "norm_scores": [0.21412521600723267, 0.1975584477186203, 0.1970096379518509, 0.19637487828731537, 0.19493184983730316]}}
{"id": "WebQTest-197", "question": "what places make up new england", "prediction": [["location.location.contains"], ["location.location.containedby"], ["book.book_subject.works"], ["book.written_work.subjects"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places make up new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.009127264842391014, -0.020411141216754913, -0.12520046532154083, -0.13208313286304474, -0.13897961378097534], "norm_scores": [0.21543750166893005, 0.2130201905965805, 0.1918277144432068, 0.19051195681095123, 0.18920262157917023]}}
{"id": "WebQTest-199", "question": "what type of government does iraq have now", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["people.person.nationality", "government.form_of_government.countries"], ["base.locations.countries.continent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does iraq have now [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.005662759765982628, -0.02131706476211548, -0.12971334159374237, -0.15196409821510315, -0.15482698380947113], "norm_scores": [0.21771655976772308, 0.2143349051475525, 0.1923166811466217, 0.18808476626873016, 0.18754707276821136]}}
{"id": "WebQTest-200", "question": "where was the vietnam war location", "prediction": [["time.event.locations"], ["location.location.events"], ["book.written_work.subjects"], ["book.book_subject.works"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["book.written_work.subjects"], ["book.book_subject.works"], ["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the vietnam war location [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.008323335088789463, -0.030176615342497826, -0.04775736853480339, -0.07847227901220322, -0.08675654232501984], "norm_scores": [0.20848378539085388, 0.20397715270519257, 0.20042243599891663, 0.19436004757881165, 0.1927565634250641]}}
{"id": "WebQTest-201", "question": "who did gerald ford select as his vice president when he became president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"], ["people.person.gender", "people.person.gender"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did gerald ford select as his vice president when he became president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.005817394237965345, -0.020081715658307076, -0.06765440851449966, -0.0776304304599762, -0.07902947813272476], "norm_scores": [0.20894354581832886, 0.20598426461219788, 0.19641448557376862, 0.1944647878408432, 0.19419290125370026]}}
{"id": "WebQTest-202", "question": "what is the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0051447246223688126, -0.0269588865339756, -0.14341332018375397, -0.14832960069179535, -0.15634024143218994], "norm_scores": [0.21855248510837555, 0.21383656561374664, 0.1903296560049057, 0.1893962323665619, 0.1878851056098938]}}
{"id": "WebQTest-204", "question": "what is the capital of modern egypt", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.capital"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.010131665505468845, -0.024349253624677658, -0.045762576162815094, -0.06477038562297821, -0.07985220104455948], "norm_scores": [0.20702387392520905, 0.2041013240814209, 0.199777290225029, 0.19601581990718842, 0.19308173656463623]}}
{"id": "WebQTest-205", "question": "what school did sir ernest rutherford go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did sir ernest rutherford go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.01367263775318861, -0.01834949664771557, -0.029404908418655396, -0.035049378871917725, -0.0524459071457386], "norm_scores": [0.20322957634925842, 0.20228131115436554, 0.20005734264850616, 0.19893129169940948, 0.19550052285194397]}}
{"id": "WebQTest-206", "question": "what position does brandon roy play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position_s"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["basketball.basketball_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position does brandon roy play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position_s </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.009495673701167107, -0.031595442444086075, -0.03915480151772499, -0.05140424519777298, -0.05838998034596443], "norm_scores": [0.2057546228170395, 0.2012573778629303, 0.19974173605442047, 0.1973099410533905, 0.195936381816864]}}
{"id": "WebQTest-207", "question": "where is harvard university", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harvard university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.003466356545686722, -0.048177607357501984, -0.06431080400943756, -0.0766388475894928, -0.07996582984924316], "norm_scores": [0.21039177477359772, 0.20119209587574005, 0.19797228276729584, 0.19554662704467773, 0.1948971450328827]}}
{"id": "WebQTest-209", "question": "where is laos in world map", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["geography.river.basin_countries"]], "ground_paths": [["book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is laos in world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.015142099931836128, -0.027731886133551598, -0.03427364304661751, -0.053567737340927124, -0.06886721402406693], "norm_scores": [0.2049795538187027, 0.2024150937795639, 0.20109525322914124, 0.19725249707698822, 0.19425758719444275]}}
{"id": "WebQTest-210", "question": "where is tyrese gibson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality"], ["music.artist.origin"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tyrese gibson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.00598965073004365, -0.024623367935419083, -0.07992761582136154, -0.08053293824195862, -0.10856305807828903], "norm_scores": [0.2109283059835434, 0.20703433454036713, 0.19589529931545258, 0.1957767754793167, 0.19036532938480377]}}
{"id": "WebQTest-211", "question": "what countries are near italy", "prediction": [["location.location.partially_containedby", "location.location.partially_contains"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["geography.river.basin_countries", "location.location.partially_contains"], ["location.location.containedby", "geography.river.basin_countries"], ["organization.organization.founders", "base.nations.supranational_body.member"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["base.locations.countries.continent", "location.location.containedby"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["location.country.official_language", "location.country.official_language"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["location.country.currency_used", "location.country.currency_used"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["organization.organization.founders", "organization.organization.founders"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.country.official_language", "language.human_language.main_country"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_contains", "geography.river.origin"], ["base.locations.countries.continent", "location.location.contains_major_portion_of"], ["location.country.currency_used", "finance.currency.countries_used"], ["military.military_combatant.includes_allies", "military.military_combatant.includes_allies"], ["location.location.partially_contains", "location.location.partially_containedby"], ["travel.guidebook.travel_destinations_covered", "travel.travel_destination.guidebooks"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.containedby", "base.locations.countries.continent"], ["fictional_universe.fictional_setting.works_set_here", "fictional_universe.work_of_fiction.setting"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are near italy [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_games.participating_countries <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03549047186970711, -0.035748932510614395, -0.0367276594042778, -0.03750826418399811, -0.040240705013275146], "norm_scores": [0.20033054053783417, 0.20027877390384674, 0.20008283853530884, 0.1999267190694809, 0.19938117265701294]}}
{"id": "WebQTest-212", "question": "which states does the colorado river run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.contains"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which states does the colorado river run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.012305790558457375, -0.019129006192088127, -0.04035032540559769, -0.06958124041557312, -0.1356838345527649], "norm_scores": [0.2086024135351181, 0.20718391239643097, 0.20283351838588715, 0.19699034094810486, 0.18438981473445892]}}
{"id": "WebQTest-213", "question": "who did scarlett johansson date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.languages", "people.person.languages"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.languages", "people.person.languages"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["people.person.children", "people.person.children"], ["film.film.executive_produced_by", "film.producer.films_executive_produced"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.spouse_s", "people.marriage.spouse"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did scarlett johansson date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.languages <SEP> people.person.languages </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.01703920029103756, -0.021886618807911873, -0.03381906449794769, -0.05059113726019859, -0.05888409540057182], "norm_scores": [0.20389239490032196, 0.20290642976760864, 0.20049965381622314, 0.1971648931503296, 0.19553658366203308]}}
{"id": "WebQTest-214", "question": "which continents were part of the roman empire", "prediction": [["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"], ["location.location.containedby", "location.location.containedby"], ["fictional_universe.fictional_language.where_spoken", "language.human_language.region"], ["location.location.containedby"]], "ground_paths": [["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.location.partiallycontains"], ["location.country.official_language", "language.human_language.region"], ["book.book_subject.works", "book.written_work.subjects"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which continents were part of the roman empire [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.region </PATH>", "<PATH> location.location.partially_contained_by <SEP> location.partial_containment_relationship.partially_contained_by </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_language.where_spoken <SEP> language.human_language.region </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01780351996421814, -0.0306424330919981, -0.03633618354797363, -0.0398327000439167, -0.04237271100282669], "norm_scores": [0.20313547551631927, 0.20054411888122559, 0.1994055062532425, 0.19870950281620026, 0.19820541143417358]}}
{"id": "WebQTest-215", "question": "where was elvis costello born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"], ["music.recording.artist"], ["music.artist.label"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was elvis costello born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.artist.label </PATH>"], "scores": [-0.016549551859498024, -0.017922310158610344, -0.03346562385559082, -0.11881224066019058, -0.1352846771478653], "norm_scores": [0.2095254510641098, 0.20923800766468048, 0.20601090788841248, 0.18915799260139465, 0.1860676109790802]}}
{"id": "WebQTest-216", "question": "where are the gobi desert located on a map", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"], ["location.location.partially_contains"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the gobi desert located on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.0015221559442579746, -0.059030309319496155, -0.07701612263917923, -0.08390723913908005, -0.10408458858728409], "norm_scores": [0.2129998654127121, 0.20109620690345764, 0.19751165807247162, 0.19615526497364044, 0.1922370195388794]}}
{"id": "WebQTest-217", "question": "what type of government does the us follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet", "base.aareas.schema.administrative_area.administrative_parent"], ["base.locations.countries.planet", "government.form_of_government.countries"], ["form.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does the us follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.locations.countries.planet <SEP> government.form_of_government.countries </PATH>", "<PATH> form.form_of_government.countries </PATH>"], "scores": [-0.006595142651349306, -0.02034224569797516, -0.1427203118801117, -0.1428469866514206, -0.1500190645456314], "norm_scores": [0.2174815982580185, 0.21451233327388763, 0.18980345129966736, 0.18977941572666168, 0.18842317163944244]}}
{"id": "WebQTest-218", "question": "in which continent is germany", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["location.location.events", "time.event.locations"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which continent is germany [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.events <SEP> time.event.locations </PATH>"], "scores": [-0.00708323298022151, -0.03237592801451683, -0.05567507818341255, -0.0632699504494667, -0.10678242146968842], "norm_scores": [0.20928966999053955, 0.2040625512599945, 0.19936302304267883, 0.1978546380996704, 0.1894301027059555]}}
{"id": "WebQTest-220", "question": "what language turkey people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["food.beer_country_region.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language turkey people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> food.beer_country_region.languages_spoken </PATH>"], "scores": [-0.007195351179689169, -0.03025420382618904, -0.052520230412483215, -0.0571959912776947, -0.1050814762711525], "norm_scores": [0.20873022079467773, 0.2039722204208374, 0.19948075711727142, 0.19855019450187683, 0.18926657736301422]}}
{"id": "WebQTest-221", "question": "who does jeremy lin play for", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["people.person.place_of_birth", "sports.sports_team.location"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy lin play for [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.013952257111668587, -0.024907775223255157, -0.025696340948343277, -0.033040113747119904, -0.03640739992260933], "norm_scores": [0.20258016884326935, 0.20037290453910828, 0.2002149522304535, 0.19875001907348633, 0.1980818808078766]}}
{"id": "WebQTest-222", "question": "where is the seychelles on world map", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.contains"], ["base.locations.countries.continent"], ["geography.island.island_group"]], "ground_paths": [["base.locations.continents.countries_within"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seychelles on world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> geography.island.island_group </PATH>"], "scores": [-0.003906446974724531, -0.04758244380354881, -0.05338713154196739, -0.05935121327638626, -0.08315271139144897], "norm_scores": [0.20925474166870117, 0.2003120332956314, 0.1991526484489441, 0.19796843826770782, 0.1933121234178543]}}
{"id": "WebQTest-224", "question": "where did william morris go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did william morris go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.015251326374709606, -0.01561097614467144, -0.026971017941832542, -0.05809125676751137, -0.060115184634923935], "norm_scores": [0.20399081707000732, 0.20391745865345, 0.20161405205726624, 0.19543640315532684, 0.1950412541627884]}}
{"id": "WebQTest-225", "question": "what state is mount st. helens in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"], ["location.mountain.mountain_type"], ["geography.mountain.mountain_range"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is mount st. helens in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>", "<PATH> location.mountain.mountain_type </PATH>", "<PATH> geography.mountain.mountain_range </PATH>"], "scores": [-0.0039939675480127335, -0.0329507440328598, -0.14001628756523132, -0.14440660178661346, -0.14785262942314148], "norm_scores": [0.21837440133094788, 0.21214164793491364, 0.19060224294662476, 0.1897672861814499, 0.18911445140838623]}}
{"id": "WebQTest-226", "question": "who is jimmy savile", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["common.topic.image"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jimmy savile [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.image </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.0007512207957915962, -0.0733550414443016, -0.07771384716033936, -0.15591208636760712, -0.16193392872810364], "norm_scores": [0.21913927793502808, 0.2037927806377411, 0.20290641486644745, 0.18764403462409973, 0.18651746213436127]}}
{"id": "WebQTest-227", "question": "where were the seneca indians located", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.location.contains"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the seneca indians located [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.0007160248933359981, -0.08456973731517792, -0.0977768823504448, -0.10046408325433731, -0.1014513224363327], "norm_scores": [0.2156890332698822, 0.19834023714065552, 0.19573797285556793, 0.19521267712116241, 0.19502006471157074]}}
{"id": "WebQTest-231", "question": "when did florida marlins join mlb", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.championships"], ["sports.sports_league_participation.league", "sports.sports_league_participation.season"], ["sports.sports_league_participation.league", "sports.sports_league_season.participation"], ["sports.sports_team_roster.team"]], "ground_paths": [["baseball.baseball_team.team_stats", "baseball.baseball_team_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did florida marlins join mlb [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_league_participation.season </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_league_season.participation </PATH>", "<PATH> sports.sports_team_roster.team </PATH>"], "scores": [-0.015851078554987907, -0.01927635446190834, -0.03872613608837128, -0.053057752549648285, -0.060225505381822586], "norm_scores": [0.20433026552200317, 0.20363156497478485, 0.1997092366218567, 0.19686751067638397, 0.1954614520072937]}}
{"id": "WebQTest-232", "question": "what countries have spanish as the national language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["base.rosetta.local_name.locale"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have spanish as the national language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>"], "scores": [-0.01119258999824524, -0.022920479997992516, -0.030627069994807243, -0.11097881197929382, -0.12971842288970947], "norm_scores": [0.20998021960258484, 0.20753195881843567, 0.20593877136707306, 0.19003859162330627, 0.18651048839092255]}}
{"id": "WebQTest-233", "question": "what to see in phoenix arizona", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["film.film_festival.location"], ["transportation.bridge.locale"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in phoenix arizona [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> transportation.bridge.locale </PATH>"], "scores": [-0.013864108361303806, -0.014090463519096375, -0.03588132932782173, -0.14416784048080444, -0.16026261448860168], "norm_scores": [0.21188059449195862, 0.21183264255523682, 0.20726655423641205, 0.18599489331245422, 0.1830253005027771]}}
{"id": "WebQTest-234", "question": "what countries does the panama canal go through", "prediction": [["location.location.containedby"], ["geography.body_of_water.bridges", "location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does the panama canal go through [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.body_of_water.bridges <SEP> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.0010463643120601773, -0.07124984264373779, -0.08248855173587799, -0.09666988998651505, -0.09751134365797043], "norm_scores": [0.2140943557024002, 0.1995796412229538, 0.1973491758108139, 0.19457024335861206, 0.19440659880638123]}}
{"id": "WebQTest-235", "question": "what is charlie chaplin famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["people.profession.people_with_this_profession"], ["film.film.directed_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is charlie chaplin famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> film.film.directed_by </PATH>"], "scores": [-0.003807264845818281, -0.03405303135514259, -0.10568542033433914, -0.11277462542057037, -0.1306198090314865], "norm_scores": [0.21500727534294128, 0.20860157907009125, 0.19418160617351532, 0.19280987977981567, 0.18939965963363647]}}
{"id": "WebQTest-237", "question": "what did fred durst do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre", "music.artist.genre"], ["music.recording.artist"], ["common.topic.notable_types", "type.type.expected_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did fred durst do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>"], "scores": [-0.0035040220245718956, -0.05780716612935066, -0.06870517134666443, -0.08658935874700546, -0.09344395995140076], "norm_scores": [0.21194152534008026, 0.20073933899402618, 0.1985635608434677, 0.19504396617412567, 0.19371159374713898]}}
{"id": "WebQTest-239", "question": "where do american bulldogs originate from", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here", "location.location.containedby"], ["biology.breed_origin.breeds_originating_here", "base.breed_origin.breeds_originating_here"]], "ground_paths": [["biology.animal_breed.place_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do american bulldogs originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.place_of_origin </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> location.location.containedby </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> base.breed_origin.breeds_originating_here </PATH>"], "scores": [-0.008205692283809185, -0.016469288617372513, -0.08135904371738434, -0.09139809012413025, -0.09585659950971603], "norm_scores": [0.2101951539516449, 0.2084653526544571, 0.19536763429641724, 0.19341613352298737, 0.1925557255744934]}}
{"id": "WebQTest-240", "question": "what colleges did albert einstein teach at", "prediction": [["people.person.employment_history", "business.employer.employees"], ["education.education.student", "education.education.institution"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["people.person.employment_history", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"], ["education.department.academics_old", "education.university.departments"], ["people.person.nationality", "organization.organization.founders"], ["people.person.education", "education.education.institution"], ["business.employment_tenure.person", "business.employer.employees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did albert einstein teach at [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.017277875915169716, -0.028759358450770378, -0.02982354909181595, -0.03195702284574509, -0.03618224710226059], "norm_scores": [0.20231373608112335, 0.20000416040420532, 0.19979141652584076, 0.19936563074588776, 0.19852504134178162]}}
{"id": "WebQTest-241", "question": "who plays the voice of brian on family guy", "prediction": [["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program_writer_relationship.tv_program", "tv.tv_writer.tv_programs"], ["tv.tv_program_creator.programs_created"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of brian on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program_writer_relationship.tv_program <SEP> tv.tv_writer.tv_programs </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>"], "scores": [-0.02266213484108448, -0.022750724107027054, -0.03044581040740013, -0.031000282615423203, -0.042747851461172104], "norm_scores": [0.20145168900489807, 0.20143386721611023, 0.19988973438739777, 0.1997789442539215, 0.19744575023651123]}}
{"id": "WebQTest-243", "question": "where did george w bush live as a child", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george w bush live as a child [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0110003761947155, -0.025395959615707397, -0.039739497005939484, -0.0488666333258152, -0.05331026762723923], "norm_scores": [0.20496878027915955, 0.20203927159309387, 0.19916199147701263, 0.19735249876976013, 0.196477472782135]}}
{"id": "WebQTest-245", "question": "what year was the first miss america pageant held", "prediction": [["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"], ["time.recurring_event.current_frequency"], ["time.event.locations"], ["organization.organization.founders"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was the first miss america pageant held [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.00789022259414196, -0.023876067250967026, -0.052470799535512924, -0.07359053194522858, -0.08335939794778824], "norm_scores": [0.20814861357212067, 0.20484764873981476, 0.19907304644584656, 0.19491274654865265, 0.19301792979240417]}}
{"id": "WebQTest-246", "question": "what was nikola tesla inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"]], "ground_paths": [["law.inventor.inventions"], ["symbols.name_source.namesakes"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>"], "scores": [-0.006021219305694103, -0.03895766660571098, -0.054903678596019745, -0.05607277527451515, -0.05818160995841026], "norm_scores": [0.20745812356472015, 0.20073647797107697, 0.19756092131137848, 0.19733010232448578, 0.19691438972949982]}}
{"id": "WebQTest-247", "question": "where did flemish people come from", "prediction": [["people.ethnicity.geographic_distribution"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["people.ethnicity.included_in_group"], ["base.popstra.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "people.ethnicity.geographic_distribution"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did flemish people come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> base.popstra.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> people.ethnicity.geographic_distribution </PATH>"], "scores": [-7.783335604472086e-05, -0.12726347148418427, -0.13057591021060944, -0.13070549070835114, -0.13073855638504028], "norm_scores": [0.22156792879104614, 0.1951061189174652, 0.1944608986377716, 0.194435715675354, 0.19442927837371826]}}
{"id": "WebQTest-248", "question": "what state new york city belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["base.biblioness.bibs_location.state"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.contains"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state new york city belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.011624893173575401, -0.0493251234292984, -0.050001583993434906, -0.050128042697906494, -0.05666634440422058], "norm_scores": [0.206460639834404, 0.19882193207740784, 0.19868746399879456, 0.19866235554218292, 0.19736766815185547]}}
{"id": "WebQTest-249", "question": "what was the cause of death for bruce lee", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["deceased_person.cause_of_death"], ["people.person.deceased_of"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for bruce lee [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_of </PATH>"], "scores": [-0.0010786213679239154, -0.0477747842669487, -0.10799175500869751, -0.12807855010032654, -0.1408759206533432], "norm_scores": [0.21723785996437073, 0.20732688903808594, 0.1952107548713684, 0.1913287192583084, 0.1888958066701889]}}
{"id": "WebQTest-250", "question": "what type of government does usa follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"], ["organization.organization.founders"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does usa follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007877239026129246, -0.017786208540201187, -0.15673065185546875, -0.16353079676628113, -0.1659654974937439], "norm_scores": [0.21922780573368073, 0.2170662134885788, 0.18890760838985443, 0.18762734532356262, 0.1871710866689682]}}
{"id": "WebQTest-251", "question": "what type of currency do they use in england", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.form_of_government"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in england [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.002278718864545226, -0.03969480097293854, -0.11604129523038864, -0.1247272789478302, -0.16087192296981812], "norm_scores": [0.21768280863761902, 0.2096884697675705, 0.19427533447742462, 0.19259515404701233, 0.18575818836688995]}}
{"id": "WebQTest-252", "question": "where was benjamin franklin educated", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was benjamin franklin educated [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.012626242823898792, -0.025404343381524086, -0.02813732996582985, -0.028802204877138138, -0.054202523082494736], "norm_scores": [0.2034529149532318, 0.20086972415447235, 0.2003214806318283, 0.20018833875656128, 0.19516752660274506]}}
{"id": "WebQTest-253", "question": "what countries included in the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.02292892150580883, -0.02543054334819317, -0.030497776344418526, -0.03623194620013237, -0.0407668836414814], "norm_scores": [0.20165085792541504, 0.20114704966545105, 0.20013035833835602, 0.19898606836795807, 0.1980857104063034]}}
{"id": "WebQTest-254", "question": "what countries in the world speak chinese", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries in the world speak chinese [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.018735503777861595, -0.02134423889219761, -0.031606126576662064, -0.0344199612736702, -0.11844975501298904], "norm_scores": [0.20516437292099, 0.20462985336780548, 0.20254069566726685, 0.20197157561779022, 0.18569345772266388]}}
{"id": "WebQTest-255", "question": "where obama went to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where obama went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.011178446933627129, -0.028665417805314064, -0.029837051406502724, -0.03359879553318024, -0.05693743750452995], "norm_scores": [0.20419494807720184, 0.2006552517414093, 0.2004202902317047, 0.1996677666902542, 0.19506174325942993]}}
{"id": "WebQTest-256", "question": "what is the primary language of china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.language_family"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the primary language of china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.007188485004007816, -0.03586199879646301, -0.03712902218103409, -0.05733777582645416, -0.1570763885974884], "norm_scores": [0.21034348011016846, 0.20439782738685608, 0.20413902401924133, 0.20005503296852112, 0.18106460571289062]}}
{"id": "WebQTest-257", "question": "who was the soviet leader during world war ii", "prediction": [["military.military_person.participated_in_conflicts"], ["people.person.nationality"], ["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speaker_s"], ["military.military_command.military_conflict", "military.military_commander.military_commands"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the soviet leader during world war ii [/INST]", "raw_output": {"paths": ["<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speaker_s </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_commander.military_commands </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.01070453505963087, -0.01340064313262701, -0.08520685881376266, -0.08665810525417328, -0.08938140422105789], "norm_scores": [0.20934909582138062, 0.20878542959690094, 0.19431892037391663, 0.19403713941574097, 0.19350942969322205]}}
{"id": "WebQTest-258", "question": "which of the following does australia export the most", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which of the following does australia export the most [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.00882391445338726, -0.03002035804092884, -0.033404640853405, -0.035502418875694275, -0.04027861729264259], "norm_scores": [0.20418769121170044, 0.1999051868915558, 0.19922979176044464, 0.19881229102611542, 0.19786500930786133]}}
{"id": "WebQTest-260", "question": "where was the city of david", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["location.location.people_born_here"], ["time.event.locations"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the city of david [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.005792811047285795, -0.03458215296268463, -0.08143337815999985, -0.10458309203386307, -0.106887586414814], "norm_scores": [0.212379589676857, 0.20635248720645905, 0.19690760970115662, 0.19240161776542664, 0.1919587254524231]}}
{"id": "WebQTest-261", "question": "what is cindy sherman known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["visual_art.visual_artist.art_forms"], ["film.film_crewmember.films_crewed", "film.film_job.films_with_this_crew_job"], ["visual_art.art_series.artist"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is cindy sherman known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> film.film_crewmember.films_crewed <SEP> film.film_job.films_with_this_crew_job </PATH>", "<PATH> visual_art.art_series.artist </PATH>"], "scores": [-0.006214682478457689, -0.032404132187366486, -0.051002368330955505, -0.06753775477409363, -0.0850340947508812], "norm_scores": [0.2085472047328949, 0.20315638184547424, 0.19941294193267822, 0.19614268839359283, 0.19274075329303741]}}
{"id": "WebQTest-262", "question": "what tv shows did shawnee smith play in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "ground_paths": [["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["film.performance.actor", "film.film.starring"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv shows did shawnee smith play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>"], "scores": [-0.022144457325339317, -0.02302669733762741, -0.03183493763208389, -0.03223637491464615, -0.03651624172925949], "norm_scores": [0.20140321552753448, 0.20122560858726501, 0.1994609385728836, 0.19938088953495026, 0.19852937757968903]}}
{"id": "WebQTest-264", "question": "what does australia use for currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"], ["common.topic.notable_types", "common.topic.notable_types"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does australia use for currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011791391298174858, -0.015165271237492561, -0.12161017209291458, -0.1373692750930786, -0.14382034540176392], "norm_scores": [0.21501070261001587, 0.21428650617599487, 0.19264884293079376, 0.18963666260242462, 0.1884172409772873]}}
{"id": "WebQTest-265", "question": "which airport to fly into rome", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>"], "scores": [-0.0060339211486279964, -0.030257802456617355, -0.060409195721149445, -0.06760669499635696, -0.0867132842540741], "norm_scores": [0.2089463472366333, 0.20394565165042877, 0.19788819551467896, 0.1964690089225769, 0.19275079667568207]}}
{"id": "WebQTest-266", "question": "what is there to do for fun in kansas city", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["base.usnationalparks.us_national_park.state"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.place_with_neighborhoods.neighborhoods"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do for fun in kansas city [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>"], "scores": [-0.01119875069707632, -0.016883444041013718, -0.03299099951982498, -0.12622544169425964, -0.126297727227211], "norm_scores": [0.21028748154640198, 0.20909544825553894, 0.20575441420078278, 0.18743813037872314, 0.18742460012435913]}}
{"id": "WebQTest-267", "question": "where was martin luther king jr raised", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr raised [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.0006708798464387655, -0.061214473098516464, -0.11146268248558044, -0.11490416526794434, -0.13150902092456818], "norm_scores": [0.21711812913417816, 0.20436303317546844, 0.19434788823127747, 0.19368019700050354, 0.19049072265625]}}
{"id": "WebQTest-268", "question": "what year was lebron james rookie season", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was lebron james rookie season [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>"], "scores": [-0.010113190859556198, -0.014908907935023308, -0.035575635731220245, -0.050348203629255295, -0.05160851404070854], "norm_scores": [0.20449930429458618, 0.20352092385292053, 0.1993579864501953, 0.1964346021413803, 0.19618719816207886]}}
{"id": "WebQTest-269", "question": "where does delaware river start", "prediction": [["geography.river.origin"], ["location.location.containedby"], ["transportation.bridge.body_of_water_spanned"], ["location.location.partial_containedby"], ["geography.lake.outflow"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does delaware river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>", "<PATH> location.location.partial_containedby </PATH>", "<PATH> geography.lake.outflow </PATH>"], "scores": [-0.0009601371712051332, -0.08041933178901672, -0.10260933637619019, -0.10551459342241287, -0.10886877030134201], "norm_scores": [0.2161976546049118, 0.1996835470199585, 0.195301353931427, 0.19473478198051453, 0.19408272206783295]}}
{"id": "WebQTest-270", "question": "what kind of monarchy does japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["royalty.monarch.kingdom"], ["royalty.kingdom.rulers"], ["royalty.monarch.form"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of monarchy does japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.monarch.form </PATH>"], "scores": [-0.007672970183193684, -0.023958737030625343, -0.045707251876592636, -0.06878180801868439, -0.09575699269771576], "norm_scores": [0.2082062065601349, 0.20484286546707153, 0.20043593645095825, 0.19586391746997833, 0.190651074051857]}}
{"id": "WebQTest-271", "question": "where did mitt romney 's parents come from", "prediction": [["people.person.place_of_birth"], ["people.person.parents"], ["people.person.children"], ["people.person.nationality"], ["people.person.religion"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mitt romney 's parents come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.religion </PATH>"], "scores": [-0.014942015521228313, -0.026959829032421112, -0.029302868992090225, -0.04390393942594528, -0.09273207932710648], "norm_scores": [0.20532160997390747, 0.20286886394023895, 0.20239408314228058, 0.19946038722991943, 0.18995507061481476]}}
{"id": "WebQTest-273", "question": "when did michael jordan return to the nba", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_award_winner.awards", "sports.sports_league_season.awards"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael jordan return to the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_league_season.awards </PATH>"], "scores": [-0.01164124347269535, -0.023511623963713646, -0.025935785844922066, -0.027023203670978546, -0.052952010184526443], "norm_scores": [0.2033233791589737, 0.2009241282939911, 0.20043763518333435, 0.2002197951078415, 0.19509506225585938]}}
{"id": "WebQTest-276", "question": "where was mission san rafael located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "location.administrative_division.second_level_division_of"], ["symbols.namesake.named_after"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mission san rafael located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.administrative_division.second_level_division_of </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0006341125117614865, -0.07562554627656937, -0.108275406062603, -0.1174580454826355, -0.13027073442935944], "norm_scores": [0.21768197417259216, 0.20195475220680237, 0.19546744227409363, 0.1936807632446289, 0.19121500849723816]}}
{"id": "WebQTest-277", "question": "who was esther 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["visual_art.art_subject.artwork_on_the_subject", "visual_art.art_subject.artwork_on_the_subject"], ["people.person.spouse_s", "people.person.spouse_s"], ["book.translated_work.translations", "book.book_subject.works"], ["visual_art.art_subject.artwork_on_the_subject", "visual_art.artwork.art_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was esther 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> freebase.valuenotation.has_value <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.028551558032631874, -0.03294996917247772, -0.037227850407361984, -0.04831080138683319, -0.04877575859427452], "norm_scores": [0.20212695002555847, 0.20123986899852753, 0.2003808170557022, 0.19817227125167847, 0.1980801522731781]}}
{"id": "WebQTest-278", "question": "who 's dating claire danes", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"]], "ground_paths": [["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["people.person.children", "people.person.parents"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who 's dating claire danes [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.romantic_relationship.celebrity </PATH>", "<PATH> celebrities.celebrity.sexual_relationships <SEP> celebrities.celebrity.sexual_relationships </PATH>"], "scores": [-0.01802119053900242, -0.02731098234653473, -0.030172400176525116, -0.04241028428077698, -0.04812251776456833], "norm_scores": [0.20304863154888153, 0.20117108523845673, 0.20059627294540405, 0.19815635681152344, 0.19702765345573425]}}
{"id": "WebQTest-279", "question": "what county is st paul va in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul va in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.018434610217809677, -0.02475438266992569, -0.03000071458518505, -0.03250911831855774, -0.07805109024047852], "norm_scores": [0.20365144312381744, 0.20236848294734955, 0.20130954682826996, 0.20080521702766418, 0.19186528027057648]}}
{"id": "WebQTest-281", "question": "what type of sports do japanese play", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["cricket.cricket_team.country"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of sports do japanese play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> cricket.cricket_team.country </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.009796272963285446, -0.015212410129606724, -0.11656368523836136, -0.13142168521881104, -0.13650892674922943], "norm_scores": [0.21460148692131042, 0.21344231069087982, 0.19286979734897614, 0.19002531468868256, 0.18906107544898987]}}
{"id": "WebQTest-282", "question": "which countries share a border with russia", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["location.location.time_zones", "time.time_zone.locations_in_this_time_zone"], ["base.locations.continents.countries_within", "base.locations.countries.continent"], ["time.event.locations", "location.location.events"], ["geography.river.basin_countries", "location.location.partially_contains"], ["base.rivalries.rival.rivals", "base.rivalries.rival.rivals"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.country.languages_spoken", "language.human_language.main_country"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.partially_containedby", "base.schemastaging.disputed_location.claimed_by"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_containedby", "base.locations.continents.countries_within"], ["location.location.events", "location.location.events"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["location.location.containedby", "location.location.containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.country.languages_spoken", "location.country.official_language"], ["location.location.partially_containedby", "location.location.containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "geography.lake.basin_countries"], ["location.location.partially_containedby", "geography.river.origin"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.rivalries.rival.rivals", "base.rivalries.rivalry.rival"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.locations.continents.countries_within", "base.locations.continents.countries_within"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["organization.organization.founders", "organization.organization.founders"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["language.human_language.main_country", "location.country.languages_spoken"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_contains", "location.location.containedby"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_contains", "geography.river.origin"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["base.locations.continents.countries_within", "location.location.partially_contains"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["location.location.partially_containedby", "base.locations.countries.continent"], ["time.event.locations", "time.event.locations"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["language.human_language.main_country", "language.human_language.countries_spoken_in"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries share a border with russia [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.0351262204349041, -0.037444818764925, -0.03771328926086426, -0.04166268929839134, -0.04598173499107361], "norm_scores": [0.20089243352413177, 0.20042718946933746, 0.20037338137626648, 0.19958359003067017, 0.1987234205007553]}}
{"id": "WebQTest-284", "question": "where did the latin language originate from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"], ["language.language_family.languages"], ["location.country.official_language"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the latin language originate from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.00464861374348402, -0.03368271142244339, -0.05080508068203926, -0.11032470315694809, -0.13631275296211243], "norm_scores": [0.21264709532260895, 0.2065618485212326, 0.20305512845516205, 0.19132199883460999, 0.18641397356987]}}
{"id": "WebQTest-286", "question": "what school did karl benz go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did karl benz go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.018033692613244057, -0.02079775556921959, -0.02097141370177269, -0.034303534775972366, -0.05663645267486572], "norm_scores": [0.2024168223142624, 0.20185808837413788, 0.20182305574417114, 0.1991501748561859, 0.19475185871124268]}}
{"id": "WebQTest-287", "question": "what movies has carmen electra been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.film_character.portrayed_in_films", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has carmen electra been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.021243102848529816, -0.026655053719878197, -0.0380280502140522, -0.038329463452100754, -0.03969812020659447], "norm_scores": [0.20231731235980988, 0.20122532546520233, 0.19894975423812866, 0.19888979196548462, 0.19861777126789093]}}
{"id": "WebQTest-289", "question": "when do world war ii end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["time.event.locations"], ["military.military_conflict.casualties", "military.military_conflict.casualties"], ["event.disaster.included_in_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when do world war ii end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>", "<PATH> event.disaster.included_in_event </PATH>"], "scores": [-0.009926275350153446, -0.03552096709609032, -0.04338020831346512, -0.06945452094078064, -0.07326321303844452], "norm_scores": [0.20735421776771545, 0.20211440324783325, 0.20053215324878693, 0.19537100195884705, 0.1946282982826233]}}
{"id": "WebQTest-291", "question": "where did morgan freeman graduate", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did morgan freeman graduate [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.01277099084109068, -0.012792362831532955, -0.04237590357661247, -0.06307773292064667, -0.07823702692985535], "norm_scores": [0.20583009719848633, 0.20582568645477295, 0.19982582330703735, 0.19573158025741577, 0.1927867978811264]}}
{"id": "WebQTest-294", "question": "who was carolus linnaeus", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was carolus linnaeus [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.0008903486304916441, -0.07022663950920105, -0.07811452448368073, -0.10792919248342514, -0.11415369063615799], "norm_scores": [0.21504905819892883, 0.20064353942871094, 0.1990671157836914, 0.1932196021080017, 0.19202065467834473]}}
{"id": "WebQTest-295", "question": "what type of government does germany have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["form_of_government.countries.country"], ["base.locations.countries.continent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does germany have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> form_of_government.countries.country </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.009665615856647491, -0.014389382675290108, -0.1402200162410736, -0.1624145209789276, -0.17024190723896027], "norm_scores": [0.21820272505283356, 0.21717442572116852, 0.19149662554264069, 0.18729326128959656, 0.18583296239376068]}}
{"id": "WebQTest-296", "question": "what kind of government does libya have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does libya have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.005804994609206915, -0.020925838500261307, -0.1488998830318451, -0.16615334153175354, -0.16655492782592773], "norm_scores": [0.21953816711902618, 0.21624353528022766, 0.19026754796504974, 0.1870129406452179, 0.1869378387928009]}}
{"id": "WebQTest-297", "question": "what time zone is anaheim california", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.city.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is anaheim california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.city.time_zones </PATH>"], "scores": [-1.0383358130638953e-05, -0.17109502851963043, -0.1830359548330307, -0.1853117197751999, -0.2018146961927414], "norm_scores": [0.23128844797611237, 0.19491839408874512, 0.19260472059249878, 0.19216689467430115, 0.18902160227298737]}}
{"id": "WebQTest-298", "question": "what year did the orioles go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_league_season.champion"]], "ground_paths": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the orioles go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_league_season.champion </PATH>"], "scores": [-0.006865883246064186, -0.017933856695890427, -0.07608047127723694, -0.11467159539461136, -0.12725505232810974], "norm_scores": [0.21247225999832153, 0.21013358235359192, 0.198263481259346, 0.19075801968574524, 0.1883726716041565]}}
{"id": "WebQTest-300", "question": "who is sir francis bacon", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir francis bacon [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.00010731181100709364, -0.11821632087230682, -0.12758682668209076, -0.14235718548297882, -0.14352984726428986], "norm_scores": [0.22208881378173828, 0.19734793901443481, 0.19550733268260956, 0.19264084100723267, 0.1924150586128235]}}
{"id": "WebQTest-301", "question": "what were amelia earhart 's achievements", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["base.activism.activist.area_of_activism"], ["fictional_universe.fictional_character.occupation"], ["symbols.name_source.namesakes"]], "ground_paths": [["fictional_universe.fictional_character.occupation"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were amelia earhart 's achievements [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.006256612949073315, -0.0339798629283905, -0.05157110095024109, -0.05766862630844116, -0.09107831865549088], "norm_scores": [0.20846718549728394, 0.20276717841625214, 0.19923146069049835, 0.1980203241109848, 0.1915138065814972]}}
{"id": "WebQTest-302", "question": "what are the official colors of the dallas cowboys", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.sports_team_colors.team"], ["sports.professional_sports_team.owner_s", "sports.sports_team.colors"], ["sports.professional_sports_team.colors"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official colors of the dallas cowboys [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team_colors.team </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.colors </PATH>"], "scores": [-0.00010753656533779576, -0.09581317007541656, -0.11427169293165207, -0.11536624282598495, -0.12095257639884949], "norm_scores": [0.2184288501739502, 0.19849318265914917, 0.19486290216445923, 0.19464972615242004, 0.19356536865234375]}}
{"id": "WebQTest-303", "question": "where did margaret hoover go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did margaret hoover go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.009735840372741222, -0.024259286001324654, -0.027866367250680923, -0.044777434319257736, -0.05778630077838898], "norm_scores": [0.20465528964996338, 0.2017044574022293, 0.20097818970680237, 0.19760803878307343, 0.19505400955677032]}}
{"id": "WebQTest-304", "question": "what is jamie oliver children names", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"], ["people.person.nationality"], ["parents.person.children"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie oliver children names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> parents.person.children </PATH>"], "scores": [-0.01171458326280117, -0.016079947352409363, -0.1542186290025711, -0.17656826972961426, -0.18472085893154144], "norm_scores": [0.2196846902370453, 0.21872778236865997, 0.19050706923007965, 0.1862965226173401, 0.1847839057445526]}}
{"id": "WebQTest-305", "question": "where does bradley walsh live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does bradley walsh live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.017651790753006935, -0.017664683982729912, -0.03623636066913605, -0.03941497951745987, -0.05103040486574173], "norm_scores": [0.2029542624950409, 0.20295165479183197, 0.199217289686203, 0.19858504831790924, 0.1962917447090149]}}
{"id": "WebQTest-306", "question": "who is the head coach of inter milan", "prediction": [["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["base.schemastaging.sports_team_extra.managers", "base.schemastaging.sports_team_manager_tenure.manager"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["soccer.football_team_management_tenure.team", "soccer.football_team_manager.team"]], "ground_paths": [["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["base.schemastaging.sports_team_manager_tenure.team", "base.schemastaging.sports_team_manager_tenure.manager"], ["soccer.football_team_management_tenure.team", "soccer.football_team_manager.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the head coach of inter milan [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> base.schemastaging.sports_team_extra.managers <SEP> base.schemastaging.sports_team_manager_tenure.manager </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> soccer.football_team_management_tenure.team <SEP> soccer.football_team_manager.team </PATH>"], "scores": [-0.02005375549197197, -0.02198345586657524, -0.024667683988809586, -0.033168621361255646, -0.037845492362976074], "norm_scores": [0.20149894058704376, 0.20111048221588135, 0.20057137310504913, 0.19887356460094452, 0.19794562458992004]}}
{"id": "WebQTest-308", "question": "who did reese witherspoon get married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did reese witherspoon get married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.017155099660158157, -0.028873812407255173, -0.031788453459739685, -0.046061839908361435, -0.04863770306110382], "norm_scores": [0.20348620414733887, 0.20111553370952606, 0.20053020119667053, 0.1976882815361023, 0.19717973470687866]}}
{"id": "WebQTest-310", "question": "what kind of money should i take to costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["common.topic.notable_types"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money should i take to costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.007656037341803312, -0.019024772569537163, -0.11800172924995422, -0.13348431885242462, -0.14459586143493652], "norm_scores": [0.21560975909233093, 0.21317242085933685, 0.19308383762836456, 0.19011738896369934, 0.1880165934562683]}}
{"id": "WebQTest-311", "question": "where is the ufc headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["sports.sports_league.sport", "sports.sports_team.location"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["sports.sports_organization.headquarters", "location.mailing_address.citytown"]], "ground_paths": [["common.topic.subject_of", "people.person.place_of_birth"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the ufc headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> sports.sports_league.sport <SEP> sports.sports_team.location </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> sports.sports_organization.headquarters <SEP> location.mailing_address.citytown </PATH>"], "scores": [-0.009636708535254002, -0.038031041622161865, -0.04583178833127022, -0.05667406693100929, -0.06045905500650406], "norm_scores": [0.20657075941562653, 0.20078781247138977, 0.19922763109207153, 0.19707919657230377, 0.19633464515209198]}}
{"id": "WebQTest-312", "question": "who plays captain kirk in star trek", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays captain kirk in star trek [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02567319944500923, -0.028123000636696815, -0.028610285371541977, -0.02937382273375988, -0.03153184801340103], "norm_scores": [0.2005983591079712, 0.20010754466056824, 0.20001006126403809, 0.19985739886760712, 0.1994265615940094]}}
{"id": "WebQTest-313", "question": "who does albert pujols play for 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does albert pujols play for 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.011052466928958893, -0.023746954277157784, -0.026187730953097343, -0.0301069188863039, -0.03496231511235237], "norm_scores": [0.20284534990787506, 0.2002865970134735, 0.1997983604669571, 0.19901683926582336, 0.19805288314819336]}}
{"id": "WebQTest-314", "question": "what kind of government is sweden", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["base.locations.countries.continent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.01140096690505743, -0.013234138488769531, -0.14527083933353424, -0.17205829918384552, -0.18470504879951477], "norm_scores": [0.2190418243408203, 0.21864064037799835, 0.19159673154354095, 0.18653248250484467, 0.18418830633163452]}}
{"id": "WebQTest-315", "question": "where was country singer george jones born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was country singer george jones born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.010267338715493679, -0.01931924745440483, -0.0502256378531456, -0.09643736481666565, -0.14484284818172455], "norm_scores": [0.21082176268100739, 0.20892204344272614, 0.20256376266479492, 0.19341595470905304, 0.1842765361070633]}}
{"id": "WebQTest-316", "question": "what did shakespeare become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.profession"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["fictional_universe.fictional_character.occupation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shakespeare become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.005114009138196707, -0.03419673070311546, -0.0558316744863987, -0.07019349187612534, -0.09446850419044495], "norm_scores": [0.20949439704418182, 0.20348946750164032, 0.19913426041603088, 0.19629478454589844, 0.19158710539340973]}}
{"id": "WebQTest-317", "question": "where does russia import from", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does russia import from [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.027815187349915504, -0.02862263098359108, -0.031004132702946663, -0.03490195795893669, -0.03526871278882027], "norm_scores": [0.20074187219142914, 0.20057985186576843, 0.20010273158550262, 0.19932429492473602, 0.1992512047290802]}}
{"id": "WebQTest-318", "question": "what is the money of switzerland called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["common.topic.notable_types"], ["location.country.currency"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the money of switzerland called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.country.currency </PATH>"], "scores": [-0.004134000279009342, -0.03059394285082817, -0.14022718369960785, -0.1591910719871521, -0.1656976193189621], "norm_scores": [0.2195955067873001, 0.21386121213436127, 0.19165445864200592, 0.18805420398712158, 0.18683457374572754]}}
{"id": "WebQTest-321", "question": "what do people in australia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in australia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.006882583722472191, -0.022528721019625664, -0.070808544754982, -0.10617412626743317, -0.15609970688819885], "norm_scores": [0.2132463902235031, 0.20993587374687195, 0.2000409960746765, 0.1930900514125824, 0.18368661403656006]}}
{"id": "WebQTest-322", "question": "what has ian somerhalder acted in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["film.film.produced_by"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.producer.film"], ["film.performance.actor", "film.film.starring"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ian somerhalder acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.018865901976823807, -0.030924325808882713, -0.032850347459316254, -0.034492116421461105, -0.03601804003119469], "norm_scores": [0.20236295461654663, 0.19993741810321808, 0.1995527148246765, 0.19922535121440887, 0.1989215761423111]}}
{"id": "WebQTest-323", "question": "what movies did ron howard director", "prediction": [["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"], ["film.producer.film"], ["people.person.nationality", "film.film.country"]], "ground_paths": [["film.producer.film"], ["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did ron howard director [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.director.film </PATH>", "<PATH> film.producer.film </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>"], "scores": [-0.022325994446873665, -0.031353119760751724, -0.033216848969459534, -0.04474685713648796, -0.045563217252492905], "norm_scores": [0.20263256132602692, 0.20081160962581635, 0.20043769478797913, 0.19813992083072662, 0.19797822833061218]}}
{"id": "WebQTest-326", "question": "what did gerald r ford die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gerald r ford die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0012335570063441992, -0.04877779632806778, -0.10327434539794922, -0.12332398444414139, -0.13504306972026825], "norm_scores": [0.21661964058876038, 0.2065616399049759, 0.19560596346855164, 0.19172319769859314, 0.189489483833313]}}
{"id": "WebQTest-327", "question": "what two continents is turkey on", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.locations.countries.continent"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two continents is turkey on [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.012192553840577602, -0.029355822131037712, -0.04603778198361397, -0.05444229021668434, -0.06009967625141144], "norm_scores": [0.20569542050361633, 0.20219512283802032, 0.1988501101732254, 0.19718587398529053, 0.1960734724998474]}}
{"id": "WebQTest-328", "question": "where did kevin love go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin love go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.015608332119882107, -0.0212628822773695, -0.02731916308403015, -0.03291558101773262, -0.04942990466952324], "norm_scores": [0.2027449905872345, 0.2016017884016037, 0.20038452744483948, 0.1992662250995636, 0.19600249826908112]}}
{"id": "WebQTest-329", "question": "what type of government was formed when italy unified", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent", "government.form_of_government.countries"], ["government.form_of_government.countries", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government was formed when italy unified [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>"], "scores": [-0.010263857431709766, -0.01288636215031147, -0.11084820330142975, -0.11334746330976486, -0.12315592914819717], "norm_scores": [0.21290236711502075, 0.21234476566314697, 0.19252949953079224, 0.19204892218112946, 0.1901744157075882]}}
{"id": "WebQTest-331", "question": "where was the temple of karnak built", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["architecture.structure.architect"], ["architecture.building_complex.building_sites"], ["architecture.building_complex.buildings_sites"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the temple of karnak built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.building_complex.building_sites </PATH>", "<PATH> architecture.building_complex.buildings_sites </PATH>"], "scores": [-0.0017745444783940911, -0.06548042595386505, -0.07440993189811707, -0.10784655809402466, -0.1201150193810463], "norm_scores": [0.21477781236171722, 0.20152193307876587, 0.19973045587539673, 0.19316254556179047, 0.19080722332000732]}}
{"id": "WebQTest-333", "question": "what movies did morgan freeman star in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.film.executive_produced_by"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["theater.theater_actor.theater_roles", "theater.theater_role.play"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["film.producer.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"], ["film.producer.films_executive_produced"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["common.topic.notable_types", "common.topic.notable_types"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did morgan freeman star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.018540848046541214, -0.03728498890995979, -0.03814433142542839, -0.038364339619874954, -0.040598925203084946], "norm_scores": [0.20322836935520172, 0.19945450127124786, 0.1992831826210022, 0.1992393434047699, 0.19879461824893951]}}
{"id": "WebQTest-334", "question": "who did john kennedy have affairs with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"]], "ground_paths": [["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["music.recording.featured_artists", "music.recording.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did john kennedy have affairs with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>"], "scores": [-0.020458245649933815, -0.033133987337350845, -0.03359021618962288, -0.048155173659324646, -0.049594759941101074], "norm_scores": [0.20332124829292297, 0.20076026022434235, 0.20066869258880615, 0.19776715338230133, 0.1974826455116272]}}
{"id": "WebQTest-335", "question": "what type of money to take to cuba", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.tourist_attraction.near_travel_destination"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money to take to cuba [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.010980227030813694, -0.015114214271306992, -0.10159684717655182, -0.10939197242259979, -0.11499426513910294], "norm_scores": [0.21201126277446747, 0.2111366093158722, 0.19364425539970398, 0.19214065372943878, 0.19106723368167877]}}
{"id": "WebQTest-336", "question": "what currency should you take to morocco", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to morocco [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.011675720103085041, -0.014264129102230072, -0.1083664745092392, -0.13373692333698273, -0.1387663334608078], "norm_scores": [0.21408693492412567, 0.21353352069854736, 0.19435599446296692, 0.18948709964752197, 0.18853649497032166]}}
{"id": "WebQTest-337", "question": "who is mary mcleod bethune for kids", "prediction": [["people.person.profession"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.profession.people_with_this_profession"], ["education.education.student", "education.education.student"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is mary mcleod bethune for kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> education.education.student <SEP> education.education.student </PATH>"], "scores": [-0.0005275836447253823, -0.08379852026700974, -0.08544690907001495, -0.09277744591236115, -0.09302031993865967], "norm_scores": [0.21449002623558044, 0.19735266268253326, 0.19702762365341187, 0.19558857381343842, 0.19554108381271362]}}
{"id": "WebQTest-338", "question": "what do they call money in japan", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"], ["language.human_language.main_country"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they call money in japan [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.013158350251615047, -0.017795659601688385, -0.039013996720314026, -0.10266093164682388, -0.11164301633834839], "norm_scores": [0.20874890685081482, 0.20778310298919678, 0.2034207284450531, 0.1908770501613617, 0.1891702562570572]}}
{"id": "WebQTest-339", "question": "what nationality is mother teresa", "prediction": [["people.person.nationality"], ["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is mother teresa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.005966152064502239, -0.03630121424794197, -0.0633828267455101, -0.08687802404165268, -0.08739541471004486], "norm_scores": [0.2101549506187439, 0.2038756161928177, 0.1984284371137619, 0.19382064044475555, 0.19372038543224335]}}
{"id": "WebQTest-340", "question": "who is richard pryor son", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"], ["people.person.parent"], ["people.person.sibling_s"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is richard pryor son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> people.person.sibling_s </PATH>"], "scores": [-0.003384415991604328, -0.03793741390109062, -0.09684094786643982, -0.14309196174144745, -0.1593276858329773], "norm_scores": [0.21729516983032227, 0.20991520583629608, 0.19790758192539215, 0.18896260857582092, 0.18591943383216858]}}
{"id": "WebQTest-341", "question": "what countries share borders with spain", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["royalty.royal_line.kingdom_s_ruled", "people.family.country"], ["geography.river.basin_countries", "location.location.partially_contains"], ["location.country.form_of_government", "location.country.form_of_government"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["location.location.primarily_containedby", "location.location.contains_major_portion_of"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.country.currency_formerly_used", "finance.currency.countries_formerly_used"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.primarily_containedby"], ["location.location.partially_containedby", "location.location.containedby"], ["location.location.primarily_containedby", "base.locations.continents.countries_within"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.primarily_containedby", "location.location.primarily_containedby"], ["location.location.containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.contains"], ["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.primarily_containedby", "location.location.containedby"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["finance.currency.countries_used", "location.country.currency_used"], ["film.film.country", "film.film.country"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["location.country.internet_tld", "location.country.internet_tld"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries share borders with spain [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.country.form_of_government <SEP> location.country.form_of_government </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.035662200301885605, -0.040682751685380936, -0.04506845772266388, -0.046594973653554916, -0.046686697751283646], "norm_scores": [0.2014588564634323, 0.20044994354248047, 0.19957275688648224, 0.19926834106445312, 0.19925005733966827]}}
{"id": "WebQTest-342", "question": "what countries have english as their official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have english as their official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.017205169424414635, -0.020115474238991737, -0.022574158385396004, -0.08691861480474472, -0.13869990408420563], "norm_scores": [0.20790037512779236, 0.2072962075471878, 0.2067871391773224, 0.19390058517456055, 0.1841156929731369]}}
{"id": "WebQTest-343", "question": "what international organizations is china part of", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization_member.member_of", "organization.organization_membership.organization"], ["organization.organization_membership.member", "organization.organization_membership.organization"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization_member.member_of", "organization.organization_membership.organization"], ["organization.organization_membership.member", "organization.membership_organization.members"], ["organization.organization_membership.member", "organization.organization_membership.organization"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what international organizations is china part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_member.member_of <SEP> organization.organization_membership.organization </PATH>", "<PATH> organization.organization_membership.member <SEP> organization.organization_membership.organization </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.02469327300786972, -0.025556104257702827, -0.028943244367837906, -0.03431583195924759, -0.03753044828772545], "norm_scores": [0.20110344886779785, 0.20093001425266266, 0.20025058090686798, 0.1991776078939438, 0.19853834807872772]}}
{"id": "WebQTest-344", "question": "what county is west st paul in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is west st paul in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.013206386007368565, -0.02545544132590294, -0.04165039211511612, -0.04922935739159584, -0.050573717802762985], "norm_scores": [0.20459423959255219, 0.20210343599319458, 0.19885675609111786, 0.19735532999038696, 0.19709019362926483]}}
{"id": "WebQTest-346", "question": "who does donnie wahlberg play in the sixth sense", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.performance.actor", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does donnie wahlberg play in the sixth sense [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.020142963156104088, -0.030494749546051025, -0.035448819398880005, -0.03714551031589508, -0.03774509206414223], "norm_scores": [0.20242074131965637, 0.20033612847328186, 0.19934611022472382, 0.19900816679000854, 0.1988888829946518]}}
{"id": "WebQTest-347", "question": "what did baron de montesquie influence", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "organization.organization.founders"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did baron de montesquie influence [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> organization.organization.founders </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.004851798061281443, -0.02315564639866352, -0.10791055113077164, -0.10856620222330093, -0.10993135720491409], "norm_scores": [0.21341654658317566, 0.20954574644565582, 0.19251751899719238, 0.1923913210630417, 0.19212886691093445]}}
{"id": "WebQTest-349", "question": "who did carlos boozer play for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did carlos boozer play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.009585732594132423, -0.015386021696031094, -0.052117057144641876, -0.05273009091615677, -0.05388737842440605], "norm_scores": [0.20546473562717438, 0.2042764127254486, 0.19690926373004913, 0.19678860902786255, 0.19656097888946533]}}
{"id": "WebQTest-351", "question": "who owns the portland press herald", "prediction": [["book.newspaper.owner"], ["periodicals.newspaper_circulation_area.newspapers", "business.business_operation.industry"], ["book.newspaper_owner.newspapers_owned"], ["periodicals.newspaper_circulation_area.newspapers", "book.periodical.language", "people.person.languages"], ["business.business_operation.industry", "business.business_operation.industry"]], "ground_paths": [["book.newspaper_owner.newspapers_owned"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns the portland press herald [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.owner </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> business.business_operation.industry </PATH>", "<PATH> book.newspaper_owner.newspapers_owned </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> book.periodical.language <SEP> people.person.languages </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>"], "scores": [-0.022669464349746704, -0.037560056895017624, -0.038512781262397766, -0.04329751059412956, -0.04608888551592827], "norm_scores": [0.2030070573091507, 0.20000655949115753, 0.19981609284877777, 0.19886231422424316, 0.19830797612667084]}}
{"id": "WebQTest-352", "question": "what did albert speer design", "prediction": [["architecture.architect.structures_designed"], ["architecture.structure.architect"], ["architecture.architecture.architect"], ["architecture.building.architect"], ["architecture.architect.structures_designed", "architecture.architect.structures_designed"]], "ground_paths": [["architecture.architect.structures_designed"], ["architecture.structure.architect"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did albert speer design [/INST]", "raw_output": {"paths": ["<PATH> architecture.architect.structures_designed </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.architecture.architect </PATH>", "<PATH> architecture.building.architect </PATH>", "<PATH> architecture.architect.structures_designed <SEP> architecture.architect.structures_designed </PATH>"], "scores": [-0.0006143796490505338, -0.08431187272071838, -0.10743887722492218, -0.11624888330698013, -0.11720233410596848], "norm_scores": [0.2174316942691803, 0.19997398555278778, 0.19540224969387054, 0.19368833303451538, 0.1935037523508072]}}
{"id": "WebQTest-354", "question": "what art movement did leonardo da vinci belong to", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["influence.influence_node.influenced_by"], ["interests.hobby.people_with_this_hobby"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art movement did leonardo da vinci belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> interests.hobby.people_with_this_hobby </PATH>"], "scores": [-0.0041449107229709625, -0.025311991572380066, -0.06080146133899689, -0.09040872007608414, -0.09291663020849228], "norm_scores": [0.21024389564990997, 0.20584040880203247, 0.1986633688211441, 0.192867711186409, 0.19238461554050446]}}
{"id": "WebQTest-356", "question": "what airport is near arlington tx", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport is near arlington tx [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>"], "scores": [-0.010939497500658035, -0.02148437686264515, -0.04327476769685745, -0.055990494787693024, -0.06958634406328201], "norm_scores": [0.20590193569660187, 0.203742116689682, 0.1993505358695984, 0.19683168828487396, 0.19417372345924377]}}
{"id": "WebQTest-359", "question": "what team does heskey play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does heskey play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.027842115610837936, -0.029082749038934708, -0.03254471719264984, -0.03507377207279205, -0.036574434489011765], "norm_scores": [0.2008770853281021, 0.20062802731990814, 0.19993466138839722, 0.19942966103553772, 0.1991305947303772]}}
{"id": "WebQTest-360", "question": "where did emperor hirohito live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor hirohito live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.023268286138772964, -0.025361446663737297, -0.028489861637353897, -0.03885706514120102, -0.05063746124505997], "norm_scores": [0.2020106017589569, 0.2015882134437561, 0.20095853507518768, 0.19888590276241302, 0.1965566873550415]}}
{"id": "WebQTest-361", "question": "what to do and see in vienna austria", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["film.film.featured_film_locations"], ["organization.organization.geographic_scope"]], "ground_paths": [["base.arthist.int_zm_nyek.helynev"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do and see in vienna austria [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> film.film.featured_film_locations </PATH>", "<PATH> organization.organization.geographic_scope </PATH>"], "scores": [-0.013290577568113804, -0.018958017230033875, -0.024748168885707855, -0.10080882161855698, -0.10907596349716187], "norm_scores": [0.20799458026885986, 0.20681911706924438, 0.20562507212162018, 0.1905650794506073, 0.18899616599082947]}}
{"id": "WebQTest-362", "question": "when did conflict start in ireland", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.events"], ["people.person.nationality", "people.person.nationality"], ["people.person.nationality", "people.person.place_of_birth"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did conflict start in ireland [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.005103768780827522, -0.043098680675029755, -0.048907727003097534, -0.08779466152191162, -0.08940045535564423], "norm_scores": [0.21009932458400726, 0.2022663801908493, 0.20109480619430542, 0.19342494010925293, 0.19311457872390747]}}
{"id": "WebQTest-365", "question": "what is there to do in peoria illinois", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do in peoria illinois [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> travel.travel_destination.tourist_attractions <PATH>"], "scores": [-0.010502713732421398, -0.017529787495732307, -0.04281081259250641, -0.058882858604192734, -0.1705959588289261], "norm_scores": [0.20981772243976593, 0.20834848284721375, 0.2031472623348236, 0.1999083310365677, 0.17877818644046783]}}
{"id": "WebQTest-366", "question": "what language do british speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do british speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.016116300597786903, -0.020523525774478912, -0.03716396540403366, -0.04499954357743263, -0.11687061190605164], "norm_scores": [0.20616601407527924, 0.20525939762592316, 0.2018720507621765, 0.20029644668102264, 0.18640607595443726]}}
{"id": "WebQTest-367", "question": "what makes elvis presley famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"], ["music.artist.origin"], ["music.artist.origin", "people.person.profession"]], "ground_paths": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what makes elvis presley famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> music.artist.origin <SEP> people.person.profession </PATH>"], "scores": [-0.004400025121867657, -0.04253536835312843, -0.0673508420586586, -0.08608640730381012, -0.09557835757732391], "norm_scores": [0.2111491709947586, 0.203248530626297, 0.1982668936252594, 0.1945868283510208, 0.192748561501503]}}
{"id": "WebQTest-370", "question": "what language do people speak in the netherlands", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["book.newspaper.circulation_areas"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the netherlands [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>"], "scores": [-0.013577383942902088, -0.016609419137239456, -0.041059285402297974, -0.05137738585472107, -0.1451098918914795], "norm_scores": [0.20792090892791748, 0.20729142427444458, 0.20228464901447296, 0.2002081722021103, 0.1822948306798935]}}
{"id": "WebQTest-371", "question": "what are the major languages spoken in greece", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in greece [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.016853181645274162, -0.018837831914424896, -0.02625054307281971, -0.05338558554649353, -0.14801838994026184], "norm_scores": [0.20704518258571625, 0.20663468539714813, 0.2051086276769638, 0.19961783289909363, 0.18159371614456177]}}
{"id": "WebQTest-374", "question": "what type of artist is henri matisse", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.art_form.artists"], ["visual_art.visual_artists.art_forms"]], "ground_paths": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of artist is henri matisse [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.art_form.artists </PATH>", "<PATH> visual_art.visual_artists.art_forms </PATH>"], "scores": [-0.0006334630306810141, -0.07336387783288956, -0.08130744099617004, -0.08785422891378403, -0.11324308067560196], "norm_scores": [0.21448548138141632, 0.1994396597146988, 0.1978616565465927, 0.1965705156326294, 0.1916426569223404]}}
{"id": "WebQTest-375", "question": "what was the first book charles dickens wrote", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"], ["film.film_story_contributor.film_story_credits"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first book charles dickens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>"], "scores": [-0.0029546075966209173, -0.03543601930141449, -0.057041145861148834, -0.060900237411260605, -0.0724550187587738], "norm_scores": [0.20868323743343353, 0.20201382040977478, 0.19769608974456787, 0.19693464040756226, 0.19467219710350037]}}
{"id": "WebQTest-376", "question": "what books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"]], "ground_paths": [["film.film_story_contributor.film_story_credits"], ["film.film.story_by"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.014961636625230312, -0.018466724082827568, -0.04019035026431084, -0.05427251756191254, -0.07161859422922134], "norm_scores": [0.20500382781028748, 0.20428651571273804, 0.19989652931690216, 0.19710129499435425, 0.19371184706687927]}}
{"id": "WebQTest-377", "question": "what school did michael jordan attend", "prediction": [["people.person.education", "education.education.institution"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did michael jordan attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01853620819747448, -0.02241678535938263, -0.025478845462203026, -0.027499740943312645, -0.03032591938972473], "norm_scores": [0.2012653946876526, 0.20048588514328003, 0.1998729258775711, 0.1994694024324417, 0.19890646636486053]}}
{"id": "WebQTest-379", "question": "what year does hitler die", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year does hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> time.event.locations </PATH>"], "scores": [-0.012832514941692352, -0.02156073786318302, -0.03880766034126282, -0.06657352298498154, -0.07446509599685669], "norm_scores": [0.20603370666503906, 0.2042432278394699, 0.20075085759162903, 0.19525350630283356, 0.19371873140335083]}}
{"id": "WebQTest-380", "question": "where is mount st helens volcano", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"], ["geography.mountain.mountain_range"], ["geography.mountain_range.mountains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount st helens volcano [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> geography.mountain_range.mountains </PATH>"], "scores": [-0.007643769960850477, -0.02888801135122776, -0.046604450792074203, -0.10031147301197052, -0.12502112984657288], "norm_scores": [0.2109030932188034, 0.20646987855434418, 0.20284418761730194, 0.19223739206790924, 0.18754547834396362]}}
{"id": "WebQTest-381", "question": "what shows are shot in new york", "prediction": [["film.film.featured_film_locations"], ["tv.tv_program.filming_locations"], ["tv.tv_location.tv_shows_filmed_here"], ["media_common.netflix_title.netflix_genres"], ["location.location.featured_in_films"]], "ground_paths": [["tv.tv_program.filming_locations"], ["film.film.featured_film_locations"], ["tv.tv_location.tv_shows_filmed_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what shows are shot in new york [/INST]", "raw_output": {"paths": ["<PATH> film.film.featured_film_locations </PATH>", "<PATH> tv.tv_program.filming_locations </PATH>", "<PATH> tv.tv_location.tv_shows_filmed_here </PATH>", "<PATH> media_common.netflix_title.netflix_genres </PATH>", "<PATH> location.location.featured_in_films </PATH>"], "scores": [-0.015756845474243164, -0.021938305348157883, -0.022222014144062996, -0.08651407063007355, -0.10835516452789307], "norm_scores": [0.207011878490448, 0.2057361751794815, 0.2056778222322464, 0.19287048280239105, 0.18870365619659424]}}
{"id": "WebQTest-382", "question": "what book did tupac write", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"], ["book.author.book_editions_published"], ["film.film.story_by"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did tupac write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.014474239200353622, -0.018701253458857536, -0.03959369659423828, -0.07819215953350067, -0.1302940398454666], "norm_scores": [0.20833851397037506, 0.2074597328901291, 0.20317034423351288, 0.1954777091741562, 0.18555372953414917]}}
{"id": "WebQTest-383", "question": "what language do chinese people write in", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.writing_system"], ["language.human_language.language_family"]], "ground_paths": [["language.human_language.writing_system"], ["language.language_writing_system.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do chinese people write in [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.writing_system </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.008348854258656502, -0.03391094505786896, -0.03950238600373268, -0.04999770596623421, -0.07657022029161453], "norm_scores": [0.20672480762004852, 0.20150744915008545, 0.20038387179374695, 0.19829179346561432, 0.19309206306934357]}}
{"id": "WebQTest-384", "question": "what was robert burns famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"], ["influence.influence_node.influenced", "people.person.profession"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.005937251262366772, -0.027555067092180252, -0.07149514555931091, -0.08834737539291382, -0.08857481181621552], "norm_scores": [0.2102281004190445, 0.20573219656944275, 0.1968880295753479, 0.19359783828258514, 0.19355380535125732]}}
{"id": "WebQTest-385", "question": "when did william mckinley died", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"], ["base.culturalevent.event.entity_involved"], ["people.cause_of_death.people"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did william mckinley died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.0037287124432623386, -0.036700475960969925, -0.05235075205564499, -0.07711375504732132, -0.08972182124853134], "norm_scores": [0.20977771282196045, 0.20297376811504364, 0.19982190430164337, 0.19493447244167328, 0.19249215722084045]}}
{"id": "WebQTest-386", "question": "how deep is lake merritt oakland", "prediction": [["geography.lake.basin_countries"], ["location.location.containedby"], ["geography.lake.depth"], ["location.location.contains"], ["geography.lake.lake_type"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how deep is lake merritt oakland [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.lake.depth </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.lake.lake_type </PATH>"], "scores": [-0.01171241793781519, -0.03923619166016579, -0.046143993735313416, -0.05535917729139328, -0.058769430965185165], "norm_scores": [0.20617148280143738, 0.20057423412799835, 0.19919349253177643, 0.19736632704734802, 0.19669440388679504]}}
{"id": "WebQTest-388", "question": "what are the important holidays of islam", "prediction": [["time.holiday.featured_in_religions"], ["religion.religion.holidays"], ["time.recurring_event.holidays"], ["location.location.events"], ["time.recurring_event.current_frequency"]], "ground_paths": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the important holidays of islam [/INST]", "raw_output": {"paths": ["<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> religion.religion.holidays </PATH>", "<PATH> time.recurring_event.holidays </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.008092246949672699, -0.01670161634683609, -0.1356392204761505, -0.14471745491027832, -0.14557300508022308], "norm_scores": [0.21665993332862854, 0.21480262279510498, 0.19071535766124725, 0.18899184465408325, 0.1888302117586136]}}
{"id": "WebQTest-389", "question": "what type of music did claude debussy play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["book.written_work.author"], ["music.composer.genre"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did claude debussy play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> music.composer.genre </PATH>"], "scores": [-0.004152076784521341, -0.030395029112696648, -0.09651800245046616, -0.11796700954437256, -0.12424198538064957], "norm_scores": [0.21435613930225372, 0.2088039666414261, 0.19544380903244019, 0.19129636883735657, 0.19009974598884583]}}
{"id": "WebQTest-390", "question": "when was the last time the toronto maple leafs were in the stanley cup finals", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs were in the stanley cup finals [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>"], "scores": [-0.010597329586744308, -0.014906538650393486, -0.025603551417589188, -0.07655219733715057, -0.08786964416503906], "norm_scores": [0.20649993419647217, 0.20561200380325317, 0.20342427492141724, 0.19331969320774078, 0.19114413857460022]}}
{"id": "WebQTest-392", "question": "what team is hank baskett on 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is hank baskett on 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.015590725466609001, -0.019430536776781082, -0.020508769899606705, -0.02400302328169346, -0.03489205986261368], "norm_scores": [0.2014598399400711, 0.20068776607513428, 0.20047149062156677, 0.1997721940279007, 0.19760867953300476]}}
{"id": "WebQTest-393", "question": "what are the two official languages of paraguay", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.language_family.geographic_distribution"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the two official languages of paraguay [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>"], "scores": [-0.015027644112706184, -0.025644497945904732, -0.027977334335446358, -0.03507441654801369, -0.17003975808620453], "norm_scores": [0.20776499807834625, 0.20557084679603577, 0.20509184896945953, 0.20364144444465637, 0.17793095111846924]}}
{"id": "WebQTest-395", "question": "what state is the steelers from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.location", "location.location.containedby"], ["sports.sports_team_location.teams", "location.location.containedby"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is the steelers from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.location <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_team_location.teams <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.004010586999356747, -0.031490176916122437, -0.09546848386526108, -0.11013416945934296, -0.11689873784780502], "norm_scores": [0.21376389265060425, 0.20796971023082733, 0.19508087635040283, 0.19224075973033905, 0.19094471633434296]}}
{"id": "WebQTest-396", "question": "what college did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.010214267298579216, -0.01877472549676895, -0.02582586742937565, -0.07730682939291, -0.08753923326730728], "norm_scores": [0.20675325393676758, 0.20499089360237122, 0.2035505771636963, 0.19333675503730774, 0.19136853516101837]}}
{"id": "WebQTest-397", "question": "what are the school colors for harvard university", "prediction": [["education.educational_institution.colors"], ["education.school_mascot.school"], ["education.educational_institution.colors", "education.educational_institution.colors"], ["education.educational_institution.colors", "education.color.colors"], ["education.educational_institution.colors", "education.school_colors.school"]], "ground_paths": [["education.educational_institution.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the school colors for harvard university [/INST]", "raw_output": {"paths": ["<PATH> education.educational_institution.colors </PATH>", "<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.colors <SEP> education.educational_institution.colors </PATH>", "<PATH> education.educational_institution.colors <SEP> education.color.colors </PATH>", "<PATH> education.educational_institution.colors <SEP> education.school_colors.school </PATH>"], "scores": [-0.003313168417662382, -0.04762301966547966, -0.06369604170322418, -0.06668339669704437, -0.0674101710319519], "norm_scores": [0.2094428837299347, 0.2003651112318039, 0.19717037677764893, 0.19658222794532776, 0.19643941521644592]}}
{"id": "WebQTest-398", "question": "where is the university of the rockies located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the university of the rockies located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>"], "scores": [-0.002766162622720003, -0.049814388155937195, -0.06153256446123123, -0.06319399923086166, -0.06887851655483246], "norm_scores": [0.2094523310661316, 0.1998262107372284, 0.19749827682971954, 0.1971704065799713, 0.19605277478694916]}}
{"id": "WebQTest-399", "question": "what disease does robin roberts have", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"], ["medicine.cancer_patient.notable_people_with_this_condition"], ["people.person.medical_condition"], ["medicine.disease_notable_person_with_this_condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease does robin roberts have [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.cancer_patient.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition </PATH>", "<PATH> medicine.disease_notable_person_with_this_condition </PATH>"], "scores": [-0.01058771088719368, -0.01219971477985382, -0.09027102589607239, -0.0967792198061943, -0.1137162521481514], "norm_scores": [0.21091583371162415, 0.2105761170387268, 0.19476152956485748, 0.19349810481071472, 0.19024841487407684]}}
{"id": "WebQTest-400", "question": "what was the capital city of the east roman empire", "prediction": [["location.location.containedby"], ["location.country.capital"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["fictional_universe.fictional_setting.languages", "language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the capital city of the east roman empire [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.006776771973818541, -0.029157837852835655, -0.051038216799497604, -0.07608006149530411, -0.07828624546527863], "norm_scores": [0.20839400589466095, 0.20378173887729645, 0.199371337890625, 0.19444070756435394, 0.19401220977306366]}}
{"id": "WebQTest-401", "question": "who plays edward scissorhands", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film.starring", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.film.country", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays edward scissorhands [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.016882814466953278, -0.04234098270535469, -0.04654660448431969, -0.04925490543246269, -0.051181044429540634], "norm_scores": [0.20491532981395721, 0.1997644007205963, 0.19892604649066925, 0.198388010263443, 0.1980062574148178]}}
{"id": "WebQTest-402", "question": "what does joey jordison play in slipknot", "prediction": [["music.group_membership.group", "music.group_membership.role"], ["music.group_member.instruments_played"], ["music.group_member.instruments_played", "music.group_member.instruments_played"], ["music.instrument.instrumentalists", "music.group_member.instruments_played"], ["music.instrument.instrumentalists"]], "ground_paths": [["music.group_member.instruments_played"], ["music.group_membership.group", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does joey jordison play in slipknot [/INST]", "raw_output": {"paths": ["<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.instrumentalists <SEP> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.instrumentalists </PATH>"], "scores": [-0.01789422705769539, -0.018822070211172104, -0.04238305613398552, -0.04764639586210251, -0.05308679863810539], "norm_scores": [0.20362505316734314, 0.2034362256526947, 0.19869907200336456, 0.19765600562095642, 0.1965836137533188]}}
{"id": "WebQTest-403", "question": "what kind of language does turkey speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["food.beer_country_region.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does turkey speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> food.beer_country_region.languages_spoken </PATH>"], "scores": [-0.006846860982477665, -0.030163869261741638, -0.04786336421966553, -0.056381914764642715, -0.09651552140712738], "norm_scores": [0.20821742713451385, 0.20341859757900238, 0.19984985888004303, 0.19815465807914734, 0.1903594583272934]}}
{"id": "WebQTest-404", "question": "what was lebron james first team", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lebron james first team [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.013629048131406307, -0.01645669899880886, -0.029643209651112556, -0.046900056302547455, -0.052863262593746185], "norm_scores": [0.2036622017621994, 0.20308712124824524, 0.20042669773101807, 0.19699764251708984, 0.19582639634609222]}}
{"id": "WebQTest-405", "question": "who fought the battle of gettysburg", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"], ["military.military_person.participated_in_conflicts"], ["military.military_conflict.casualties", "military.military_person.casualties"], ["military.military_command.military_conflict", "military.military_command.military_commander"]], "ground_paths": [["military.military_command.military_conflict", "military.military_combatant.military_commanders"], ["location.location.events", "location.administrative_division.first_level_division_of"], ["book.written_work.subjects", "book.written_work.subjects"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["location.location.events", "location.administrative_division.second_level_division_of"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["time.event.included_in_event", "time.event.locations"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved", "people.person.nationality"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["military.military_person.participated_in_conflicts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought the battle of gettysburg [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_person.casualties </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_command.military_commander </PATH>"], "scores": [-0.006793790031224489, -0.02687808685004711, -0.046034932136535645, -0.0564405582845211, -0.05831257998943329], "norm_scores": [0.20648418366909027, 0.20237846672534943, 0.1985384225845337, 0.19648320972919464, 0.19611573219299316]}}
{"id": "WebQTest-406", "question": "who is the coach of the sf giants", "prediction": [["baseball.baseball_team.current_coaches"], ["baseball.baseball_team.current_manager"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["baseball.baseball_team.current_coach"]], "ground_paths": [["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"], ["baseball.current_coaching_tenure.baseball_team", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach of the sf giants [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> baseball.baseball_team.current_manager </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> baseball.baseball_team.current_coach </PATH>"], "scores": [-0.010899344459176064, -0.029268929734826088, -0.04674269258975983, -0.047137558460235596, -0.04843438044190407], "norm_scores": [0.20516352355480194, 0.20142914354801178, 0.19794000685214996, 0.19786188006401062, 0.1976054459810257]}}
{"id": "WebQTest-408", "question": "who was the first president of the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first president of the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.0006040467415004969, -0.08116133511066437, -0.08611076325178146, -0.09452422708272934, -0.09700851142406464], "norm_scores": [0.21463368833065033, 0.19802147150039673, 0.1970437914133072, 0.1953929364681244, 0.19490811228752136]}}
{"id": "WebQTest-409", "question": "where did giuliana rancic grow up", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did giuliana rancic grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.0034356112591922283, -0.04139849543571472, -0.047890838235616684, -0.11592020094394684, -0.13615640997886658], "norm_scores": [0.21328383684158325, 0.2053387314081192, 0.20400993525981903, 0.19059281051158905, 0.18677470088005066]}}
{"id": "WebQTest-410", "question": "what are the major cities in ukraine", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major cities in ukraine [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.02060544490814209, -0.02322840690612793, -0.03187639266252518, -0.044440094381570816, -0.05530969053506851], "norm_scores": [0.20290105044841766, 0.2023695558309555, 0.20062699913978577, 0.19812215864658356, 0.19598031044006348]}}
{"id": "WebQTest-414", "question": "who does brian dawkins play for 2011", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does brian dawkins play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.012666678056120872, -0.012799081392586231, -0.04298917576670647, -0.046235665678977966, -0.051431864500045776], "norm_scores": [0.20412474870681763, 0.204097718000412, 0.1980280876159668, 0.197386234998703, 0.19636324048042297]}}
{"id": "WebQTest-415", "question": "what movies did tupac star in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["music.producer.releases_produced"], ["music.recording.artist", "music.recording.releases"], ["music.featured_artist.recordings", "music.recording.releases"], ["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did tupac star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.02112709730863571, -0.023385055363178253, -0.03960614278912544, -0.040942806750535965, -0.04876112565398216], "norm_scores": [0.20273452997207642, 0.202277272939682, 0.1990225911140442, 0.19875673949718475, 0.19720885157585144]}}
{"id": "WebQTest-416", "question": "what form of currency does china have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does china have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.011153995990753174, -0.015324262902140617, -0.12000862509012222, -0.1361139416694641, -0.14418134093284607], "norm_scores": [0.21502266824245453, 0.21412783861160278, 0.1928454041481018, 0.18976444005966187, 0.1882397085428238]}}
{"id": "WebQTest-417", "question": "who plays kenneth", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays kenneth [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.023030037060379982, -0.024909507483243942, -0.03124476596713066, -0.03182258829474449, -0.03498005494475365], "norm_scores": [0.20123524963855743, 0.20085740089416504, 0.19958890974521637, 0.19947364926338196, 0.1988447904586792]}}
{"id": "WebQTest-418", "question": "what league are the indianapolis colts in", "prediction": [["american_football.football_team.division"], ["sports.sports_team.league"], ["sports.sports_team.sport"], ["american_football.football_team.league"], ["american_football.football_team.current_division"]], "ground_paths": [["american_football.football_team.division"], ["american_football.football_team.conference"], ["american_football.football_team.conference", "american_football.football_conference.league"], ["sports.sports_team.sport", "sports.sports_league.sport"], ["sports.sports_league_participation.team", "sports.sports_league_participation.league"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what league are the indianapolis colts in [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_team.division </PATH>", "<PATH> sports.sports_team.league </PATH>", "<PATH> sports.sports_team.sport </PATH>", "<PATH> american_football.football_team.league </PATH>", "<PATH> american_football.football_team.current_division </PATH>"], "scores": [-0.017268776893615723, -0.03795977309346199, -0.04366207495331764, -0.04689434915781021, -0.0486019141972065], "norm_scores": [0.2043554037809372, 0.20017053186893463, 0.19903235137462616, 0.19839005172252655, 0.1980515867471695]}}
{"id": "WebQTest-419", "question": "what country did magellan represent", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.people_born_here", "location.location.containedby"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did magellan represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.0033430338371545076, -0.04575832188129425, -0.07607956975698471, -0.07783519476652145, -0.0779302641749382], "norm_scores": [0.21076332032680511, 0.20201067626476288, 0.19597738981246948, 0.19563362002372742, 0.1956150382757187]}}
{"id": "WebQTest-420", "question": "where is perpignan located", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is perpignan located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.0075181773863732815, -0.026233816519379616, -0.05821037292480469, -0.0704890638589859, -0.07817396521568298], "norm_scores": [0.20821261405944824, 0.204352006316185, 0.19792090356349945, 0.1955055594444275, 0.19400887191295624]}}
{"id": "WebQTest-421", "question": "who did kim richards marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kim richards marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.017161360010504723, -0.026137664914131165, -0.046648621559143066, -0.04820158705115318, -0.05111229792237282], "norm_scores": [0.2041623443365097, 0.20233793556690216, 0.1982300579547882, 0.1979224532842636, 0.19734719395637512]}}
{"id": "WebQTest-422", "question": "what happened in bosnia in the 90s", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["time.event.included_in_event"], ["book.written_work.subjects"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened in bosnia in the 90s [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.006306516472250223, -0.035201627761125565, -0.05279136821627617, -0.05679504573345184, -0.07868986576795578], "norm_scores": [0.2080283761024475, 0.202103391289711, 0.19857953488826752, 0.19778606295585632, 0.19350264966487885]}}
{"id": "WebQTest-423", "question": "when 's the last time the steelers won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the steelers won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.01019758265465498, -0.013699919916689396, -0.05270850658416748, -0.07654055953025818, -0.08405330777168274], "norm_scores": [0.20749042928218842, 0.20676501095294952, 0.19885468482971191, 0.19417157769203186, 0.1927182972431183]}}
{"id": "WebQTest-425", "question": "where english is spoken", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["language.human_language.region"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where english is spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.region </PATH>"], "scores": [-0.013816448859870434, -0.024898352101445198, -0.02945907786488533, -0.07770641148090363, -0.15741810202598572], "norm_scores": [0.2093013972043991, 0.20699472725391388, 0.20605282485485077, 0.19634735584259033, 0.1813037246465683]}}
{"id": "WebQTest-426", "question": "where is giza pyramids located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["location.location.contains"], ["location.country.administrative_divisions"], ["architecture.structure.architect"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is giza pyramids located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> architecture.structure.architect </PATH>"], "scores": [-0.0024825758300721645, -0.046512749046087265, -0.07977128028869629, -0.11419744789600372, -0.13527101278305054], "norm_scores": [0.21493832767009735, 0.20567989349365234, 0.19895176589488983, 0.19221919775009155, 0.18821083009243011]}}
{"id": "WebQTest-427", "question": "what is the zip code for midland tx", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.citytown.postal_ codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for midland tx [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.citytown.postal_ codes </PATH>"], "scores": [-7.620143151143566e-05, -0.10958002507686615, -0.1500234156847, -0.17798812687397003, -0.18047446012496948], "norm_scores": [0.22578531503677368, 0.20236657559871674, 0.19434547424316406, 0.18898595869541168, 0.18851664662361145]}}
{"id": "WebQTest-428", "question": "what states make up the midwest us", "prediction": [["location.location.contains"], ["location.location.containedby"], ["location.location.partially_containedby"], ["base.locations.countries.states_provinces_within"], ["location.location.partially_contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states make up the midwest us [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.011610331945121288, -0.01597100868821144, -0.12944526970386505, -0.12974479794502258, -0.1396360546350479], "norm_scores": [0.2149197906255722, 0.2139846235513687, 0.191029891371727, 0.1909726858139038, 0.18909302353858948]}}
{"id": "WebQTest-430", "question": "what instruments does justin bieber use", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_member.instruments_played"], ["music.recording_contribution.contributor", "music.track_contribution.role"], ["music.instrument.family"]], "ground_paths": [["music.group_member.membership", "music.group_membership.role"], ["music.artist.track_contributions", "music.track_contribution.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments does justin bieber use [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.recording_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.00459415465593338, -0.03857265040278435, -0.050995174795389175, -0.06481349468231201, -0.07563762366771698], "norm_scores": [0.2085840255022049, 0.20161572098731995, 0.19912663102149963, 0.1963939666748047, 0.19427962601184845]}}
{"id": "WebQTest-431", "question": "what language does australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.007716787979006767, -0.022499671205878258, -0.06179000809788704, -0.10131245106458664, -0.1612386703491211], "norm_scores": [0.21271973848342896, 0.20959824323654175, 0.20152276754379272, 0.19371342658996582, 0.18244589865207672]}}
{"id": "WebQTest-432", "question": "who did annie oakley married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did annie oakley married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.025705428794026375, -0.037398725748062134, -0.03869261592626572, -0.04017138108611107, -0.04350379481911659], "norm_scores": [0.20228712260723114, 0.19993548095226288, 0.19967696070671082, 0.19938190281391144, 0.1987185776233673]}}
{"id": "WebQTest-433", "question": "who has played lex luthor", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has played lex luthor [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.019641229882836342, -0.02561630867421627, -0.028182296082377434, -0.03393322601914406, -0.04030420631170273], "norm_scores": [0.20198361575603485, 0.20078033208847046, 0.2002658098936081, 0.19911739230155945, 0.19785284996032715]}}
{"id": "WebQTest-434", "question": "where does the zambezi river start", "prediction": [["geography.river.origin"], ["location.location.containedby"], [], ["location.location.partial_containedby"], ["geography.river.source"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> </PATH>", "<PATH> location.location.partial_containedby </PATH>", "<PATH> geography.river.source </PATH>"], "scores": [-0.0003604378434829414, -0.10298454761505127, -0.11889660358428955, -0.12863466143608093, -0.13826721906661987], "norm_scores": [0.22019262611865997, 0.198716402053833, 0.1955794245004654, 0.19368410110473633, 0.19182740151882172]}}
{"id": "WebQTest-436", "question": "what is the nigeria time", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"], ["location.country.form_of_government"], ["common.topic.notable_types"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the nigeria time [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.00017717106675263494, -0.08355797082185745, -0.11949573457241058, -0.1481945961713791, -0.1588408350944519], "norm_scores": [0.2210812121629715, 0.20339487493038177, 0.19621510803699493, 0.19066399335861206, 0.1886449009180069]}}
{"id": "WebQTest-437", "question": "where did they find jenni rivera 's body", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_burial"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they find jenni rivera 's body [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>"], "scores": [-0.0005017282092012465, -0.08310280740261078, -0.08430811762809753, -0.09081939607858658, -0.09662993252277374], "norm_scores": [0.21448566019535065, 0.19748088717460632, 0.19724300503730774, 0.19596287608146667, 0.19482754170894623]}}
{"id": "WebQTest-438", "question": "what university did romney graduated from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["common.topic.notable_types"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.religion", "religion.religion.organizations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did romney graduated from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.01462901383638382, -0.021602807566523552, -0.023773498833179474, -0.04633331671357155, -0.0663134753704071], "norm_scores": [0.20398300886154175, 0.20256541669368744, 0.20212619006633759, 0.1976173222064972, 0.19370807707309723]}}
{"id": "WebQTest-439", "question": "where is the time zone line in south dakota", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.time_zones"], ["location.location.partially_contains"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the time zone line in south dakota [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.time_zones </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.0019067891407757998, -0.046848297119140625, -0.08014123141765594, -0.09396928548812866, -0.11068505793809891], "norm_scores": [0.213229700922966, 0.20385898649692535, 0.19718365371227264, 0.19447575509548187, 0.1912519484758377]}}
{"id": "WebQTest-440", "question": "where is jason mraz from", "prediction": [["music.artist.origin"], ["location.location.people_born_here"], ["people.person.place_of_birth"], ["music.recording.artist"], ["music.album.artist"]], "ground_paths": [["people.person.place_of_birth"], ["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jason mraz from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.0047753178514540195, -0.0393376499414444, -0.051644135266542435, -0.09288852661848068, -0.11258580535650253], "norm_scores": [0.21125143766403198, 0.20407482981681824, 0.2015787959098816, 0.19343391060829163, 0.18966105580329895]}}
{"id": "WebQTest-441", "question": "what killed john bonham", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["music.group_member.instruments_played", "people.deceased_person.cause_of_death"], ["music.group_member.instruments_played", "music.group_member.instruments_played"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what killed john bonham [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> music.group_member.instruments_played <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.001796418335288763, -0.04418959468603134, -0.09408016502857208, -0.12729725241661072, -0.12944632768630981], "norm_scores": [0.21586355566978455, 0.20690368115901947, 0.19683440029621124, 0.1904035210609436, 0.18999478220939636]}}
{"id": "WebQTest-442", "question": "what industry does walmart operate in", "prediction": [["business.business_operation.industry"], ["business.industry.companies"], ["business.business_operation.industry", "business.business_operation.industry"], ["business.business_operation.industry", "business.industry.parent_industry"], ["organization.organization.industry"]], "ground_paths": [["organization.organization.sectors"], ["business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what industry does walmart operate in [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> business.business_operation.industry <SEP> business.industry.parent_industry </PATH>", "<PATH> organization.organization.industry </PATH>"], "scores": [-0.005606419872492552, -0.04839202016592026, -0.05138832703232765, -0.06964811682701111, -0.07422736287117004], "norm_scores": [0.208985835313797, 0.2002328485250473, 0.19963379204273224, 0.19602158665657043, 0.195126011967659]}}
{"id": "WebQTest-443", "question": "what county is texarkana arkansas in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.hud_county_place.county"], ["location.location.contains"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is texarkana arkansas in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.00824558176100254, -0.0211982112377882, -0.05777423083782196, -0.07286360114812851, -0.09018094092607498], "norm_scores": [0.20843905210494995, 0.20575661957263947, 0.19836683571338654, 0.1953960657119751, 0.19204145669937134]}}
{"id": "WebQTest-446", "question": "what music period did beethoven live in", "prediction": [["music.artist.genre"], ["music.artist.period"], ["music.artist.periods"], ["influence.influence_node.influenced_by"], ["music.genre.artists"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music period did beethoven live in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.artist.period </PATH>", "<PATH> music.artist.periods </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.0016785553889349103, -0.068384550511837, -0.09009917825460434, -0.09108048677444458, -0.0955236479640007], "norm_scores": [0.21386952698230743, 0.20006856322288513, 0.1957709789276123, 0.19557896256446838, 0.19471189379692078]}}
{"id": "WebQTest-447", "question": "what did laura ingalls wilder wrote", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"]], "ground_paths": [["book.book_edition.author_editor"], ["book.literary_series.author_s"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did laura ingalls wilder wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.01067851297557354, -0.025498682633042336, -0.036598097532987595, -0.061732202768325806, -0.0843404158949852], "norm_scores": [0.20665785670280457, 0.20361773669719696, 0.20137019455432892, 0.19637201726436615, 0.1919821947813034]}}
{"id": "WebQTest-448", "question": "what type of government system does italy have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government system does italy have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> location.country.form_of_government <PATH>"], "scores": [-0.009394945576786995, -0.014824895188212395, -0.14406083524227142, -0.17632202804088593, -0.18245097994804382], "norm_scores": [0.21949346363544464, 0.218304842710495, 0.19183902442455292, 0.18574883043766022, 0.18461385369300842]}}
{"id": "WebQTest-449", "question": "what did john irving wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"]], "ground_paths": [["book.author.book_editions_published"], ["film.film.story_by"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["film.film_story_contributor.film_story_credits"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john irving wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>"], "scores": [-0.015159125439822674, -0.02559962309896946, -0.029647458344697952, -0.06562556326389313, -0.07578104734420776], "norm_scores": [0.2054571658372879, 0.20332324504852295, 0.20250187814235687, 0.19534577429294586, 0.1933719664812088]}}
{"id": "WebQTest-450", "question": "what is the national flower of hawaii", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.symbol_of"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national flower of hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.symbol_of </PATH>"], "scores": [-0.010314341634511948, -0.019708190113306046, -0.022653326392173767, -0.030240654945373535, -0.050750553607940674], "norm_scores": [0.20329217612743378, 0.20139142870903015, 0.20079918205738068, 0.19928140938282013, 0.19523581862449646]}}
{"id": "WebQTest-451", "question": "what did alice paul accomplish", "prediction": [["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"], ["common.topic.notable_types"], ["people.person.profession"], ["base.activism.activists.area_of_activism"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did alice paul accomplish [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> base.activism.activists.area_of_activism </PATH>"], "scores": [-0.005658865440636873, -0.02732716128230095, -0.07248872518539429, -0.07552740722894669, -0.11400367319583893], "norm_scores": [0.2108035385608673, 0.20628492534160614, 0.197175994515419, 0.1965777426958084, 0.18915782868862152]}}
{"id": "WebQTest-453", "question": "what languages do they speak in costa rica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["book.newspaper.circulation_areas", "book.periodical.language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do they speak in costa rica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.language </PATH>"], "scores": [-0.0043857810087502, -0.028140872716903687, -0.08879602700471878, -0.09145145118236542, -0.1657349169254303], "norm_scores": [0.21444544196128845, 0.20941129326820374, 0.19708697497844696, 0.19656430184841156, 0.1824919879436493]}}
{"id": "WebQTest-454", "question": "where did hank marvin come from", "prediction": [["people.person.place_of_birth"], ["music.artist.origin"], ["location.location.people_born_here"], ["music.recording.artist"], ["music.recording_artist.origin"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hank marvin come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.recording_artist.origin </PATH>"], "scores": [-0.011307214386761189, -0.025042129680514336, -0.05200710892677307, -0.05767957866191864, -0.07097113132476807], "norm_scores": [0.2064732015132904, 0.20365670323371887, 0.19823847711086273, 0.1971171498298645, 0.19451449811458588]}}
{"id": "WebQTest-455", "question": "when did chipper jones get drafted", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_league_draft.picks", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did chipper jones get drafted [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_league_draft.picks <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.021784670650959015, -0.02283104695379734, -0.022920966148376465, -0.02484882064163685, -0.03835335746407509], "norm_scores": [0.20087070763111115, 0.20066063106060028, 0.20064258575439453, 0.20025615394115448, 0.19756995141506195]}}
{"id": "WebQTest-456", "question": "what part did winona ryder play in star trek", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did winona ryder play in star trek [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.02024732530117035, -0.028231101110577583, -0.02942405827343464, -0.030093559995293617, -0.03837333247065544], "norm_scores": [0.20181013643741608, 0.20020534098148346, 0.1999666392803192, 0.19983282685279846, 0.19818507134914398]}}
{"id": "WebQTest-459", "question": "where did jovan belcher kill himself", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["people.person.nationality", "location.location.containedby"], ["location.location.people_born_here", "location.location.containedby"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jovan belcher kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.001923249219544232, -0.06905023008584976, -0.07702500373125076, -0.0806182399392128, -0.08162714540958405], "norm_scores": [0.2122945338487625, 0.19851161539554596, 0.19693483412265778, 0.19622847437858582, 0.1960306018590927]}}
{"id": "WebQTest-461", "question": "where did clay matthews go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did clay matthews go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.012558059766888618, -0.014718763530254364, -0.03428371250629425, -0.04877515509724617, -0.05223814398050308], "norm_scores": [0.2040034532546997, 0.2035631239414215, 0.1996191293001175, 0.19674724340438843, 0.19606706500053406]}}
{"id": "WebQTest-462", "question": "what time is it in texas houston right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time is it in texas houston right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-1.6334017345798202e-05, -0.1303078979253769, -0.15826697647571564, -0.17545494437217712, -0.17806382477283478], "norm_scores": [0.22688768804073334, 0.19917096197605133, 0.19367945194244385, 0.19037894904613495, 0.18988291919231415]}}
{"id": "WebQTest-463", "question": "who is princess leia in star wars", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is princess leia in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.0117053696885705, -0.029006587341427803, -0.03581488877534866, -0.04820682853460312, -0.04911474138498306], "norm_scores": [0.20464691519737244, 0.2011367231607437, 0.1997719705104828, 0.19731169939041138, 0.1971326321363449]}}
{"id": "WebQTest-464", "question": "what is the political structure of china", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political structure of china [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.006934931501746178, -0.019585702568292618, -0.14421392977237701, -0.14532379806041718, -0.146977961063385], "norm_scores": [0.21742737293243408, 0.214694082736969, 0.18953730165958405, 0.18932707607746124, 0.18901415169239044]}}
{"id": "WebQTest-465", "question": "what is new york city airport", "prediction": [["aviation.airport.serves"], ["location.location.containedby"], ["location.location.nearby_airports"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"]], "ground_paths": [["aviation.airport.serves"], ["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is new york city airport [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>"], "scores": [-0.007171929813921452, -0.03135213255882263, -0.04631386324763298, -0.06521162390708923, -0.08246631175279617], "norm_scores": [0.20795176923274994, 0.2029837667942047, 0.19996938109397888, 0.19622591137886047, 0.1928691267967224]}}
{"id": "WebQTest-466", "question": "what language brazil speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language brazil speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "</PATH> location.country.languages_spoken </PATH>"], "scores": [-0.00019225635332986712, -0.10112757235765457, -0.11019641906023026, -0.1280522346496582, -0.19456924498081207], "norm_scores": [0.2220669835805893, 0.20074667036533356, 0.19893436133861542, 0.1954137533903122, 0.1828382909297943]}}
{"id": "WebQTest-467", "question": "who played obi wan in episode 2", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played obi wan in episode 2 [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.02744966372847557, -0.028871428221464157, -0.03309928625822067, -0.033957820385694504, -0.043535973876714706], "norm_scores": [0.20118696987628937, 0.20090113580226898, 0.20005354285240173, 0.19988186657428741, 0.1979764997959137]}}
{"id": "WebQTest-468", "question": "where did venus williams come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["people.ethnicity.people"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did venus williams come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.004189585335552692, -0.038166455924510956, -0.050415970385074615, -0.10543295741081238, -0.12316444516181946], "norm_scores": [0.2121807336807251, 0.20509260892868042, 0.20259565114974976, 0.19175051152706146, 0.18838045001029968]}}
{"id": "WebQTest-469", "question": "what currency is used in panama", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is used in panama [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.009434141218662262, -0.01821242831647396, -0.15088911354541779, -0.15474411845207214, -0.16958367824554443], "norm_scores": [0.2185254991054535, 0.21661560237407684, 0.18970072269439697, 0.18897083401679993, 0.186187282204628]}}
{"id": "WebQTest-470", "question": "where does the zambezi river originate", "prediction": [["geography.river.origin"], ["location.location.containedby"], ["location.location.partial_containedby"], [], ["geography.river.basin_countries"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river originate [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partial_containedby </PATH>", "<PATH> </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.0005505980225279927, -0.09372033923864365, -0.09408249706029892, -0.10855849087238312, -0.11981164664030075], "norm_scores": [0.21706363558769226, 0.1977533996105194, 0.19768179953098297, 0.19484075903892517, 0.19266048073768616]}}
{"id": "WebQTest-472", "question": "what team did ronaldo play for in 2003", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did ronaldo play for in 2003 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.01958954706788063, -0.02401766926050186, -0.02991030365228653, -0.03723787143826485, -0.03808720037341118], "norm_scores": [0.2020409256219864, 0.20114824175834656, 0.1999664306640625, 0.19850650429725647, 0.19833798706531525]}}
{"id": "WebQTest-473", "question": "where does robin williams live 2011", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.children", "people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["film.producer.film", "film.film.featured_film_locations"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.parents", "people.person.place_of_birth"], ["people.person.nationality", "location.administrative_division.second_level_division_of"], ["people.place_lived.person", "people.place_lived.location"], ["film.film.produced_by", "film.film_location.featured_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robin williams live 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> film.producer.film <SEP> film.film.featured_film_locations </PATH>"], "scores": [-0.014711950905621052, -0.022941607981920242, -0.03020835481584072, -0.0338013656437397, -0.04719499871134758], "norm_scores": [0.20302270352840424, 0.20135875046253204, 0.199900820851326, 0.1991838663816452, 0.19653385877609253]}}
{"id": "WebQTest-474", "question": "who played jacob black in twilight", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["film.film.country", "people.person.nationality"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jacob black in twilight [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.029671642929315567, -0.03096683695912361, -0.03216841444373131, -0.03252831846475601, -0.03488217666745186], "norm_scores": [0.20047461986541748, 0.200215145945549, 0.19997470080852509, 0.1999027580022812, 0.19943276047706604]}}
{"id": "WebQTest-477", "question": "which airport to fly into in buenos aires", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"]], "ground_paths": [["aviation.airport.serves"], ["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into in buenos aires [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>"], "scores": [-0.00736019155010581, -0.026152949780225754, -0.04572700709104538, -0.057932883501052856, -0.07156040519475937], "norm_scores": [0.20694315433502197, 0.20309044420719147, 0.19915379583835602, 0.19673772156238556, 0.1940748691558838]}}
{"id": "WebQTest-479", "question": "what team did adrian peterson play for in college", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did adrian peterson play for in college [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.010009216144680977, -0.01707506738603115, -0.03866366669535637, -0.04968617856502533, -0.05402949079871178], "norm_scores": [0.20480279624462128, 0.20336078107357025, 0.19901758432388306, 0.1968359500169754, 0.19598287343978882]}}
{"id": "WebQTest-480", "question": "where does name pennsylvania come from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes", "symbols.namesake.named_after"], ["symbols.namesake.named_after", "symbols.namesake.named_after"], ["location.location.containedby"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does name pennsylvania come from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes <SEP> symbols.namesake.named_after </PATH>", "<PATH> symbols.namesake.named_after <SEP> symbols.namesake.named_after </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.004501413553953171, -0.031122280284762383, -0.09433285892009735, -0.10040910542011261, -0.11224302649497986], "norm_scores": [0.21302780508995056, 0.2074316442012787, 0.19472555816173553, 0.19354595243930817, 0.19126905500888824]}}
{"id": "WebQTest-481", "question": "what kind of money do they use in germany", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do they use in germany [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.005205092485994101, -0.025630246847867966, -0.1127271056175232, -0.13511674106121063, -0.16380873322486877], "norm_scores": [0.21695050597190857, 0.21256418526172638, 0.1948338747024536, 0.19052007794380188, 0.18513134121894836]}}
{"id": "WebQTest-482", "question": "who plays blaine in batman", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays blaine in batman [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.014851379208266735, -0.025449391454458237, -0.03321786969900131, -0.03672505542635918, -0.042558182030916214], "norm_scores": [0.2031572312116623, 0.20101551711559296, 0.1994599997997284, 0.19876167178153992, 0.19760563969612122]}}
{"id": "WebQTest-484", "question": "what language does australia use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> country.languages_spoken </PATH>"], "scores": [-0.008339093998074532, -0.021111857146024704, -0.0648028776049614, -0.09214360266923904, -0.1774890273809433], "norm_scores": [0.21292893588542938, 0.21022653579711914, 0.20123930275440216, 0.19581179320812225, 0.17979341745376587]}}
{"id": "WebQTest-487", "question": "who did veronica lake mary", "prediction": [["people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did veronica lake mary [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.02814490720629692, -0.03045404516160488, -0.03815510496497154, -0.04609622061252594, -0.04711946099996567], "norm_scores": [0.20197343826293945, 0.2015075981616974, 0.19996172189712524, 0.19838008284568787, 0.19817721843719482]}}
{"id": "WebQTest-488", "question": "who did sir francis drake marry", "prediction": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sir francis drake marry [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.0262640118598938, -0.029092008247971535, -0.0373387411236763, -0.04012846574187279, -0.040284764021635056], "norm_scores": [0.20167510211467743, 0.20110557973384857, 0.1994539201259613, 0.19889827072620392, 0.19886720180511475]}}
{"id": "WebQTest-489", "question": "who plays stephanie plum in one for the money", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["film.film.executive_produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stephanie plum in one for the money [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.02274298295378685, -0.03398518264293671, -0.03583129122853279, -0.042330868542194366, -0.04566100984811783], "norm_scores": [0.20268504321575165, 0.20041917264461517, 0.2000495195388794, 0.1987535059452057, 0.1980927288532257]}}
{"id": "WebQTest-490", "question": "what did the ancient romans speak", "prediction": [["fictional_universe.fictional_setting.languages"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the ancient romans speak [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.012728127650916576, -0.031135816127061844, -0.041017815470695496, -0.04258979856967926, -0.0511428602039814], "norm_scores": [0.20463451743125916, 0.20090211927890778, 0.19892658293247223, 0.19861412048339844, 0.19692261517047882]}}
{"id": "WebQTest-491", "question": "what does ringo sing", "prediction": [["music.recording.artist"], ["music.artist.track"], ["music.album.artist"], ["music.composition.composer"], ["music.producer.releases_produced"]], "ground_paths": [["music.featured_artist.recordings"], ["theater.theatrical_composer.plays_composed", "film.film.sequel"], ["music.artist.label", "music.record_label.releases"], ["music.artist.album"], ["music.recording.artist"], ["music.album.artist"], ["music.album.artist", "music.album.compositions"], ["music.lyricist.lyrics_written"], ["film.performance.actor", "film.performance.film"], ["music.composer.compositions"], ["music.composition.composer"], ["organization.organization.founders", "film.film.production_companies"], ["music.recording.featured_artists"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does ringo sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.producer.releases_produced </PATH>"], "scores": [-0.0202651210129261, -0.023369768634438515, -0.05969482660293579, -0.06039649620652199, -0.06776395440101624], "norm_scores": [0.2052329182624817, 0.20459672808647156, 0.1972980946302414, 0.19715972244739532, 0.19571247696876526]}}
{"id": "WebQTest-492", "question": "where was st. lucy born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["people.deceased_person.place_of_death"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st. lucy born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0060990964993834496, -0.02543504163622856, -0.08476938307285309, -0.09134933352470398, -0.12135900557041168], "norm_scores": [0.21210655570030212, 0.20804466307163239, 0.19605955481529236, 0.19477371871471405, 0.1890154629945755]}}
{"id": "WebQTest-493", "question": "what type of government does australia have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["type.type_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does australia have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> type.type_of_government.countries </PATH>"], "scores": [-0.007043719291687012, -0.019130731001496315, -0.13443610072135925, -0.1397608071565628, -0.15365749597549438], "norm_scores": [0.21702735126018524, 0.21441993117332458, 0.19106830656528473, 0.19005364179611206, 0.18743078410625458]}}
{"id": "WebQTest-497", "question": "who was the father of king george vi", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["royalty.monarch.kingdom"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the father of king george vi [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> people.person.sibling_s <SEP> people.sibling_relationship.sibling </PATH>", "<PATH> royalty.monarch.kingdom </PATH>"], "scores": [-0.012016288004815578, -0.01868703030049801, -0.05210340395569801, -0.11184055358171463, -0.11824645847082138], "norm_scores": [0.21016035974025726, 0.20876310765743256, 0.20190227031707764, 0.19019438326358795, 0.18897990882396698]}}
{"id": "WebQTest-500", "question": "what is the zip code for trenton", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["base.biblioness.bibs_location.country", "location.location.containedby"], ["location.location.contains"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for trenton [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-9.658286580815911e-05, -0.10414434224367142, -0.15739350020885468, -0.17004792392253876, -0.1724785715341568], "norm_scores": [0.2251747101545334, 0.2029234617948532, 0.19240060448646545, 0.18998120725154877, 0.18952001631259918]}}
{"id": "WebQTest-501", "question": "what has ashley greene been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ashley greene been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.017301149666309357, -0.03218914195895195, -0.03568696230649948, -0.0422077402472496, -0.046730801463127136], "norm_scores": [0.2035248726606369, 0.20051723718643188, 0.19981709122657776, 0.19851835072040558, 0.19762247800827026]}}
{"id": "WebQTest-502", "question": "where did al qaeda attack", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"], ["organization.organization_founder.organizations_founded", "base.terrorism.terrorist_attack.organizations_involved"], ["organization.organization.founders", "base.terrorism.terrorist_attack.organizations_involved"]], "ground_paths": [["base.terrorism.terrorist_organization.involved_in_attacks"], ["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.disaster2.attack_process.attacker", "base.disaster2.attack.attack"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.conflict"], ["film.film.subjects", "film.film.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did al qaeda attack [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.terrorism.terrorist_attack.organizations_involved </PATH>", "<PATH> organization.organization.founders <SEP> base.terrorism.terrorist_attack.organizations_involved </PATH>"], "scores": [-0.01889437437057495, -0.031901028007268906, -0.034806326031684875, -0.03890005871653557, -0.04250096157193184], "norm_scores": [0.2029157429933548, 0.20029357075691223, 0.19971249997615814, 0.1988966017961502, 0.19818168878555298]}}
{"id": "WebQTest-503", "question": "what language is spoken in switzerland", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["book.newspaper.circulation_areas"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>"], "scores": [-0.01740318536758423, -0.02120204269886017, -0.0232439786195755, -0.1107882484793663, -0.15062858164310455], "norm_scores": [0.20935854315757751, 0.20856471359729767, 0.20813927054405212, 0.1906926929950714, 0.18324477970600128]}}
{"id": "WebQTest-504", "question": "who was philip in acts chapter 8", "prediction": [["people.person.gender", "people.person.gender"], ["people.person.profession"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.profession", "people.person.profession"], ["film.film_subject.films", "film.film_character.portrayed_in_films"]], "ground_paths": [["fictional_universe.person_in_fiction.representations_in_fiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was philip in acts chapter 8 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> film.film_subject.films <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.014061108231544495, -0.01715291291475296, -0.05680626630783081, -0.06609523296356201, -0.07178319245576859], "norm_scores": [0.2062588334083557, 0.2056221067905426, 0.19762803614139557, 0.19580078125, 0.19469024240970612]}}
{"id": "WebQTest-506", "question": "what the time zone in england", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in england [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.0012710971059277654, -0.04616161063313484, -0.1482839435338974, -0.14921247959136963, -0.16306009888648987], "norm_scores": [0.22062940895557404, 0.210944265127182, 0.19046558439731598, 0.19028881192207336, 0.18767192959785461]}}
{"id": "WebQTest-508", "question": "what is the best currency to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_formerly_used"], ["location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the best currency to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.01689750701189041, -0.017363253980875015, -0.037451986223459244, -0.037508513778448105, -0.10929122567176819], "norm_scores": [0.2053162157535553, 0.20522060990333557, 0.20113912224769592, 0.20112775266170502, 0.1871962547302246]}}
{"id": "WebQTest-509", "question": "what system of government is practiced in nigeria", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government is practiced in nigeria [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005433047655969858, -0.02133578434586525, -0.13288237154483795, -0.13668295741081238, -0.13938134908676147], "norm_scores": [0.21662864089012146, 0.21321091055870056, 0.19070643186569214, 0.18998301029205322, 0.1894710510969162]}}
{"id": "WebQTest-512", "question": "who were mary shelley", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were mary shelley [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.0018300903029739857, -0.0702725350856781, -0.07435807585716248, -0.07808782905340195, -0.08146117627620697], "norm_scores": [0.2121376246213913, 0.1981041133403778, 0.1972964107990265, 0.19656191766262054, 0.19589996337890625]}}
{"id": "WebQTest-513", "question": "what language do most australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.0066271694377064705, -0.024096494540572166, -0.06364992260932922, -0.09329366683959961, -0.16351202130317688], "norm_scores": [0.21280939877033234, 0.20912404358386993, 0.2010139375925064, 0.19514256715774536, 0.18190999329090118]}}
{"id": "WebQTest-514", "question": "where is denmark situated", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["common.topic.notable_types"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark situated [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.01982206664979458, -0.0214396920055151, -0.03487630933523178, -0.0525851771235466, -0.09766653925180435], "norm_scores": [0.20507276058197021, 0.20474129915237427, 0.2020086646080017, 0.1984628140926361, 0.18971452116966248]}}
{"id": "WebQTest-515", "question": "where was teddy roosevelt 's house", "prediction": [["symbols.name_source.namesakes", "location.location.containedby"], ["symbols.namesake.named_after", "location.location.containedby"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.parents", "people.person.place_of_birth"]], "ground_paths": [["symbols.namesake.named_after", "transportation.bridge.locale"], ["base.usnris.nris_listing.significant_person", "travel.travel_destination.tourist_attractions"], ["people.person.parents", "people.person.place_of_birth"], ["book.written_work.author", "people.deceased_person.place_of_death"], ["people.person.parents", "people.deceased_person.place_of_death"], ["people.person.place_of_birth", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.children", "people.deceased_person.place_of_death"], ["symbols.namesake.named_after", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["symbols.name_source.namesakes", "location.location.containedby"], ["book.book_edition.author_editor", "book.book_edition.place_of_publication"], ["people.person.children", "people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["book.written_work.author", "base.biblioness.bibs_location.state"], ["book.written_work.subjects", "location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was teddy roosevelt 's house [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> base.inaugurations.inauguration.president <SEP> time.event.locations </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.023548495024442673, -0.028400178998708725, -0.0290023535490036, -0.03177909180521965, -0.037650786340236664], "norm_scores": [0.20130766928195953, 0.2003333568572998, 0.2002127468585968, 0.19965757429599762, 0.19848868250846863]}}
{"id": "WebQTest-516", "question": "what is the government system of malaysia", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of malaysia [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.010206039994955063, -0.013760547153651714, -0.11223489046096802, -0.11785885691642761, -0.1401592493057251], "norm_scores": [0.21387943625450134, 0.21312054991722107, 0.1931338757276535, 0.1920507550239563, 0.1878153532743454]}}
{"id": "WebQTest-517", "question": "what do people speak in canada", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in canada [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.01618785224854946, -0.01758774369955063, -0.038045961409807205, -0.05007767677307129, -0.1574224829673767], "norm_scores": [0.2078162133693695, 0.20752549171447754, 0.20332302153110504, 0.200891375541687, 0.18044382333755493]}}
{"id": "WebQTest-518", "question": "what type of money do they use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries", "location.country.currency_used"], ["government.form_of_government.countries"], ["government.form_of_government.countries", "government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money do they use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.006067155860364437, -0.022487439215183258, -0.11009721457958221, -0.11850953847169876, -0.13006480038166046], "norm_scores": [0.21450285613536835, 0.2110094130039215, 0.19330959022045135, 0.19169023633003235, 0.18948794901371002]}}
{"id": "WebQTest-519", "question": "what timezone is nashville tn", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is nashville tn [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-8.672210242366418e-06, -0.17920109629631042, -0.1894986480474472, -0.19016282260417938, -0.20961955189704895], "norm_scores": [0.23250094056129456, 0.19435802102088928, 0.19236688315868378, 0.19223915040493011, 0.1885349601507187]}}
{"id": "WebQTest-521", "question": "who was anakin skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was anakin skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.004132755100727081, -0.03978266566991806, -0.047814302146434784, -0.05315062031149864, -0.060904767364263535], "norm_scores": [0.20750288665294647, 0.20023572444915771, 0.19863395392894745, 0.19757682085037231, 0.19605068862438202]}}
{"id": "WebQTest-522", "question": "where did adolf hitler die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people"], ["deceased_person.place_of_death"], ["people.place_of_death.people", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did adolf hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> location.location.containedby </PATH>"], "scores": [-0.0005307121900841594, -0.08328993618488312, -0.10270023345947266, -0.10621742159128189, -0.1123991534113884], "norm_scores": [0.2165759652853012, 0.19937393069267273, 0.19554132223129272, 0.19485479593276978, 0.19365397095680237]}}
{"id": "WebQTest-523", "question": "what is the largest nation in europe", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"], ["base.locations.countries.continent"], ["location.location.partially_contains"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the largest nation in europe [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.01255700085312128, -0.02384815737605095, -0.04112718626856804, -0.057732511311769485, -0.06074182689189911], "norm_scores": [0.2053643763065338, 0.2030586153268814, 0.1995801031589508, 0.19629336893558502, 0.19570355117321014]}}
{"id": "WebQTest-524", "question": "where was the first gulf war fought", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first gulf war fought [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.0044335355050861835, -0.03233984857797623, -0.11500034481287003, -0.13838361203670502, -0.14859558641910553], "norm_scores": [0.21700507402420044, 0.21103297173976898, 0.1942903995513916, 0.189799964427948, 0.18787159025669098]}}
{"id": "WebQTest-525", "question": "who are the senators of virginia 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of virginia 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.016392966732382774, -0.02474438026547432, -0.02497805468738079, -0.026813413947820663, -0.03184125944972038], "norm_scores": [0.2017170637845993, 0.20003943145275116, 0.19999270141124725, 0.19962598383426666, 0.19862481951713562]}}
{"id": "WebQTest-526", "question": "what band was george clinton in", "prediction": [["music.artist.genre", "music.artist.genre"], ["music.artist.label", "music.artist.label"], ["music.recording.artist"], ["music.recording.artist", "music.recording.artist"], ["music.group_membership.member", "music.musical_group.member"]], "ground_paths": [["music.featured_artist.recordings", "music.recording.featured_artists"], ["music.recording.artist", "music.recording.artist"], ["music.recording.producer", "music.recording.artist"], ["music.producer.tracks_produced", "music.recording.artist"], ["music.group_member.membership", "music.musical_group.member"], ["music.composition.composer", "music.artist.track"], ["music.album.artist", "music.album.artist"], ["music.composer.compositions", "music.artist.track"], ["music.artist.genre", "music.artist.genre"], ["music.recording.featured_artists", "music.artist.album"], ["music.artist.album", "music.album.artist"], ["music.artist.origin", "music.artist.origin"], ["music.album.artist", "music.artist.album"], ["music.record_label.artist", "music.record_label.artist"], ["music.group_membership.member", "music.group_membership.group"], ["music.artist.album", "music.artist.album"], ["music.recording.featured_artists", "music.featured_artist.recordings"], ["music.featured_artist.recordings", "music.recording.artist"], ["music.producer.releases_produced", "music.album.artist"], ["base.saturdaynightlive.snl_episode.musical_guest", "base.saturdaynightlive.snl_musical_guest.episodes_as_musical_guest"], ["music.group_membership.member", "music.musical_group.member"], ["music.composition.composer", "music.recording.artist"], ["music.artist.label", "music.artist.label"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.recording.featured_artists", "music.recording.featured_artists"], ["music.recording.artist", "music.album.artist"], ["music.artist.genre", "music.genre.artists"], ["music.composer.compositions", "music.recording.artist"], ["broadcast.artist.content", "broadcast.artist.content"], ["music.recording.producer", "music.album.artist"], ["people.person.nationality", "music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what band was george clinton in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>", "<PATH> music.group_membership.member <SEP> music.musical_group.member </PATH>"], "scores": [-0.014958756975829601, -0.036394793540239334, -0.05178392678499222, -0.052032843232154846, -0.05873391032218933], "norm_scores": [0.2056170105934143, 0.20125629007816315, 0.1981828510761261, 0.19813354313373566, 0.1968102604150772]}}
{"id": "WebQTest-527", "question": "what country borders slovakia", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["finance.currency.countries_formerly_used", "location.country.currency_formerly_used"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "location.location.contains"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.country.currency_used", "location.country.currency_used"], ["location.location.partially_containedby", "location.location.containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_containedby", "geography.river.origin"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.datedlocationtest.dated_location_break_up.new_locations", "base.datedlocationtest.dated_location_break_up.new_locations"], ["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_contains", "location.location.partially_contains"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["common.topic.notable_types", "common.topic.notable_types"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["base.locations.countries.continent", "base.locations.continents.countries_within"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["government.form_of_government.countries", "government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country borders slovakia [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03653639554977417, -0.03941020742058754, -0.04109247401356697, -0.04195025935769081, -0.04348557069897652], "norm_scores": [0.20079271495342255, 0.20021650195121765, 0.1998799592256546, 0.19970859587192535, 0.19940219819545746]}}
{"id": "WebQTest-528", "question": "what was james garfield most known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["people.profession.people_with_this_profession"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was james garfield most known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.0011427730787545443, -0.059090472757816315, -0.08074171096086502, -0.08499801903963089, -0.086086705327034], "norm_scores": [0.21252566576004028, 0.2005603313446045, 0.1962646096944809, 0.19543102383613586, 0.19521836936473846]}}
{"id": "WebQTest-529", "question": "where did paula deen go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did paula deen go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.017919285222887993, -0.018347861245274544, -0.027538128197193146, -0.031453728675842285, -0.03893216326832771], "norm_scores": [0.20178532600402832, 0.20169885456562042, 0.19985368847846985, 0.19907265901565552, 0.1975894719362259]}}
{"id": "WebQTest-530", "question": "where does robert irvine come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robert irvine come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.004085126332938671, -0.037752941250801086, -0.04915745183825493, -0.10791640728712082, -0.12483525276184082], "norm_scores": [0.21229378879070282, 0.20526529848575592, 0.20293764770030975, 0.1913568079471588, 0.18814650177955627]}}
{"id": "WebQTest-531", "question": "what style of music does michael jackson perform", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.artist.album", "music.album.genre"], ["book.author.works_written"], ["music.album.genre"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does michael jackson perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.artist.album <SEP> music.album.genre </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.album.genre </PATH>"], "scores": [-0.0008914392092265189, -0.06025337427854538, -0.1315060555934906, -0.13311628997325897, -0.13981959223747253], "norm_scores": [0.21899299323558807, 0.2063714861869812, 0.19217859208583832, 0.19186939299106598, 0.19058753550052643]}}
{"id": "WebQTest-532", "question": "what does david beckham play", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_player_stats.position"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["book.written_work.subjects", "book.book_subject.works"], ["symbols.namesake.named_after", "cvg.computer_videogame.cvg_genre"], ["soccer.football_player.position_s", "sports.sport.positions"], ["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does david beckham play [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.position </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.027744324877858162, -0.030399585142731667, -0.03215867280960083, -0.042340394109487534, -0.043306879699230194], "norm_scores": [0.201490581035614, 0.20095627009868622, 0.20060308277606964, 0.19857096672058105, 0.19837914407253265]}}
{"id": "WebQTest-533", "question": "what does julian lennon do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["music.artist.genre"], ["music.artist.album"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does julian lennon do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> music.artist.album </PATH>"], "scores": [-0.0010327881900593638, -0.062428344041109085, -0.10140394419431686, -0.11672380566596985, -0.11971690505743027], "norm_scores": [0.21627271175384521, 0.20339392125606537, 0.19561903178691864, 0.19264502823352814, 0.19206926226615906]}}
{"id": "WebQTest-534", "question": "what is the milwaukee brewers mascot", "prediction": [["sports.mascot.team"], ["sports.sports_team.team_mascot"], ["sports.mascot.team", "sports.mascot.team"], ["sports.defunct_sports_team.later_known_as"], ["sports.sports_team.team_mascot", "sports.mascot.team"]], "ground_paths": [["sports.sports_team.team_mascot"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers mascot [/INST]", "raw_output": {"paths": ["<PATH> sports.mascot.team </PATH>", "<PATH> sports.sports_team.team_mascot </PATH>", "<PATH> sports.mascot.team <SEP> sports.mascot.team </PATH>", "<PATH> sports.defunct_sports_team.later_known_as </PATH>", "<PATH> sports.sports_team.team_mascot <SEP> sports.mascot.team </PATH>"], "scores": [-0.0011969668557867408, -0.045179858803749084, -0.14110182225704193, -0.1411227136850357, -0.1438860148191452], "norm_scores": [0.21916089951992035, 0.20973047614097595, 0.19054746627807617, 0.19054348766803741, 0.1900176852941513]}}
{"id": "WebQTest-535", "question": "what is the second biggest state in the united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.containedby", "location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"]], "ground_paths": [["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the second biggest state in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0016449119430035353, -0.057191651314496994, -0.07494068145751953, -0.07659144699573517, -0.0779336541891098], "norm_scores": [0.2114327996969223, 0.20000861585140228, 0.19648997485637665, 0.19616590440273285, 0.1959027796983719]}}
{"id": "WebQTest-537", "question": "what has charlie hunnam been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has charlie hunnam been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.022740310057997704, -0.026485087350010872, -0.031606607139110565, -0.03310656175017357, -0.03666456788778305], "norm_scores": [0.20147907733917236, 0.2007260024547577, 0.19970060884952545, 0.19940127432346344, 0.19869308173656464]}}
{"id": "WebQTest-538", "question": "what are all the movies taylor swift has been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["music.album.genre", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the movies taylor swift has been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.026879850775003433, -0.03409998118877411, -0.03790966421365738, -0.03883926942944527, -0.04085623100399971], "norm_scores": [0.2017727941274643, 0.20032121241092682, 0.1995595246553421, 0.19937407970428467, 0.19897237420082092]}}
{"id": "WebQTest-540", "question": "what college did brett favre", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did brett favre [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.016602223739027977, -0.019128507003188133, -0.02814619429409504, -0.03271602466702461, -0.04819381237030029], "norm_scores": [0.20247353613376617, 0.2019626796245575, 0.20014962553977966, 0.19923704862594604, 0.19617705047130585]}}
{"id": "WebQTest-542", "question": "where did alex chilton die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alex chilton die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.00029719818849116564, -0.0955628827214241, -0.10011664777994156, -0.11646890640258789, -0.12721915543079376], "norm_scores": [0.21809031069278717, 0.1982727348804474, 0.19737188518047333, 0.19417066872119904, 0.19209447503089905]}}
{"id": "WebQTest-543", "question": "what college did martin luther king jr go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["common.topic.notable_types"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did martin luther king jr go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0145037192851305, -0.01799558848142624, -0.027453718706965446, -0.03220071643590927, -0.06207532808184624], "norm_scores": [0.20326654613018036, 0.2025579959154129, 0.20065121352672577, 0.1997009813785553, 0.19382323324680328]}}
{"id": "WebQTest-545", "question": "what kind of cancer did farrah fawcett die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.people", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did farrah fawcett die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.people <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.008167289197444916, -0.01829400658607483, -0.04296322166919708, -0.06690508872270584, -0.07915995270013809], "norm_scores": [0.2070329189300537, 0.20494693517684937, 0.19995291531085968, 0.19522252678871155, 0.1928446888923645]}}
{"id": "WebQTest-546", "question": "who is michael j fox 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.021138271316885948, -0.033349599689245224, -0.03714971989393234, -0.04227438196539879, -0.04502704739570618], "norm_scores": [0.20294436812400818, 0.20048122107982635, 0.1997208148241043, 0.1986999213695526, 0.19815371930599213]}}
{"id": "WebQTest-547", "question": "what money do i use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries", "location.country.currency_used"], ["government.form_of_government.countries"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do i use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.00596523517742753, -0.02418171986937523, -0.11727543920278549, -0.12806181609630585, -0.12994270026683807], "norm_scores": [0.2152819186449051, 0.2113957405090332, 0.19260437786579132, 0.19053803384304047, 0.1901799887418747]}}
{"id": "WebQTest-548", "question": "what guitar did joe walsh play", "prediction": [["music.guitarist.guitars_played"], ["music.group_member.instruments_played"], ["music.guitar.guitarists"], ["music.instrument.family"], ["music.musical_group.member", "music.group_membership.group"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did joe walsh play [/INST]", "raw_output": {"paths": ["<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>"], "scores": [-0.01874419115483761, -0.020548000931739807, -0.032024674117565155, -0.04522547125816345, -0.07481049001216888], "norm_scores": [0.20390070974826813, 0.20353324711322784, 0.20121072232723236, 0.19857202470302582, 0.19278332591056824]}}
{"id": "WebQTest-549", "question": "what are the official languages of the eu", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "language.human_language.countries_spoken_in"], ["organization.organization.geographic_scope", "location.country.languages_spoken"], ["organization.organization.geographic_scope", "location.country.official_language"], ["organization.membership_organization.members", "organization.organization_membership.member"]], "ground_paths": [["organization.organization.founders", "location.country.official_language"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_language"], ["organization.organization_founder.organizations_founded", "location.country.official_language"], ["organization.organization.geographic_scope", "language.human_language.region"], ["organization.organization.founders", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.country.languages_spoken </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.country.official_language </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_membership.member </PATH>"], "scores": [-0.024259168654680252, -0.029852289706468582, -0.03762945532798767, -0.03846285119652748, -0.041073933243751526], "norm_scores": [0.20200535655021667, 0.20087867975234985, 0.19932247698307037, 0.19915641844272614, 0.19863708317279816]}}
{"id": "WebQTest-550", "question": "what is ryan seacrest radio show", "prediction": [["broadcast.content.producer"], ["media_common.netflix_title.netflix_genres"], ["internet.website.owner"], ["tv.tv_program.program_creator"], ["broadcast.producer.content", "broadcast.content.producer"]], "ground_paths": [["broadcast.artist.content"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ryan seacrest radio show [/INST]", "raw_output": {"paths": ["<PATH> broadcast.content.producer </PATH>", "<PATH> media_common.netflix_title.netflix_genres </PATH>", "<PATH> internet.website.owner </PATH>", "<PATH> tv.tv_program.program_creator </PATH>", "<PATH> broadcast.producer.content <SEP> broadcast.content.producer </PATH>"], "scores": [-0.035908255726099014, -0.03773411363363266, -0.054396748542785645, -0.05574573948979378, -0.05784299969673157], "norm_scores": [0.20248983800411224, 0.20212046802043915, 0.19878049194812775, 0.19851252436637878, 0.1980966329574585]}}
{"id": "WebQTest-552", "question": "what language do most italians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most italians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.016755053773522377, -0.020455578342080116, -0.027684789150953293, -0.053219761699438095, -0.12129219621419907], "norm_scores": [0.20616985857486725, 0.20540833473205566, 0.20392875373363495, 0.19878734648227692, 0.18570572137832642]}}
{"id": "WebQTest-554", "question": "what money do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.004435418173670769, -0.02984604425728321, -0.1385229527950287, -0.15131139755249023, -0.17168451845645905], "norm_scores": [0.21935534477233887, 0.21385160088539124, 0.19182920455932617, 0.189391627907753, 0.18557217717170715]}}
{"id": "WebQTest-555", "question": "what did drew barrymore go to rehab for", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.person.profession", "people.profession.specializations"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["people.person.profession", "people.profession.specializations_of"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did drew barrymore go to rehab for [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> base.popstra.celebrity.substance_abuse <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations_of </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.02159738726913929, -0.02557707019150257, -0.037569560110569, -0.04102388024330139, -0.047543905675411224], "norm_scores": [0.20262064039707184, 0.2018158882856369, 0.19941005110740662, 0.19872240722179413, 0.19743096828460693]}}
{"id": "WebQTest-556", "question": "when is saint george day celebrated", "prediction": [["time.recurring_event.current_frequency"], ["time.day_of_year.holidays"], ["time.holiday.featured_in_religions"], ["time.recurring_event.instances"], ["time.event.included_in_event"]], "ground_paths": [["time.holiday.day_of_year"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is saint george day celebrated [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.day_of_year.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.included_in_event </PATH>"], "scores": [-0.015456105582416058, -0.02616211026906967, -0.033998116850852966, -0.05943315848708153, -0.08140630275011063], "norm_scores": [0.20558644831180573, 0.20339718461036682, 0.2018095850944519, 0.19674129784107208, 0.1924654096364975]}}
{"id": "WebQTest-557", "question": "what government did the roman empire have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["fictional_universe.fictional_setting.languages", "location.country.form_of_government"], ["fictional_universe.fictional_setting.languages", "government.form_of_government.countries"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government did the roman empire have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> location.country.form_of_government </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> government.form_of_government.countries </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.005073242820799351, -0.024854538962244987, -0.08265268802642822, -0.0875122919678688, -0.09157954156398773], "norm_scores": [0.2108030766248703, 0.20667406916618347, 0.1950673609972, 0.19412170350551605, 0.19333377480506897]}}
{"id": "WebQTest-559", "question": "what instrument did robin gibb play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.group_membership.group", "music.group_membership.role"], ["music.musical_group.member", "music.group_membership.role"], ["music.instrument.instrumentalists"]], "ground_paths": [["music.group_member.instruments_played"], ["music.group_membership.member", "music.group_membership.role"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_membership.member", "music.performance_role.regular_performances"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument did robin gibb play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.instrument.instrumentalists </PATH>"], "scores": [-0.0008514324435964227, -0.07535865157842636, -0.0807698592543602, -0.08641138672828674, -0.0974477007985115], "norm_scores": [0.21379795670509338, 0.19844742119312286, 0.19737648963928223, 0.19626611471176147, 0.19411197304725647]}}
{"id": "WebQTest-560", "question": "who voiced darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_guest_role.special_performance_type", "tv.tv_actor.guest_roles"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_guest_role.special_performance_type <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.025655565783381462, -0.028687091544270515, -0.030642563477158546, -0.03674281761050224, -0.040847714990377426], "norm_scores": [0.20137356221675873, 0.2007640302181244, 0.20037180185317993, 0.19915322959423065, 0.1983373761177063]}}
{"id": "WebQTest-561", "question": "when was abe lincoln president", "prediction": [["government.us_president.vice_president"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "location.location.events"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "location.location.containedby"]], "ground_paths": [["base.inaugurations.inauguration.president"], ["base.inaugurations.inauguration_speaker.inauguration"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was abe lincoln president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> location.location.events </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> location.location.containedby </PATH>"], "scores": [-0.025836989283561707, -0.02747456170618534, -0.039922915399074554, -0.041183046996593475, -0.045139677822589874], "norm_scores": [0.20201896131038666, 0.20168842375278473, 0.19919328391551971, 0.19894243776798248, 0.19815684854984283]}}
{"id": "WebQTest-562", "question": "where are the texas rangers playing", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.professional_sports_team.owner_s", "sports.sports_team.arena_stadium"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the texas rangers playing [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.arena_stadium </PATH>"], "scores": [-0.0012129187816753983, -0.05321308597922325, -0.09054219722747803, -0.09662922471761703, -0.10065396875143051], "norm_scores": [0.21375693380832672, 0.20292559266090393, 0.19549021124839783, 0.19430387020111084, 0.19352342188358307]}}
{"id": "WebQTest-564", "question": "what is arkansas state capitol", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.state"], ["location.us_state.capital"], ["government.governmental_jurisdiction.government_bodies"], ["base.aareas.schema.administrative_area.capital"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is arkansas state capitol [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.us_state.capital </PATH>", "<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>"], "scores": [-0.020089633762836456, -0.024598967283964157, -0.027928104624152184, -0.04761674627661705, -0.04967989772558212], "norm_scores": [0.20278280973434448, 0.20187044143676758, 0.2011995017528534, 0.19727690517902374, 0.19687031209468842]}}
{"id": "WebQTest-567", "question": "what is the zip code for nogales az", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.city.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for nogales az [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.city.postal_codes </PATH>", "<PATH> location.citytown.postal_codes <PATH>"], "scores": [-0.0003468439681455493, -0.0815957635641098, -0.11174528300762177, -0.12825676798820496, -0.171298548579216], "norm_scores": [0.22029629349708557, 0.20310528576374054, 0.19707316160202026, 0.19384589791297913, 0.18567943572998047]}}
{"id": "WebQTest-568", "question": "when did michael vick start playing for the eagles", "prediction": [["american_football.football_player.rushing", "american_football.player_rushing_statistics.season"], ["american_football.football_player.games", "american_football.player_game_statistics.season"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.season"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.season"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael vick start playing for the eagles [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.season </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.season </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.season </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.season </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.020687388256192207, -0.02214031107723713, -0.027155539020895958, -0.028612103313207626, -0.034814562648534775], "norm_scores": [0.2011999785900116, 0.2009078562259674, 0.19990278780460358, 0.1996118128299713, 0.1983775794506073]}}
{"id": "WebQTest-570", "question": "what is william taft famous for", "prediction": [["people.person.profession"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is william taft famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.009738467633724213, -0.03906770050525665, -0.04042923450469971, -0.040718283504247665, -0.04429105296730995], "norm_scores": [0.20506910979747772, 0.19914191961288452, 0.19887098670005798, 0.1988135129213333, 0.19810445606708527]}}
{"id": "WebQTest-572", "question": "where did robin cook die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin cook die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.00032333674607798457, -0.10238930583000183, -0.11052867025136948, -0.11141927540302277, -0.11236291378736496], "norm_scores": [0.21798451244831085, 0.19683347642421722, 0.1952378749847412, 0.19506408274173737, 0.19488008320331573]}}
{"id": "WebQTest-573", "question": "where did melba beals live", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.nationality", "location.location.containedby"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth", "location.us_state.capital"], ["people.person.nationality", "location.country.first_level_divisions"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did melba beals live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.017055928707122803, -0.027192512527108192, -0.033670470118522644, -0.035868290811777115, -0.055144164711236954], "norm_scores": [0.20335830748081207, 0.20130735635757446, 0.20000751316547394, 0.19956840574741364, 0.1957584023475647]}}
{"id": "WebQTest-575", "question": "who is louisiana state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.locations.countries.states_provinces_within", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "symbols.name_source.namesakes"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.election.district", "government.election.winner"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["location.location.partially_containedby", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is louisiana state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.022576961666345596, -0.025845661759376526, -0.029522396624088287, -0.03289641812443733, -0.036764442920684814], "norm_scores": [0.20139114558696747, 0.20073392987251282, 0.19999724626541138, 0.19932357966899872, 0.19855408370494843]}}
{"id": "WebQTest-576", "question": "what are the 7 countries that are part of central america", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"], ["base.locations.countries.continent"], ["book.book_subject.works"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the 7 countries that are part of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.007674186956137419, -0.02796478196978569, -0.03492966294288635, -0.11036259680986404, -0.11831211298704147], "norm_scores": [0.21049542725086212, 0.20626738667488098, 0.20483574271202087, 0.1899527758359909, 0.1884487271308899]}}
{"id": "WebQTest-577", "question": "where did derek fisher go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"]], "ground_paths": [["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did derek fisher go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>"], "scores": [-0.01787276566028595, -0.02351413667201996, -0.024508798494935036, -0.03461761772632599, -0.04010916128754616], "norm_scores": [0.20205430686473846, 0.20091766119003296, 0.20071791112422943, 0.19869911670684814, 0.19761092960834503]}}
{"id": "WebQTest-578", "question": "who was kobe bryant drafted by", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kobe bryant drafted by [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>"], "scores": [-0.025139737874269485, -0.027254069223999977, -0.030584264546632767, -0.030910316854715347, -0.03426939249038696], "norm_scores": [0.2008993774652481, 0.2004750669002533, 0.1998085379600525, 0.19974340498447418, 0.19907358288764954]}}
{"id": "WebQTest-579", "question": "where are the new orleans hornets moving to", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.previously_known_as"], ["sports.sports_team.arena_stadium"], ["sports.defunct_sports_team.later_known_as"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the new orleans hornets moving to [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.defunct_sports_team.later_known_as </PATH>"], "scores": [-0.01110819447785616, -0.029867172241210938, -0.044308848679065704, -0.052166327834129333, -0.05668775737285614], "norm_scores": [0.2055930644273758, 0.2017722874879837, 0.19887930154800415, 0.19732274115085602, 0.19643257558345795]}}
{"id": "WebQTest-580", "question": "what language does algerian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does algerian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.011965890415012836, -0.01770024001598358, -0.05199150741100311, -0.054890528321266174, -0.12553910911083221], "norm_scores": [0.20808756351470947, 0.20689772069454193, 0.19992321729660034, 0.1993444710969925, 0.18574705719947815]}}
{"id": "WebQTest-581", "question": "where was anne boleyn executed", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["location.location.people_executed"], ["people.person.place_of_death"], ["location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was anne boleyn executed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> location.location.people_executed </PATH>", "<PATH> people.person.place_of_death </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0012099046725779772, -0.08056273311376572, -0.082650326192379, -0.08297640830278397, -0.08391611278057098], "norm_scores": [0.2133283168077469, 0.19705435633659363, 0.1966434121131897, 0.19657929241657257, 0.1963946372270584]}}
{"id": "WebQTest-582", "question": "what did galileo do to become famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["symbols.namesake.named_after"], ["law.invention.inventor"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did galileo do to become famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.001744600827805698, -0.058803629130125046, -0.0736311674118042, -0.08867243677377701, -0.10028889775276184], "norm_scores": [0.21285320818424225, 0.20104801654815674, 0.19808895885944366, 0.19513173401355743, 0.1928781270980835]}}
{"id": "WebQTest-583", "question": "what school did martin luther king jr received his doctorate degree from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did martin luther king jr received his doctorate degree from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>"], "scores": [-0.006772924680262804, -0.023592809215188026, -0.029862860217690468, -0.04286697879433632, -0.06719993054866791], "norm_scores": [0.20549039542675018, 0.20206297934055328, 0.20080000162124634, 0.19820567965507507, 0.19344095885753632]}}
{"id": "WebQTest-584", "question": "where buddha come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004570883233100176, -0.04442794620990753, -0.04640466347336769, -0.08161335438489914, -0.10172867774963379], "norm_scores": [0.21038374304771423, 0.20216336846351624, 0.20176415145397186, 0.19478389620780945, 0.1909048855304718]}}
{"id": "WebQTest-585", "question": "what did king henry viii rule", "prediction": [["royalty.kingdom.rulers"], ["royalty.monarch.kingdom"], ["people.person.nationality"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"]], "ground_paths": [["royalty.monarch.kingdom"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did king henry viii rule [/INST]", "raw_output": {"paths": ["<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>"], "scores": [-0.015338102355599403, -0.029677782207727432, -0.03045465238392353, -0.04833909869194031, -0.049678072333335876], "norm_scores": [0.20389272272586823, 0.20098982751369476, 0.2008337378501892, 0.1972738653421402, 0.1970098912715912]}}
{"id": "WebQTest-586", "question": "what type of cancer did larry hagman have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did larry hagman have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.people_with_this_condition </PATH>"], "scores": [-0.014753861352801323, -0.015092565678060055, -0.03239868953824043, -0.03895599767565727, -0.10499662160873413], "norm_scores": [0.20525580644607544, 0.20518630743026733, 0.20166586339473724, 0.20034781098365784, 0.18754416704177856]}}
{"id": "WebQTest-588", "question": "what wars did robert e lee served in", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_command.military_commander", "military.military_conflict.commanders"], ["military.military_person.participated_in_conflicts"], ["military.military_commander.military_commands", "military.military_conflict.commanders"]], "ground_paths": [["military.military_command.military_commander", "military.military_conflict.commanders"], ["people.person.nationality", "base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_command.military_conflict"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["military.military_conflict.military_personnel_involved"], ["military.military_person.participated_in_conflicts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wars did robert e lee served in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> military.military_command.military_commander <SEP> military.military_conflict.commanders </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> military.military_commander.military_commands <SEP> military.military_conflict.commanders </PATH>"], "scores": [-0.006373740267008543, -0.025788629427552223, -0.04680314287543297, -0.04785418510437012, -0.048437412828207016], "norm_scores": [0.20578983426094055, 0.20183299481868744, 0.19763582944869995, 0.19742821156978607, 0.1973131000995636]}}
{"id": "WebQTest-589", "question": "what russian language called", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.language_family"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what russian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014626062475144863, -0.025494946166872978, -0.026785410940647125, -0.0799882635474205, -0.10825954377651215], "norm_scores": [0.2072775810956955, 0.20503689348697662, 0.2047724723815918, 0.19416272640228271, 0.18875037133693695]}}
{"id": "WebQTest-590", "question": "who is in charge of libya now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in charge of libya now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.010183666832745075, -0.02411360666155815, -0.03649580478668213, -0.03741460293531418, -0.04104260727763176], "norm_scores": [0.20395898818969727, 0.20113755762577057, 0.1986623853445053, 0.19847992062568665, 0.1977611482143402]}}
{"id": "WebQTest-592", "question": "when did shays rebellion start", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["time.recurring_event.included_in_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shays rebellion start [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> time.recurring_event.included_in_event </PATH>"], "scores": [-0.014334374107420444, -0.020871762186288834, -0.03356211259961128, -0.07831403613090515, -0.08250797539949417], "norm_scores": [0.20633181929588318, 0.2049873322248459, 0.20240242779254913, 0.1935442090034485, 0.19273419678211212]}}
{"id": "WebQTest-593", "question": "what kind of disease does montel williams have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.person.medical_condition"], ["people.person.profession", "medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition", "medical_condition.notable_people_with_this_condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of disease does montel williams have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.medical_condition </PATH>", "<PATH> people.person.profession <SEP> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition <SEP> medical_condition.notable_people_with_this_condition </PATH>"], "scores": [-0.009445304982364178, -0.01300796214491129, -0.08284901827573776, -0.09109661728143692, -0.10964790731668472], "norm_scores": [0.21044139564037323, 0.20969299972057343, 0.19554753601551056, 0.1939413696527481, 0.1903766691684723]}}
{"id": "WebQTest-595", "question": "when is the last time the ravens won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the ravens won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.010314314626157284, -0.020992890000343323, -0.033217139542102814, -0.04560770466923714, -0.09206941723823547], "norm_scores": [0.20603451132774353, 0.2038460671901703, 0.20136937499046326, 0.19888968765735626, 0.18986031413078308]}}
{"id": "WebQTest-596", "question": "where did kansas city royals come from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.previously_known_as"], ["sports.sports_team.place_founded"], ["sports.defunct_sports_team.later_known_as"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kansas city royals come from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>", "<PATH> sports.sports_team.place_founded </PATH>", "<PATH> sports.defunct_sports_team.later_known_as </PATH>"], "scores": [-0.014986829832196236, -0.022130081430077553, -0.0363599918782711, -0.04433557018637657, -0.06130004674196243], "norm_scores": [0.2041834592819214, 0.20273011922836304, 0.19986571371555328, 0.19827800989151, 0.19494271278381348]}}
{"id": "WebQTest-597", "question": "what style of music does john mayer play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.musical_group.member", "music.group_membership.group"], ["music.album.genre"], ["music.artist.album", "music.artist.genre"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does john mayer play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>", "<PATH> music.album.genre </PATH>", "<PATH> music.artist.album <SEP> music.artist.genre </PATH>"], "scores": [-0.0006491029052995145, -0.06409560889005661, -0.1328515112400055, -0.14947070181369781, -0.15241014957427979], "norm_scores": [0.22047777473926544, 0.20692375302314758, 0.1931746006011963, 0.18999074399471283, 0.18943309783935547]}}
{"id": "WebQTest-599", "question": "what inventions did thomas edison invent list", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions did thomas edison invent list [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.008954684250056744, -0.022778408601880074, -0.05320647731423378, -0.055764276534318924, -0.08503513783216476], "norm_scores": [0.20729704201221466, 0.20445114374160767, 0.1983237862586975, 0.19781716167926788, 0.1921108067035675]}}
{"id": "WebQTest-600", "question": "where did djokovic come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.ethnicity.people"], ["people.person.ethnicity"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did djokovic come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.008758723735809326, -0.021345941349864006, -0.05577768385410309, -0.08221174031496048, -0.13102993369102478], "norm_scores": [0.21027621626853943, 0.20764601230621338, 0.20061808824539185, 0.1953844130039215, 0.18607518076896667]}}
{"id": "WebQTest-602", "question": "what part of the country is new england", "prediction": [["location.location.contains"], ["location.location.containedby"], ["base.locations.countries.states_provinces_within"], ["location.administrative_division.first_level_division_of"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.006834918167442083, -0.023381030187010765, -0.11913362145423889, -0.1344955712556839, -0.13802340626716614], "norm_scores": [0.21576964855194092, 0.21222887933254242, 0.1928500086069107, 0.18991009891033173, 0.18924130499362946]}}
{"id": "WebQTest-603", "question": "what money to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.013598671182990074, -0.023475613445043564, -0.036074090749025345, -0.04622350260615349, -0.10256580263376236], "norm_scores": [0.20615477859973907, 0.20412862300872803, 0.20157304406166077, 0.19953754544258118, 0.18860597908496857]}}
{"id": "WebQTest-604", "question": "what kind of music did jimi hendrix play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["book.written_work.author"], ["music.music_artist.genre"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did jimi hendrix play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> music.music_artist.genre </PATH>"], "scores": [-0.0008985883905552328, -0.05603453144431114, -0.14197400212287903, -0.14677877724170685, -0.17002664506435394], "norm_scores": [0.2210705727338791, 0.2092115730047226, 0.1919829547405243, 0.19106273353099823, 0.1866721659898758]}}
{"id": "WebQTest-605", "question": "what degrees did obama get in college", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did obama get in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.011282863095402718, -0.019634965807199478, -0.026914728805422783, -0.06821415573358536, -0.07998456805944443], "norm_scores": [0.20599713921546936, 0.20428378880023956, 0.20280206203460693, 0.1945970505475998, 0.19231998920440674]}}
{"id": "WebQTest-606", "question": "who were alexander graham bell 's siblings", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.sibling_s", "people.sibling_relationship.sibling"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were alexander graham bell 's siblings [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.sibling_s <SEP> people.sibling_relationship.sibling </PATH>"], "scores": [-0.02096458151936531, -0.02262273244559765, -0.0396696999669075, -0.04161812365055084, -0.04747805744409561], "norm_scores": [0.20270797610282898, 0.20237211883068085, 0.19895154237747192, 0.19856426119804382, 0.19740408658981323]}}
{"id": "WebQTest-608", "question": "when did liverpool fc last win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did liverpool fc last win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.004608887247741222, -0.029191847890615463, -0.05715794488787651, -0.057292431592941284, -0.06674863398075104], "norm_scores": [0.20777250826358795, 0.20272712409496307, 0.19713616371154785, 0.197109654545784, 0.19525453448295593]}}
{"id": "WebQTest-609", "question": "who played jeannie", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jeannie [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.01580922305583954, -0.03101033717393875, -0.032826829701662064, -0.03980206698179245, -0.046305883675813675], "norm_scores": [0.20348793268203735, 0.20041808485984802, 0.2000543475151062, 0.19866378605365753, 0.19737590849399567]}}
{"id": "WebQTest-611", "question": "where was st peter killed", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["location.location.people_born_here"], ["fictional_universe.fictional_character.place_of_death"], ["symbols.namesake.named_after"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st peter killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> fictional_universe.fictional_character.place_of_death </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0009065798949450254, -0.08080936968326569, -0.09222222119569778, -0.09619361907243729, -0.10317178070545197], "norm_scores": [0.21515381336212158, 0.19863131642341614, 0.1963772475719452, 0.19559891521930695, 0.19423873722553253]}}
{"id": "WebQTest-612", "question": "who does jeremy shockey play for in 2012", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy shockey play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.013665574602782726, -0.01744353026151657, -0.02698918990790844, -0.030494634062051773, -0.03530794754624367], "norm_scores": [0.20222873985767365, 0.2014661580324173, 0.19955219328403473, 0.19885389506816864, 0.19789904356002808]}}
{"id": "WebQTest-613", "question": "what college did deion sanders jr go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did deion sanders jr go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.019671279937028885, -0.023857956752181053, -0.023894796147942543, -0.028501024469733238, -0.04276450723409653], "norm_scores": [0.2016133815050125, 0.20077106356620789, 0.20076367259025574, 0.19984102249145508, 0.1970108449459076]}}
{"id": "WebQTest-614", "question": "when did the battle of alamo start", "prediction": [["time.event.locations"], ["time.event.included_in_event"], ["time.event.includes_event"], ["location.location.events"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the battle of alamo start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> location.location.events </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.011097058653831482, -0.02839501202106476, -0.044455450028181076, -0.06948739290237427, -0.0716349333524704], "norm_scores": [0.20684310793876648, 0.20329588651657104, 0.20005697011947632, 0.19511131942272186, 0.19469274580478668]}}
{"id": "WebQTest-615", "question": "what were the names of henry viii 's three children", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"], ["people.person.offspring"], ["people.deceased_person.place_of_burial"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were the names of henry viii 's three children [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> people.person.offspring </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>"], "scores": [-0.012268065474927425, -0.012885271571576595, -0.13048848509788513, -0.14766143262386322, -0.14878186583518982], "norm_scores": [0.2158125936985016, 0.21567943692207336, 0.19174954295158386, 0.18848474323749542, 0.18827368319034576]}}
{"id": "WebQTest-616", "question": "what was queen elizabeth ii childhood nickname", "prediction": [["people.person.children"], ["people.person.parents"], ["royalty.monarch.kingdom"], ["people.person.nationality", "people.person.nationality"], ["royalty.kingdom.rulers"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was queen elizabeth ii childhood nickname [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> royalty.kingdom.rulers </PATH>"], "scores": [-0.023877952247858047, -0.02952537313103676, -0.034054748713970184, -0.035757314413785934, -0.04993385821580887], "norm_scores": [0.20215436816215515, 0.20101594924926758, 0.20010749995708466, 0.1997670978307724, 0.19695506989955902]}}
{"id": "WebQTest-617", "question": "where did tim tebow grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tim tebow grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.010182751342654228, -0.019185001030564308, -0.06116876006126404, -0.07281732559204102, -0.0937279462814331], "norm_scores": [0.20831309258937836, 0.20644621551036835, 0.19795826077461243, 0.19566570222377777, 0.19161668419837952]}}
{"id": "WebQTest-618", "question": "where is chesapeake bay bridge", "prediction": [["transportation.bridge.locale"], ["transportation.bridge.body_of_water_spanned"], ["location.location.containedby"], ["geography.body_of_water.bridges"], ["location.location.contains"]], "ground_paths": [["transportation.bridge.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chesapeake bay bridge [/INST]", "raw_output": {"paths": ["<PATH> transportation.bridge.locale </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.body_of_water.bridges </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0009878771379590034, -0.06499006599187851, -0.08479547500610352, -0.0983010083436966, -0.11408641934394836], "norm_scores": [0.21468691527843475, 0.20137695968151093, 0.19742785394191742, 0.1947794109582901, 0.191728875041008]}}
{"id": "WebQTest-619", "question": "who played victoria barkley", "prediction": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["film.film.country", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played victoria barkley [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.02012753300368786, -0.021009810268878937, -0.023863887414336205, -0.03416480869054794, -0.04689258337020874], "norm_scores": [0.20181474089622498, 0.20163676142692566, 0.20106209814548492, 0.19900159537792206, 0.19648480415344238]}}
{"id": "WebQTest-620", "question": "where did kennedy 's inaugural address take place", "prediction": [["time.event.locations"], ["location.location.events"], ["event.speech_or_presentation.speaker_s", "event.speech_or_presentation.speech_topic"], ["architecture.architect.structures_designed"], ["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speech_venue"]], "ground_paths": [["symbols.name_source.namesakes", "people.person.place_of_birth"], ["common.image.appears_in_topic_gallery", "common.image.image_location"], ["people.person.nationality", "location.administrative_division.country"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["symbols.name_source.namesakes", "location.location.containedby"], ["base.inaugurations.inauguration.president", "location.location.events"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kennedy 's inaugural address take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.speech_or_presentation.speaker_s <SEP> event.speech_or_presentation.speech_topic </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speech_venue </PATH>"], "scores": [-0.0007094386382959783, -0.06875378638505936, -0.0985998585820198, -0.09883464872837067, -0.09934911131858826], "norm_scores": [0.21488869190216064, 0.2007530927658081, 0.19484993815422058, 0.19480420649051666, 0.19470399618148804]}}
{"id": "WebQTest-621", "question": "what countries have german as the official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.local_name.locale"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have german as the official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>"], "scores": [-0.015278760343790054, -0.020686563104391098, -0.024375149980187416, -0.1156565248966217, -0.12352188676595688], "norm_scores": [0.20887918770313263, 0.20775264501571655, 0.20698773860931396, 0.18893033266067505, 0.18745015561580658]}}
{"id": "WebQTest-622", "question": "where did the taliban began", "prediction": [["organization.organization.place_founded"], ["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["organization.organization.geographic_scope", "location.location.containedby"]], "ground_paths": [["organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the taliban began [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.location.containedby </PATH>"], "scores": [-0.019550230354070663, -0.02535616233944893, -0.03012220747768879, -0.05459060147404671, -0.05590939149260521], "norm_scores": [0.20351862907409668, 0.20234042406082153, 0.20137837529182434, 0.19651076197624207, 0.196251779794693]}}
{"id": "WebQTest-624", "question": "who plays elena gilbert on vampire diaries", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays elena gilbert on vampire diaries [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.025698712095618248, -0.031071797013282776, -0.03199319168925285, -0.035678476095199585, -0.03642938658595085], "norm_scores": [0.2012978345155716, 0.20021915435791016, 0.2000347524881363, 0.1992989331483841, 0.19914932548999786]}}
{"id": "WebQTest-626", "question": "what does the missouri river bisect", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"], ["geography.river.basin_countries"], ["base.locations.countries.states_provinces_within"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does the missouri river bisect [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>"], "scores": [-0.01115905586630106, -0.019582560285925865, -0.05756724253296852, -0.06312941759824753, -0.09298226237297058], "norm_scores": [0.20759567618370056, 0.20585434138774872, 0.1981816589832306, 0.19708240032196045, 0.1912858933210373]}}
{"id": "WebQTest-627", "question": "what nationality is taylor lautner jacob black", "prediction": [["people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.actor", "film.performance.character"], ["fictional_universe.fictional_character.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is taylor lautner jacob black [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> fictional_universe.fictional_character.nationality </PATH>"], "scores": [-0.020996125414967537, -0.02257697470486164, -0.04258868470788002, -0.04713013395667076, -0.04936471953988075], "norm_scores": [0.2031160444021225, 0.20279520750045776, 0.19877725839614868, 0.19787657260894775, 0.19743488729000092]}}
{"id": "WebQTest-628", "question": "what animal represents california", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what animal represents california [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>"], "scores": [-0.01908918097615242, -0.02707725018262863, -0.02862115204334259, -0.03190947696566582, -0.0322541743516922], "norm_scores": [0.20174551010131836, 0.20014037191867828, 0.19983160495758057, 0.19917558133602142, 0.19910693168640137]}}
{"id": "WebQTest-629", "question": "what company does nike own", "prediction": [["business.business_operation.industry"], ["business.business_operation.industry", "business.business_operation.industry"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_language"], ["business.industry.companies"]], "ground_paths": [["business.business_operation.industry", "business.business_operation.industry"], ["business.acquisition.acquiring_company", "business.acquisition.company_acquired"], ["business.industry.companies", "business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what company does nike own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_language </PATH>", "<PATH> business.industry.companies </PATH>"], "scores": [-0.029114561155438423, -0.029377246275544167, -0.03179539367556572, -0.04534435644745827, -0.046861130744218826], "norm_scores": [0.20147596299648285, 0.2014230489730835, 0.20093657076358795, 0.19823242723941803, 0.19793199002742767]}}
{"id": "WebQTest-630", "question": "what language do colombian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do colombian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.00883440114557743, -0.018166113644838333, -0.07130561769008636, -0.0894850343465805, -0.15944667160511017], "norm_scores": [0.21218588948249817, 0.2102150321006775, 0.19933593273162842, 0.1957448571920395, 0.18251831829547882]}}
{"id": "WebQTest-631", "question": "what year did ray allen join the nba", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["sports.sports_award.award_winner", "sports.sports_league_season.awards"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did ray allen join the nba [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_league_season.awards </PATH>"], "scores": [-0.018131937831640244, -0.022557901218533516, -0.023669064044952393, -0.029764235019683838, -0.03658860921859741], "norm_scores": [0.20160438120365143, 0.20071405172348022, 0.20049114525318146, 0.19927284121513367, 0.19791755080223083]}}
{"id": "WebQTest-633", "question": "what is claire danes famous for", "prediction": [["people.person.profession"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is claire danes famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0009771599434316158, -0.07521986216306686, -0.07554858177900314, -0.0898590162396431, -0.0951542928814888], "norm_scores": [0.2135993391275406, 0.19831551611423492, 0.19825033843517303, 0.19543348252773285, 0.194401353597641]}}
{"id": "WebQTest-634", "question": "what county is san antonio in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"]], "ground_paths": [["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san antonio in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00846080668270588, -0.026780415326356888, -0.03900749608874321, -0.08868658542633057, -0.0984785184264183], "norm_scores": [0.20882995426654816, 0.2050390988588333, 0.2025473266839981, 0.19273081421852112, 0.1908528208732605]}}
{"id": "WebQTest-635", "question": "what characters does matt stone do", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.fictional_character.character_created_by"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_no_value"], ["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["people.person.profession", "people.person.profession"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.nationality", "people.person.nationality"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"], ["people.person.ethnicity", "people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what characters does matt stone do [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.017038658261299133, -0.03639286011457443, -0.03832460194826126, -0.039617881178855896, -0.039890557527542114], "norm_scores": [0.20346492528915405, 0.1995648741722107, 0.1991797387599945, 0.1989223212003708, 0.19886808097362518]}}
{"id": "WebQTest-636", "question": "what timezone is the state of michigan and", "prediction": [["location.location.time_zones"], ["base.locations.states_and_provences.country", "location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is the state of michigan and [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> base.locations.states_and_provences.country <SEP> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-1.1961728887399659e-05, -0.14528825879096985, -0.15391381084918976, -0.18686091899871826, -0.18838141858577728], "norm_scores": [0.22830960154533386, 0.19743835926055908, 0.1957426369190216, 0.18939857184886932, 0.18911081552505493]}}
{"id": "WebQTest-637", "question": "what type of government did soviet union have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government did soviet union have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.007543237879872322, -0.017810173332691193, -0.14514105021953583, -0.14528928697109222, -0.1491348296403885], "norm_scores": [0.21736624836921692, 0.21514597535133362, 0.18942363560199738, 0.18939554691314697, 0.1886686235666275]}}
{"id": "WebQTest-638", "question": "what languages are there in japan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages are there in japan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.014650475233793259, -0.021773770451545715, -0.03197069093585014, -0.05254102498292923, -0.13489480316638947], "norm_scores": [0.20724302530288696, 0.20577199757099152, 0.20368443429470062, 0.19953738152980804, 0.18376317620277405]}}
{"id": "WebQTest-639", "question": "what year did the bulls get rodman", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_league_draft_pick.draft"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the bulls get rodman [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.01736373081803322, -0.03402882069349289, -0.0365666039288044, -0.04083602502942085, -0.04378589242696762], "norm_scores": [0.20345142483711243, 0.2000889629125595, 0.19958184659481049, 0.19873155653476715, 0.19814617931842804]}}
{"id": "WebQTest-640", "question": "who was saint paul the apostle", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was saint paul the apostle [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.003098001703619957, -0.050739679485559464, -0.06957270205020905, -0.08037117123603821, -0.1034969836473465], "norm_scores": [0.21189692616462708, 0.2020384967327118, 0.1982690989971161, 0.1961396336555481, 0.19165578484535217]}}
{"id": "WebQTest-642", "question": "where did vince scully go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did vince scully go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.014705558307468891, -0.020568357780575752, -0.02365010417997837, -0.03135964646935463, -0.05142208933830261], "norm_scores": [0.20272943377494812, 0.20154434442520142, 0.20092418789863586, 0.19938111305236816, 0.19542090594768524]}}
{"id": "WebQTest-643", "question": "where does princess leia live", "prediction": [["fictional_universe.fictional_character.places_lived"], ["fictional_universe.fictional_character.place_of_birth"], ["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"]], "ground_paths": [["fictional_universe.fictional_character.places_lived"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does princess leia live [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.places_lived </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>"], "scores": [-0.003953801468014717, -0.03664962202310562, -0.06759016215801239, -0.07116856426000595, -0.07324240356683731], "norm_scores": [0.20945781469345093, 0.20272016525268555, 0.19654394686222076, 0.19584189355373383, 0.19543616473674774]}}
{"id": "WebQTest-644", "question": "what famous people are from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["location.location.containedby", "people.person.nationality"], ["location.location.containedby", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.006422176957130432, -0.035618990659713745, -0.048327650874853134, -0.061375442892313004, -0.07184695452451706], "norm_scores": [0.2077540159225464, 0.2017759531736374, 0.19922788441181183, 0.19664528965950012, 0.19459685683250427]}}
{"id": "WebQTest-645", "question": "what type of art did leonardo da vinci make", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"], ["visual_art.art_form.artists"], ["visual_art.visual_artists.art_forms"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did leonardo da vinci make [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.art_form.artists </PATH>", "<PATH> visual_art.visual_artists.art_forms </PATH>"], "scores": [-0.00020072121697012335, -0.09798169136047363, -0.10298463702201843, -0.10826899111270905, -0.12443604320287704], "norm_scores": [0.21786931157112122, 0.19757424294948578, 0.19658824801445007, 0.19555217027664185, 0.19241607189178467]}}
{"id": "WebQTest-646", "question": "where did mary bell live", "prediction": [["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mary bell live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.016052313148975372, -0.03342684730887413, -0.03510400280356407, -0.036385584622621536, -0.04889991134405136], "norm_scores": [0.20360535383224487, 0.20009836554527283, 0.19976304471492767, 0.1995071917772293, 0.19702604413032532]}}
{"id": "WebQTest-648", "question": "who did kimberly williams marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["film.film.produced_by", "film.music_contributor.film"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kimberly williams marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.01986473612487316, -0.021612897515296936, -0.04291526973247528, -0.04625358432531357, -0.04857705906033516], "norm_scores": [0.20320582389831543, 0.20285089313983917, 0.19857540726661682, 0.19791360199451447, 0.1974542737007141]}}
{"id": "WebQTest-649", "question": "what other movies has ryan gosling been in", "prediction": [["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.film.executive_produced_by"], ["film.actor.film", "film.performance.film"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.featured_film_locations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has ryan gosling been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.02549206279218197, -0.028307974338531494, -0.03215739130973816, -0.03330110013484955, -0.035327520221471786], "norm_scores": [0.20108671486377716, 0.20052126049995422, 0.1997508704662323, 0.19952252507209778, 0.19911862909793854]}}
{"id": "WebQTest-650", "question": "what political party does julia gillard belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["government.election_campaign.candidate", "government.election_campaign.party"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does julia gillard belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>"], "scores": [-0.008216760121285915, -0.022004809230566025, -0.022509917616844177, -0.07487465441226959, -0.07637691497802734], "norm_scores": [0.20653727650642395, 0.20370906591415405, 0.20360620319843292, 0.19321875274181366, 0.1929287165403366]}}
{"id": "WebQTest-651", "question": "what is the currency of germany now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of germany now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009075208567082882, -0.020798005163669586, -0.05943647772073746, -0.08179125189781189, -0.16493600606918335], "norm_scores": [0.2116503119468689, 0.20918366312980652, 0.20125527679920197, 0.1968061923980713, 0.18110458552837372]}}
{"id": "WebQTest-652", "question": "what kind of hispanic is selena gomez", "prediction": [["people.ethnicity.included_in_group"], ["people.person.ethnicity"], ["people.ethnicity.people"], ["people.ethnicity.includes_people"], ["people.ethnicity.includes_groups"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of hispanic is selena gomez [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.ethnicity.includes_people </PATH>", "<PATH> people.ethnicity.includes_groups </PATH>"], "scores": [-0.011791767552495003, -0.02707560546696186, -0.03273347392678261, -0.04151248559355736, -0.07107548415660858], "norm_scores": [0.20503298938274384, 0.20192313194274902, 0.2007838934659958, 0.19902892410755157, 0.19323116540908813]}}
{"id": "WebQTest-653", "question": "who did denise van outen married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did denise van outen married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.01665908843278885, -0.03426356613636017, -0.03807338327169418, -0.04030101001262665, -0.04671218991279602], "norm_scores": [0.2037327140569687, 0.20017749071121216, 0.19941630959510803, 0.19897256791591644, 0.19770100712776184]}}
{"id": "WebQTest-654", "question": "what are egyptian money called", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["people.person.nationality", "people.person.gender", "finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are egyptian money called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> people.person.nationality <SEP> people.person.gender <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.00853869691491127, -0.02844342589378357, -0.057681988924741745, -0.07177625596523285, -0.07344991713762283], "norm_scores": [0.20797769725322723, 0.20387887954711914, 0.19800406694412231, 0.19523291289806366, 0.19490642845630646]}}
{"id": "WebQTest-655", "question": "what is the zip code for wyoming michigan", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.city_town.postal_codes"], ["location.city_state.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for wyoming michigan [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.city_town.postal_codes </PATH>", "<PATH> location.city_state.postal_codes </PATH>"], "scores": [-0.00145777128636837, -0.052093394100666046, -0.08343283087015152, -0.1188037246465683, -0.12491720914840698], "norm_scores": [0.21528270840644836, 0.20465311408042908, 0.19833886623382568, 0.19144608080387115, 0.19027923047542572]}}
{"id": "WebQTest-656", "question": "who is the president of costa rica in 2012", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of costa rica in 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.0034675400238484144, -0.03236745297908783, -0.04174555838108063, -0.049089860171079636, -0.05273408442735672], "norm_scores": [0.2065565586090088, 0.20067252218723297, 0.19879938662052155, 0.19734470546245575, 0.19662684202194214]}}
{"id": "WebQTest-657", "question": "where did martin luther king got shot at", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did martin luther king got shot at [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0002177199348807335, -0.11295333504676819, -0.11971693485975266, -0.12270612269639969, -0.13088074326515198], "norm_scores": [0.22011977434158325, 0.1966521143913269, 0.1953265219926834, 0.19474352896213531, 0.19315806031227112]}}
{"id": "WebQTest-658", "question": "who wrote the gospel according to john", "prediction": [["book.book_subject.works"], ["book.author.works_written"], ["book.written_work.author"], ["book.written_work.subjects"], ["book.book.genre"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the gospel according to john [/INST]", "raw_output": {"paths": ["<PATH> book.book_subject.works </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book.genre </PATH>"], "scores": [-0.013500252738595009, -0.02378428727388382, -0.03393036499619484, -0.07594195008277893, -0.08795487135648727], "norm_scores": [0.2067284733057022, 0.2046133577823639, 0.2025478333234787, 0.19421474635601044, 0.19189561903476715]}}
{"id": "WebQTest-659", "question": "what form of government is the uk", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["form.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government is the uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0075368271209299564, -0.018730221316218376, -0.13703356683254242, -0.15146540105342865, -0.1654200255870819], "norm_scores": [0.2179928869009018, 0.21556641161441803, 0.19151492416858673, 0.1887708604335785, 0.18615491688251495]}}
{"id": "WebQTest-661", "question": "who did nasri play for before arsenal", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did nasri play for before arsenal [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.026046033948659897, -0.02884669043123722, -0.033673834055662155, -0.03566300868988037, -0.03821513429284096], "norm_scores": [0.20129074156284332, 0.20072779059410095, 0.19976118206977844, 0.19936420023441315, 0.19885605573654175]}}
{"id": "WebQTest-662", "question": "what currency does the czech republic use now", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does the czech republic use now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.01345270685851574, -0.021931249648332596, -0.028075069189071655, -0.12003899365663528, -0.12074441462755203], "norm_scores": [0.2094590663909912, 0.20769067108631134, 0.20641855895519257, 0.18828223645687103, 0.18814946711063385]}}
{"id": "WebQTest-663", "question": "what age did william penn get married", "prediction": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what age did william penn get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.01398311648517847, -0.03508312255144119, -0.03691631183028221, -0.04038342088460922, -0.04487369954586029], "norm_scores": [0.2040826380252838, 0.1998216062784195, 0.1994556486606598, 0.19876529276371002, 0.1978747844696045]}}
{"id": "WebQTest-664", "question": "what was firefox programmed in", "prediction": [["computer.software_developer.software"], ["film.film.story_by"], ["computer.software.language"], ["computer.software.program"], ["common.topic.notable_types"]], "ground_paths": [["computer.software.languages_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was firefox programmed in [/INST]", "raw_output": {"paths": ["<PATH> computer.software_developer.software </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> computer.software.language </PATH>", "<PATH> computer.software.program </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.052984561771154404, -0.05345743149518967, -0.05782124400138855, -0.06742983311414719, -0.07070474326610565], "norm_scores": [0.20149928331375122, 0.20140403509140015, 0.20052704215049744, 0.19860948622226715, 0.19796013832092285]}}
{"id": "WebQTest-665", "question": "what government does new zealand have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.main_country", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does new zealand have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.0059622288681566715, -0.02262619510293007, -0.07789791375398636, -0.10663022100925446, -0.126853808760643], "norm_scores": [0.21256570518016815, 0.20905286073684692, 0.19781169295310974, 0.19220896065235138, 0.18836084008216858]}}
{"id": "WebQTest-666", "question": "where is isthmus of panama located on the map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["geography.isthorough_connection.isthorough_connections"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is isthmus of panama located on the map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> geography.isthorough_connection.isthorough_connections </PATH>"], "scores": [-0.001128121162764728, -0.059357739984989166, -0.08769766241312027, -0.09126390516757965, -0.11631832271814346], "norm_scores": [0.21433886885643005, 0.20221443474292755, 0.19656413793563843, 0.19586437940597534, 0.19101808965206146]}}
{"id": "WebQTest-667", "question": "what is the capital of republic of ireland", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.location.contains"], ["location.country.capital"], ["base.aareas.schema.administrative_area.capital"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of republic of ireland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>"], "scores": [-0.0073540667071938515, -0.028370270505547523, -0.05605217441916466, -0.05917022377252579, -0.07141514867544174], "norm_scores": [0.20750664174556732, 0.2031911313533783, 0.19764356315135956, 0.197028249502182, 0.19463038444519043]}}
{"id": "WebQTest-668", "question": "what has kristen stewart acted in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.place_of_birth", "film.film.subjects"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has kristen stewart acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.026965564116835594, -0.027072228491306305, -0.034657757729291916, -0.03805386275053024, -0.03848978877067566], "norm_scores": [0.20121754705905914, 0.20119608938694, 0.19967566430568695, 0.19899870455265045, 0.19891196489334106]}}
{"id": "WebQTest-669", "question": "what did dr josef mengele do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["base.activism.activist.area_of_activism"], ["medicine.disease.notable_people_with_this_condition"], ["base.crime.crime.victim_s"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr josef mengele do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> base.crime.crime.victim_s </PATH>"], "scores": [-0.002219133311882615, -0.05707981064915657, -0.07085441052913666, -0.07444937527179718, -0.10797116160392761], "norm_scores": [0.21230299770832062, 0.2009696513414383, 0.19822034239768982, 0.1975090205669403, 0.190997913479805]}}
{"id": "WebQTest-670", "question": "what kind of language does egypt speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.culturalevent.event.entity_involved", "location.location.events", "language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does egypt speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.018839441239833832, -0.02459588274359703, -0.026113681495189667, -0.03628651425242424, -0.09381860494613647], "norm_scores": [0.20418666303157806, 0.20301464200019836, 0.2027067393064499, 0.20065510272979736, 0.18943680822849274]}}
{"id": "WebQTest-671", "question": "who are the senators of hawaii 2012", "prediction": [["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.place_of_birth"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of hawaii 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.02332225628197193, -0.02527618408203125, -0.03076872043311596, -0.031615179032087326, -0.03341325744986534], "norm_scores": [0.20111295580863953, 0.2007203847169876, 0.19962093234062195, 0.19945204257965088, 0.19909372925758362]}}
{"id": "WebQTest-672", "question": "what form of government does north korea have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["people.person.nationality", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does north korea have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.010173830203711987, -0.013395000249147415, -0.13142810761928558, -0.14015071094036102, -0.149009108543396], "norm_scores": [0.21593236923217773, 0.21523793041706085, 0.19127477705478668, 0.18961361050605774, 0.18794135749340057]}}
{"id": "WebQTest-673", "question": "where did robert kardashian go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["common.topic.notable_types"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert kardashian go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.012900165282189846, -0.018225766718387604, -0.029512174427509308, -0.04199199005961418, -0.05291781947016716], "norm_scores": [0.2036529779434204, 0.20257127285003662, 0.20029784739017487, 0.1978137046098709, 0.195664182305336]}}
{"id": "WebQTest-675", "question": "what county is st james city fl in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st james city fl in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.020206546410918236, -0.023410111665725708, -0.029219960793852806, -0.03177996352314949, -0.058043718338012695], "norm_scores": [0.20246227085590363, 0.20181472599506378, 0.2006455957889557, 0.2001326084136963, 0.19494479894638062]}}
{"id": "WebQTest-676", "question": "what did the akkadian empire include", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the akkadian empire include [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.012043153867125511, -0.029821952804923058, -0.03450506180524826, -0.059297364205121994, -0.07210031151771545], "norm_scores": [0.20594258606433868, 0.20231352746486664, 0.2013682723045349, 0.1964372843503952, 0.19393834471702576]}}
{"id": "WebQTest-677", "question": "after how many years are the olympic games held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.current_frequency"], ["time.recurring_event.instances", "time.event.instance_of_recurring_event"], ["time.event.recurring_event"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: after how many years are the olympic games held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.recurring_event.instances <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> time.event.recurring_event </PATH>"], "scores": [-0.010587322525680065, -0.01671086624264717, -0.047871582210063934, -0.09327701479196548, -0.0934567004442215], "norm_scores": [0.20840255916118622, 0.2071302831172943, 0.20077548921108246, 0.1918630599975586, 0.19182857871055603]}}
{"id": "WebQTest-678", "question": "what system of government does guyana have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government does guyana have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.007642061915248632, -0.017639897763729095, -0.1340024769306183, -0.13659289479255676, -0.14201495051383972], "norm_scores": [0.216233491897583, 0.21408240497112274, 0.19056595861911774, 0.19007296860218048, 0.18904517590999603]}}
{"id": "WebQTest-680", "question": "what team did aguero play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did aguero play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.024113278836011887, -0.025840764865279198, -0.030314700677990913, -0.03564968705177307, -0.038309261202812195], "norm_scores": [0.2013479769229889, 0.20100045204162598, 0.20010319352149963, 0.19903850555419922, 0.1985098421573639]}}
{"id": "WebQTest-681", "question": "who is the president of the european union 2012", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["organization.organization_membership.organization", "organization.organization_membership.member"], ["organization.organization.geographic_scope", "people.person.nationality"], ["organization.organization_membership.organization", "organization.organization_member.member_of"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of the european union 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_membership.member </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>"], "scores": [-0.010886075906455517, -0.017698366194963455, -0.029184745624661446, -0.04124774411320686, -0.04885619506239891], "norm_scores": [0.20375250279903412, 0.20236919820308685, 0.2000580132007599, 0.1976592093706131, 0.19616104662418365]}}
{"id": "WebQTest-683", "question": "what year was kenya moore crowned miss usa", "prediction": [["people.person.nationality", "location.location.events"], ["people.person.nationality", "time.event.locations"], ["time.event.instance_of_recurring_event"], ["award.competitor.competitions", "award.competitor.competitions"], ["award.competitor.competitions_won", "award.competitor.competitions_won"]], "ground_paths": [["time.recurring_event.instances"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was kenya moore crowned miss usa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.events </PATH>", "<PATH> people.person.nationality <SEP> time.event.locations </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> award.competitor.competitions <SEP> award.competitor.competitions </PATH>", "<PATH> award.competitor.competitions_won <SEP> award.competitor.competitions_won </PATH>"], "scores": [-0.016425689682364464, -0.03167521953582764, -0.04546097666025162, -0.04667148366570473, -0.05815410614013672], "norm_scores": [0.20468375086784363, 0.20158609747886658, 0.1988261491060257, 0.1985856145620346, 0.1963183730840683]}}
{"id": "WebQTest-684", "question": "what sort of government does nigeria have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.locations.countries.continent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sort of government does nigeria have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.0059166536666452885, -0.02130640670657158, -0.13305920362472534, -0.13815921545028687, -0.1454170048236847], "norm_scores": [0.2168605774641037, 0.2135487049818039, 0.19096919894218445, 0.18999771773815155, 0.18862375617027283]}}
{"id": "WebQTest-685", "question": "where does chef ina garten live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does chef ina garten live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.019716130569577217, -0.02229478769004345, -0.02926691062748432, -0.045298825949430466, -0.059264060109853745], "norm_scores": [0.20309166610240936, 0.2025686353445053, 0.2011612206697464, 0.1979619264602661, 0.195216566324234]}}
{"id": "WebQTest-686", "question": "when the atlanta braves won the world series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.mascot.team"], ["time.event.instance_of_recurring_event"], ["sports.mascot.team", "sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when the atlanta braves won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.mascot.team </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.mascot.team <SEP> sports.sports_team.championships </PATH>"], "scores": [-0.010199226438999176, -0.015297756530344486, -0.05203554034233093, -0.08794087916612625, -0.10063687711954117], "norm_scores": [0.20865170657634735, 0.20759057998657227, 0.2001025676727295, 0.1930452585220337, 0.1906098574399948]}}
{"id": "WebQTest-688", "question": "what kind of language do they speak in iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do they speak in iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.013683696277439594, -0.019145647063851357, -0.03455129265785217, -0.048144031316041946, -0.17761598527431488], "norm_scores": [0.20881935954093933, 0.20768190920352936, 0.20450696349143982, 0.20174595713615417, 0.17724578082561493]}}
{"id": "WebQTest-689", "question": "what was the first language spoken in spain", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first language spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.01810416206717491, -0.021155042573809624, -0.02118402160704136, -0.06843073666095734, -0.1491694301366806], "norm_scores": [0.2073846310377121, 0.20675288140773773, 0.206746906042099, 0.19720594584941864, 0.18190960586071014]}}
{"id": "WebQTest-690", "question": "what did scott joplin play", "prediction": [["music.group_member.instruments_played"], ["people.person.profession"], ["book.written_work.author"], ["music.instrument.family"], ["music.artist.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did scott joplin play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.artist.instruments_played </PATH>"], "scores": [-0.001999486004933715, -0.07555124163627625, -0.0824904516339302, -0.08529399335384369, -0.09847525507211685], "norm_scores": [0.21368147432804108, 0.19852888584136963, 0.19715604186058044, 0.19660407304763794, 0.19402958452701569]}}
{"id": "WebQTest-691", "question": "what school did miley cyrus", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did miley cyrus [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.0198189839720726, -0.02454364486038685, -0.02596653625369072, -0.03304664045572281, -0.04237842559814453], "norm_scores": [0.20186889171600342, 0.20091737806797028, 0.20063170790672302, 0.19921623170375824, 0.19736583530902863]}}
{"id": "WebQTest-693", "question": "what language do most people speak in afghanistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most people speak in afghanistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.015881258994340897, -0.018378494307398796, -0.03234931454062462, -0.04380042105913162, -0.12056857347488403], "norm_scores": [0.20600521564483643, 0.20549140870571136, 0.20264048874378204, 0.20033325254917145, 0.18552954494953156]}}
{"id": "WebQTest-694", "question": "who is the prime minister of jamaica west indies", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of jamaica west indies [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.010963484644889832, -0.027252458035945892, -0.031211499124765396, -0.03565285727381706, -0.039232514798641205], "norm_scores": [0.20360220968723297, 0.20031258463859558, 0.19952110946178436, 0.1986369490623474, 0.19792716205120087]}}
{"id": "WebQTest-695", "question": "what role did paul mccartney play in the beatles", "prediction": [["music.musical_group.member", "music.group_membership.role"], ["music.musical_group.member", "music.performance_role.regular_performances"], ["music.producer.releases_produced", "music.group_membership.member", "music.group_membership.role"], ["music.producer.releases_produced", "music.group_membership.member", "music.performance_role.regular_performances"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["music.group_membership.member", "music.group_membership.role"], ["music.group_member.membership", "music.performance_role.regular_performances"], ["music.group_member.instruments_played", "music.instrument.family"], ["music.group_member.instruments_played", "music.instrument.variation"], ["music.group_member.membership", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did paul mccartney play in the beatles [/INST]", "raw_output": {"paths": ["<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.musical_group.member <SEP> music.performance_role.regular_performances </PATH>", "<PATH> music.producer.releases_produced <SEP> music.group_membership.member <SEP> music.group_membership.role </PATH>", "<PATH> music.producer.releases_produced <SEP> music.group_membership.member <SEP> music.performance_role.regular_performances </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.013412157073616982, -0.021154986694455147, -0.02512608841061592, -0.04973529279232025, -0.04975322261452675], "norm_scores": [0.20369578897953033, 0.202124685049057, 0.20132362842559814, 0.19642968475818634, 0.1964261531829834]}}
{"id": "WebQTest-696", "question": "when did juventus win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did juventus win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.005317944101989269, -0.02406601794064045, -0.08712531626224518, -0.09056807309389114, -0.09431864321231842], "norm_scores": [0.2111484706401825, 0.2072267234325409, 0.1945626437664032, 0.19389396905899048, 0.19316811859607697]}}
{"id": "WebQTest-698", "question": "who is robin roberts father", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"], ["fictional_universe.fictional_character.parents"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is robin roberts father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.008355080150067806, -0.022557085379958153, -0.10119028389453888, -0.13809725642204285, -0.16455161571502686], "norm_scores": [0.21593819558620453, 0.21289311349391937, 0.19679391384124756, 0.18966324627399445, 0.18471160531044006]}}
{"id": "WebQTest-699", "question": "what is the capital of australia victoria state", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["base.biblioness.bibs_location.state"], ["base.aareas.schema.administrative_area.capital_city"], ["base.aareas.schema.administrative_area.capital"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of australia victoria state [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.aareas.schema.administrative_area.capital_city </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>"], "scores": [-0.015600640326738358, -0.03777780756354332, -0.03855886310338974, -0.04402272403240204, -0.04604469612240791], "norm_scores": [0.20419153571128845, 0.19971297681331635, 0.19955706596374512, 0.19846968352794647, 0.1980687975883484]}}
{"id": "WebQTest-700", "question": "where is french spoken most", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.local_name.locale"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["base.rosetta.local_name.locale"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is french spoken most [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>"], "scores": [-0.016709960997104645, -0.023274213075637817, -0.023900074884295464, -0.09056831896305084, -0.1322789043188095], "norm_scores": [0.2080751657485962, 0.20671379566192627, 0.20658445358276367, 0.19326089322566986, 0.1853656768798828]}}
{"id": "WebQTest-701", "question": "what team is reggie bush on 2011", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is reggie bush on 2011 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.020437346771359444, -0.022736649960279465, -0.023843660950660706, -0.027143215760588646, -0.028745464980602264], "norm_scores": [0.20082959532737732, 0.20036835968494415, 0.20014667510986328, 0.1994873583316803, 0.19916798174381256]}}
{"id": "WebQTest-702", "question": "what highschool did emily osment go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did emily osment go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.014894336462020874, -0.02157084085047245, -0.02250344678759575, -0.02652648836374283, -0.05271882563829422], "norm_scores": [0.20254875719547272, 0.201200932264328, 0.20101340115070343, 0.20020632445812225, 0.19503054022789001]}}
{"id": "WebQTest-703", "question": "what currency should i take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.0054845428094267845, -0.0328066311776638, -0.05872563645243645, -0.06833869218826294, -0.0718386247754097], "norm_scores": [0.20850366353988647, 0.2028840184211731, 0.19769303500652313, 0.19580170512199402, 0.19511760771274567]}}
{"id": "WebQTest-704", "question": "who is thor heyerdahl", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is thor heyerdahl [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.001291276654228568, -0.05784158036112785, -0.09904059022665024, -0.10330146551132202, -0.12512405216693878], "norm_scores": [0.21558958292007446, 0.20373624563217163, 0.1955130696296692, 0.1946817934513092, 0.1904793381690979]}}
{"id": "WebQTest-706", "question": "where did troy aikman play football", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality", "location.location.containedby"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did troy aikman play football [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.011650556698441505, -0.014042885974049568, -0.056715287268161774, -0.058524057269096375, -0.05895756557583809], "norm_scores": [0.20569577813148499, 0.20520427823066711, 0.19663192331790924, 0.19627657532691956, 0.19619151949882507]}}
{"id": "WebQTest-707", "question": "who played cletus hogg", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played cletus hogg [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.02059510350227356, -0.02710273489356041, -0.03152346983551979, -0.03335762023925781, -0.0362207256257534], "norm_scores": [0.20183837413787842, 0.20052914321422577, 0.19964461028575897, 0.19927877187728882, 0.19870904088020325]}}
{"id": "WebQTest-708", "question": "who won fa cup 1976", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fa cup 1976 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_award.award <SEP> sports.sports_award_winner.awards </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.champion </PATH>"], "scores": [-0.019036147743463516, -0.023253148421645164, -0.024278663098812103, -0.029906727373600006, -0.03368673101067543], "norm_scores": [0.20140144228935242, 0.20055393874645233, 0.200348362326622, 0.19922396540641785, 0.1984723061323166]}}
{"id": "WebQTest-709", "question": "what did general robert e lee die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["base.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did general robert e lee die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> base.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0017233274411410093, -0.04064545780420303, -0.10816828161478043, -0.11373782902956009, -0.12993483245372772], "norm_scores": [0.21577073633670807, 0.2075338214635849, 0.19398318231105804, 0.19290581345558167, 0.18980646133422852]}}
{"id": "WebQTest-710", "question": "what style of art did henri matisse do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of art did henri matisse do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.0018665231764316559, -0.03690742701292038, -0.08214778453111649, -0.10078337043523788, -0.1193879023194313], "norm_scores": [0.213521808385849, 0.20616938173770905, 0.1970500349998474, 0.1934119015932083, 0.18984682857990265]}}
{"id": "WebQTest-711", "question": "who plays chuck bass in gossip girl", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["base.gossipgirl.character.played_by"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays chuck bass in gossip girl [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.021695850417017937, -0.03103666380047798, -0.0340709388256073, -0.034876901656389236, -0.03904121741652489], "norm_scores": [0.20209722220897675, 0.20021826028823853, 0.19961166381835938, 0.19945085048675537, 0.19862200319766998]}}
{"id": "WebQTest-712", "question": "what countries have territories in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["book.written_work.subjects"], ["book.book_subject.works"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have territories in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.008777602575719357, -0.03486521914601326, -0.04417169839143753, -0.06304403394460678, -0.06453841924667358], "norm_scores": [0.20693567395210266, 0.20160703361034393, 0.1997394859790802, 0.19600526988506317, 0.1957125961780548]}}
{"id": "WebQTest-713", "question": "what county is st. louis park in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.us_county.hud_county_place"], ["location.location.contains"], ["location.hud_county_place.county"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st. louis park in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.006475839298218489, -0.03957022726535797, -0.047001469880342484, -0.051548078656196594, -0.05170696973800659], "norm_scores": [0.20663562417030334, 0.19990907609462738, 0.19842898845672607, 0.19752885401248932, 0.19749747216701508]}}
{"id": "WebQTest-714", "question": "what is the political system of south africa", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system of south africa [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.011423748917877674, -0.01284866314381361, -0.07650002092123032, -0.09129265695810318, -0.09206908196210861], "norm_scores": [0.20914682745933533, 0.2088490128517151, 0.19596974551677704, 0.19309216737747192, 0.1929423063993454]}}
{"id": "WebQTest-716", "question": "what language group does germany belong to", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.human_language.language_family"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language group does germany belong to [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.012727063149213791, -0.027875777333974838, -0.03565977141261101, -0.03960457444190979, -0.06951446086168289], "norm_scores": [0.20489414036273956, 0.20181365311145782, 0.2002488225698471, 0.1994604468345642, 0.1935829371213913]}}
{"id": "WebQTest-717", "question": "what timezone is tucson arizona in right now", "prediction": [["location.location.time_zones"], ["location.location.time_Zones"], ["location.location.time_zonessymbol"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is tucson arizona in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.location.time_zonessymbol </PATH>"], "scores": [-8.939661711337976e-06, -0.1741858422756195, -0.18192872405052185, -0.20424988865852356, -0.21861541271209717], "norm_scores": [0.2329525202512741, 0.19571465253829956, 0.19420510530471802, 0.18991823494434357, 0.18720947206020355]}}
{"id": "WebQTest-718", "question": "when was lucy lawless born", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["people.person.nationality", "location.location.containedby"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was lucy lawless born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>"], "scores": [-0.0007202457636594772, -0.07640226930379868, -0.0829431489109993, -0.09088486433029175, -0.10066072642803192], "norm_scores": [0.21427731215953827, 0.19865885376930237, 0.1973636895418167, 0.1958024799823761, 0.19389767944812775]}}
{"id": "WebQTest-719", "question": "which political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.election_campaigns", "government.election_campaign.party"]], "ground_paths": [["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.politician.election_campaigns <SEP> government.election_campaign.party </PATH>"], "scores": [-0.019210966303944588, -0.02146047353744507, -0.021973757073283195, -0.03482916206121445, -0.039944861084222794], "norm_scores": [0.20165450870990753, 0.2012014091014862, 0.20109814405441284, 0.19852948188781738, 0.19751647114753723]}}
{"id": "WebQTest-721", "question": "what kind of leukemia did jill clayburgh have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.deceased_person.cause_of_death"], ["people.person.medical_condition"], ["people.person.medical_condition", "medical_condition.condition"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of leukemia did jill clayburgh have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.medical_condition </PATH>", "<PATH> people.person.medical_condition <SEP> medical_condition.condition </PATH>"], "scores": [-0.011924478225409985, -0.01275575626641512, -0.04731297865509987, -0.05901286005973816, -0.06331001967191696], "norm_scores": [0.20540988445281982, 0.2052392065525055, 0.1982678771018982, 0.19596166908740997, 0.1951213926076889]}}
{"id": "WebQTest-723", "question": "what is there to see in sydney australia", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["film.film_festival.location"], ["transportation.bridge.locale"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to see in sydney australia [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> film.film_festival.location </PATH>", "<PATH> transportation.bridge.locale </PATH>"], "scores": [-0.014345856383442879, -0.016088437288999557, -0.02846352942287922, -0.1111675426363945, -0.12223980575799942], "norm_scores": [0.2087821662425995, 0.20841866731643677, 0.2058553695678711, 0.18951530754566193, 0.1874285191297531]}}
{"id": "WebQTest-724", "question": "where was marilyn monroe found dead", "prediction": [["people.deceased_person.place_of_death"], ["people.cause_of_death.people"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people_found_here"], ["people.place_of_death.people", "people.place_of_death.location"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was marilyn monroe found dead [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_found_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>"], "scores": [-0.00030197142041288316, -0.08563829213380814, -0.10136141628026962, -0.11681275814771652, -0.12181627750396729], "norm_scores": [0.21750091016292572, 0.19971007108688354, 0.19659458100795746, 0.1935802698135376, 0.1926141083240509]}}
{"id": "WebQTest-725", "question": "what states does the sierra nevada run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"], ["location.location.contains"], ["geography.mountain_range.mountains"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states does the sierra nevada run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain_range.mountains </PATH>"], "scores": [-0.006979158613830805, -0.0276869535446167, -0.043241340667009354, -0.09238060563802719, -0.13623492419719696], "norm_scores": [0.21093550324440002, 0.20661242306232452, 0.20342354476451874, 0.19366909563541412, 0.1853594034910202]}}
{"id": "WebQTest-726", "question": "what language is spoken in greek", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["language.human_language.language_family"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in greek [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.014315642416477203, -0.018231980502605438, -0.042921796441078186, -0.053426265716552734, -0.08939429372549057], "norm_scores": [0.20587970316410065, 0.2050749808549881, 0.200073704123497, 0.197983056306839, 0.19098854064941406]}}
{"id": "WebQTest-728", "question": "what is the name of the speaker of the house of representatives now 2011", "prediction": [["common.topic.image", "common.topic.image"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "ground_paths": [["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the speaker of the house of representatives now 2011 [/INST]", "raw_output": {"paths": ["<PATH> common.topic.image <SEP> common.topic.image </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.010118388570845127, -0.01653100550174713, -0.025897057726979256, -0.03028680942952633, -0.04321960732340813], "norm_scores": [0.20302806794643402, 0.20173028111457825, 0.1998496949672699, 0.19897432625293732, 0.19641759991645813]}}
{"id": "WebQTest-729", "question": "who plays stanley hudson", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stanley hudson [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.021946314722299576, -0.0304158516228199, -0.03267236053943634, -0.03701012209057808, -0.039642129093408585], "norm_scores": [0.20208527147769928, 0.20038093626499176, 0.19992928206920624, 0.19906389713287354, 0.19854064285755157]}}
{"id": "WebQTest-730", "question": "what record label is kelly clarkson signed with", "prediction": [["music.artist.label"], ["music.record_label.artist"], ["music.recording.artist"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nominee"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["music.record_label.artist"], ["music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what record label is kelly clarkson signed with [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.001855627284385264, -0.05834764242172241, -0.07277225703001022, -0.07425963878631592, -0.08623842895030975], "norm_scores": [0.21160222589969635, 0.1999797821044922, 0.19711583852767944, 0.1968228816986084, 0.19447924196720123]}}
{"id": "WebQTest-731", "question": "where does tim roth come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["music.artist.origin"], ["music.recording.artist"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim roth come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.006783321034163237, -0.03280609846115112, -0.04980715736746788, -0.06650962680578232, -0.12214981019496918], "norm_scores": [0.20985198020935059, 0.2044615000486374, 0.20101481676101685, 0.19768525660037994, 0.18698641657829285]}}
{"id": "WebQTest-733", "question": "what currency should you take to vietnam", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "ground_paths": [["base.monetarydebase.debased_currency.issuing_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to vietnam [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>"], "scores": [-0.011368785053491592, -0.016251953318715096, -0.07122671604156494, -0.07558483630418777, -0.08544997870922089], "norm_scores": [0.20818474888801575, 0.20717063546180725, 0.19608886539936066, 0.19523614645004272, 0.19331957399845123]}}
{"id": "WebQTest-734", "question": "who did roger federer married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.person.children", "people.person.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did roger federer married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.02466115728020668, -0.034951839596033096, -0.04118049517273903, -0.04798731580376625, -0.05008554831147194], "norm_scores": [0.20303668081760406, 0.2009580135345459, 0.1997101753950119, 0.19835542142391205, 0.1979396641254425]}}
{"id": "WebQTest-735", "question": "what kind of money do i need in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i need in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.006417305674403906, -0.02171647921204567, -0.1046561747789383, -0.13459765911102295, -0.1349402666091919], "norm_scores": [0.21503815054893494, 0.21177327632904053, 0.19491752982139587, 0.18916791677474976, 0.1891031265258789]}}
{"id": "WebQTest-737", "question": "what is the name of the capital of belgium", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"], ["location.location.containedby"], ["location.location.contains"], ["location.administrative_division.country"]], "ground_paths": [["location.country.capital"], ["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the capital of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.011882101185619831, -0.022034790366888046, -0.030103376135230064, -0.087844617664814, -0.10400508344173431], "norm_scores": [0.20787055790424347, 0.20577079057693481, 0.20411719381809235, 0.19266502559185028, 0.18957649171352386]}}
{"id": "WebQTest-738", "question": "what kind of government does japan have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["base.onephylogeny.type_of_thing.things_of_this_type"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does japan have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type </PATH>"], "scores": [-0.009330535307526588, -0.014852779917418957, -0.12180931121110916, -0.1319405883550644, -0.13266025483608246], "norm_scores": [0.21474480628967285, 0.21356219053268433, 0.19189947843551636, 0.18996509909629822, 0.18982841074466705]}}
{"id": "WebQTest-739", "question": "what pieces of music did claude debussy compose", "prediction": [["music.composition.composer"], ["music.artist.genre"], ["music.recording.artist"], ["music.composer.compositions"], ["music.composition.lyricist"]], "ground_paths": [["book.author.works_written", "book.book.editions"], ["base.musiteca.music_teacher.composition"], ["book.written_work.author"], ["opera.librettist.libretti"], ["music.recording.artist"], ["book.author.works_written"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["opera.opera.librettist"], ["music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what pieces of music did claude debussy compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.composition.lyricist </PATH>"], "scores": [-0.0074781812727451324, -0.03408380597829819, -0.05354968458414078, -0.05640336498618126, -0.07096211612224579], "norm_scores": [0.20749211311340332, 0.20204444229602814, 0.19814950227737427, 0.1975848376750946, 0.19472910463809967]}}
{"id": "WebQTest-740", "question": "who was selena gomez in barney and friends", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.character"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["film.actor.film", "film.performance.character"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.character"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.character"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was selena gomez in barney and friends [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.character </PATH>"], "scores": [-0.024143913760781288, -0.02674981765449047, -0.033624377101659775, -0.04080645740032196, -0.04168374091386795], "norm_scores": [0.20185501873493195, 0.20132970809936523, 0.19995038211345673, 0.19851946830749512, 0.1983453929424286]}}
{"id": "WebQTest-741", "question": "who plays caesar flickerman in the hunger games", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film.starring", "film.actor.film"], ["film.film.country", "people.person.nationality"]], "ground_paths": [["film.film.starring", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays caesar flickerman in the hunger games [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.026418698951601982, -0.030256330966949463, -0.038735296577215195, -0.04297289252281189, -0.04369205981492996], "norm_scores": [0.20200446248054504, 0.2012307345867157, 0.19953171908855438, 0.19868797063827515, 0.19854512810707092]}}
{"id": "WebQTest-742", "question": "where did the gettysburg address", "prediction": [["book.written_work.subjects"], ["book.author.book_editions_published"], ["book.book_subject.works"], ["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speech_topic"], ["location.location.events"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gettysburg address [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speech_topic </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.01206883043050766, -0.04429668188095093, -0.04836835712194443, -0.058860599994659424, -0.060467444360256195], "norm_scores": [0.2066253274679184, 0.20007240772247314, 0.1992594301700592, 0.1971796751022339, 0.1968630999326706]}}
{"id": "WebQTest-743", "question": "what was the name of jfk 's brothers", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["film.film_subject.films", "film.film.subjects"], ["people.person.parents", "people.person.children"], ["people.family.members", "people.family_member.family"], ["people.family.members", "people.family.members"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["people.person.religion", "people.person.religion"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.gender", "people.person.gender"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of jfk 's brothers [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.03132523223757744, -0.03430847078561783, -0.036770690232515335, -0.03810804337263107, -0.038503263145685196], "norm_scores": [0.20089687407016754, 0.20029844343662262, 0.1998058706521988, 0.19953882694244385, 0.1994599997997284]}}
{"id": "WebQTest-744", "question": "who was tupac signed to", "prediction": [["music.artist.label"], ["music.record_label.artist"], ["music.recording.artist"], ["music.producer.releases_produced"], ["music.album.artist"]], "ground_paths": [["music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was tupac signed to [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.producer.releases_produced </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.009407280012965202, -0.0241331085562706, -0.05739111825823784, -0.09276844561100006, -0.10432363301515579], "norm_scores": [0.2097315788269043, 0.2066657394170761, 0.19990548491477966, 0.19295699894428253, 0.1907401829957962]}}
{"id": "WebQTest-745", "question": "what was augustus caesar famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was augustus caesar famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.004857046529650688, -0.04465620592236519, -0.05198449641466141, -0.05765315517783165, -0.0638064295053482], "norm_scores": [0.20806127786636353, 0.19994322955608368, 0.19848334789276123, 0.19736139476299286, 0.19615070521831512]}}
{"id": "WebQTest-747", "question": "what language does colombia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does colombia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.01046801544725895, -0.019446464255452156, -0.06000419706106186, -0.06428875029087067, -0.15383033454418182], "norm_scores": [0.21022649109363556, 0.20834743976593018, 0.20006638765335083, 0.1992110311985016, 0.18214866518974304]}}
{"id": "WebQTest-748", "question": "what is the zip code for schenectady new york", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.postal_code.citytown"], ["location.mailing_address.citytown"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for schenectady new york [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.postal_code.citytown </PATH>", "<PATH> location.mailing_address.citytown </PATH>"], "scores": [-0.0004785638302564621, -0.06546731293201447, -0.1101691871881485, -0.15210022032260895, -0.15304921567440033], "norm_scores": [0.2197318822145462, 0.20590589940547943, 0.1969042271375656, 0.1888185441493988, 0.18863943219184875]}}
{"id": "WebQTest-749", "question": "who was president in 1988 in the united states", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"]], "ground_paths": [["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president in 1988 in the united states [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>"], "scores": [-0.023361273109912872, -0.023736800998449326, -0.026111168786883354, -0.03015998750925064, -0.03387966379523277], "norm_scores": [0.2008177489042282, 0.20074234902858734, 0.2002662867307663, 0.1994570791721344, 0.19871655106544495]}}
{"id": "WebQTest-751", "question": "who owns google 2012", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["people.person.gender", "people.person.gender"], ["organization.organization.board_members", "organization.organization_board_membership.member"], ["organization.organization.parent", "organization.organization.child"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.leadership.organization", "organization.leadership.person"], ["organization.leadership.organization", "business.board_member.leader_of"], ["organization.organization.leadership", "business.board_member.leader_of"], ["conferences.conference.focus", "time.event.people_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns google 2012 [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> organization.organization.board_members <SEP> organization.organization_board_membership.member </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>"], "scores": [-0.02063925750553608, -0.02248433791100979, -0.051572367548942566, -0.05353680998086929, -0.0593811571598053], "norm_scores": [0.20419274270534515, 0.20381633937358856, 0.1979731172323227, 0.1975845992565155, 0.1964332014322281]}}
{"id": "WebQTest-752", "question": "what drink did john pemberton create", "prediction": [["food.beer_country_region.beers_from_here"], ["food.beer_country_region.beers_from_here", "food.beer_country_region.beers_from_here"], ["food.beer.from_region"], ["food.beer.from_region", "food.beer_country_region.beers_from_here"], ["food.beverage_creator.beverages_created"]], "ground_paths": [["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drink did john pemberton create [/INST]", "raw_output": {"paths": ["<PATH> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer_country_region.beers_from_here <SEP> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer.from_region </PATH>", "<PATH> food.beer.from_region <SEP> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beverage_creator.beverages_created </PATH>"], "scores": [-0.008645427413284779, -0.02682608924806118, -0.045110952109098434, -0.05000001937150955, -0.05491302162408829], "norm_scores": [0.2057422548532486, 0.20203551650047302, 0.1983749121427536, 0.19740740954875946, 0.19643990695476532]}}
{"id": "WebQTest-753", "question": "what position did stanley matthews play", "prediction": [["sports.pro_athlete.teams", "sports.sports_position.teams"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["sports.sports_position.sport", "sports.sports_position.sport"]], "ground_paths": [["soccer.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did stanley matthews play [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_position.teams </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_position.sport </PATH>"], "scores": [-0.030845627188682556, -0.0345466174185276, -0.03587792068719864, -0.0369107648730278, -0.044384416192770004], "norm_scores": [0.20113472640514374, 0.2003917098045349, 0.2001250982284546, 0.19991850852966309, 0.19842997193336487]}}
{"id": "WebQTest-755", "question": "what was bonnie prince charlie full name", "prediction": [["people.person.nationality"], ["common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["people.person.children"], ["symbols.name_source.namesakes"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was bonnie prince charlie full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.00770376855507493, -0.05174719914793968, -0.05198599025607109, -0.05540158227086067, -0.06182895973324776], "norm_scores": [0.20771315693855286, 0.19876329600811005, 0.19871583580970764, 0.1980382651090622, 0.19676949083805084]}}
{"id": "WebQTest-756", "question": "who makes the voice of stewie griffin", "prediction": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.tv_program_creator.programs_created"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who makes the voice of stewie griffin [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.019353972747921944, -0.021998312324285507, -0.022612834349274635, -0.050300996750593185, -0.051075030118227005], "norm_scores": [0.20274068415164948, 0.20220527052879333, 0.20208103954792023, 0.19656254351139069, 0.19641046226024628]}}
{"id": "WebQTest-757", "question": "who is angelina jolie mom and dad", "prediction": [["people.person.parents"], ["people.person.children"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.award_nominee"], ["award.award_nomination.award_nominee", "award.award_nominee.award_nominations"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie mom and dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.01041748933494091, -0.0158194862306118, -0.09601646661758423, -0.0998670682311058, -0.10137436538934708], "norm_scores": [0.21096749603748322, 0.20983092486858368, 0.1936602145433426, 0.1929159313440323, 0.19262537360191345]}}
{"id": "WebQTest-758", "question": "who is the governor of indiana 2009", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["common.topic.image"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of indiana 2009 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.019138485193252563, -0.01917361468076706, -0.02374819852411747, -0.028095092624425888, -0.028152041137218475], "norm_scores": [0.20090502500534058, 0.2008979767560959, 0.19998104870319366, 0.1991136372089386, 0.19910229742527008]}}
{"id": "WebQTest-759", "question": "who brad pitt has dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"]], "ground_paths": [["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.insult_victim", "base.popstra.public_insult.perpetrator"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.nationality", "people.person.place_of_birth"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.lived_with.participant", "base.popstra.celebrity.lived_with"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who brad pitt has dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> base.popstra.dated.participant <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.01634306274354458, -0.030462514609098434, -0.036572229117155075, -0.05046313628554344, -0.05338059365749359], "norm_scores": [0.20424629747867584, 0.20138271152973175, 0.2001560777425766, 0.1973949670791626, 0.19681990146636963]}}
{"id": "WebQTest-760", "question": "what party does barack obama belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party does barack obama belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.017369739711284637, -0.0200092364102602, -0.027710944414138794, -0.035223692655563354, -0.044419754296541214], "norm_scores": [0.20231884717941284, 0.20178553462028503, 0.20023740828037262, 0.1987387239933014, 0.19691947102546692]}}
{"id": "WebQTest-761", "question": "what is san francisco zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is san francisco zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.citytown.postal_codes <PATH>"], "scores": [-0.00023248005891218781, -0.08640427142381668, -0.1617346554994583, -0.17243143916130066, -0.17680944502353668], "norm_scores": [0.22480766475200653, 0.2062467783689499, 0.19128090143203735, 0.18924571573734283, 0.18841899931430817]}}
{"id": "WebQTest-762", "question": "who has stephen ireland played for", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has stephen ireland played for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.031903669238090515, -0.03208378329873085, -0.03319495543837547, -0.037248652428388596, -0.03731723874807358], "norm_scores": [0.20048920810222626, 0.20045310258865356, 0.20023049414157867, 0.19942046701908112, 0.19940678775310516]}}
{"id": "WebQTest-764", "question": "where did john mayer go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john mayer go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.015168539248406887, -0.019200818613171577, -0.021065091714262962, -0.041911713778972626, -0.07746810466051102], "norm_scores": [0.2039438933134079, 0.20312318205833435, 0.20274485647678375, 0.19856207072734833, 0.19162596762180328]}}
{"id": "WebQTest-765", "question": "what celebrities has brody jenner dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["base.popstra.breakup.participant", "base.popstra.celebrity.breakup"], ["base.popstra.canoodled.participant", "base.popstra.celebrity.canoodled"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.breakup.participant", "base.popstra.breakup.participant"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what celebrities has brody jenner dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.canoodled.participant </PATH>"], "scores": [-0.016738755628466606, -0.027846775949001312, -0.03332609683275223, -0.03601651266217232, -0.038356803357601166], "norm_scores": [0.20275650918483734, 0.2005167454481125, 0.19942104816436768, 0.1988852471113205, 0.19842036068439484]}}
{"id": "WebQTest-766", "question": "what is malaysian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_used", "location.country.currency_used"], ["location.country.currency_used", "location.location.time_zones"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is malaysian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <SEP> location.location.time_zones </PATH>"], "scores": [-0.0069458382204174995, -0.023781178519129753, -0.1280594915151596, -0.14777150750160217, -0.15667566657066345], "norm_scores": [0.21744707226753235, 0.21381692588329315, 0.19264359772205353, 0.18888337910175323, 0.18720901012420654]}}
{"id": "WebQTest-768", "question": "who owns volkswagen car company", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization_board_membership.organization", "organization.organization_board_membership.member"], ["organization.organization.board_members", "business.board_member.organization_board_memberships"], ["business.employer.employees", "business.employment_tenure.person"]], "ground_paths": [["business.business_operation.industry", "business.business_operation.industry"], ["base.schemastaging.corresponding_entity.entities", "base.schemastaging.entity_hint.corresponds_to"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["organization.organization.parent", "organization.organization_relationship.parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns volkswagen car company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_board_membership.organization <SEP> organization.organization_board_membership.member </PATH>", "<PATH> organization.organization.board_members <SEP> business.board_member.organization_board_memberships </PATH>", "<PATH> business.employer.employees <SEP> business.employment_tenure.person </PATH>"], "scores": [-0.02544246055185795, -0.030527183786034584, -0.032447606325149536, -0.03918488323688507, -0.039285726845264435], "norm_scores": [0.20159049332141876, 0.2005680501461029, 0.20018325746059418, 0.19883909821510315, 0.19881905615329742]}}
{"id": "WebQTest-769", "question": "which is the main train station in rome", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["trains.travel_destination.tourist_attractions"], ["travel.transportation.mode_of_transportation"], ["trainsdaily.travel_destination.travel_destinations"]], "ground_paths": [["travel.travel_destination.how_to_get_here", "travel.transportation.transport_terminus"], ["location.location.containedby", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which is the main train station in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> trains.travel_destination.tourist_attractions </PATH>", "<PATH> travel.transportation.mode_of_transportation </PATH>", "<PATH> trainsdaily.travel_destination.travel_destinations </PATH>"], "scores": [-0.0029188571497797966, -0.06394381076097488, -0.07250689715147018, -0.08639714121818542, -0.09129024296998978], "norm_scores": [0.2123633772134781, 0.19979141652584076, 0.1980878859758377, 0.19535541534423828, 0.19440186023712158]}}
{"id": "WebQTest-770", "question": "what did steve mcnair died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["deceased_person.cause_of_death"], ["people.person.deceased_cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did steve mcnair died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_cause_of_death </PATH>"], "scores": [-0.0009998367168009281, -0.05038956552743912, -0.11027535796165466, -0.11106518656015396, -0.1197844073176384], "norm_scores": [0.21588629484176636, 0.20548273622989655, 0.19353847205638885, 0.19338567554950714, 0.1917068362236023]}}
{"id": "WebQTest-771", "question": "who is the newly elected governor of california", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the newly elected governor of california [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.021045733243227005, -0.02314019575715065, -0.027619512751698494, -0.029362745583057404, -0.030386123806238174], "norm_scores": [0.20105449855327606, 0.20063383877277374, 0.19973714649677277, 0.1993892639875412, 0.1991852968931198]}}
{"id": "WebQTest-772", "question": "who plays roxanne in ghost rider", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.film.starring", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays roxanne in ghost rider [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.film.language <SEP> people.person.languages </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.021221552044153214, -0.02412048913538456, -0.029039088636636734, -0.032454706728458405, -0.04439195618033409], "norm_scores": [0.20180641114711761, 0.201222226023674, 0.20023491978645325, 0.19955217838287354, 0.197184219956398]}}
{"id": "WebQTest-773", "question": "where is the sony ericsson company", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the sony ericsson company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.0052216979674994946, -0.033420272171497345, -0.04572730138897896, -0.06909947097301483, -0.0704050064086914], "norm_scores": [0.20800770819187164, 0.20222410559654236, 0.1997506022453308, 0.19513611495494843, 0.19488151371479034]}}
{"id": "WebQTest-774", "question": "what kind of currency does the dominican republic have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"], ["location.country. currency_used"], ["location.location.containedby"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does the dominican republic have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.country. currency_used </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-6.341834523482248e-05, -0.10886675864458084, -0.134495347738266, -0.1768578290939331, -0.17902377247810364], "norm_scores": [0.22496122121810913, 0.20176924765110016, 0.19666388630867004, 0.18850672245025635, 0.18809886276721954]}}
{"id": "WebQTest-775", "question": "what to do this weekend in manchester with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["organization.organization.geographic_scope"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do this weekend in manchester with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.geographic_scope </PATH>"], "scores": [-0.012516629882156849, -0.016411935910582542, -0.028912780806422234, -0.11129920929670334, -0.11455190926790237], "norm_scores": [0.2088211476802826, 0.20800931751728058, 0.2054252028465271, 0.18917936086654663, 0.18856501579284668]}}
{"id": "WebQTest-776", "question": "what sport is popular in italy", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["sports.sports_teamlocation.teams"]], "ground_paths": [["sports.sports_team_location.teams"], ["cricket.cricket_team.country"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sport is popular in italy [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.sports_teamlocation.teams </PATH>"], "scores": [-0.010908057913184166, -0.014788194559514523, -0.12920303642749786, -0.15460538864135742, -0.172703355550766], "norm_scores": [0.21732863783836365, 0.21648702025413513, 0.19308215379714966, 0.18823915719985962, 0.18486306071281433]}}
{"id": "WebQTest-777", "question": "where is the main headquarters of google", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["location.location.contains"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.phone_number", "common.phone_number.service_location"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the main headquarters of google [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.015879768878221512, -0.025622889399528503, -0.035423099994659424, -0.050429172813892365, -0.056609030812978745], "norm_scores": [0.2042033076286316, 0.20222339034080505, 0.20025122165679932, 0.19726867973804474, 0.19605335593223572]}}
{"id": "WebQTest-778", "question": "what was jesse owens education", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse owens education [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.013005881570279598, -0.019359663128852844, -0.0327504463493824, -0.03360438719391823, -0.0690518245100975], "norm_scores": [0.20411399006843567, 0.20282119512557983, 0.2001233696937561, 0.19995254278182983, 0.19298888742923737]}}
{"id": "WebQTest-779", "question": "where does carlos santana live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.children", "people.person.place_of_birth"], ["symbols.namesake.named_after", "music.artist.origin"], ["people.person.nationality", "location.administrative_division.second_level_division_of"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.country.first_level_divisions"], ["symbols.namesake.named_after", "music.artist.origin"], ["people.person.nationality", "location.location.containedby"], ["music.artist.origin", "location.location.containedby"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.children", "people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does carlos santana live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after <SEP> music.artist.origin </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.second_level_division_of </PATH>"], "scores": [-0.01967656798660755, -0.02257789857685566, -0.028667764738202095, -0.04606054723262787, -0.06174571439623833], "norm_scores": [0.20321416854858398, 0.20262543857097626, 0.2013952136039734, 0.19792267680168152, 0.19484245777130127]}}
{"id": "WebQTest-780", "question": "what is the new movie john carter about", "prediction": [["film.film_story_contributor.film_story_credits"], ["film.film.subjects"], ["film.film.story_by"], ["film.film_subject.films"], ["film.film.film_casting_director"]], "ground_paths": [["film.film.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the new movie john carter about [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> film.film_subject.films </PATH>", "<PATH> film.film.film_casting_director </PATH>"], "scores": [-0.021520432084798813, -0.021726783365011215, -0.032762784510850906, -0.056709565222263336, -0.07874462008476257], "norm_scores": [0.20414747297763824, 0.20410533249378204, 0.20186524093151093, 0.19708862900733948, 0.19279326498508453]}}
{"id": "WebQTest-782", "question": "what countries does nafta include", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["common.topic.notable_types"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization.founders"]], "ground_paths": [["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_member.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does nafta include [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_membership.member </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.010876386426389217, -0.025840844959020615, -0.03805883973836899, -0.05472048744559288, -0.06639016419649124], "norm_scores": [0.20570066571235657, 0.20264537632465363, 0.20018452405929565, 0.19687676429748535, 0.19459262490272522]}}
{"id": "WebQTest-783", "question": "what county is houston tx in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.citytown.postal_codes"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is houston tx in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.citytown.postal_codes </PATH>"], "scores": [-0.005502518732100725, -0.034649088978767395, -0.04423169419169426, -0.07841389626264572, -0.1015562191605568], "norm_scores": [0.20958280563354492, 0.20356234908103943, 0.20162101089954376, 0.19484561681747437, 0.19038820266723633]}}
{"id": "WebQTest-784", "question": "where do the seattle seahawks play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the seattle seahawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.009105258621275425, -0.01581980660557747, -0.08489811420440674, -0.10320442914962769, -0.11426877975463867], "norm_scores": [0.2113858461380005, 0.20997123420238495, 0.19595640897750854, 0.19240181148052216, 0.19028474390506744]}}
{"id": "WebQTest-785", "question": "who does michael keaton play in cars", "prediction": [["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["film.performance.film", "film.film_character.portrayed_in_films"], ["fictional_universe.fictional_universe.works_set_here", "fictional_universe.fictional_universe.characters"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael keaton play in cars [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.02826201729476452, -0.0316239669919014, -0.03280101343989372, -0.037109170109033585, -0.03771412745118141], "norm_scores": [0.20104950666427612, 0.20037472248077393, 0.20013901591300964, 0.19927863776683807, 0.19915810227394104]}}
{"id": "WebQTest-787", "question": "where is the riviera casino", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"], ["government.governmental_jurisdiction.government"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the riviera casino [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>"], "scores": [-0.008931539952754974, -0.03672712296247482, -0.04449991509318352, -0.0465620793402195, -0.0655578225851059], "norm_scores": [0.20637023448944092, 0.20071303844451904, 0.19915898144245148, 0.19874869287014008, 0.19500896334648132]}}
{"id": "WebQTest-788", "question": "what is the holy book of the buddhist religion", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"], ["book.written_work.subjects"], ["religion.religion.texts", "religion.religious_text.religious_text_of"], ["common.topic.notable_types"]], "ground_paths": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the holy book of the buddhist religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> religion.religion.texts <SEP> religion.religious_text.religious_text_of </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.004655029624700546, -0.0273834690451622, -0.062135715037584305, -0.08101508021354675, -0.08312366157770157], "norm_scores": [0.20952580869197845, 0.20481732487678528, 0.1978217214345932, 0.19412200152873993, 0.19371311366558075]}}
{"id": "WebQTest-789", "question": "who became president when henry harrison died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who became president when henry harrison died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.02311115525662899, -0.025278611108660698, -0.03201015293598175, -0.043303195387125015, -0.051405493170022964], "norm_scores": [0.2023845911026001, 0.2019464075565338, 0.2005915492773056, 0.19833901524543762, 0.19673849642276764]}}
{"id": "WebQTest-790", "question": "where did richard arkwright grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["influence.influence_node.influenced_by", "location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard arkwright grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0016346651827916503, -0.04510187357664108, -0.11396948248147964, -0.1272241175174713, -0.1272459328174591], "norm_scores": [0.2166779786348343, 0.20746135711669922, 0.1936548501253128, 0.1911049634218216, 0.19110079109668732]}}
{"id": "WebQTest-791", "question": "what other books did louis sachar write", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"], ["book.author.book_editions_published"], ["film.film.story_by"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did louis sachar write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.01211878377944231, -0.02636711299419403, -0.028975345194339752, -0.05715518444776535, -0.07618667930364609], "norm_scores": [0.20563262701034546, 0.20272348821163177, 0.2021954208612442, 0.19657713174819946, 0.1928713470697403]}}
{"id": "WebQTest-792", "question": "when did roth ira originate", "prediction": [["finance.currency.countries_used"], ["common.topic.notable_types"], ["finance.currency.countries_formerly_used"], ["organization.organization.founders"], ["symbols.namesake.named_after"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did roth ira originate [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.013792595826089382, -0.02997387945652008, -0.03955643251538277, -0.06140661984682083, -0.08307339251041412], "norm_scores": [0.20639492571353912, 0.20308206975460052, 0.20114532113075256, 0.1967979222536087, 0.19257979094982147]}}
{"id": "WebQTest-794", "question": "who does the voice for darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02318977564573288, -0.024274319410324097, -0.028318120166659355, -0.03247243911027908, -0.04691701382398605], "norm_scores": [0.20156767964363098, 0.2013491839170456, 0.2005365937948227, 0.19970524311065674, 0.19684132933616638]}}
{"id": "WebQTest-796", "question": "what did thomas malthus do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas malthus do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0018148228991776705, -0.05601400136947632, -0.07033035904169083, -0.09683775901794434, -0.11286768317222595], "norm_scores": [0.21343505382537842, 0.20217494666576385, 0.19930115342140198, 0.19408762454986572, 0.19100120663642883]}}
{"id": "WebQTest-797", "question": "what is the zip code for newark california", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"], ["location.mailing_address.citytown"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for newark california [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.mailing_address.citytown </PATH>"], "scores": [-0.00013050014968030155, -0.09316153824329376, -0.15150946378707886, -0.1606956571340561, -0.16977368295192719], "norm_scores": [0.22389821708202362, 0.20400826632976532, 0.19244542717933655, 0.1906856745481491, 0.18896245956420898]}}
{"id": "WebQTest-798", "question": "what states share a border with tennessee", "prediction": [["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["time.event.locations", "location.location.events"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_children"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.location.containedby", "location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["base.locations.countries.states_provinces_within", "location.country.first_level_divisions"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.locations.countries.states_provinces_within", "location.administrative_division.first_level_division_of"], ["location.location.containedby", "base.usnationalparks.us_national_park.state"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["book.written_work.subjects", "book.written_work.subjects"], ["location.location.partially_contains", "geography.river.origin"], ["location.location.partially_contains", "location.location.partially_containedby"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "time.event.locations"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_parent"], ["base.locations.countries.states_provinces_within", "base.locations.countries.states_provinces_within"], ["location.location.events", "time.event.locations"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["base.locations.countries.states_provinces_within", "location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states share a border with tennessee [/INST]", "raw_output": {"paths": ["<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03324713185429573, -0.033479463309049606, -0.0353643074631691, -0.04074333235621452, -0.04129656404256821], "norm_scores": [0.20071586966514587, 0.20066924393177032, 0.20029136538505554, 0.19921687245368958, 0.19910669326782227]}}
{"id": "WebQTest-800", "question": "what war did the us lose the most soldiers", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.military_conflict"], ["military.military_command.military_combatant", "military.military_command.military_conflict"], ["military.casualties.combatant", "military.casualties.military_conflict"], ["military.military_conflict.casualties", "military.military_conflict.casualties"]], "ground_paths": [["military.military_combatant.military_commanders", "military.military_command.military_conflict"], ["military.military_command.military_combatant", "military.military_command.military_conflict"], ["location.location.partially_containedby", "location.location.events"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_combatant.casualties", "military.casualties.military_conflict"], ["people.person.nationality", "military.military_person.participated_in_conflicts"], ["military.military_combatant.military_commanders", "military.military_conflict.commanders"], ["military.military_command.military_combatant", "military.military_conflict.commanders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what war did the us lose the most soldiers [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.military_conflict </PATH>", "<PATH> military.military_command.military_combatant <SEP> military.military_command.military_conflict </PATH>", "<PATH> military.casualties.combatant <SEP> military.casualties.military_conflict </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>"], "scores": [-0.015094798058271408, -0.03029450960457325, -0.0358663871884346, -0.03765113651752472, -0.03820343688130379], "norm_scores": [0.20328465104103088, 0.20021815598011017, 0.1991056650876999, 0.19875061511993408, 0.1986408829689026]}}
{"id": "WebQTest-801", "question": "what kind of government is egypt", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is egypt [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>"], "scores": [-0.006207185797393322, -0.021887313574552536, -0.12182626873254776, -0.1241283267736435, -0.14022494852542877], "norm_scores": [0.21558240056037903, 0.2122284173965454, 0.19204393029212952, 0.19160234928131104, 0.188542902469635]}}
{"id": "WebQTest-802", "question": "what musical instruments does justin bieber play", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.recording_contribution.contributor", "music.track_contribution.role"], ["people.person.profession", "music.music_profession.specializations"]], "ground_paths": [["music.group_member.membership", "music.group_membership.role"], ["music.artist.track_contributions", "music.track_contribution.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what musical instruments does justin bieber play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.recording_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> people.person.profession <SEP> music.music_profession.specializations </PATH>"], "scores": [-0.0036600488238036633, -0.044990718364715576, -0.05504037067294121, -0.06308165192604065, -0.0987665206193924], "norm_scores": [0.21003927290439606, 0.201535165309906, 0.19951993227005005, 0.1979219764471054, 0.19098369777202606]}}
{"id": "WebQTest-805", "question": "where do john lennon die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["location.location.people_born_here"], ["people.person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do john lennon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_death </PATH>"], "scores": [-0.0006567227537743747, -0.07858260720968246, -0.09173382818698883, -0.1094454899430275, -0.11520528793334961], "norm_scores": [0.21613822877407074, 0.19993498921394348, 0.1973228007555008, 0.19385866820812225, 0.19274529814720154]}}
{"id": "WebQTest-809", "question": "what is the current time in bangalore india", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in bangalore india [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-0.00018056051339954138, -0.07134795933961868, -0.1765785813331604, -0.17966294288635254, -0.1931629329919815], "norm_scores": [0.2257443517446518, 0.210237056016922, 0.18923793733119965, 0.18865516781806946, 0.18612544238567352]}}
{"id": "WebQTest-810", "question": "what language do people from greece speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from greece speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.014114072546362877, -0.014209315180778503, -0.04948420077562332, -0.059741903096437454, -0.153764545917511], "norm_scores": [0.20875899493694305, 0.20873911678791046, 0.20150423049926758, 0.19944781064987183, 0.1815498024225235]}}
{"id": "WebQTest-811", "question": "where is the phillies spring training stadium", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["location.location.containedby"], ["location.location.contains"], ["baseball.baseball_team.arena_stadium"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the phillies spring training stadium [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> baseball.baseball_team.arena_stadium </PATH>"], "scores": [-0.003469844814389944, -0.046372123062610626, -0.04990842938423157, -0.07915695011615753, -0.08862362802028656], "norm_scores": [0.21016769111156464, 0.20134171843528748, 0.20063096284866333, 0.19484777748584747, 0.19301190972328186]}}
{"id": "WebQTest-812", "question": "what movies does matt dallas play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does matt dallas play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.012451025657355785, -0.03426679968833923, -0.03446044400334358, -0.03702085465192795, -0.037358589470386505], "norm_scores": [0.20375806093215942, 0.19936107099056244, 0.19932246208190918, 0.19881276786327362, 0.19874562323093414]}}
{"id": "WebQTest-813", "question": "who was john f kennedy 's family", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.parents", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.religion", "people.person.religion"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["film.film_subject.films", "film.film.subjects"], ["people.person.parents", "people.person.children"], ["people.family.members", "people.family_member.family"], ["people.family.members", "people.family.members"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.sibling_relationship.sibling", "people.person.sibling_s"], ["people.person.parents"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["people.person.profession", "people.person.profession"], ["people.person.profession", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["people.person.religion", "people.person.religion"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.gender", "people.person.gender"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["symbols.name_source.namesakes"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was john f kennedy 's family [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>"], "scores": [-0.032598551362752914, -0.032862838357686996, -0.03434627130627632, -0.04004976153373718, -0.04017707332968712], "norm_scores": [0.200681671500206, 0.2006286382675171, 0.20033124089241028, 0.19919191300868988, 0.19916655123233795]}}
{"id": "WebQTest-814", "question": "where is harley davidson corporate headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.place_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harley davidson corporate headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.005125387106090784, -0.04084678366780281, -0.04330485686659813, -0.04628640040755272, -0.05187464877963066], "norm_scores": [0.20654965937137604, 0.19930164515972137, 0.1988123506307602, 0.19822046160697937, 0.19711585342884064]}}
{"id": "WebQTest-818", "question": "where was the ottoman empire based", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.country.capital"], ["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the ottoman empire based [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.006983610335737467, -0.041280608624219894, -0.05995992198586464, -0.07775301486253738, -0.08045429736375809], "norm_scores": [0.2094009667634964, 0.20234091579914093, 0.19859641790390015, 0.1950940042734146, 0.1945677250623703]}}
{"id": "WebQTest-819", "question": "where was bin laden found and killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_conflict.military_personnel_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was bin laden found and killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> military.military_conflict.military_personnel_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.020581291988492012, -0.03144126012921333, -0.03436177596449852, -0.03907407820224762, -0.03991420567035675], "norm_scores": [0.20250938832759857, 0.20032203197479248, 0.19973786175251007, 0.19879883527755737, 0.1986318975687027]}}
{"id": "WebQTest-820", "question": "what are the names of harry potter movies in order", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["fictional_universe.fictional_universe.works_set_here"], ["film.film_series.films_in_series"], ["book.book_character.appears_in_book"], ["book.written_work.subjects"]], "ground_paths": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["film.film_series.films_in_series"], ["fictional_universe.fictional_universe.works_set_here"], ["film.film.film_series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of harry potter movies in order [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>", "<PATH> film.film_series.films_in_series </PATH>", "<PATH> book.book_character.appears_in_book </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.006587139796465635, -0.03297458961606026, -0.03774338215589523, -0.041424307972192764, -0.05904298648238182], "norm_scores": [0.2058485448360443, 0.20048774778842926, 0.19953396916389465, 0.19880083203315735, 0.19532889127731323]}}
{"id": "WebQTest-821", "question": "what did pamela courson die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did pamela courson die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.002355810720473528, -0.0371447317302227, -0.11456578969955444, -0.12316638976335526, -0.1325361430644989], "norm_scores": [0.21627210080623627, 0.2088775932788849, 0.1933162361383438, 0.19166073203086853, 0.18987329304218292]}}
{"id": "WebQTest-822", "question": "what did dr jack kevorkian do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["base.activism.activist.area_of_activism"], ["film.film.subjects"]], "ground_paths": [["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr jack kevorkian do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> film.film.subjects </PATH>"], "scores": [-0.0017838135827332735, -0.049072250723838806, -0.07177595049142838, -0.13102295994758606, -0.16036532819271088], "norm_scores": [0.21652844548225403, 0.2065274715423584, 0.20189137756824493, 0.1902773529291153, 0.18477527797222137]}}
{"id": "WebQTest-823", "question": "what does bolivia border", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["time.event.locations", "location.location.events"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["location.country.languages_spoken", "location.country.official_language"], ["location.location.partially_containedby", "location.location.partially_contains"], ["people.deceased_person.place_of_death", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["language.language_family.geographic_distribution", "language.human_language.main_country"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["symbols.name_source.namesakes", "people.person.nationality"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_containedby"], ["time.event.locations", "time.event.locations"], ["language.language_family.geographic_distribution", "location.country.official_language"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.contains", "location.location.contains"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["location.country.official_language", "location.country.languages_spoken"], ["location.location.contains", "location.location.containedby"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.country.official_language", "language.human_language.main_country"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does bolivia border [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03487269580364227, -0.03628968447446823, -0.03851880133152008, -0.04193640127778053, -0.043175071477890015], "norm_scores": [0.20081782341003418, 0.20053347945213318, 0.20008695125579834, 0.1994042992591858, 0.1991574615240097]}}
{"id": "WebQTest-825", "question": "who does nolan ryan play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does nolan ryan play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.008013376966118813, -0.017922930419445038, -0.04077600687742233, -0.04875180497765541, -0.07906236499547958], "norm_scores": [0.2062108963727951, 0.20417752861976624, 0.1995643526315689, 0.19797900319099426, 0.1920681744813919]}}
{"id": "WebQTest-827", "question": "what country does justin bieber come from", "prediction": [["people.person.nationality"], ["music.artist.origin"], ["people.person.place_of_birth"], ["people.person.ethnicity"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does justin bieber come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.00410600658506155, -0.04249389097094536, -0.050419874489307404, -0.08516207337379456, -0.09700973331928253], "norm_scores": [0.21050392091274261, 0.20257626473903656, 0.20097698271274567, 0.19411450624465942, 0.19182826578617096]}}
{"id": "WebQTest-828", "question": "what is the head judge of the supreme court called", "prediction": [["law.judicial_tenure.title", "law.judge.courts"], ["law.court.jurisdiction", "people.person.nationality"], ["law.judicial_tenure.court", "law.judge.courts"], ["law.court.jurisdiction", "government.government_office_or_title.office_holders"], ["law.court.jurisdiction", "government.government_position_held.office_holder"]], "ground_paths": [["government.governmental_body.offices_positions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the head judge of the supreme court called [/INST]", "raw_output": {"paths": ["<PATH> law.judicial_tenure.title <SEP> law.judge.courts </PATH>", "<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> law.judicial_tenure.court <SEP> law.judge.courts </PATH>", "<PATH> law.court.jurisdiction <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> law.court.jurisdiction <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.028298240154981613, -0.03413369879126549, -0.039229582995176315, -0.04334316402673721, -0.04444386065006256], "norm_scores": [0.2019238919019699, 0.2007489949464798, 0.1997286081314087, 0.19890868663787842, 0.19868987798690796]}}
{"id": "WebQTest-829", "question": "who was frida kahlo influenced by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was frida kahlo influenced by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.011476590298116207, -0.012283439747989178, -0.07342125475406647, -0.07425206899642944, -0.08472944796085358], "norm_scores": [0.20800188183784485, 0.2078341394662857, 0.19550824165344238, 0.19534587860107422, 0.19330984354019165]}}
{"id": "WebQTest-830", "question": "where is taylor swift from", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["music.album.artist"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is taylor swift from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.015308448113501072, -0.02169809117913246, -0.031698886305093765, -0.10462018102407455, -0.10537609457969666], "norm_scores": [0.20808173716068268, 0.20675639808177948, 0.20469897985458374, 0.19030332565307617, 0.19015952944755554]}}
{"id": "WebQTest-831", "question": "what type of religion does argentina have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["religion.religion.countries"], ["royalty.kingdom.rulers", "people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion does argentina have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> religion.religion.countries </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>"], "scores": [-0.009161077439785004, -0.014042716473340988, -0.07491124421358109, -0.08623756468296051, -0.090655617415905], "norm_scores": [0.20924653112888336, 0.20822754502296448, 0.195931077003479, 0.19372442364692688, 0.19287042319774628]}}
{"id": "WebQTest-832", "question": "when is the last time the giants won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["time.participant.event"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the giants won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.participant.event </PATH>"], "scores": [-0.0073603433556854725, -0.017218947410583496, -0.08146824687719345, -0.08856169134378433, -0.10322509706020355], "norm_scores": [0.21055485308170319, 0.20848926901817322, 0.1955152451992035, 0.19413326680660248, 0.19130738079547882]}}
{"id": "WebQTest-833", "question": "what is the main language used in china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.administrative_divisions"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language used in china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.006637252401560545, -0.030707698315382004, -0.04169381037354469, -0.07239328324794769, -0.1389831006526947], "norm_scores": [0.21034188568592072, 0.20533931255340576, 0.20309579372406006, 0.1969555914402008, 0.18426747620105743]}}
{"id": "WebQTest-834", "question": "where did the battle of waterloo occur", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["military.military_conflict.casualties"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of waterloo occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> military.military_conflict.casualties </PATH>"], "scores": [-0.0030861201230436563, -0.03807714208960533, -0.11486928164958954, -0.1288774609565735, -0.14037324488162994], "norm_scores": [0.2167612463235855, 0.2093077003955841, 0.19383618235588074, 0.1911398023366928, 0.18895508348941803]}}
{"id": "WebQTest-836", "question": "who did vladimir lenin marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did vladimir lenin marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.025300223380327225, -0.0442955456674099, -0.04456208273768425, -0.04649501293897629, -0.0479561947286129], "norm_scores": [0.20330436527729034, 0.19947898387908936, 0.1994258165359497, 0.1990407109260559, 0.1987500935792923]}}
{"id": "WebQTest-837", "question": "what year did the baltimore ravens win superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the baltimore ravens win superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.013352468609809875, -0.014688689261674881, -0.03229910880327225, -0.06570379436016083, -0.0908803790807724], "norm_scores": [0.20600323379039764, 0.20572815835475922, 0.20213690400123596, 0.1954961121082306, 0.19063563644886017]}}
{"id": "WebQTest-838", "question": "what team does cristiano ronaldo play for 2011", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does cristiano ronaldo play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01994761824607849, -0.02219296619296074, -0.024506280198693275, -0.03612392395734787, -0.040274713188409805], "norm_scores": [0.20173326134681702, 0.20128081738948822, 0.20081572234630585, 0.19849620759487152, 0.1976739913225174]}}
{"id": "WebQTest-841", "question": "what is money called in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is money called in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.009981396608054638, -0.017607057467103004, -0.10255515575408936, -0.11906345188617706, -0.13963373005390167], "norm_scores": [0.21371892094612122, 0.21209536492824554, 0.19482232630252838, 0.19163253903388977, 0.18773087859153748]}}
{"id": "WebQTest-843", "question": "where did theodor schwann study", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann study [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.021134471520781517, -0.02166927047073841, -0.02390890382230282, -0.028435571119189262, -0.06020861119031906], "norm_scores": [0.20197534561157227, 0.20186737179756165, 0.2014157474040985, 0.2005060613155365, 0.19423553347587585]}}
{"id": "WebQTest-845", "question": "who are the virginia senators 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "ground_paths": [["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the virginia senators 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.016123438253998756, -0.02034018561244011, -0.027911316603422165, -0.03331996500492096, -0.03440960869193077], "norm_scores": [0.20206494629383087, 0.20121468603610992, 0.19969700276851654, 0.19861984252929688, 0.1984035223722458]}}
{"id": "WebQTest-846", "question": "what was dr seuss 's real name", "prediction": [["book.book.interior_illustrations_by", "people.person.nationality"], ["people.person.profession"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["book.book.interior_illustrations_by", "people.person.interred_names"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss 's real name [/INST]", "raw_output": {"paths": ["<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.interred_names </PATH>"], "scores": [-0.022366084158420563, -0.0319109708070755, -0.03649970516562462, -0.043046735227108, -0.04925942420959473], "norm_scores": [0.20286186039447784, 0.2009347826242447, 0.2000148445367813, 0.1987096220254898, 0.19747892022132874]}}
{"id": "WebQTest-847", "question": "what countries are members of the security council", "prediction": [["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_body_membership.member"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_body_membership.member"], ["government.governmental_body.members", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_body_membership.member"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are members of the security council [/INST]", "raw_output": {"paths": ["<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body.members <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_body_membership.member </PATH>", "<PATH> base.unitednations.united_nations_body.members <SEP> base.unitednations.united_nations_body_membership.member </PATH>", "<PATH> government.governmental_body.members <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.010378656908869743, -0.012842315249145031, -0.026097305119037628, -0.032588571310043335, -0.0687781572341919], "norm_scores": [0.2039462924003601, 0.2034444659948349, 0.20076559484004974, 0.19946660101413727, 0.19237704575061798]}}
{"id": "WebQTest-848", "question": "where are the nordic countries", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nordic countries [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.008705084212124348, -0.02616853080689907, -0.05323414131999016, -0.11519575119018555, -0.14924709498882294], "norm_scores": [0.21244925260543823, 0.20877137780189514, 0.20319662988185883, 0.19098836183547974, 0.18459443747997284]}}
{"id": "WebQTest-849", "question": "what country sponsored ferdinand magellan", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved"], ["people.person.sponsorship", "people.person.nationality"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country sponsored ferdinand magellan [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.person.sponsorship <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.003618198214098811, -0.05732649564743042, -0.06761794537305832, -0.06902012228965759, -0.07939691841602325], "norm_scores": [0.21055176854133606, 0.19954168796539307, 0.19749866425991058, 0.19722191989421844, 0.19518597424030304]}}
{"id": "WebQTest-850", "question": "what did vera ellen die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["people.people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did vera ellen die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>"], "scores": [-0.002376520074903965, -0.03750362992286682, -0.1304304450750351, -0.13074922561645508, -0.1381186842918396], "norm_scores": [0.21749143302440643, 0.2099841982126236, 0.19135025143623352, 0.1912892609834671, 0.18988476693630219]}}
{"id": "WebQTest-851", "question": "what is the capital of modern day egypt", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.capital"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern day egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.010420899838209152, -0.024710403755307198, -0.03868182376027107, -0.06715993583202362, -0.07454103231430054], "norm_scores": [0.20658256113529205, 0.2036515772342682, 0.20082606375217438, 0.19518759846687317, 0.1937521994113922]}}
{"id": "WebQTest-852", "question": "what province is toronto in ( 7 letters )", "prediction": [["location.province.capital"], ["base.biblioness.bibs_location.state"], ["base.biblioness.bibs_location.province"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.province.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is toronto in ( 7 letters ) [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.biblioness.bibs_location.province </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.002837679348886013, -0.0427415706217289, -0.05916932597756386, -0.07635073363780975, -0.07948791235685349], "norm_scores": [0.2100202739238739, 0.20180466771125793, 0.19851654767990112, 0.19513487815856934, 0.1945236623287201]}}
{"id": "WebQTest-855", "question": "what is the currency of brazil now", "prediction": [["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_used"], ["location.country.currency_used", "location.country.currency_used"], ["location.country.currency_used", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>", "<PATH> location.country.currency_used <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.0012762082042172551, -0.05973437801003456, -0.06764031946659088, -0.15914610028266907, -0.1623861938714981], "norm_scores": [0.21814213693141937, 0.20575553178787231, 0.2041352540254593, 0.1862848550081253, 0.1856822520494461]}}
{"id": "WebQTest-856", "question": "what types of art did andy warhol do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.art_form.artists"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what types of art did andy warhol do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.art_form.artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.006767767947167158, -0.020268553867936134, -0.059279266744852066, -0.0791151374578476, -0.09728552401065826], "norm_scores": [0.20924444496631622, 0.20643846690654755, 0.19854021072387695, 0.19464078545570374, 0.19113603234291077]}}
{"id": "WebQTest-857", "question": "what other states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["location.location.partially_contains", "location.location.partially_containedby"]], "ground_paths": [["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.containedby", "location.location.contains"], ["location.location.contains", "location.location.contains"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.02574295923113823, -0.03685680404305458, -0.03884919360280037, -0.04040955379605293, -0.04045654088258743], "norm_scores": [0.20215246081352234, 0.19991819560527802, 0.1995202749967575, 0.19920921325683594, 0.199199840426445]}}
{"id": "WebQTest-859", "question": "what are the predominant religions in russia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["language.human_language.main_country", "location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the predominant religions in russia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.008359105326235294, -0.01270546019077301, -0.10692006349563599, -0.10929480195045471, -0.12771691381931305], "norm_scores": [0.21307037770748138, 0.21214629709720612, 0.19307169318199158, 0.19261373579502106, 0.18909786641597748]}}
{"id": "WebQTest-861", "question": "who will coach carolina panthers in 2013", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team.sport", "sports.sport.team_coaches"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will coach carolina panthers in 2013 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.team_coaches </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.004110388457775116, -0.034904591739177704, -0.03595193475484848, -0.03695523738861084, -0.040870361030101776], "norm_scores": [0.20534171164035797, 0.19911473989486694, 0.1989063173532486, 0.19870685040950775, 0.19793041050434113]}}
{"id": "WebQTest-862", "question": "where did louis sachar grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did louis sachar grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.0043587712571024895, -0.028060581535100937, -0.09660203009843826, -0.10126861929893494, -0.10581545531749725], "norm_scores": [0.21278323233127594, 0.20779919624328613, 0.19403347373008728, 0.19313012063503265, 0.192253977060318]}}
{"id": "WebQTest-865", "question": "what to do for fun in houston", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["metropolitan_transit.transit_system.area_served"], ["base.usnationalparks.us_national_park.state"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do for fun in houston [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>"], "scores": [-0.01354123279452324, -0.018111398443579674, -0.027500376105308533, -0.12038809061050415, -0.13115207850933075], "norm_scores": [0.20967510342597961, 0.20871905982494354, 0.20676855742931366, 0.18842732906341553, 0.18640996515750885]}}
{"id": "WebQTest-866", "question": "where did otto frank go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did otto frank go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.014377262443304062, -0.018288100138306618, -0.026731669902801514, -0.044425804167985916, -0.06927710771560669], "norm_scores": [0.20404845476150513, 0.20325200259685516, 0.20154304802417755, 0.19800829887390137, 0.1931481659412384]}}
{"id": "WebQTest-867", "question": "what else has jennifer lawrence played in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else has jennifer lawrence played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.023116296157240868, -0.03138766065239906, -0.032539002597332, -0.033620383590459824, -0.035192083567380905], "norm_scores": [0.20161566138267517, 0.19995489716529846, 0.19972480833530426, 0.19950895011425018, 0.19919562339782715]}}
{"id": "WebQTest-868", "question": "who is the governor of pennsylvania state now", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of pennsylvania state now [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>"], "scores": [-0.024136876687407494, -0.02466907724738121, -0.025143269449472427, -0.027540573850274086, -0.03100326471030712], "norm_scores": [0.20047226548194885, 0.2003656029701233, 0.2002706080675125, 0.1997910887002945, 0.19910044968128204]}}
{"id": "WebQTest-869", "question": "where is ancient phoenician", "prediction": [["location.location.containedby"], ["language.human_language.countries_spoken_in"], ["location.location.contains"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ancient phoenician [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.02307872660458088, -0.026808103546500206, -0.0304032564163208, -0.05268486216664314, -0.05525803938508034], "norm_scores": [0.20291629433631897, 0.20216093957424164, 0.20143544673919678, 0.1969967782497406, 0.19649052619934082]}}
{"id": "WebQTest-870", "question": "where does the celtics practice", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.professional_sports_team.owner_s", "sports.sports_team.arena_stadium"], ["sports.professional_sports_team.owner_s", "sports.sports_facility.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the celtics practice [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.002139192307367921, -0.039986997842788696, -0.11368408799171448, -0.1258438676595688, -0.1401871144771576], "norm_scores": [0.21682527661323547, 0.20877227187156677, 0.19393964111804962, 0.1915956437587738, 0.18886716663837433]}}
{"id": "WebQTest-872", "question": "where bin laden got killed", "prediction": [["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_conflict.military_personnel_involved", "time.event.locations"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where bin laden got killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> military.military_conflict.military_personnel_involved <SEP> time.event.locations </PATH>"], "scores": [-0.013367542065680027, -0.02775372564792633, -0.04415716975927353, -0.046056993305683136, -0.048431333154439926], "norm_scores": [0.20455004274845123, 0.2016284018754959, 0.19834798574447632, 0.19797152280807495, 0.1975020468235016]}}
{"id": "WebQTest-873", "question": "where does toronto get its water from", "prediction": [["location.location.containedby"], ["base.infrastructure.sewage_treatment_plant.serves_location_s"], ["base.infrastructure.sewage_treatment_plant.serves_location_s", "base.infrastructure.sewage_treatment_plant.serves_location_s"], ["location.location.containedby", "base.infrastructure.sewage_treatment_plant.serves_location_s"], ["base.infrastructure.sewage_treatment_plant.serves_location"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does toronto get its water from [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s <SEP> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> location.location.containedby <SEP> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location </PATH>"], "scores": [-0.014731502160429955, -0.024912148714065552, -0.04059208929538727, -0.05010875687003136, -0.05696164816617966], "norm_scores": [0.20457293093204498, 0.20250080525875092, 0.19935038685798645, 0.19746221601963043, 0.1961136758327484]}}
{"id": "WebQTest-874", "question": "what language do people from bosnia speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from bosnia speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.01459224708378315, -0.02115226909518242, -0.028626954182982445, -0.06399539858102798, -0.1558820754289627], "norm_scores": [0.2083524465560913, 0.20699012279510498, 0.205448716878891, 0.19830931723117828, 0.18089944124221802]}}
{"id": "WebQTest-875", "question": "where did hugo chavez go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.contains"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hugo chavez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>"], "scores": [-0.015518445521593094, -0.02068229764699936, -0.028305809944868088, -0.028338173404335976, -0.049010660499334335], "norm_scores": [0.2025739848613739, 0.2015306055545807, 0.2000000774860382, 0.19999362528324127, 0.19590167701244354]}}
{"id": "WebQTest-876", "question": "who does japan import from", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.country.form_of_government", "location.country.form_of_government"], ["location.imports_and_exports.exported_to", "location.statistical_region.places_exported_to"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["base.aareas.schema.administrative_area.administrative_parent", "base.locations.countries.planet"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["film.film.featured_film_locations", "film.film.country"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["travel.travel_destination.tour_operators", "organization.organization.geographic_scope"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["fictional_universe.fictional_universe.locations", "fictional_universe.fictional_universe.locations"], ["cvg.computer_game_region.versions_released_in_this_region", "cvg.computer_game_region.versions_released_in_this_region"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.statistical_region.places_imported_from", "location.imports_and_exports.imported_from"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does japan import from [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.026762982830405235, -0.03343173488974571, -0.035908788442611694, -0.03735740855336189, -0.0379839725792408], "norm_scores": [0.20150917768478394, 0.20016983151435852, 0.19967462122440338, 0.19938558340072632, 0.1992606818675995]}}
{"id": "WebQTest-877", "question": "what is the name of sonia gandhi daughter", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"], ["people.person.parent"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of sonia gandhi daughter [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.008549513295292854, -0.019098471850156784, -0.11000264436006546, -0.14096033573150635, -0.14302439987659454], "norm_scores": [0.21536937355995178, 0.21310938894748688, 0.19459129869937897, 0.18865950405597687, 0.18827049434185028]}}
{"id": "WebQTest-878", "question": "who do islamic people pray to", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["base.argumentmaps.intellectual_dispute.positions"], ["base.argumentmaps.intellectual_dispute.positions", "book.book_subject.works"], ["religion.belief.belief_of"]], "ground_paths": [["religion.deity.deity_of"], ["religion.religion.deities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people pray to [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions <SEP> book.book_subject.works </PATH>", "<PATH> religion.belief.belief_of </PATH>"], "scores": [-0.01312527246773243, -0.013724480755627155, -0.06417979300022125, -0.071212038397789, -0.09871067851781845], "norm_scores": [0.2078496217727661, 0.20772512257099152, 0.19750431180000305, 0.1961202770471573, 0.1908007115125656]}}
{"id": "WebQTest-879", "question": "where was mary todd lincoln from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mary todd lincoln from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.003594303270801902, -0.032793253660202026, -0.09395382553339005, -0.09549804031848907, -0.10544443130493164], "norm_scores": [0.2127576321363449, 0.20663514733314514, 0.1943759322166443, 0.19407601654529572, 0.19215522706508636]}}
{"id": "WebQTest-880", "question": "where did charles darwin die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>"], "scores": [-0.0008638306171633303, -0.07093995809555054, -0.07486574351787567, -0.0866786390542984, -0.0977310836315155], "norm_scores": [0.21338188648223877, 0.19894079864025116, 0.19816134870052338, 0.19583424925804138, 0.1936817318201065]}}
{"id": "WebQTest-881", "question": "what movies does nick cannon play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["music.artist.track", "music.recording.releases"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does nick cannon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.01633538492023945, -0.03535301983356476, -0.03564905375242233, -0.038489650934934616, -0.042586516588926315], "norm_scores": [0.20349137485027313, 0.19965800642967224, 0.19959890842437744, 0.1990327388048172, 0.19821900129318237]}}
{"id": "WebQTest-882", "question": "when did the big earthquake hit san francisco", "prediction": [["time.event.locations"], ["location.location.events"], ["event.disaster.areas_affected"], ["meteorology.tropical_cyclone.affected_areas"], ["time.recurring_event.current_frequency"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the big earthquake hit san francisco [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.0071335830725729465, -0.0290687195956707, -0.0487184002995491, -0.07126395404338837, -0.07428957521915436], "norm_scores": [0.2078784555196762, 0.20336824655532837, 0.19941113889217377, 0.19496560096740723, 0.194376602768898]}}
{"id": "WebQTest-883", "question": "who was king george vi married to", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.spouse_s", "people.person.spouse_s"], ["film.film.subjects", "film.film_subject.films"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.family.members", "people.family.members"], ["people.person.religion", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king george vi married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>"], "scores": [-0.03480067104101181, -0.040071602910757065, -0.04087068513035774, -0.040976084768772125, -0.04373598471283913], "norm_scores": [0.20106001198291779, 0.2000030279159546, 0.19984325766563416, 0.19982220232486725, 0.19927147030830383]}}
{"id": "WebQTest-884", "question": "where was jesus after he died on the cross", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["time.event.locations"], ["symbols.name_source.namesakes"], ["people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was jesus after he died on the cross [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.008949718438088894, -0.02832387574017048, -0.06663056463003159, -0.0701189860701561, -0.07318119704723358], "norm_scores": [0.20819386839866638, 0.20419910550117493, 0.19652482867240906, 0.1958404779434204, 0.19524168968200684]}}
{"id": "WebQTest-885", "question": "who is sir james dyson", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced_by"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir james dyson [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0006789044127799571, -0.06801861524581909, -0.10299643874168396, -0.11974594742059708, -0.12602142989635468], "norm_scores": [0.2170344740152359, 0.20290066301822662, 0.19592632353305817, 0.1926719844341278, 0.19146664440631866]}}
{"id": "WebQTest-886", "question": "who voiced darth vader in the original star wars movies", "prediction": [["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader in the original star wars movies [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.022288603708148003, -0.022514397278428078, -0.025504274293780327, -0.029486212879419327, -0.03615894168615341], "norm_scores": [0.200980082154274, 0.20093470811843872, 0.200334832072258, 0.1995387077331543, 0.198211669921875]}}
{"id": "WebQTest-887", "question": "what high school did maya angelou go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what high school did maya angelou go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.016225462779402733, -0.020547950640320778, -0.022250160574913025, -0.03184707462787628, -0.03644237667322159], "norm_scores": [0.2018503099679947, 0.20097969472408295, 0.20063787698745728, 0.19872158765792847, 0.19781050086021423]}}
{"id": "WebQTest-888", "question": "what 5 countries border switzerland", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["time.event.locations", "location.location.events"], ["location.country.form_of_government", "location.country.form_of_government"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.languages_spoken", "language.human_language.main_country"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["base.mystery.cryptid_classification.area_of_occurrence", "base.mystery.cryptid_observation_location.cryptid_s_occurring_here"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["location.location.events", "location.location.events"], ["location.country.official_language", "location.country.official_language"], ["geography.river.origin", "location.location.partially_contains"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_contains"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["geography.lake.basin_countries", "location.location.partially_contains"], ["film.film_location.featured_in_films", "film.film.featured_film_locations"], ["base.aareas.schema.administrative_area.subdivides_place"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["geography.lake.basin_countries", "location.location.partially_containedby"], ["location.location.partially_contains", "location.location.partially_contains"], ["book.written_work.subjects", "book.written_work.subjects"], ["location.location.containedby", "base.locations.countries.continent"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "time.event.locations"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["fictional_universe.work_of_fiction.setting", "fictional_universe.fictional_setting.works_set_here"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["film.film.country", "film.film.country"], ["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.is_reviewed"], ["location.location.contains", "location.location.contains"], ["location.location.contains", "location.location.containedby"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["people.person.nationality", "people.person.nationality"], ["location.location.containedby", "base.nations.supranational_body.member"], ["location.location.events", "time.event.locations"], ["location.location.contains", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 5 countries border switzerland [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03585440665483475, -0.0401269756257534, -0.046643540263175964, -0.0467986986041069, -0.04773074388504028], "norm_scores": [0.20151886343955994, 0.20065969228744507, 0.1993563324213028, 0.19932541251182556, 0.19913971424102783]}}
{"id": "WebQTest-889", "question": "what country did benito mussolini govern", "prediction": [["people.person.nationality"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.governmental_jurisdiction.governing_officials"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did benito mussolini govern [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.01615769974887371, -0.02707407809793949, -0.03049818053841591, -0.03567207604646683, -0.0499604195356369], "norm_scores": [0.20315533876419067, 0.20094968378543854, 0.20026277005672455, 0.19922931492328644, 0.1964028924703598]}}
{"id": "WebQTest-891", "question": "who was the leader of the first communist party of china", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization.founders", "influence.influence_node.influenced"], ["government.political_party_tenure.party", "government.political_party_tenure.politician"], ["organization.organization.founders", "influence.influence_node.influenced_by"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the first communist party of china [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.founders <SEP> influence.influence_node.influenced </PATH>", "<PATH> government.political_party_tenure.party <SEP> government.political_party_tenure.politician </PATH>", "<PATH> organization.organization.founders <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.003870669286698103, -0.029104985296726227, -0.08360413461923599, -0.08375301957130432, -0.0851479321718216], "norm_scores": [0.21081006526947021, 0.2055569738149643, 0.19465409219264984, 0.1946251094341278, 0.19435381889343262]}}
{"id": "WebQTest-892", "question": "what did miles davis died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.people.deceased_person.cause_of_death"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did miles davis died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.002172359498217702, -0.03898392617702484, -0.12625160813331604, -0.14761237800121307, -0.1513935774564743], "norm_scores": [0.21866585314273834, 0.21076278388500214, 0.19314968585968018, 0.18906763195991516, 0.18835407495498657]}}
{"id": "WebQTest-893", "question": "what is the zip code for azle texas", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for azle texas [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.Citytown.postal_codes </PATH>"], "scores": [-0.00013004637730773538, -0.09780392795801163, -0.1310117542743683, -0.1703309267759323, -0.17372703552246094], "norm_scores": [0.2237938940525055, 0.20296867191791534, 0.1963392198085785, 0.1887691169977188, 0.18812911212444305]}}
{"id": "WebQTest-894", "question": "what date did sally pearson won gold", "prediction": [["people.person.nationality", "olympics.olympic_participating_country.olympics_participated_in"], ["people.person.profession", "common.topic.notable_types"], ["people.person.nationality", "olympics.olympic_games.participating_countries"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.olympics"], ["common.topic.image", "common.image.appears_in_topic_gallery"]], "ground_paths": [["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.olympics"], ["olympics.olympic_athlete_affiliation.athlete", "olympics.olympic_athlete_affiliation.olympics"], ["people.person.nationality", "olympics.olympic_games.participating_countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what date did sally pearson won gold [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.olympics </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>"], "scores": [-0.02037152834236622, -0.027586117386817932, -0.033005207777023315, -0.03690128028392792, -0.03922510892152786], "norm_scores": [0.20221686363220215, 0.20076318085193634, 0.1996781826019287, 0.1989017277956009, 0.1984400600194931]}}
{"id": "WebQTest-895", "question": "when did the detroit pistons last win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.runner_up"], ["sports.sports_teams.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the detroit pistons last win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_teams.champion </PATH>"], "scores": [-0.011059578508138657, -0.011570645496249199, -0.11428362876176834, -0.11950764805078506, -0.15162597596645355], "norm_scores": [0.2142472118139267, 0.2141377478837967, 0.1932348906993866, 0.1922280490398407, 0.1861521303653717]}}
{"id": "WebQTest-896", "question": "when did the philadelphia flyers win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_teams.champion"]], "ground_paths": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the philadelphia flyers win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_teams.champion </PATH>"], "scores": [-0.009122321382164955, -0.014289257116615772, -0.10631083697080612, -0.12666606903076172, -0.1475454717874527], "norm_scores": [0.21449731290340424, 0.21339187026023865, 0.1946316361427307, 0.1907099038362503, 0.1867692917585373]}}
{"id": "WebQTest-898", "question": "what county is st paul nc in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul nc in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01027058344334364, -0.0282636359333992, -0.03609541431069374, -0.048474833369255066, -0.07501349598169327], "norm_scores": [0.20590968430042267, 0.20223785936832428, 0.20066016912460327, 0.19819141924381256, 0.1930008828639984]}}
{"id": "WebQTest-899", "question": "where is olympic national park wa", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park wa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.008827951736748219, -0.030949126929044724, -0.031207559630274773, -0.07740987837314606, -0.09630455076694489], "norm_scores": [0.20807550847530365, 0.2035231590270996, 0.20347057282924652, 0.1942836195230484, 0.190647155046463]}}
{"id": "WebQTest-900", "question": "where did nat turner 1831 rebellion occur", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["event.disaster.areas_attack_process"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did nat turner 1831 rebellion occur [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> event.disaster.areas_attack_process </PATH>"], "scores": [-0.012213882990181446, -0.013904980383813381, -0.05838287994265556, -0.08681174367666245, -0.10162805020809174], "norm_scores": [0.2085168957710266, 0.20816458761692047, 0.19910873472690582, 0.19352799654006958, 0.19068177044391632]}}
{"id": "WebQTest-901", "question": "who would play luke skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who would play luke skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.02074906975030899, -0.021671099588274956, -0.022997774183750153, -0.036099374294281006, -0.03887733817100525], "norm_scores": [0.20146529376506805, 0.2012796252965927, 0.2010127604007721, 0.19839635491371155, 0.19784598052501678]}}
{"id": "WebQTest-903", "question": "where turkish people originate", "prediction": [["people.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "people.ethnicity.geographic_distribution"], ["people.ethnicity.included_in_group"], ["people.ethnicity.geographic_distribution", "base.locations.continents.countries_within"], ["people.ethnicity.geographic_distribution", "base.locations.countries.continent"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where turkish people originate [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.continents.countries_within </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.countries.continent </PATH>"], "scores": [-7.328471838263795e-05, -0.1263764649629593, -0.13429374992847443, -0.14026036858558655, -0.1414414495229721], "norm_scores": [0.2225629836320877, 0.1961553543806076, 0.19460846483707428, 0.19345077872276306, 0.19322243332862854]}}
{"id": "WebQTest-904", "question": "what state is barack obama from", "prediction": [["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_parent"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["base.popstra.celebrity.vacations_in", "base.popstra.location.vacationers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is barack obama from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.state </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.01789906993508339, -0.023080788552761078, -0.04015621915459633, -0.044571977108716965, -0.044784896075725555], "norm_scores": [0.20325316488742828, 0.202202707529068, 0.19877929985523224, 0.19790348410606384, 0.19786134362220764]}}
{"id": "WebQTest-905", "question": "what years did joe montana win super bowl", "prediction": [["sports.sports_championship_event.champion"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_championship_event.champion"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team.championships"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what years did joe montana win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.006039489526301622, -0.02913433127105236, -0.04886569455265999, -0.04897338151931763, -0.05085274577140808], "norm_scores": [0.2062111347913742, 0.20150329172611237, 0.19756631553173065, 0.19754505157470703, 0.19717414677143097]}}
{"id": "WebQTest-908", "question": "what awards gary paulsen won", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_winner.awards_won", "award.award_category.winners"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_winner.awards_won", "award.award_category.winners"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_honor.award_winner", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards gary paulsen won [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_category.winners </PATH>"], "scores": [-0.020454902201890945, -0.024533141404390335, -0.024844564497470856, -0.03041810169816017, -0.03054867312312126], "norm_scores": [0.20114275813102722, 0.20032411813735962, 0.20026174187660217, 0.19914868474006653, 0.19912268221378326]}}
{"id": "WebQTest-910", "question": "where did alexander graham bell die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people", "people.place_of_death.location"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alexander graham bell die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.00039197292062453926, -0.09051529318094254, -0.09441859275102615, -0.10416010022163391, -0.11413326114416122], "norm_scores": [0.21654395759105682, 0.19788186252117157, 0.19711099565029144, 0.1952001452445984, 0.19326308369636536]}}
{"id": "WebQTest-911", "question": "what year did the golden state warriors win their first nba championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the golden state warriors win their first nba championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.010523294098675251, -0.011453725397586823, -0.10148473083972931, -0.10784564167261124, -0.12640641629695892], "norm_scores": [0.21231576800346375, 0.21211831271648407, 0.19385553896427155, 0.19262635707855225, 0.1890840381383896]}}
{"id": "WebQTest-912", "question": "what currency should i take to turkey", "prediction": [["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.016630057245492935, -0.017264723777770996, -0.041411615908145905, -0.04169895499944687, -0.11990741640329361], "norm_scores": [0.2061002403497696, 0.20596948266029358, 0.20105551183223724, 0.20099776983261108, 0.1858770251274109]}}
{"id": "WebQTest-914", "question": "what countries are in the uk yahoo", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the uk yahoo [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.020430108532309532, -0.02612665668129921, -0.02789733186364174, -0.03947156295180321, -0.04449985548853874], "norm_scores": [0.2022557109594345, 0.20110681653022766, 0.20075103640556335, 0.19844089448451996, 0.1974455863237381]}}
{"id": "WebQTest-915", "question": "what superbowl did the baltimore ravens win", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what superbowl did the baltimore ravens win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.011439098045229912, -0.013677314855158329, -0.04617137834429741, -0.08685873448848724, -0.09767866134643555], "norm_scores": [0.2079712599515915, 0.20750629901885986, 0.20087194442749023, 0.19286303222179413, 0.19078750908374786]}}
{"id": "WebQTest-916", "question": "what is the language of russia s spoken", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.language_family"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the language of russia s spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.019276384264230728, -0.019823461771011353, -0.020313743501901627, -0.07885003834962845, -0.1474023461341858], "norm_scores": [0.2074548900127411, 0.20734143257141113, 0.2072398066520691, 0.19545698165893555, 0.18250688910484314]}}
{"id": "WebQTest-918", "question": "who is the king of cambodia now", "prediction": [["people.person.nationality"], ["royalty.monarch.kingdom"], ["royalty.kingdom.rulers"], ["royalty.monarch.kingdom", "people.person.nationality"], ["people.person.place_of_birth"]], "ground_paths": [["royalty.kingdom.rulers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the king of cambodia now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.016209809109568596, -0.016863711178302765, -0.03710266947746277, -0.08785132318735123, -0.08984093368053436], "norm_scores": [0.2066735476255417, 0.20653843879699707, 0.2024003267288208, 0.19238507747650146, 0.19200268387794495]}}
{"id": "WebQTest-919", "question": "who are serena williams parents", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.profession"], ["influence.influence_node.influenced_by"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are serena williams parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.008617437444627285, -0.021550219506025314, -0.15831199288368225, -0.16834433376789093, -0.16938935220241547], "norm_scores": [0.2196844071149826, 0.21686159074306488, 0.1891418844461441, 0.18725383281707764, 0.1870582401752472]}}
{"id": "WebQTest-920", "question": "what schools did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.010734718292951584, -0.023037085309624672, -0.02862650342285633, -0.029612639918923378, -0.060123760253190994], "norm_scores": [0.20395049452781677, 0.20145680010318756, 0.20033390820026398, 0.2001364380121231, 0.1941222846508026]}}
{"id": "WebQTest-921", "question": "what was the cause of death for huell howser", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["deceased_person.cause_of_death"], ["people.person.deceased_people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for huell howser [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_people </PATH>"], "scores": [-0.0010799235897138715, -0.049359388649463654, -0.09275731444358826, -0.10965447872877121, -0.1268206536769867], "norm_scores": [0.21532130241394043, 0.20517265796661377, 0.19645904004573822, 0.1931673139333725, 0.18987968564033508]}}
{"id": "WebQTest-922", "question": "what books are written by suzanne collins", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.author.book_editions_published"], ["book.written_work.author"], ["film.film_story_contributor.film_story_credits"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.book_editions_published"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books are written by suzanne collins [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>"], "scores": [-0.014153552241623402, -0.020994946360588074, -0.03921390697360039, -0.04330310598015785, -0.07772020250558853], "norm_scores": [0.2049971967935562, 0.20359951257705688, 0.19992372393608093, 0.19910787045955658, 0.19237174093723297]}}
{"id": "WebQTest-923", "question": "what countries does england border", "prediction": [["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["location.location.events", "location.location.events"], ["location.location.events", "time.event.locations"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"]], "ground_paths": [["base.biblioness.bibs_location.country", "base.aareas.schema.administrative_area.administrative_children"], ["time.event.locations", "location.location.events"], ["location.country.form_of_government", "location.country.form_of_government"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["law.court_jurisdiction_area.courts", "law.court_jurisdiction_area.courts"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["sports.sports_team_location.teams", "cricket.cricket_team.country"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["government.government_agency.jurisdiction", "government.governmental_jurisdiction.agencies"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["cricket.cricket_tournament_event.host", "time.event.locations"], ["periodicals.newspaper_circulation_area.newspapers", "periodicals.newspaper_circulation_area.newspapers"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["government.government_agency.jurisdiction", "government.government_agency.jurisdiction"], ["location.country.languages_spoken", "location.country.official_language"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["cricket.cricket_tournament_event.host", "location.location.events"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["law.court_jurisdiction_area.courts", "law.court.jurisdiction"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "time.event.locations"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["finance.currency.countries_used", "location.country.currency_used"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization.geographic_scope", "common.topic.subject_of"], ["location.country.internet_tld", "location.country.internet_tld"], ["people.person.nationality", "people.person.nationality"], ["location.location.events", "time.event.locations"], ["government.governmental_jurisdiction.government_bodies", "government.governmental_jurisdiction.government_bodies"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does england border [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> location.location.events <SEP> location.location.events </PATH>", "<PATH> location.location.events <SEP> time.event.locations </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.04236501455307007, -0.042959947139024734, -0.043031658977270126, -0.044950731098651886, -0.045502934604883194], "norm_scores": [0.20027945935726166, 0.20016033947467804, 0.20014598965644836, 0.1997622549533844, 0.19965197145938873]}}
{"id": "WebQTest-924", "question": "what team did messi play for in 2010", "prediction": [["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"]], "ground_paths": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did messi play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>"], "scores": [-0.02466452866792679, -0.024805035442113876, -0.024865956977009773, -0.033223193138837814, -0.03392622992396355], "norm_scores": [0.20072594285011292, 0.20069774985313416, 0.200685515999794, 0.19901533424854279, 0.19887547194957733]}}
{"id": "WebQTest-925", "question": "what time zone is greensburg indiana", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.location.contains", "location.location.time_zones"], ["location.location.contains"], ["location.location.containedby", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is greensburg indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains <SEP> location.location.time_zones </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.time_zones </PATH>"], "scores": [-2.2444906790042296e-05, -0.15071320533752441, -0.1512303501367569, -0.16145718097686768, -0.16588488221168518], "norm_scores": [0.22635354101657867, 0.1946897804737091, 0.19458912312984467, 0.19260922074317932, 0.19175830483436584]}}
{"id": "WebQTest-927", "question": "who plays voldemort", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays voldemort [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02172890119254589, -0.022814759984612465, -0.023960279300808907, -0.04023837670683861, -0.04348479583859444], "norm_scores": [0.2017420083284378, 0.2015230655670166, 0.20129235088825226, 0.19804221391677856, 0.19740033149719238]}}
{"id": "WebQTest-928", "question": "what colleges did harper lee attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did harper lee attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.013969301246106625, -0.023617355152964592, -0.02846362628042698, -0.02897048182785511, -0.050432007759809494], "norm_scores": [0.20303276181221008, 0.20108331739902496, 0.20011116564273834, 0.2000097632408142, 0.1957629919052124]}}
{"id": "WebQTest-929", "question": "who founded the kkk", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["base.activism.activism_issue.activists"], ["base.activism.activist_organization.founders"], ["base.activism.activist_issue.activists"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the kkk [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> base.activism.activist_organization.founders </PATH>", "<PATH> base.activism.activist_issue.activists </PATH>"], "scores": [-0.010035298764705658, -0.018071506172418594, -0.08822063356637955, -0.09761769324541092, -0.1137881875038147], "norm_scores": [0.2112208604812622, 0.20953024923801422, 0.19533558189868927, 0.19350861012935638, 0.19040462374687195]}}
{"id": "WebQTest-931", "question": "what jobs did john adams have before he was president", "prediction": [["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["tv.tv_program.country_of_origin", "government.government_office_or_title.jurisdiction"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["common.topic.image", "common.topic.image"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did john adams have before he was president [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> people.person.nationality <SEP> government.government_office_or_title.jurisdiction </PATH>"], "scores": [-0.022426055744290352, -0.023390445858240128, -0.024017030373215675, -0.03068101964890957, -0.03245840221643448], "norm_scores": [0.20083372294902802, 0.20064015686511993, 0.20051446557044983, 0.1991826891899109, 0.19882896542549133]}}
{"id": "WebQTest-932", "question": "what year did super mario bros 2 come out", "prediction": [["film.film.sequel"], ["film.film.country"], ["common.topic.notable_types"], [], ["computer.software_genre.games"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did super mario bros 2 come out [/INST]", "raw_output": {"paths": ["<PATH> film.film.sequel </PATH>", "<PATH> film.film.country </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> </PATH>", "<PATH> computer.software_genre.games </PATH>"], "scores": [-0.022580865770578384, -0.03628045693039894, -0.04627582058310509, -0.053971778601408005, -0.06710325181484222], "norm_scores": [0.2045605331659317, 0.20177726447582245, 0.1997704654932022, 0.19823895394802094, 0.1956527978181839]}}
{"id": "WebQTest-933", "question": "what political party did andrew johnson belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.us_president.vice_president", "organization.organization.founders"], ["government.politician.party", "government.political_party_tenure.party"], ["organization.organization.founders"]], "ground_paths": [["government.us_president.vice_president", "organization.organization.founders"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party did andrew johnson belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.us_president.vice_president <SEP> organization.organization.founders </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.01620623469352722, -0.02559836208820343, -0.026494011282920837, -0.02943703718483448, -0.043306149542331696], "norm_scores": [0.20240715146064758, 0.2005150020122528, 0.20033550262451172, 0.199746772646904, 0.1969955861568451]}}
{"id": "WebQTest-935", "question": "where was osama bin laden killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was osama bin laden killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.01670243591070175, -0.024753909558057785, -0.03595411777496338, -0.04276774823665619, -0.044704411178827286], "norm_scores": [0.20326976478099823, 0.20163971185684204, 0.19939389824867249, 0.1980399191379547, 0.1976567506790161]}}
{"id": "WebQTest-936", "question": "where were greek temples built", "prediction": [["location.location.containedby"], ["location.location.contains"], ["architecture.architect.structures_designed"], ["architecture.building.building_function"], ["architecture.structure.architect"]], "ground_paths": [["architecture.building.building_function", "location.location.containedby"], ["architecture.building.building_function", "travel.tourist_attraction.near_travel_destination"], ["architecture.building_function.buildings", "location.location.contains"], ["architecture.building_function.buildings", "location.location.containedby"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were greek temples built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> architecture.building.building_function </PATH>", "<PATH> architecture.structure.architect </PATH>"], "scores": [-0.006359178107231855, -0.047093912959098816, -0.06708560138940811, -0.07684127986431122, -0.08679049462080002], "norm_scores": [0.21026834845542908, 0.201875239610672, 0.19787949323654175, 0.19595842063426971, 0.19401845335960388]}}
{"id": "WebQTest-937", "question": "where are people who speak arabic from", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["people.ethnicity.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are people who speak arabic from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.016423819586634636, -0.02350827120244503, -0.03551580384373665, -0.03901447728276253, -0.07202690839767456], "norm_scores": [0.20418135821819305, 0.20273993909358978, 0.20032010972499847, 0.19962048530578613, 0.19313810765743256]}}
{"id": "WebQTest-938", "question": "who was queen elizabeth ii mom", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.parent"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen elizabeth ii mom [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.0052627576515078545, -0.029431695118546486, -0.12733176350593567, -0.1356087327003479, -0.15413619577884674], "norm_scores": [0.2173592895269394, 0.21216891705989838, 0.19238194823265076, 0.1907961666584015, 0.18729375302791595]}}
{"id": "WebQTest-939", "question": "what type of art did pablo picasso paint", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"], ["film.film_genre.films_in_this_genre"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did pablo picasso paint [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> film.film_genre.films_in_this_genre </PATH>"], "scores": [-0.0017277542501688004, -0.04025750607252121, -0.07132494449615479, -0.10629074275493622, -0.10745292901992798], "norm_scores": [0.2129760980606079, 0.2049262523651123, 0.1986575871706009, 0.19183140993118286, 0.19160860776901245]}}
{"id": "WebQTest-941", "question": "what is ella fitzgerald parents name", "prediction": [["people.person.children"], ["people.person.parents"], ["music.artist.genre", "music.artist.genre"], ["people.person.parent"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ella fitzgerald parents name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.01072254590690136, -0.016826240345835686, -0.13226816058158875, -0.13298845291137695, -0.13544170558452606], "norm_scores": [0.21518687903881073, 0.21387745440006256, 0.19055889546871185, 0.1904216855764389, 0.18995508551597595]}}
{"id": "WebQTest-942", "question": "who is anne hathaway parents", "prediction": [["people.person.parents"], ["people.person.children"], ["film.actor.film", "film.performance.character"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is anne hathaway parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.013959412463009357, -0.014244748279452324, -0.13931122422218323, -0.14460410177707672, -0.14717437326908112], "norm_scores": [0.21576334536075592, 0.21570180356502533, 0.19034351408481598, 0.18933869898319244, 0.1888526827096939]}}
{"id": "WebQTest-943", "question": "who does don draper gets engaged to", "prediction": [["fictional_universe.fictional_character.character_created_by", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.character_created_by", "fictional_universe.fictional_character.children"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created", "fictional_universe.fictional_character.character_created_by"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does don draper gets engaged to [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> fictional_universe.fictional_character.children </PATH>"], "scores": [-0.022362450137734413, -0.02577851712703705, -0.0337168350815773, -0.04033118486404419, -0.043117791414260864], "norm_scores": [0.20214475691318512, 0.20145539939403534, 0.1998625099658966, 0.19854490458965302, 0.19799239933490753]}}
{"id": "WebQTest-944", "question": "what was van halen first album", "prediction": [["music.artist.album"], ["music.album.artist"], ["music.recording.artist"], ["music.composition.composer"], ["music.producer.releases_produced"]], "ground_paths": [["music.album.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was van halen first album [/INST]", "raw_output": {"paths": ["<PATH> music.artist.album </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.producer.releases_produced </PATH>"], "scores": [-0.021669108420610428, -0.025112081319093704, -0.030957164242863655, -0.059319838881492615, -0.06973858922719955], "norm_scores": [0.20393867790699005, 0.20323772728443146, 0.2020532637834549, 0.19640299677848816, 0.19436733424663544]}}
{"id": "WebQTest-945", "question": "what political system did spain have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["base.locations.countries.continent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political system did spain have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.country.form_of_government <PATH>"], "scores": [-0.00436044717207551, -0.026762736961245537, -0.14480862021446228, -0.16752135753631592, -0.17992863059043884], "norm_scores": [0.22049500048160553, 0.2156103253364563, 0.19160327315330505, 0.18730045855045319, 0.18499094247817993]}}
{"id": "WebQTest-948", "question": "what are the supreme court cases", "prediction": [["law.court.jurisdiction", "government.governmental_jurisdiction.government_bodies"], ["law.court.jurisdiction", "book.written_work.subjects"], ["law.court.jurisdiction", "government.governmental_body.jurisdiction"], ["book.written_work.subjects"], ["law.court.jurisdiction", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the supreme court cases [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> law.court.jurisdiction <SEP> book.written_work.subjects </PATH>", "<PATH> law.court.jurisdiction <SEP> government.governmental_body.jurisdiction </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> law.court.jurisdiction <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.0215621255338192, -0.03323616087436676, -0.03920258209109306, -0.04087675362825394, -0.04230102524161339], "norm_scores": [0.20278820395469666, 0.200434610247612, 0.19924227893352509, 0.1989089995622635, 0.19862590730190277]}}
{"id": "WebQTest-949", "question": "what is penn state 's main campus", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.parent_institution"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is penn state 's main campus [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>"], "scores": [-0.003166936570778489, -0.04838591068983078, -0.05165039747953415, -0.07210579514503479, -0.07518312335014343], "norm_scores": [0.20953990519046783, 0.20027576386928558, 0.1996230185031891, 0.19558113813400269, 0.194980189204216]}}
{"id": "WebQTest-952", "question": "what country does alaska belong to", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does alaska belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.016004469245672226, -0.033857282251119614, -0.03824993968009949, -0.046660199761390686, -0.05800444260239601], "norm_scores": [0.2045414298772812, 0.2009221911430359, 0.20004154741764069, 0.1983661949634552, 0.19612860679626465]}}
{"id": "WebQTest-953", "question": "where is kansas city", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.biblioness.bibs_location.state"], ["base.biblioness.bibs_location.country"], ["location.us_county.county_seat"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is kansas city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.us_county.county_seat </PATH>"], "scores": [-0.0059674507938325405, -0.05043112859129906, -0.055882371962070465, -0.06412926316261292, -0.07390798628330231], "norm_scores": [0.2089587301015854, 0.19987118244171143, 0.19878460466861725, 0.19715198874473572, 0.19523349404335022]}}
{"id": "WebQTest-954", "question": "what was pink floyd 's first album", "prediction": [["music.recording.artist"], ["music.artist.album"], ["music.artist.genre", "music.album.genre"], ["music.album.artist"], ["music.recording.artist", "music.recording.releases"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was pink floyd 's first album [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.artist.genre <SEP> music.album.genre </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.recording.artist <SEP> music.recording.releases </PATH>"], "scores": [-0.027519317343831062, -0.03503204137086868, -0.04237396642565727, -0.05355163291096687, -0.061269957572221756], "norm_scores": [0.20329803228378296, 0.2017764300107956, 0.2003004401922226, 0.19807399809360504, 0.1965510994195938]}}
{"id": "WebQTest-955", "question": "who did granville t. woods married", "prediction": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.children"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did granville t. woods married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.03119705244898796, -0.03393888846039772, -0.03739967197179794, -0.03877777233719826, -0.03985939919948578], "norm_scores": [0.20100902020931244, 0.20045863091945648, 0.19976608455181122, 0.1994909793138504, 0.19927531480789185]}}
{"id": "WebQTest-957", "question": "what language did the sioux indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["language.human_language.language_family"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the sioux indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.000737150723580271, -0.07962410897016525, -0.07972447574138641, -0.08051124215126038, -0.09847384691238403], "norm_scores": [0.21374788880348206, 0.19753392040729523, 0.1975141018629074, 0.19735875725746155, 0.19384531676769257]}}
{"id": "WebQTest-958", "question": "what are some famous people from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["location.location.containedby", "people.person.nationality"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some famous people from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0027388716116547585, -0.04361007735133171, -0.07511439919471741, -0.07897815108299255, -0.0886780172586441], "norm_scores": [0.21122094988822937, 0.20276214182376862, 0.19647382199764252, 0.19571617245674133, 0.19382692873477936]}}
{"id": "WebQTest-959", "question": "who did brett gardner play for in 2008", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did brett gardner play for in 2008 [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.00972102489322424, -0.016473116353154182, -0.031669143587350845, -0.03649010881781578, -0.05261033773422241], "norm_scores": [0.20394989848136902, 0.20257745683193207, 0.19952234625816345, 0.19856277108192444, 0.1953875720500946]}}
{"id": "WebQTest-960", "question": "who plays dwight in the office", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["tv.tv_series_season.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_series_season.episodes", "tv.tv_director.episodes_directed"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays dwight in the office [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.015752824023365974, -0.03727143257856369, -0.040233172476291656, -0.04320645332336426, -0.04638516902923584], "norm_scores": [0.20419499278068542, 0.19984793663024902, 0.19925692677497864, 0.19866535067558289, 0.1980348527431488]}}
{"id": "WebQTest-962", "question": "what county is san diego ca", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.location.containedby"], ["location.us_county.county_seat"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego ca [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.016315534710884094, -0.024174803867936134, -0.03036942519247532, -0.03949756175279617, -0.08213018625974655], "norm_scores": [0.20443187654018402, 0.20283149182796478, 0.20157891511917114, 0.199747234582901, 0.19141045212745667]}}
{"id": "WebQTest-963", "question": "who is the miami dolphins quarterback", "prediction": [["american_football.football_player.position_s"], ["american_football.game_rushing_statistics.team", "american_football.game_rushing_statistics.player"], ["american_football.game_passing_statistics.team", "american_football.game_passing_statistics.player"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["american_football.football_team.current_head_coach"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the miami dolphins quarterback [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.game_rushing_statistics.team <SEP> american_football.game_rushing_statistics.player </PATH>", "<PATH> american_football.game_passing_statistics.team <SEP> american_football.game_passing_statistics.player </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>"], "scores": [-0.007764660287648439, -0.036998625844717026, -0.03823303058743477, -0.0397694930434227, -0.050783421844244], "norm_scores": [0.20544108748435974, 0.19952215254306793, 0.19927601516246796, 0.19897007942199707, 0.1967906653881073]}}
{"id": "WebQTest-965", "question": "where did anne frank go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["book.book_subject.works", "book.written_work.subjects"], ["book.book_subject.works", "book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did anne frank go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.021174104884266853, -0.023974016308784485, -0.028705574572086334, -0.030092723667621613, -0.040403466671705246], "norm_scores": [0.2015407234430313, 0.20097723603248596, 0.20002852380275726, 0.19975125789642334, 0.19770225882530212]}}
{"id": "WebQTest-966", "question": "what basketball player is married to monica", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.gender", "people.person.gender"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball player is married to monica [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.030891157686710358, -0.0330498181283474, -0.04060520604252815, -0.04454243183135986, -0.0488973893225193], "norm_scores": [0.2017441689968109, 0.20130914449691772, 0.19979390501976013, 0.19900880753993988, 0.19814401865005493]}}
{"id": "WebQTest-967", "question": "what did hitler use to kill himself", "prediction": [["people.deceased_person.cause_of_death"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["base.activism.activist.area_of_activism", "base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism", "people.deceased_person.cause_of_death"], ["base.activism.activist.area_of_activism", "base.activism.activist_issue.activists_organizations"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hitler use to kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activism_issue.activists </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> people.deceased_person.cause_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist_issue.activists_organizations </PATH>"], "scores": [-0.0084809809923172, -0.0297851525247097, -0.04687310755252838, -0.056102607399225235, -0.05927891284227371], "norm_scores": [0.2063889056444168, 0.2020384669303894, 0.19861537218093872, 0.1967906951904297, 0.19616660475730896]}}
{"id": "WebQTest-968", "question": "what do people speak in germany", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in germany [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.013314363546669483, -0.01986241526901722, -0.03647049516439438, -0.06532119959592819, -0.1699955314397812], "norm_scores": [0.20942924916744232, 0.20806238055229187, 0.2046353965997696, 0.19881586730480194, 0.17905710637569427]}}
{"id": "WebQTest-969", "question": "when is the last time the denver broncos won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the denver broncos won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.015596073120832443, -0.015713239088654518, -0.023871582001447678, -0.06128585711121559, -0.07378870993852615], "norm_scores": [0.20448024570941925, 0.20445628464221954, 0.20279504358768463, 0.19534780085086823, 0.19292061030864716]}}
{"id": "WebQTest-970", "question": "where was caesar when he was stabbed", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.place_of_interment.interred_here"], ["people.person.nationality"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was caesar when he was stabbed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0016187283908948302, -0.06876447796821594, -0.07001739740371704, -0.07609561085700989, -0.09275335818529129], "norm_scores": [0.2123110443353653, 0.19852332770824432, 0.19827474653720856, 0.19707325100898743, 0.19381766021251678]}}
{"id": "WebQTest-971", "question": "what style of music did bessie smith perform", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.written_work.author"], ["book.written_work.subjects"], ["book.author.works_written"]], "ground_paths": [["book.book_subject.works"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music did bessie smith perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0005593281821347773, -0.07235249131917953, -0.10571541637182236, -0.10742776840925217, -0.11595845967531204], "norm_scores": [0.2164239138364792, 0.2014307826757431, 0.1948213279247284, 0.1944880336523056, 0.1928359717130661]}}
{"id": "WebQTest-972", "question": "what team did jackie robinson", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did jackie robinson [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.014876216650009155, -0.01732523925602436, -0.028957927599549294, -0.03304643929004669, -0.04936616122722626], "norm_scores": [0.2027713805437088, 0.2022753804922104, 0.19993601739406586, 0.19912023842334747, 0.19589702785015106]}}
{"id": "WebQTest-973", "question": "what time zone is chicago in right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is chicago in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-8.776584763836581e-06, -0.1647188514471054, -0.18526631593704224, -0.18902826309204102, -0.20197635889053345], "norm_scores": [0.23127153515815735, 0.196150541305542, 0.19216126203536987, 0.19143973290920258, 0.18897691369056702]}}
{"id": "WebQTest-975", "question": "who was the vice president under ronald reagan", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the vice president under ronald reagan [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.0063839806243777275, -0.020068738609552383, -0.07404983043670654, -0.08055125176906586, -0.099854476749897], "norm_scores": [0.21007244288921356, 0.2072172462940216, 0.19632796943187714, 0.19505569338798523, 0.19132661819458008]}}
{"id": "WebQTest-976", "question": "where do the chicago white sox play home games", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_championship_event.champion"], ["sports.sports_team_location.teams"], ["sports.sports_team_location.team"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the chicago white sox play home games [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team_location.team </PATH>"], "scores": [-0.0005533499061129987, -0.05989494174718857, -0.10925479233264923, -0.134232759475708, -0.14363744854927063], "norm_scores": [0.21829606592655182, 0.20571890473365784, 0.1958111822605133, 0.19098079204559326, 0.18919309973716736]}}
{"id": "WebQTest-977", "question": "what part did jeff conaway play in grease", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"]], "ground_paths": [["film.performance.actor", "film.performance.character"], ["theater.theater_character.plays_appears_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did jeff conaway play in grease [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>"], "scores": [-0.02651817351579666, -0.03189645707607269, -0.03241165354847908, -0.035377297550439835, -0.04277616739273071], "norm_scores": [0.20145800709724426, 0.20037741959095, 0.20027421414852142, 0.1996811479330063, 0.19820916652679443]}}
{"id": "WebQTest-978", "question": "what book did benjamin franklin published", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.book_editions_published"], ["book.author.works_written"], ["people.person.profession", "common.topic.subject_of"]], "ground_paths": [["book.author.book_editions_published"], ["people.person.profession", "common.topic.subject_of"], ["book.book_edition.author_editor", "book.book.genre"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did benjamin franklin published [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> people.person.profession <SEP> common.topic.subject_of </PATH>"], "scores": [-0.011849501170217991, -0.029202761128544807, -0.03303786739706993, -0.04820450022816658, -0.078858382999897], "norm_scores": [0.20570562779903412, 0.20216675102710724, 0.20139291882514954, 0.19836150109767914, 0.19237321615219116]}}
{"id": "WebQTest-979", "question": "what currency to take to barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["location.country.form_of_government"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency to take to barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.010639840736985207, -0.015980545431375504, -0.12500786781311035, -0.12518881261348724, -0.1374141126871109], "norm_scores": [0.2146243453025818, 0.213481143116951, 0.19142982363700867, 0.19139517843723297, 0.18906956911087036]}}
{"id": "WebQTest-980", "question": "where did they get the name stanley cup", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["award.award_honor.award", "award.award_honor.award_winner"], ["sports.sports_championship_event.championship"], ["symbols.namesake.named_after", "symbols.namesake.named_after"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they get the name stanley cup [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> award.award_honor.award <SEP> award.award_honor.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> symbols.namesake.named_after <SEP> symbols.namesake.named_after </PATH>"], "scores": [-0.013191090896725655, -0.022770211100578308, -0.04934881627559662, -0.05787564441561699, -0.068443663418293], "norm_scores": [0.2058672159910202, 0.20390459895133972, 0.19855649769306183, 0.1968706250190735, 0.19480106234550476]}}
{"id": "WebQTest-982", "question": "what is the zip code of syracuse ny", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"], ["location.mailing_address.citytown"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code of syracuse ny [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.Citytown.postal_codes </PATH>", "<PATH> location.mailing_address.citytown </PATH>"], "scores": [-0.00027005866286344826, -0.08016505837440491, -0.1361311376094818, -0.16965167224407196, -0.17057761549949646], "norm_scores": [0.22302666306495667, 0.2059011608362198, 0.19469422101974487, 0.18827612698078156, 0.1881018728017807]}}
{"id": "WebQTest-983", "question": "where did tennessee williams go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tennessee williams go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.013778647407889366, -0.02091631293296814, -0.021259808912873268, -0.04558949172496796, -0.06897798925638199], "norm_scores": [0.20406411588191986, 0.2026127576828003, 0.20254316926002502, 0.19767482578754425, 0.19310514628887177]}}
{"id": "WebQTest-984", "question": "what is the name of airport in new york", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of airport in new york [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.009704232215881348, -0.026605652645230293, -0.03169994428753853, -0.07597778737545013, -0.0873783752322197], "norm_scores": [0.20735593140125275, 0.20388075709342957, 0.2028447836637497, 0.19405919313430786, 0.19185936450958252]}}
{"id": "WebQTest-985", "question": "who was queen victoria 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"]], "ground_paths": [["base.crime.crime_victim.crime", "base.crime.crime_victim.crime"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.parents"], ["royalty.kingdom.rulers", "people.person.nationality"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen victoria 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.036747027188539505, -0.03857968747615814, -0.0390559546649456, -0.04062687233090401, -0.0425112321972847], "norm_scores": [0.20055179297924042, 0.20018458366394043, 0.20008927583694458, 0.19977518916130066, 0.19939909875392914]}}
{"id": "WebQTest-986", "question": "where did benjamin franklin work", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employer.employees"], ["people.person.children", "people.person.place_of_birth"], ["people.person.profession"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["people.person.employment_history", "business.employer.employees"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.nationality", "government.government_agency.jurisdiction"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin work [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.01702134869992733, -0.02795282192528248, -0.03811933845281601, -0.04310545697808266, -0.055553942918777466], "norm_scores": [0.20388585329055786, 0.20166921615600586, 0.19962935149669647, 0.19863644242286682, 0.19617904722690582]}}
{"id": "WebQTest-987", "question": "what does barbara bush do for work", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.person.children"], ["base.activism.activist.area_of_activism"], ["people.person.parents"]], "ground_paths": [["government.politician.government_positions_held", "government.government_office_category.officeholders"], ["business.employment_tenure.person", "business.job_title.people_with_this_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does barbara bush do for work [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.children </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.0020116858649998903, -0.0668100118637085, -0.07575741410255432, -0.08068155497312546, -0.09002235531806946], "norm_scores": [0.21248294413089752, 0.19915100932121277, 0.1973770707845688, 0.19640754163265228, 0.19458147883415222]}}
{"id": "WebQTest-988", "question": "who is jacob latimore", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["common.topic.notable_types", "common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jacob latimore [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>"], "scores": [-0.0003885698679368943, -0.07768435031175613, -0.14446915686130524, -0.15027542412281036, -0.15327933430671692], "norm_scores": [0.22170674800872803, 0.20521531999111176, 0.19195768237113953, 0.19084635376930237, 0.1902739405632019]}}
{"id": "WebQTest-989", "question": "what happened to dunkirk during ww2", "prediction": [["time.event.locations"], ["military.military_conflict.casualties", "military.casualties.event"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.military_conflict.casualties"], ["time.event.included_in_event"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to dunkirk during ww2 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>", "<PATH> time.event.included_in_event </PATH>"], "scores": [-0.030237559229135513, -0.032164037227630615, -0.03865908458828926, -0.03966880962252617, -0.040944904088974], "norm_scores": [0.20122133195400238, 0.20083405077457428, 0.1995338499546051, 0.19933247566223145, 0.1990782767534256]}}
{"id": "WebQTest-990", "question": "what was scottie pippen good at", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position_s"], ["sports.sports_position.sport", "sports.sports_position.sport"]], "ground_paths": [["basketball.basketball_player.position_s", "sports.sport.positions"], ["common.topic.notable_types", "type.type.domain"], ["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen good at [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_position.sport </PATH>"], "scores": [-0.01677844300866127, -0.029286976903676987, -0.032254528254270554, -0.048100776970386505, -0.0487394817173481], "norm_scores": [0.20366930961608887, 0.20113758742809296, 0.20054157078266144, 0.19738879799842834, 0.19726276397705078]}}
{"id": "WebQTest-991", "question": "who played donna on west wing", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played donna on west wing [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.022043516859412193, -0.03577115014195442, -0.03892149031162262, -0.04287770763039589, -0.043023306876420975], "norm_scores": [0.20291179418563843, 0.20014531910419464, 0.19951578974723816, 0.1987280249595642, 0.19869908690452576]}}
{"id": "WebQTest-993", "question": "who played meg in season 1 of family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played meg in season 1 of family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>"], "scores": [-0.0224528219550848, -0.03052603267133236, -0.0322277657687664, -0.03657356649637222, -0.0421275831758976], "norm_scores": [0.2020721286535263, 0.20044732093811035, 0.20010650157928467, 0.19923877716064453, 0.19813525676727295]}}
{"id": "WebQTest-994", "question": "when was the last time the dallas cowboys won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dallas cowboys won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.006019692402333021, -0.025703631341457367, -0.03861793130636215, -0.05876387283205986, -0.06784907728433609], "norm_scores": [0.20673535764217377, 0.20270578563213348, 0.20010480284690857, 0.19611383974552155, 0.19434018433094025]}}
{"id": "WebQTest-996", "question": "where do ireland play rugby union", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_team_location.teams"], ["sports.sports_facility.teams"], ["sports.sports_team.location"], ["location.location.contains"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do ireland play rugby union [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0051256874576210976, -0.04307141155004501, -0.04802634194493294, -0.06596076488494873, -0.06652422994375229], "norm_scores": [0.20823796093463898, 0.2004842609167099, 0.19949333369731903, 0.19594742357730865, 0.19583705067634583]}}
{"id": "WebQTest-997", "question": "who killed lee harvey oswald shot", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.gender", "people.person.gender"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who killed lee harvey oswald shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.deceased_person.cause_of_death <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.010556862689554691, -0.026519307866692543, -0.04392503574490547, -0.047397367656230927, -0.048951949924230576], "norm_scores": [0.2050226777791977, 0.20177599787712097, 0.19829432666301727, 0.19760698080062866, 0.1973000317811966]}}
{"id": "WebQTest-998", "question": "when was the last time knicks won the championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.runner_up"], ["sports.sports_teams.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time knicks won the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_teams.champion </PATH>"], "scores": [-0.008617637678980827, -0.015416419133543968, -0.11433938145637512, -0.13181670010089874, -0.1582503616809845], "norm_scores": [0.21560966968536377, 0.21414875984191895, 0.19397862255573273, 0.19061784446239471, 0.18564513325691223]}}
{"id": "WebQTest-999", "question": "when did celtics win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_teams.champion"]], "ground_paths": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did celtics win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_teams.champion </PATH>"], "scores": [-0.011915119364857674, -0.012265977449715137, -0.1492152214050293, -0.14980533719062805, -0.16906775534152985], "norm_scores": [0.21752724051475525, 0.21745093166828156, 0.18962036073207855, 0.18950848281383514, 0.18589302897453308]}}
{"id": "WebQTest-1000", "question": "what countries included in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["book.book_subject.works"], ["book.written_work.subjects"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.006413018796592951, -0.03760749101638794, -0.05659047141671181, -0.0649392232298851, -0.07416711002588272], "norm_scores": [0.208420529961586, 0.2020193189382553, 0.19822056591510773, 0.19657257199287415, 0.19476695358753204]}}
{"id": "WebQTest-1001", "question": "what country is nicki minaj from", "prediction": [["people.person.nationality"], ["music.artist.origin"], ["people.person.place_of_birth"], ["music.recording.artist"], ["music.album.artist"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nicki minaj from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.006283072289079428, -0.03980795294046402, -0.053107619285583496, -0.058494310826063156, -0.07543684542179108], "norm_scores": [0.20817731320858002, 0.20131388306617737, 0.1986542046070099, 0.1975869983434677, 0.19426757097244263]}}
{"id": "WebQTest-1002", "question": "who played harry potter dumbledore actor", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["people.person.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played harry potter dumbledore actor [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.016425510868430138, -0.023869195953011513, -0.02387453429400921, -0.04296625405550003, -0.04329252615571022], "norm_scores": [0.20273852348327637, 0.20123501121997833, 0.20123392343521118, 0.19742846488952637, 0.19736406207084656]}}
{"id": "WebQTest-1003", "question": "who was the prime minister of pakistan in 1999", "prediction": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the prime minister of pakistan in 1999 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.009863744489848614, -0.01618237979710102, -0.02088397927582264, -0.03466111049056053, -0.0480131171643734], "norm_scores": [0.2032182365655899, 0.2019382119178772, 0.20099101960659027, 0.19824092090129852, 0.1956116110086441]}}
{"id": "WebQTest-1004", "question": "what movies did christopher plummer play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["music.artist.album", "film.film.soundtrack"], ["film.performance.actor"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["music.recording.artist", "music.recording.releases"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.producer.films_executive_produced"], ["film.actor.dubbing_performances", "film.film.dubbing_performances"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did christopher plummer play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.01562773436307907, -0.03418264910578728, -0.03587675467133522, -0.036261849105358124, -0.03707478940486908], "norm_scores": [0.20325493812561035, 0.1995183378458023, 0.19918061792850494, 0.1991039365530014, 0.1989421397447586]}}
{"id": "WebQTest-1005", "question": "what is capital city of morocco", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is capital city of morocco [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.006227726582437754, -0.034267786890268326, -0.04241495579481125, -0.0938280001282692, -0.11877672374248505], "norm_scores": [0.21068201959133148, 0.2048565298318863, 0.20319433510303497, 0.19301147758960724, 0.1882556825876236]}}
{"id": "WebQTest-1006", "question": "what movies is omar epps in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies is omar epps in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.01452195830643177, -0.03045334853231907, -0.03665686026215553, -0.03948776051402092, -0.03977884352207184], "norm_scores": [0.20355381071567535, 0.20033662021160126, 0.1990976631641388, 0.19853484630584717, 0.19847705960273743]}}
{"id": "WebQTest-1007", "question": "what district does nancy pelosi represent", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["government.government_position_held.office_holder", "government.political_district.representatives"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["government.politician.government_positions_held", "government.political_district.representatives"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["base.government2.elected_government_positions_held.officeholder", "base.government2.elected_government_positions_held.district_represented"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what district does nancy pelosi represent [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.political_district.representatives </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.022221215069293976, -0.02500544860959053, -0.030142100527882576, -0.035095300525426865, -0.041644204407930374], "norm_scores": [0.20172259211540222, 0.201161727309227, 0.20013108849525452, 0.1991422325372696, 0.19784234464168549]}}
{"id": "WebQTest-1008", "question": "where is walton indiana", "prediction": [["location.location.containedby"], ["location.hud_county_place.county"], ["location.location.contains"], ["location.us_county.hud_county_place"], ["location.us_county.county_seat"]], "ground_paths": [["location.us_county.hud_county_place"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>"], "scores": [-0.005590982269495726, -0.04604458436369896, -0.05074552074074745, -0.05104868859052658, -0.08131569623947144], "norm_scores": [0.20838391780853271, 0.20012226700782776, 0.19918371737003326, 0.1991233378648758, 0.19318677484989166]}}
{"id": "WebQTest-1009", "question": "where did george herbert walker bush go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.educational_institution.parent_institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["base.yalebase.person.residential_college", "education.educational_institution.parent_institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george herbert walker bush go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.parent_institution </PATH>"], "scores": [-0.015594332478940487, -0.017698846757411957, -0.019053995609283447, -0.05040803551673889, -0.061809852719306946], "norm_scores": [0.2034561038017273, 0.2030283659696579, 0.20275342464447021, 0.1964949071407318, 0.19426722824573517]}}
{"id": "WebQTest-1010", "question": "what did bruce jenner win gold medal for", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_event_competition.medalists"], ["olympics.olympic_athlete_affiliation.athlete", "olympics.olympic_event_competition.medalists"]], "ground_paths": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bruce jenner win gold medal for [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> olympics.olympic_athlete_affiliation.athlete <SEP> olympics.olympic_event_competition.medalists </PATH>"], "scores": [-0.007288898341357708, -0.019849350675940514, -0.044542212039232254, -0.04593244567513466, -0.053703416138887405], "norm_scores": [0.20543624460697174, 0.20287202298641205, 0.19792388379573822, 0.19764891266822815, 0.19611893594264984]}}
{"id": "WebQTest-1011", "question": "where do they speak tibetan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak tibetan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.013245202600955963, -0.03398991748690605, -0.035469528287649155, -0.03983389586210251, -0.052581787109375], "norm_scores": [0.20438703894615173, 0.20019076764583588, 0.1998947709798813, 0.1990242600440979, 0.19650322198867798]}}
{"id": "WebQTest-1012", "question": "what to do in hollywood ca this weekend", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.neighborhood.neighborhood_of"], ["location.location.contains"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in hollywood ca this weekend [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.neighborhood.neighborhood_of </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01512623205780983, -0.015382401645183563, -0.027968069538474083, -0.11249741166830063, -0.11618617177009583], "norm_scores": [0.20841659605503082, 0.20836323499679565, 0.2057572603225708, 0.18907955288887024, 0.18838335573673248]}}
{"id": "WebQTest-1014", "question": "where did kim kardashian come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim kardashian come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.003967772237956524, -0.039266519248485565, -0.04527907818555832, -0.10546038299798965, -0.13537487387657166], "norm_scores": [0.2125309854745865, 0.2051597684621811, 0.20392994582653046, 0.19201917946338654, 0.18636007606983185]}}
{"id": "WebQTest-1015", "question": "what town did justin bieber grew up in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.person.nationality"], ["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town did justin bieber grew up in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.013283606618642807, -0.025985797867178917, -0.033219583332538605, -0.03788323700428009, -0.08030932396650314], "norm_scores": [0.20498058199882507, 0.2023933380842209, 0.20093455910682678, 0.19999966025352478, 0.19169192016124725]}}
{"id": "WebQTest-1017", "question": "who plays charlie in the santa clause movies", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays charlie in the santa clause movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.027325352653861046, -0.027735229581594467, -0.028594084084033966, -0.03451884910464287, -0.03779459744691849], "norm_scores": [0.20077337324619293, 0.2006911039352417, 0.20051881670951843, 0.1993343085050583, 0.19868239760398865]}}
{"id": "WebQTest-1020", "question": "what highschool did r. kelly attend", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did r. kelly attend [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01708051934838295, -0.018568193539977074, -0.019864629954099655, -0.0404924675822258, -0.04972333461046219], "norm_scores": [0.20240959525108337, 0.202108696103096, 0.20184685289859772, 0.19772584736347198, 0.1959090530872345]}}
{"id": "WebQTest-1021", "question": "what happened at benghazi", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["time.event.includes_event"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened at benghazi [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.0050417096354067326, -0.04428153112530708, -0.05106651037931442, -0.07680685073137283, -0.09747573733329773], "norm_scores": [0.2101283073425293, 0.2020425945520401, 0.20067638158798218, 0.19557680189609528, 0.19157594442367554]}}
{"id": "WebQTest-1022", "question": "what country is nike based in", "prediction": [["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded", "people.person.nationality"]], "ground_paths": [["film.film.production_companies", "film.film.country"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.founders", "people.person.nationality"], ["organization.organization.phone_number", "common.phone_number.service_location"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nike based in [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> people.person.nationality </PATH>"], "scores": [-0.00488514918833971, -0.03983357921242714, -0.04432960972189903, -0.05577786639332771, -0.06073760241270065], "norm_scores": [0.20733819901943207, 0.20021721720695496, 0.1993190497159958, 0.19705019891262054, 0.19607530534267426]}}
{"id": "WebQTest-1024", "question": "what kind of money do i bring to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i bring to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>"], "scores": [-0.007210800424218178, -0.026397811248898506, -0.056773051619529724, -0.06067173555493355, -0.06326014548540115], "norm_scores": [0.20720762014389038, 0.203269824385643, 0.19718827307224274, 0.19642099738121033, 0.19591324031352997]}}
{"id": "WebQTest-1025", "question": "who did steve spurrier play pro football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["common.topic.notable_types"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did steve spurrier play pro football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>"], "scores": [-0.008286937139928341, -0.022168243303894997, -0.03163628280162811, -0.059138912707567215, -0.06412732601165771], "norm_scores": [0.20579327642917633, 0.20295633375644684, 0.20104379951953888, 0.19558991491794586, 0.1946166604757309]}}
{"id": "WebQTest-1027", "question": "where do the blackhawks play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"], ["sports.sports_championship_event.champion"], ["sports.sports_arena_stadium.team"], ["sports.sports_facility.teams", "sports.sports_facility.teams"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the blackhawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_arena_stadium.team </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_facility.teams </PATH>"], "scores": [-0.002552624326199293, -0.035654280334711075, -0.10103463381528854, -0.10663669556379318, -0.13437514007091522], "norm_scores": [0.21499373018741608, 0.20799358189105988, 0.19482988119125366, 0.19374148547649384, 0.18844126164913177]}}
{"id": "WebQTest-1028", "question": "who plays john connor", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays john connor [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.02170533500611782, -0.021867875009775162, -0.02677387371659279, -0.03653347119688988, -0.038289669901132584], "norm_scores": [0.20146605372428894, 0.20143331587314606, 0.20044749975204468, 0.19850073754787445, 0.19815242290496826]}}
{"id": "WebQTest-1029", "question": "what is the main language spoken in mexico", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["film.film.featured_film_locations"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language spoken in mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> film.film.featured_film_locations </PATH>"], "scores": [-0.017487306147813797, -0.025913648307323456, -0.028196539729833603, -0.04558787867426872, -0.04858337342739105], "norm_scores": [0.2031434327363968, 0.20143885910511017, 0.20097951591014862, 0.19751444458961487, 0.19692367315292358]}}
{"id": "WebQTest-1030", "question": "what are italians classified as", "prediction": [["people.ethnicity.included_in_group"], ["people.ethnicity.includes_groups"], ["people.person.ethnicity"], ["people.ethnicity.languages_spoken"], ["people.ethnicity.people"]], "ground_paths": [["people.ethnicity.includes_groups"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are italians classified as [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.ethnicity.includes_groups </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.009923797100782394, -0.02063407003879547, -0.06401823461055756, -0.07482611387968063, -0.0797203779220581], "norm_scores": [0.2080545276403427, 0.20583811402320862, 0.19709892570972443, 0.194980189204216, 0.19402821362018585]}}
{"id": "WebQTest-1031", "question": "what has angelina jolie accomplished", "prediction": [["people.person.profession"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has angelina jolie accomplished [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>"], "scores": [-0.014823832549154758, -0.028358424082398415, -0.030715929344296455, -0.04329322278499603, -0.045383270829916], "norm_scores": [0.20355716347694397, 0.2008206695318222, 0.20034778118133545, 0.1978437304496765, 0.19743065536022186]}}
{"id": "WebQTest-1032", "question": "who played captain kirk in star trek movie", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played captain kirk in star trek movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.024300944060087204, -0.026501866057515144, -0.02747242897748947, -0.029698604717850685, -0.035463929176330566], "norm_scores": [0.20087780058383942, 0.20043617486953735, 0.20024172961711884, 0.19979645311832428, 0.1986478865146637]}}
{"id": "WebQTest-1033", "question": "who played dana scully in x files", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.quotations", "people.person.quotations"], ["film.performance.character", "film.actor.film"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played dana scully in x files [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.02422253228724003, -0.030708014965057373, -0.03192169591784477, -0.03390990197658539, -0.03463025018572807], "norm_scores": [0.20137453079223633, 0.2000727355480194, 0.1998300552368164, 0.19943314790725708, 0.19928953051567078]}}
{"id": "WebQTest-1035", "question": "what was f. scott fitzgerald", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was f. scott fitzgerald [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>"], "scores": [-0.0005447563016787171, -0.06900089234113693, -0.09711023420095444, -0.11125104129314423, -0.12150903791189194], "norm_scores": [0.21630731225013733, 0.20199522376060486, 0.19639632105827332, 0.1936386674642563, 0.1916624754667282]}}
{"id": "WebQTest-1036", "question": "what is the dominant language spoken in jamaica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.region"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the dominant language spoken in jamaica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.region </PATH>"], "scores": [-0.01553890947252512, -0.021892163902521133, -0.023173751309514046, -0.06157003715634346, -0.1480533331632614], "norm_scores": [0.20759965479373932, 0.20628489553928375, 0.20602069795131683, 0.19826021790504456, 0.18183453381061554]}}
{"id": "WebQTest-1037", "question": "who is the coach for the new york knicks", "prediction": [["basketball.basketball_team.head_coach"], ["basketball.basketball_coach.team"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"]], "ground_paths": [["basketball.basketball_team.head_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach for the new york knicks [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_team.head_coach </PATH>", "<PATH> basketball.basketball_coach.team </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>"], "scores": [-0.004386596381664276, -0.02906647138297558, -0.06139783933758736, -0.06161466985940933, -0.06532634049654007], "norm_scores": [0.208096444606781, 0.2030234932899475, 0.1965644508600235, 0.1965218335390091, 0.1957937777042389]}}
{"id": "WebQTest-1038", "question": "who was wesley snipes", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was wesley snipes [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.00038349576061591506, -0.08409707248210907, -0.09931270778179169, -0.13770145177841187, -0.13974934816360474], "norm_scores": [0.2189571112394333, 0.20137368142604828, 0.19833284616470337, 0.19086340069770813, 0.19047293066978455]}}
{"id": "WebQTest-1039", "question": "where does sherrod brown live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.nationality", "location.location.containedby"], ["location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sherrod brown live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.01510307565331459, -0.01804179884493351, -0.03614933788776398, -0.05263032764196396, -0.06046994403004646], "norm_scores": [0.2042878270149231, 0.2036883682012558, 0.2000332623720169, 0.1967635303735733, 0.1952270269393921]}}
{"id": "WebQTest-1040", "question": "what kind of language does colombia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does colombia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.006391857285052538, -0.022445255890488625, -0.07265742868185043, -0.08289946615695953, -0.16982011497020721], "norm_scores": [0.21296848356723785, 0.20957691967487335, 0.1993134319782257, 0.19728247821331024, 0.18085867166519165]}}
{"id": "WebQTest-1041", "question": "what currency does england use now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does england use now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0031678557861596346, -0.04365811496973038, -0.06061990559101105, -0.0878206342458725, -0.1355411559343338], "norm_scores": [0.21279625594615936, 0.20435218513011932, 0.20091523230075836, 0.19552385807037354, 0.1864124834537506]}}
{"id": "WebQTest-1042", "question": "who was the original voice of meg griffin on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the original voice of meg griffin on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.014228242449462414, -0.028659509494900703, -0.032771773636341095, -0.03343880921602249, -0.038365766406059265], "norm_scores": [0.20306943356990814, 0.2001599371433258, 0.19933849573135376, 0.19920559227466583, 0.19822652637958527]}}
{"id": "WebQTest-1043", "question": "what kind of government is mali", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.locations.countries.continent"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is mali [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.00800954457372427, -0.018106898292899132, -0.12994098663330078, -0.13727791607379913, -0.13999615609645844], "norm_scores": [0.21597088873386383, 0.21380113065242767, 0.19117939472198486, 0.18978185951709747, 0.18926666676998138]}}
{"id": "WebQTest-1044", "question": "who did cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.0196794793009758, -0.022315485402941704, -0.025508057326078415, -0.03442118316888809, -0.035532064735889435], "norm_scores": [0.20156434178352356, 0.20103371143341064, 0.2003929167985916, 0.19861474633216858, 0.19839422404766083]}}
{"id": "WebQTest-1045", "question": "what is the zip code for morgantown wv", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.citytown.postal_ codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for morgantown wv [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.citytown.postal_ codes </PATH>"], "scores": [-6.239113281480968e-05, -0.106056809425354, -0.15338771045207977, -0.1664821058511734, -0.17301523685455322], "norm_scores": [0.22496436536312103, 0.20233961939811707, 0.19298581779003143, 0.19047527015209198, 0.1892349123954773]}}
{"id": "WebQTest-1046", "question": "what is jacksonville fl timezone", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jacksonville fl timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-9.526613212074153e-06, -0.1902138590812683, -0.19328388571739197, -0.19472138583660126, -0.20794221758842468], "norm_scores": [0.23329712450504303, 0.19288776814937592, 0.19229650497436523, 0.19202028214931488, 0.18949832022190094]}}
{"id": "WebQTest-1047", "question": "who does amy stiller play in dodgeball", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["film.film.starring", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does amy stiller play in dodgeball [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.02294834703207016, -0.024734100326895714, -0.026651816442608833, -0.034513622522354126, -0.04102412238717079], "norm_scores": [0.20140551030635834, 0.2010461688041687, 0.20066100358963013, 0.19908961653709412, 0.19779765605926514]}}
{"id": "WebQTest-1048", "question": "who were king henry vii children", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"], ["fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.children"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were king henry vii children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>"], "scores": [-0.012192914262413979, -0.015561908483505249, -0.14121384918689728, -0.14359216392040253, -0.1664164811372757], "norm_scores": [0.21694138646125793, 0.21621173620224, 0.1906818300485611, 0.1902288794517517, 0.18593621253967285]}}
{"id": "WebQTest-1050", "question": "who does mila kunis play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does mila kunis play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.character </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_universe.characters </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.020172974094748497, -0.023014187812805176, -0.023666733875870705, -0.03311564773321152, -0.036621615290641785], "norm_scores": [0.201430082321167, 0.20085857808589935, 0.20072756707668304, 0.19883982837200165, 0.19814392924308777]}}
{"id": "WebQTest-1051", "question": "when did the san francisco earthquake occur", "prediction": [["time.event.locations"], ["event.disaster.areas_affected"], ["location.location.events"], ["time.recurring_event.instances"], ["time.recurring_event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the san francisco earthquake occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.recurring_event.locations </PATH>"], "scores": [-0.0029054039623588324, -0.05005573481321335, -0.06166287139058113, -0.07059428840875626, -0.08432366698980331], "norm_scores": [0.2103828638792038, 0.20069347321987152, 0.19837747514247894, 0.19661356508731842, 0.19393260776996613]}}
{"id": "WebQTest-1052", "question": "who played damien in the omen 2006", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.film", "film.actor.film"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played damien in the omen 2006 [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>"], "scores": [-0.017038801684975624, -0.023705998435616493, -0.037082489579916, -0.037385035306215286, -0.03947336971759796], "norm_scores": [0.2027909755706787, 0.20144343376159668, 0.1987667679786682, 0.19870665669441223, 0.1982921063899994]}}
{"id": "WebQTest-1054", "question": "who played denver in four christmases", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["people.person.place_of_birth", "people.person.gender", "people.person.gender"], ["film.performance.film", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played denver in four christmases [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.01963679865002632, -0.031169794499874115, -0.04653823748230934, -0.04732297360897064, -0.05139179155230522], "norm_scores": [0.20393897593021393, 0.20160046219825745, 0.19852586090564728, 0.19837012887001038, 0.19756463170051575]}}
{"id": "WebQTest-1055", "question": "what to do with kids in essex", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["organization.organization.geographic_scope"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in essex [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.geographic_scope </PATH>"], "scores": [-0.012176807038486004, -0.01752736233174801, -0.035547807812690735, -0.06728217750787735, -0.15112824738025665], "norm_scores": [0.20884589850902557, 0.2077314406633377, 0.20402155816555023, 0.19764870405197144, 0.18175238370895386]}}
{"id": "WebQTest-1056", "question": "who did the cleveland cavaliers draft", "prediction": [["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["sports.sports_team.roster", "sports.pro_athlete.teams"]], "ground_paths": [["sports.sports_team_roster.team", "sports.pro_athlete.teams"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"], ["sports.sports_team.roster", "sports.pro_athlete.teams"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the cleveland cavaliers draft [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.team <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.player </PATH>", "<PATH> sports.sports_team.roster <SEP> sports.pro_athlete.teams </PATH>"], "scores": [-0.02129984274506569, -0.02349933050572872, -0.024330127984285355, -0.02890266664326191, -0.030340302735567093], "norm_scores": [0.2008756697177887, 0.20043432712554932, 0.20026788115501404, 0.19935424625873566, 0.1990678310394287]}}
{"id": "WebQTest-1057", "question": "what concentration camp did anne frank died in", "prediction": [["base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["film.film.story_by", "base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["book.written_work.subjects"], ["book.written_work.subjects", "base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["base.fictionaluniverse.fictional_character.occupation", "base.fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what concentration camp did anne frank died in [/INST]", "raw_output": {"paths": ["<PATH> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> film.film.story_by <SEP> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.subjects <SEP> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> base.fictionaluniverse.fictional_character.occupation <SEP> base.fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.0067406902089715, -0.04052821546792984, -0.045240603387355804, -0.057160235941410065, -0.0794864147901535], "norm_scores": [0.20791420340538025, 0.20100665092468262, 0.20006166398525238, 0.1976911425590515, 0.19332636892795563]}}
{"id": "WebQTest-1059", "question": "what language does east timor speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does east timor speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.007689675781875849, -0.029346782714128494, -0.047200556844472885, -0.05308179929852486, -0.1444040685892105], "norm_scores": [0.20974573493003845, 0.20525208115577698, 0.20162008702754974, 0.2004377841949463, 0.18294428288936615]}}
{"id": "WebQTest-1060", "question": "what is the currency in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005395154934376478, -0.027012204751372337, -0.14369480311870575, -0.1516815572977066, -0.15580573678016663], "norm_scores": [0.2186405211687088, 0.21396487951278687, 0.1904004067182541, 0.18888579308986664, 0.1881083846092224]}}
{"id": "WebQTest-1061", "question": "where does greek language come from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"], ["language.language_family.languages"], ["common.topic.notable_types"]], "ground_paths": [["base.rosetta.languoid.child"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does greek language come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.008613700978457928, -0.02514062449336052, -0.055328402668237686, -0.05564123019576073, -0.11574607342481613], "norm_scores": [0.20874923467636108, 0.2053276002407074, 0.19922184944152832, 0.19915953278541565, 0.18754172325134277]}}
{"id": "WebQTest-1062", "question": "what is the name of the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.005133552942425013, -0.02469998225569725, -0.14039304852485657, -0.14574210345745087, -0.14576253294944763], "norm_scores": [0.21778170764446259, 0.21356192231178284, 0.19022998213768005, 0.18921515345573425, 0.18921127915382385]}}
{"id": "WebQTest-1063", "question": "where is audrey hepburn from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is audrey hepburn from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.0011433993931859732, -0.053206346929073334, -0.09957565367221832, -0.14559997618198395, -0.15469852089881897], "norm_scores": [0.2184039205312729, 0.2073240876197815, 0.19793009757995605, 0.18902695178985596, 0.18731486797332764]}}
{"id": "WebQTest-1065", "question": "who does darth vaders voice", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>"], "scores": [-0.021691281348466873, -0.022342972457408905, -0.037145547568798065, -0.03721044212579727, -0.041493698954582214], "norm_scores": [0.2020607739686966, 0.20192915201187134, 0.19896207749843597, 0.1989491730928421, 0.19809883832931519]}}
{"id": "WebQTest-1067", "question": "what school did douglas macarthur go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did douglas macarthur go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.015230648219585419, -0.01716727390885353, -0.03079097531735897, -0.034942977130413055, -0.035117257386446], "norm_scores": [0.20228928327560425, 0.20189790427684784, 0.1991659551858902, 0.19834072887897491, 0.19830617308616638]}}
{"id": "WebQTest-1068", "question": "what famous people are from barbados", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["location.location.containedby", "people.person.nationality"], ["location.location.containedby", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"], ["location.location.contains", "location.location.people_born_here"], ["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from barbados [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.013151435181498528, -0.025790706276893616, -0.041006967425346375, -0.05155401676893234, -0.06596551835536957], "norm_scores": [0.20530293881893158, 0.2027243822813034, 0.19966301321983337, 0.19756823778152466, 0.1947413980960846]}}
{"id": "WebQTest-1069", "question": "what is the name of the currency used in china", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "finance.currency.countries_used"], ["location.country.form_of_government"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in china [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.00670821126550436, -0.020725125446915627, -0.12895700335502625, -0.14594674110412598, -0.1459948569536209], "norm_scores": [0.21687230467796326, 0.2138536274433136, 0.19191642105579376, 0.18868336081504822, 0.18867427110671997]}}
{"id": "WebQTest-1070", "question": "what are the names of michael jackson 's 3 children", "prediction": [["people.person.parents"], ["people.person.children"], ["music.composition.composer", "people.person.parents"], ["music.recording.artist", "music.recording.artist"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's 3 children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.composition.composer <SEP> people.person.parents </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.008979745209217072, -0.017231224104762077, -0.09457610547542572, -0.09965883195400238, -0.10650483518838882], "norm_scores": [0.21141035854816437, 0.20967307686805725, 0.19406723976135254, 0.19308334589004517, 0.19176599383354187]}}
{"id": "WebQTest-1072", "question": "who is the current governor of arizona 2010", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.country.first_level_divisions", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current governor of arizona 2010 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.019584832713007927, -0.020591381937265396, -0.02371738664805889, -0.02485153079032898, -0.0287295114248991], "norm_scores": [0.20078249275684357, 0.20058049261569977, 0.19995446503162384, 0.19972780346870422, 0.1989547610282898]}}
{"id": "WebQTest-1074", "question": "who is lamar odom playing for this year", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is lamar odom playing for this year [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.016961321234703064, -0.019436681643128395, -0.031686194241046906, -0.032388102263212204, -0.0338929146528244], "norm_scores": [0.20198701322078705, 0.20148763060569763, 0.19903457164764404, 0.1988949179649353, 0.1985958367586136]}}
{"id": "WebQTest-1075", "question": "where were the great pyramids of giza built", "prediction": [["architecture.building.building_complex"], ["location.location.containedby"], ["architecture.architect.structures_designed"], ["architecture.structure.architect"], ["architecture.building.building_sites"]], "ground_paths": [["architecture.building.building_complex"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the great pyramids of giza built [/INST]", "raw_output": {"paths": ["<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> architecture.architect.structures_designed </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.building.building_sites </PATH>"], "scores": [-0.007193542085587978, -0.02334064431488514, -0.08668570965528488, -0.09371880441904068, -0.09583249688148499], "norm_scores": [0.21097688376903534, 0.20759756863117218, 0.19485513865947723, 0.19348950684070587, 0.19308096170425415]}}
{"id": "WebQTest-1077", "question": "what language do the russian speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.language_family"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the russian speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01684693805873394, -0.020181702449917793, -0.024916885420680046, -0.11039426922798157, -0.12215132266283035], "norm_scores": [0.20836029946804047, 0.20766663551330566, 0.2066856175661087, 0.18975266814231873, 0.18753479421138763]}}
{"id": "WebQTest-1078", "question": "what language do australia people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do australia people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.007843350060284138, -0.01988055743277073, -0.07827907800674438, -0.1245821863412857, -0.16692937910556793], "norm_scores": [0.21446511149406433, 0.2118990272283554, 0.19987882673740387, 0.19083482027053833, 0.18292219936847687]}}
{"id": "WebQTest-1079", "question": "what country was king tut 's tomb found", "prediction": [["people.person.nationality"], ["people.deceased_person.place_of_death"], ["location.location.containedby"], ["people.deceased_person.place_of_burial"], ["people.person.place_of_birth"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was king tut 's tomb found [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0166071318089962, -0.030192693695425987, -0.030963022261857986, -0.030985500663518906, -0.06551821529865265], "norm_scores": [0.2036559134721756, 0.2009078413248062, 0.2007531374692917, 0.20074862241744995, 0.19393455982208252]}}
{"id": "WebQTest-1080", "question": "where did abraham and his descendants live", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.deceased_person.place_of_burial"]], "ground_paths": [["location.location.people_born_here"], ["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did abraham and his descendants live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>"], "scores": [-0.01946924440562725, -0.033944305032491684, -0.03553047776222229, -0.04276938736438751, -0.05147489160299301], "norm_scores": [0.20345191657543182, 0.20052814483642578, 0.2002103179693222, 0.19876626133918762, 0.19704340398311615]}}
{"id": "WebQTest-1081", "question": "where did george washington carver go to collage", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george washington carver go to collage [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.017089569941163063, -0.017785202711820602, -0.018561551347374916, -0.03363373130559921, -0.05969538539648056], "norm_scores": [0.20244085788726807, 0.2023000717163086, 0.2021430879831314, 0.1991191804409027, 0.1939968466758728]}}
{"id": "WebQTest-1082", "question": "what are the sports in spain", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"], ["sports.sports_facility.teams"], ["sports.sports_teamlocation.teams"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sports in spain [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_teamlocation.teams </PATH>"], "scores": [-0.010585392825305462, -0.015180001966655254, -0.13953416049480438, -0.18300899863243103, -0.1835007220506668], "norm_scores": [0.21942979097366333, 0.21842391788959503, 0.1928829699754715, 0.18467707931995392, 0.1845862716436386]}}
{"id": "WebQTest-1083", "question": "where president obama was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where president obama was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0034251054748892784, -0.03973077982664108, -0.08011970669031143, -0.08136499673128128, -0.10574784874916077], "norm_scores": [0.21194182336330414, 0.20438514649868011, 0.19629472494125366, 0.19605042040348053, 0.19132795929908752]}}
{"id": "WebQTest-1084", "question": "what sports do people play in poland", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["cricket.cricket_team.country"], ["book.book_subject.works", "book.book_subject.works"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports do people play in poland [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> cricket.cricket_team.country </PATH>", "<PATH> book.book_subject.works <SEP> book.book_subject.works </PATH>"], "scores": [-0.010257828049361706, -0.015063226222991943, -0.15231749415397644, -0.15881651639938354, -0.16016694903373718], "norm_scores": [0.21807819604873657, 0.21703274548053741, 0.18919798731803894, 0.18797236680984497, 0.1877187043428421]}}
{"id": "WebQTest-1085", "question": "who was demi lovato dating", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.artist.genre", "music.artist.genre"], ["people.person.profession", "people.person.profession"], ["music.composition.composer", "music.composer.compositions"], ["music.artist.album", "music.album.artist"]], "ground_paths": [["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["music.recording.featured_artists", "music.album.featured_artists"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["music.recording.artist", "music.composition.composer"], ["music.album.artist", "music.album.artist"], ["music.artist.genre", "music.artist.genre"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.languages", "people.person.languages"], ["music.artist.album", "music.composer.compositions"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["people.person.ethnicity", "people.person.ethnicity"], ["music.composer.compositions", "music.composer.compositions"], ["music.artist.label", "music.artist.label"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.recording.artist", "music.composer.compositions"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["music.composer.compositions", "music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was demi lovato dating [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> music.composition.composer <SEP> music.composer.compositions </PATH>", "<PATH> music.artist.album <SEP> music.album.artist </PATH>"], "scores": [-0.028897786512970924, -0.030484981834888458, -0.03990757092833519, -0.050022438168525696, -0.0542246550321579], "norm_scores": [0.20236554741859436, 0.20204460620880127, 0.2001497596502304, 0.19813549518585205, 0.1973046213388443]}}
{"id": "WebQTest-1087", "question": "what films has scarlett johansson been in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.character"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has scarlett johansson been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.023814449086785316, -0.024204377084970474, -0.03297814726829529, -0.033876027911901474, -0.038512036204338074], "norm_scores": [0.201373890042305, 0.2012953907251358, 0.199536994099617, 0.19935791194438934, 0.19843582808971405]}}
{"id": "WebQTest-1088", "question": "who originally voiced meg on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced meg on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_guest_role.actor </PATH>"], "scores": [-0.024327516555786133, -0.02591499127447605, -0.0260486863553524, -0.038557395339012146, -0.049272630363702774], "norm_scores": [0.20169714093208313, 0.20137719810009003, 0.20135028660297394, 0.19884733855724335, 0.19672800600528717]}}
{"id": "WebQTest-1089", "question": "who played amy squirrel in bad teacher", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.film.language", "people.person.languages"], ["film.film.starring", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played amy squirrel in bad teacher [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.028143906965851784, -0.03212323784828186, -0.03585338592529297, -0.03604024648666382, -0.03982292115688324], "norm_scores": [0.20125289261341095, 0.20045363903045654, 0.1997072994709015, 0.1996699869632721, 0.19891612231731415]}}
{"id": "WebQTest-1090", "question": "where are boeing headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization.place_founded"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are boeing headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.005020129960030317, -0.04427340626716614, -0.05309021845459938, -0.05537134036421776, -0.056227654218673706], "norm_scores": [0.20766057074069977, 0.19966712594032288, 0.19791443645954132, 0.19746346771717072, 0.1972944587469101]}}
{"id": "WebQTest-1091", "question": "what type of religions are in the united states", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical.subjects"], ["royalty.monarch.kingdom", "people.person.religion"], ["religion.religion.countries"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religions are in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.subjects </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> religion.religion.countries </PATH>"], "scores": [-0.007861519232392311, -0.015909690409898758, -0.06525351852178574, -0.07444217801094055, -0.07994190603494644], "norm_scores": [0.20823565125465393, 0.2065664529800415, 0.19662107527256012, 0.19482265412807465, 0.19375410676002502]}}
{"id": "WebQTest-1092", "question": "who did viggo mortensen play in lord of the rings", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.film_series.films_in_series", "book.book.characters"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["media_common.adaptation.adapted_from", "book.book_character.appears_in_book"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["media_common.adapted_work.adaptations", "book.book.characters"], ["film.performance.film", "film.performance.character"], ["film.film.starring", "film.performance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["music.featured_artist.recordings", "book.book_character.appears_in_book"], ["film.film.story_by", "fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did viggo mortensen play in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.021870778873562813, -0.02274891920387745, -0.02779686450958252, -0.0325794443488121, -0.0405239537358284], "norm_scores": [0.20144712924957275, 0.20127031207084656, 0.20025686919689178, 0.19930140674114227, 0.1977243274450302]}}
{"id": "WebQTest-1094", "question": "who was isaac newton and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was isaac newton and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.0006574711296707392, -0.06503605097532272, -0.08150241523981094, -0.09684262424707413, -0.11455356329679489], "norm_scores": [0.21456313133239746, 0.20118510723114014, 0.1978994458913803, 0.19488678872585297, 0.1914655566215515]}}
{"id": "WebQTest-1095", "question": "who did william shakespeare marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.children", "people.person.parents"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["book.book_subject.works", "book.book_subject.works"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did william shakespeare marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.has_value <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.018220722675323486, -0.0357196219265461, -0.04387466982007027, -0.04506165534257889, -0.04820771515369415], "norm_scores": [0.20402751863002777, 0.20048832893371582, 0.19885997474193573, 0.19862407445907593, 0.19800017774105072]}}
{"id": "WebQTest-1096", "question": "what movies queen latifah is in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.film.executive_produced_by"], ["people.person.nationality", "film.film.country"], ["film.producer.film", "film.film.sequel"], ["music.recording.artist"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["film.producer.film"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.producer.films_executive_produced"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.film.produced_by"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies queen latifah is in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.024953026324510574, -0.02728245221078396, -0.03949684277176857, -0.04239780455827713, -0.04484343156218529], "norm_scores": [0.20217348635196686, 0.20170308649539948, 0.19925439357757568, 0.198677197098732, 0.19819189608097076]}}
{"id": "WebQTest-1097", "question": "where did michelle obama graduated from", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did michelle obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.015978479757905006, -0.01962604746222496, -0.021022433415055275, -0.0516190342605114, -0.05446555092930794], "norm_scores": [0.20331157743930817, 0.2025713324546814, 0.2022886723279953, 0.19619305431842804, 0.1956353783607483]}}
{"id": "WebQTest-1098", "question": "what kind of music did franz liszt compose", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.composition.composer"], ["music.composer.genre"], ["music.composer.compositions"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.composer.genre </PATH>", "<PATH> music.composer.compositions </PATH>"], "scores": [-0.005050719249993563, -0.03244861587882042, -0.06290105730295181, -0.0858682170510292, -0.09812099486589432], "norm_scores": [0.21051494777202606, 0.20482556521892548, 0.19868215918540955, 0.1941709965467453, 0.1918063759803772]}}
{"id": "WebQTest-1099", "question": "what country does david luiz play for", "prediction": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["people.person.nationality", "sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does david luiz play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.030819397419691086, -0.031527549028396606, -0.03160381689667702, -0.033731114119291306, -0.03855740651488304], "norm_scores": [0.2004854679107666, 0.2003435492515564, 0.2003282755613327, 0.19990256428718567, 0.19894009828567505]}}
{"id": "WebQTest-1100", "question": "what document did thomas jefferson wrote", "prediction": [["book.written_work.author"], ["book.author.book_editions_published"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["book.written_work.subjects"]], "ground_paths": [["book.book_subject.works"], ["book.author.book_editions_published"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["book.written_work.subjects"], []], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did thomas jefferson wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.010971887037158012, -0.02716793492436409, -0.03679606318473816, -0.0700700506567955, -0.07508242875337601], "norm_scores": [0.20665612816810608, 0.20333607494831085, 0.20138771831989288, 0.19479700922966003, 0.19382305443286896]}}
{"id": "WebQTest-1101", "question": "who is martin luther king jr facts", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["base.activism.activist.area_of_activism"]], "ground_paths": [["base.activism.activist.area_of_activism"], ["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is martin luther king jr facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.001739578670822084, -0.06598608195781708, -0.06695005297660828, -0.07042833417654037, -0.10070261359214783], "norm_scores": [0.21213237941265106, 0.19893218576908112, 0.19874051213264465, 0.19805043935775757, 0.1921444684267044]}}
{"id": "WebQTest-1102", "question": "where is victoria british columbia", "prediction": [["location.location.containedby"], ["location.province.capital"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.state"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["base.aareas.schema.administrative_area.capital"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is victoria british columbia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.province.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.009736820124089718, -0.019911006093025208, -0.06845342367887497, -0.07155639678239822, -0.08576628565788269], "norm_scores": [0.2083466500043869, 0.20623764395713806, 0.19646547734737396, 0.19585679471492767, 0.19309337437152863]}}
{"id": "WebQTest-1103", "question": "how many mary mary sisters", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["music.artist.genre", "music.artist.genre"], ["music.recording.artist", "music.recording.artist"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.children"]], "ground_paths": [["music.musical_group.member", "music.group_member.membership"], ["music.album.artist", "music.composition.composer"], ["music.album.artist", "music.composer.compositions"], ["music.artist.origin", "people.person.place_of_birth"], ["music.group_membership.group", "music.group_membership.member"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many mary mary sisters [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03866303712129593, -0.040006931871175766, -0.045812081545591354, -0.047192562371492386, -0.048443958163261414], "norm_scores": [0.20107343792915344, 0.20080341398715973, 0.19964109361171722, 0.19936567544937134, 0.19911634922027588]}}
{"id": "WebQTest-1104", "question": "what do we call the currency of france", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do we call the currency of france [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.00865831971168518, -0.018674854189157486, -0.09828163683414459, -0.1057373508810997, -0.13640257716178894], "norm_scores": [0.2131345570087433, 0.21101035177707672, 0.1948637068271637, 0.1934162825345993, 0.18757513165473938]}}
{"id": "WebQTest-1106", "question": "what team does david beckham play for in 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "ground_paths": [["book.written_work.subjects", "book.written_work.subjects"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does david beckham play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.020609084516763687, -0.02530454471707344, -0.02915584109723568, -0.02949647791683674, -0.03086143173277378], "norm_scores": [0.2012980878353119, 0.20035509765148163, 0.1995849609375, 0.19951699674129486, 0.19924484193325043]}}
{"id": "WebQTest-1107", "question": "what movies did nick cannon star in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["music.artist.track", "music.recording.releases"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did nick cannon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.01698647439479828, -0.03521543741226196, -0.03578510135412216, -0.0388621911406517, -0.03903648629784584], "norm_scores": [0.20325756072998047, 0.1995859593153, 0.19947229325771332, 0.19885943830013275, 0.19882477819919586]}}
{"id": "WebQTest-1108", "question": "what radio station is npr on in nyc", "prediction": [["broadcast.radio_station.owner"], ["broadcast.radio_station_owner.radio_stations"], ["broadcast.content.location"], ["broadcast.content.location", "broadcast.content.location"], ["broadcast.broadcast.content", "broadcast.broadcast.area_served"]], "ground_paths": [["broadcast.producer.location", "broadcast.broadcast.area_served"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["broadcast.radio_network.affiliates", "broadcast.radio_affiliation_duration.station"], ["broadcast.producer.produces", "broadcast.producer.produces"], ["award.award_winner.awards_won", "award.award_honor.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what radio station is npr on in nyc [/INST]", "raw_output": {"paths": ["<PATH> broadcast.radio_station.owner </PATH>", "<PATH> broadcast.radio_station_owner.radio_stations </PATH>", "<PATH> broadcast.content.location </PATH>", "<PATH> broadcast.content.location <SEP> broadcast.content.location </PATH>", "<PATH> broadcast.broadcast.content <SEP> broadcast.broadcast.area_served </PATH>"], "scores": [-0.018984314054250717, -0.03555091470479965, -0.044321879744529724, -0.05029032751917839, -0.05567954480648041], "norm_scores": [0.20442794263362885, 0.2010691612958908, 0.19931329786777496, 0.19812725484371185, 0.1970623880624771]}}
{"id": "WebQTest-1109", "question": "who did tom landry play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom landry play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.011387531645596027, -0.012307400815188885, -0.05599866062402725, -0.05716235190629959, -0.05751730501651764], "norm_scores": [0.20552344620227814, 0.20533446967601776, 0.19655632972717285, 0.19632773101329803, 0.1962580382823944]}}
{"id": "WebQTest-1110", "question": "where was john jay born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"], ["people.deceased_person.place_of_death"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john jay born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.010325836017727852, -0.016323033720254898, -0.12320385128259659, -0.12361527234315872, -0.1296122670173645], "norm_scores": [0.21423707902431488, 0.21295608580112457, 0.19136933982372284, 0.19129063189029694, 0.19014687836170197]}}
{"id": "WebQTest-1111", "question": "where is mount fuji located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_range"], ["geography.mountain.mountain_type"], ["geography.mountain_range.mountains"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount fuji located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> geography.mountain.mountain_type </PATH>", "<PATH> geography.mountain_range.mountains </PATH>"], "scores": [-0.011906030587852001, -0.02313871681690216, -0.050510335713624954, -0.08964535593986511, -0.0954490676522255], "norm_scores": [0.2085062861442566, 0.20617730915546417, 0.2006104290485382, 0.19291117787361145, 0.19179481267929077]}}
{"id": "WebQTest-1113", "question": "where was toussaint l'ouverture born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"], ["film.film_subject.films"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was toussaint l'ouverture born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> film.film_subject.films </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.004176184069365263, -0.029935473576188087, -0.08526734262704849, -0.09462476521730423, -0.10870447754859924], "norm_scores": [0.21227142214775085, 0.20687328279018402, 0.19573751091957092, 0.1939144730567932, 0.19120334088802338]}}
{"id": "WebQTest-1114", "question": "what countries makes up the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries makes up the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.019161703065037727, -0.03092561662197113, -0.031093204393982887, -0.03492453694343567, -0.0453747883439064], "norm_scores": [0.20263700187206268, 0.20026716589927673, 0.2002336084842682, 0.19946791231632233, 0.19739428162574768]}}
{"id": "WebQTest-1115", "question": "what is lawton ok zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.city.postal_codes"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is lawton ok zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.city.postal_codes </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>"], "scores": [-0.0001822773483581841, -0.09792478382587433, -0.1357094645500183, -0.14050434529781342, -0.17957496643066406], "norm_scores": [0.22296574711799622, 0.2022036910057068, 0.1947060525417328, 0.19377470016479492, 0.1863497793674469]}}
{"id": "WebQTest-1116", "question": "what is the capital city of spain wiki answers", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.capital"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital city of spain wiki answers [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.006930655334144831, -0.0325961671769619, -0.04047849401831627, -0.05854807421565056, -0.08852231502532959], "norm_scores": [0.2077701836824417, 0.20250549912452698, 0.20091556012630463, 0.1973176896572113, 0.1914910078048706]}}
{"id": "WebQTest-1117", "question": "where do they speak german", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["book.newspaper.circulation_areas"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak german [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> book.newspaper.circulation_areas </PATH>"], "scores": [-0.017067918553948402, -0.022837772965431213, -0.02329937368631363, -0.10882486402988434, -0.15879347920417786], "norm_scores": [0.20972183346748352, 0.2085152566432953, 0.20841901004314423, 0.19133487343788147, 0.18200907111167908]}}
{"id": "WebQTest-1118", "question": "when did mark mcgwire retired", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.ending_season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did mark mcgwire retired [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>"], "scores": [-0.013132140040397644, -0.015150167979300022, -0.026296274736523628, -0.04649621620774269, -0.05482872575521469], "norm_scores": [0.2036140114068985, 0.20320351421833038, 0.20095117390155792, 0.19693270325660706, 0.19529856741428375]}}
{"id": "WebQTest-1119", "question": "what is colorado technical university", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["common.topic.notable_types"], ["education.educational_institution.colors"], ["education.educational_institution.parent_institution"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is colorado technical university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> education.educational_institution.colors </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>"], "scores": [-0.025485731661319733, -0.03309698775410652, -0.039343614131212234, -0.04322008788585663, -0.04619790241122246], "norm_scores": [0.20240546762943268, 0.20087075233459473, 0.19961990416049957, 0.19884757697582245, 0.19825634360313416]}}
{"id": "WebQTest-1120", "question": "what was the currency used in france before the euro", "prediction": [["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"], ["location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the currency used in france before the euro [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_formerly_used </PATH>"], "scores": [-0.005938888993114233, -0.02567138895392418, -0.04839954897761345, -0.08106403052806854, -0.11884037405252457], "norm_scores": [0.21009483933448792, 0.2059897780418396, 0.2013608068227768, 0.19488972425460815, 0.18766483664512634]}}
{"id": "WebQTest-1121", "question": "what is kentucky state slogan", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is kentucky state slogan [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.013690314255654812, -0.020001757889986038, -0.02350032888352871, -0.028149951249361038, -0.040571797639131546], "norm_scores": [0.20230352878570557, 0.2010307013988495, 0.20032863318920135, 0.19939933717250824, 0.19693775475025177]}}
{"id": "WebQTest-1124", "question": "who played samantha stephens in bewitched", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played samantha stephens in bewitched [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.01612582616508007, -0.032269611954689026, -0.03358185291290283, -0.03423059359192848, -0.03503922373056412], "norm_scores": [0.20283961296081543, 0.19959129393100739, 0.19932955503463745, 0.19920028746128082, 0.1990392804145813]}}
{"id": "WebQTest-1125", "question": "what area of science did sir isaac newton study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"], ["law.invention.inventor"], ["symbols.namesake.named_after"], ["book.written_work.subjects"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what area of science did sir isaac newton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.0056586419232189655, -0.03271602839231491, -0.044945232570171356, -0.06742604076862335, -0.07586684077978134], "norm_scores": [0.20802642405033112, 0.2024732381105423, 0.2000122219324112, 0.19556595385074615, 0.19392217695713043]}}
{"id": "WebQTest-1126", "question": "what county is san diego in", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.location.containedby"], ["location.us_county.county_seat"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.us_county.county_seat"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.019078336656093597, -0.02287488617002964, -0.025417843833565712, -0.045147914439439774, -0.0791289284825325], "norm_scores": [0.20383723080158234, 0.20306481420993805, 0.20254908502101898, 0.1985919326543808, 0.19195696711540222]}}
{"id": "WebQTest-1127", "question": "which country in north america is divided into provinces", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["location.location.contains"], ["location.administrative_division.first_level_division_of"]], "ground_paths": [["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in north america is divided into provinces [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.016372548416256905, -0.021251171827316284, -0.022743239998817444, -0.07230360805988312, -0.08310732245445251], "norm_scores": [0.20534595847129822, 0.20434659719467163, 0.20404191315174103, 0.19417601823806763, 0.1920894980430603]}}
{"id": "WebQTest-1130", "question": "what is real name of santa claus", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.occupation", "people.person.profession"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["fictional_universe.fictional_character.character_created_by", "people.person.profession"]], "ground_paths": [["fictional_universe.fictional_character.based_on"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is real name of santa claus [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> people.person.profession </PATH>"], "scores": [-0.005410294979810715, -0.04013507068157196, -0.05298975110054016, -0.05569099262356758, -0.07024621218442917], "norm_scores": [0.2080044448375702, 0.20090550184249878, 0.19833946228027344, 0.19780442118644714, 0.19494618475437164]}}
{"id": "WebQTest-1131", "question": "when did kelly slater go pro", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["sports.sports_award_winner.awards", "sports.sports_award_season.awards"], ["sports.pro_sports_played.athlete", "sports.pro_sports_played.seasons"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kelly slater go pro [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award_season.awards </PATH>", "<PATH> sports.pro_sports_played.athlete <SEP> sports.pro_sports_played.seasons </PATH>"], "scores": [-0.0180371031165123, -0.029744572937488556, -0.038848500698804855, -0.039865728467702866, -0.04227745160460472], "norm_scores": [0.20316021144390106, 0.20079559087753296, 0.19897586107254028, 0.1987735480070114, 0.1982947438955307]}}
{"id": "WebQTest-1132", "question": "what did coretta scott king died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did coretta scott king died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>"], "scores": [-0.0014413391472771764, -0.04432068020105362, -0.11337408423423767, -0.1309310346841812, -0.15500177443027496], "norm_scores": [0.21794450283050537, 0.20879670977592468, 0.1948651373386383, 0.19147376716136932, 0.18691986799240112]}}
{"id": "WebQTest-1133", "question": "what does wh smith stand for", "prediction": [["symbols.name_source.namesakes"], ["common.topic.notable_types", "common.topic.notable_types"], ["symbols.namesake.named_after"], ["organization.organization.founders"], []], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does wh smith stand for [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> </PATH>"], "scores": [-0.024844257161021233, -0.039858654141426086, -0.04182768240571022, -0.047443270683288574, -0.05392226204276085], "norm_scores": [0.20336559414863586, 0.20033498108386993, 0.19994090497493744, 0.1988212764263153, 0.19753728806972504]}}
{"id": "WebQTest-1135", "question": "who was vp for nixon", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["government.political_appointer.appointees", "government.politician.government_positions_held"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for nixon [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> base.americancomedy.impersonated_celebrity.impersonated_by <SEP> base.americancomedy.impersonated_celebrity.impersonated_by </PATH>", "<PATH> government.political_appointer.appointees <SEP> government.politician.government_positions_held </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.005762591026723385, -0.022781385108828545, -0.0787750780582428, -0.08855070173740387, -0.11669272184371948], "norm_scores": [0.21149420738220215, 0.20792528986930847, 0.1966027319431305, 0.19469018280506134, 0.18928758800029755]}}
{"id": "WebQTest-1136", "question": "what religion did adolf hitler follow", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"], ["religion.religion.founding_figures"], ["organization.organization_founder.organizations_founded"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion did adolf hitler follow [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0012380362022668123, -0.06009408459067345, -0.08089013397693634, -0.09359771013259888, -0.10990360379219055], "norm_scores": [0.21389971673488617, 0.20167376101016998, 0.19752302765846252, 0.1950288712978363, 0.19187454879283905]}}
{"id": "WebQTest-1137", "question": "where is the illinois river on a map", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the illinois river on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.011282375082373619, -0.026749681681394577, -0.03484633192420006, -0.05157821252942085, -0.09007702767848969], "norm_scores": [0.20635159313678741, 0.20318444073200226, 0.20154596865177155, 0.19820177555084229, 0.19071626663208008]}}
{"id": "WebQTest-1138", "question": "where honduras in the world", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"], ["base.locations.continents.countries_within"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.contains"], ["base.locations.continents.countries_within"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where honduras in the world [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.016840346157550812, -0.019124696031212807, -0.0388905294239521, -0.05128706991672516, -0.10455146431922913], "norm_scores": [0.20584286749362946, 0.20537318289279938, 0.20135366916656494, 0.19887299835681915, 0.18855729699134827]}}
{"id": "WebQTest-1142", "question": "who does ronaldinho play for now 2011", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ronaldinho play for now 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02463557943701744, -0.025696370750665665, -0.028356455266475677, -0.02904321253299713, -0.03140793740749359], "norm_scores": [0.20063890516757965, 0.20042617619037628, 0.19989372789859772, 0.19975650310516357, 0.19928468763828278]}}
{"id": "WebQTest-1143", "question": "what galileo galilei was famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"], ["people.profession.people_with_this_profession"], ["symbols.name_source.namesakes"]], "ground_paths": [["astronomy.astronomer.astronomical_objects_discovered", "astronomy.star_system_body.star_system"], ["book.written_work.author", "book.book_subject.works"], ["astronomy.astronomical_discovery.discoverer", "astronomy.star_system_body.star_system"], ["influence.influence_node.influenced", "base.argumentmaps.innovator.original_ideas"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["symbols.name_source.namesakes", "astronomy.star_system_body.star_system"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what galileo galilei was famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0012805649312213063, -0.06433208286762238, -0.07636499404907227, -0.08216771483421326, -0.09675765037536621], "norm_scores": [0.21286572515964508, 0.19985859096050262, 0.1974681168794632, 0.19632558524608612, 0.19348201155662537]}}
{"id": "WebQTest-1144", "question": "what are the major trading partners of germany", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major trading partners of germany [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.statistical_region.places_imported_from </PATH>"], "scores": [-0.028581777587532997, -0.03200775757431984, -0.03416064381599426, -0.03919362649321556, -0.04112081229686737], "norm_scores": [0.20128822326660156, 0.200599804520607, 0.20016838610172272, 0.19916348159313202, 0.19878003001213074]}}
{"id": "WebQTest-1145", "question": "where did romney graduated college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.religion", "religion.religion.organizations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did romney graduated college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.017273791134357452, -0.01922995038330555, -0.024157175794243813, -0.047728002071380615, -0.05900842323899269], "norm_scores": [0.20323900878429413, 0.2028418332338333, 0.20184484124183655, 0.1971428245306015, 0.19493147730827332]}}
{"id": "WebQTest-1146", "question": "who did johnny depp play in corpse bride", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did johnny depp play in corpse bride [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.01785369962453842, -0.024885054677724838, -0.032533444464206696, -0.03341495618224144, -0.03889235854148865], "norm_scores": [0.20234063267707825, 0.200922891497612, 0.19939200580120087, 0.1992163211107254, 0.1981281340122223]}}
{"id": "WebQTest-1147", "question": "what is the zip code for jonesboro ar", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for jonesboro ar [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.Citytown.postal_codes </PATH>"], "scores": [-7.576882489956915e-05, -0.10537182539701462, -0.1512071192264557, -0.16804450750350952, -0.17193703353405], "norm_scores": [0.22485707700252533, 0.20238441228866577, 0.1933174431324005, 0.19008973240852356, 0.18935124576091766]}}
{"id": "WebQTest-1148", "question": "what college did se hinton go", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did se hinton go [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0176535677164793, -0.01979476772248745, -0.023088006302714348, -0.05257444083690643, -0.06173253804445267], "norm_scores": [0.2034587264060974, 0.20302355289459229, 0.2023560255765915, 0.19647639989852905, 0.19468526542186737]}}
{"id": "WebQTest-1149", "question": "when was taylor swift fearless tour", "prediction": [["music.concert_tour.artist"], ["time.event.locations"], ["music.artist.concert_tours", "time.event.locations"], ["time.event.instance_of_recurring_event"], ["time.event.included_in_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was taylor swift fearless tour [/INST]", "raw_output": {"paths": ["<PATH> music.concert_tour.artist </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> music.artist.concert_tours <SEP> time.event.locations </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.event.included_in_event </PATH>"], "scores": [-0.004350313451141119, -0.04937578737735748, -0.06220513954758644, -0.0700296014547348, -0.07419470697641373], "norm_scores": [0.20969943702220917, 0.20046700537204742, 0.1979115903377533, 0.19636908173561096, 0.19555288553237915]}}
{"id": "WebQTest-1151", "question": "where did charles darwin and his wife live", "prediction": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects", "location.location.contains"], ["book.written_work.subjects", "book.written_work.subjects"]], "ground_paths": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_burial", "travel.travel_destination.tourist_attractions"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["book.written_work.subjects", "location.location.contains"], ["people.person.nationality", "location.country.capital"], ["people.person.children", "people.deceased_person.place_of_death"], ["book.written_work.subjects"], ["book.written_work.subjects", "location.location.containedby"], ["book.written_work.subjects", "location.country.capital"], ["book.book_subject.works", "book.written_work.subjects"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin and his wife live [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.contains </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.subjects </PATH>"], "scores": [-0.03018813394010067, -0.03427131474018097, -0.03436610847711563, -0.040357016026973724, -0.04123222082853317], "norm_scores": [0.20118072628974915, 0.20036093890666962, 0.20034195482730865, 0.19914531707763672, 0.19897109270095825]}}
{"id": "WebQTest-1152", "question": "who is the voice of kitt", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of kitt [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.03216390684247017, -0.032350681722164154, -0.039878640323877335, -0.041681814938783646, -0.04266839474439621], "norm_scores": [0.20111796259880066, 0.20108039677143097, 0.19957235455513, 0.19921281933784485, 0.19901637732982635]}}
{"id": "WebQTest-1153", "question": "what is eminem 's real full name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.nationality"], ["music.artist.genre", "music.artist.genre"], ["music.artist.album", "music.composer.compositions"], ["music.artist.album"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is eminem 's real full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.album <SEP> music.composer.compositions </PATH>", "<PATH> music.artist.album </PATH>"], "scores": [-0.03364603593945503, -0.039809200912714005, -0.04148057475686073, -0.04217800125479698, -0.047257836908102036], "norm_scores": [0.2014489471912384, 0.20021119713783264, 0.19987685978412628, 0.19973748922348022, 0.19872543215751648]}}
{"id": "WebQTest-1154", "question": "who is amy grant 's first husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.artist.genre", "music.artist.genre"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["music.artist.origin", "music.artist.origin"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is amy grant 's first husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.025407912209630013, -0.033779654651880264, -0.03380022570490837, -0.04310649260878563, -0.04354175925254822], "norm_scores": [0.20211033523082733, 0.20042537152767181, 0.2004212588071823, 0.19856473803520203, 0.1984783262014389]}}
{"id": "WebQTest-1155", "question": "what are the seven nations of central america", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["book.book_subject.works"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the seven nations of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.009076935239136219, -0.0258347000926733, -0.04303707554936409, -0.09839212894439697, -0.11495593935251236], "norm_scores": [0.20990398526191711, 0.20641577243804932, 0.20289529860019684, 0.19196921586990356, 0.1888156682252884]}}
{"id": "WebQTest-1156", "question": "what is jamie spears baby name", "prediction": [["people.person.children"], ["people.person.parents"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["people.person.parent"], ["common.topic.notable_types"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie spears baby name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.celebrity.canoodled </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.009095337241888046, -0.020250597968697548, -0.12947548925876617, -0.13287831842899323, -0.134121835231781], "norm_scores": [0.21544598042964935, 0.21305598318576813, 0.19101081788539886, 0.19036193192005157, 0.19012537598609924]}}
{"id": "WebQTest-1160", "question": "who is the mother of prince michael jackson", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"], ["music.artist.genre", "music.artist.genre"], ["people.person.profession"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the mother of prince michael jackson [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.007530447095632553, -0.03201695531606674, -0.04045398533344269, -0.08597732335329056, -0.11712976545095444], "norm_scores": [0.20990024507045746, 0.20482292771339417, 0.20310211181640625, 0.19406352937221527, 0.18811117112636566]}}
{"id": "WebQTest-1161", "question": "what is the clemson mascot name", "prediction": [["education.school_mascot.school"], ["education.educational_institution.mascot"], [], ["sports.school_mascot.school"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["education.school_mascot.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the clemson mascot name [/INST]", "raw_output": {"paths": ["<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.mascot </PATH>", "<PATH> </PATH>", "<PATH> sports.school_mascot.school </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.012337880209088326, -0.014345052652060986, -0.06703012436628342, -0.08009470254182816, -0.08079571276903152], "norm_scores": [0.20776666700839996, 0.20735006034374237, 0.1967085748910904, 0.19415539503097534, 0.19401933252811432]}}
{"id": "WebQTest-1163", "question": "what rainforest is in south america", "prediction": [["location.location.containedby"], ["common.topic.notable_types"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.contains"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rainforest is in south america [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0036033843643963337, -0.03994753584265709, -0.062420256435871124, -0.08498635143041611, -0.09045016765594482], "norm_scores": [0.21071060001850128, 0.20318999886512756, 0.19867469370365143, 0.19424158334732056, 0.19318315386772156]}}
{"id": "WebQTest-1164", "question": "what type of art does claude monet do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"], ["visual_art.artist.associated_periods_or_movements"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does claude monet do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.005878344643861055, -0.026685692369937897, -0.036237750202417374, -0.08375882357358932, -0.1064542904496193], "norm_scores": [0.20925335586071014, 0.20494432747364044, 0.20299601554870605, 0.19357503950595856, 0.1892312467098236]}}
{"id": "WebQTest-1166", "question": "when did we start war with iraq", "prediction": [["base.culturalevent.event.entity_involved"], ["time.event.locations"], ["time.event.includes_event"], ["time.event.included_in_event"], ["military.military_combatant_group.conflict", "military.military_combatant_group.conflict"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did we start war with iraq [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.conflict </PATH>"], "scores": [-0.02527010813355446, -0.028062913566827774, -0.032027654349803925, -0.037634916603565216, -0.039362747222185135], "norm_scores": [0.20144258439540863, 0.20088078081607819, 0.20008592307567596, 0.1989671140909195, 0.1986236274242401]}}
{"id": "WebQTest-1168", "question": "what guitar did kurt cobain", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"], ["music.artist.guitars_played"]], "ground_paths": [["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did kurt cobain [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> music.artist.guitars_played </PATH>"], "scores": [-0.01227802224457264, -0.031818099319934845, -0.0370509959757328, -0.04640424996614456, -0.07564150542020798], "norm_scores": [0.20570914447307587, 0.20172858238220215, 0.20067571103572845, 0.198807492852211, 0.19307906925678253]}}
{"id": "WebQTest-1169", "question": "who did jennifer lawrence play in x men", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jennifer lawrence play in x men [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.021454470232129097, -0.0278193149715662, -0.03057217039167881, -0.034954410046339035, -0.04006526246666908], "norm_scores": [0.20190879702568054, 0.20062775909900665, 0.20007622241973877, 0.19920136034488678, 0.19818586111068726]}}
{"id": "WebQTest-1170", "question": "who is the governor of virginia 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of virginia 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.017313653603196144, -0.019047604873776436, -0.02484819106757641, -0.026373349130153656, -0.029163967818021774], "norm_scores": [0.20120877027511597, 0.2008601874113083, 0.19969844818115234, 0.1993941068649292, 0.19883845746517181]}}
{"id": "WebQTest-1171", "question": "who did darth vaders voice", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.02263719029724598, -0.023241790011525154, -0.03610115125775337, -0.03834342956542969, -0.03894520923495293], "norm_scores": [0.20184637606143951, 0.20172438025474548, 0.19914694130420685, 0.1987009048461914, 0.19858136773109436]}}
{"id": "WebQTest-1173", "question": "where is walton county ga", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.hud_county_place.county"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton county ga [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00899149477481842, -0.0315946489572525, -0.04582061618566513, -0.05160592123866081, -0.0656765028834343], "norm_scores": [0.206412672996521, 0.2017994374036789, 0.1989489644765854, 0.19780130684375763, 0.1950376182794571]}}
{"id": "WebQTest-1174", "question": "what cancer did jackie kennedy die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.parent_disease"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did jackie kennedy die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.parent_disease </PATH>"], "scores": [-0.014282732270658016, -0.014607842080295086, -0.03564189374446869, -0.044304259121418, -0.10649719089269638], "norm_scores": [0.20572420954704285, 0.20565733313560486, 0.20137672126293182, 0.19963984191417694, 0.18760187923908234]}}
{"id": "WebQTest-1175", "question": "what does lauren conrad major in", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["education.education.student", "education.education.field_of_study"]], "ground_paths": [["people.person.education", "education.field_of_study.students_majoring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does lauren conrad major in [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> education.education.student <SEP> education.education.field_of_study </PATH>"], "scores": [-0.00897628627717495, -0.021262243390083313, -0.045177385210990906, -0.045272912830114365, -0.052673861384391785], "norm_scores": [0.20517730712890625, 0.20267193019390106, 0.1978825181722641, 0.1978636234998703, 0.19640463590621948]}}
{"id": "WebQTest-1176", "question": "what time zone is pennsylvania in right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is pennsylvania in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-1.6166426576091908e-05, -0.12387301027774811, -0.18261559307575226, -0.18652676045894623, -0.20512966811656952], "norm_scores": [0.22930455207824707, 0.20259201526641846, 0.19103401899337769, 0.19028833508491516, 0.1867811381816864]}}
{"id": "WebQTest-1177", "question": "where did mario lopez go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mario lopez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.01263825036585331, -0.024186350405216217, -0.025665365159511566, -0.033044103533029556, -0.04671330749988556], "norm_scores": [0.20317457616329193, 0.20084179937839508, 0.20054496824741364, 0.19907064735889435, 0.196368008852005]}}
{"id": "WebQTest-1178", "question": "where is tupac from originally", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["music.artist.origin"], ["people.ethnicity.people"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tupac from originally [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.004620186053216457, -0.033085331320762634, -0.0698503851890564, -0.09353089332580566, -0.11212415993213654], "norm_scores": [0.21178416907787323, 0.20584069192409515, 0.19841037690639496, 0.19376710057258606, 0.1901976317167282]}}
{"id": "WebQTest-1179", "question": "what role did george lucas play in star wars", "prediction": [["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.writer.film"], ["film.film.produced_by"], ["film.producer.films_executive_produced"], ["film.film.directed_by"]], "ground_paths": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did george lucas play in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.writer.film </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.producer.films_executive_produced </PATH>", "<PATH> film.film.directed_by </PATH>"], "scores": [-0.029712555930018425, -0.03857512027025223, -0.042896874248981476, -0.043161790817976, -0.045352790504693985], "norm_scores": [0.20205283164978027, 0.20027002692222595, 0.19940638542175293, 0.19935356080532074, 0.19891725480556488]}}
{"id": "WebQTest-1180", "question": "what sarah dessen books are movies", "prediction": [["film.film_story_contributor.film_story_credits"], ["book.book_edition.author_editor"], ["book.author.works_written"], ["film.film.story_by"], ["book.written_work.author"]], "ground_paths": [["film.film.story_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sarah dessen books are movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.025202777236700058, -0.03139691427350044, -0.032844845205545425, -0.03360799327492714, -0.03526052460074425], "norm_scores": [0.20129495859146118, 0.20005196332931519, 0.1997625082731247, 0.19961011409759521, 0.1992805302143097]}}
{"id": "WebQTest-1181", "question": "what is the current time in kauai hawaii", "prediction": [["location.location.time_zones"], ["geography.island.island_group", "location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby", "location.location.time_zones"], ["location.location.time_zones", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in kauai hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> geography.island.island_group <SEP> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>"], "scores": [-0.00023672354291193187, -0.0819760262966156, -0.09349799901247025, -0.094187431037426, -0.09478824585676193], "norm_scores": [0.21493467688560486, 0.1980649083852768, 0.1957959234714508, 0.19566096365451813, 0.19554345309734344]}}
{"id": "WebQTest-1183", "question": "where was farrah fawcett buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"], ["people.deceased_person.place_of_death"], ["people.person.place_of_burial"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was farrah fawcett buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_burial </PATH>"], "scores": [-0.002533893333747983, -0.03829246014356613, -0.0682026669383049, -0.09604107588529587, -0.10798221081495285], "norm_scores": [0.21222561597824097, 0.20477080345153809, 0.198736771941185, 0.19328054785728455, 0.1909862905740738]}}
{"id": "WebQTest-1185", "question": "what inspired steinbeck", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired steinbeck [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <PATH>"], "scores": [-0.010320034809410572, -0.01476072333753109, -0.11988524347543716, -0.11996070295572281, -0.17640666663646698], "norm_scores": [0.21575336158275604, 0.21479739248752594, 0.1933632791042328, 0.193348690867424, 0.18273724615573883]}}
{"id": "WebQTest-1186", "question": "what kind of government does cuba have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does cuba have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> government.form_of_government.countries <PATH>", "<PATH> location.country.form_of_government <PATH>"], "scores": [-0.011834383010864258, -0.011911366134881973, -0.14772602915763855, -0.1773291677236557, -0.1774815320968628], "norm_scores": [0.21892954409122467, 0.21891270577907562, 0.1911117434501648, 0.1855371594429016, 0.18550889194011688]}}
{"id": "WebQTest-1187", "question": "where are english speakers distributed", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are english speakers distributed [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.014058561995625496, -0.02442874386906624, -0.031205972656607628, -0.055496178567409515, -0.07962429523468018], "norm_scores": [0.2053966522216797, 0.20327766239643097, 0.20190465450286865, 0.19705943763256073, 0.19236166775226593]}}
{"id": "WebQTest-1189", "question": "what is the capital of italy today", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"], ["location.location.containedby"], ["location.country.first_level_divisions"], ["base.aareas.schema.administrative_area.capital"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of italy today [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.first_level_divisions </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>"], "scores": [-0.013156742788851261, -0.02685517445206642, -0.02727009728550911, -0.07753832638263702, -0.07986342161893845], "norm_scores": [0.20637714862823486, 0.20356939733028412, 0.2034849375486374, 0.19350895285606384, 0.1930595487356186]}}
{"id": "WebQTest-1191", "question": "what language do people speak in the bahamas", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.language_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.language_spoken </PATH>"], "scores": [-0.004838799592107534, -0.03287547081708908, -0.04259294271469116, -0.07575240731239319, -0.1708272248506546], "norm_scores": [0.21213854849338531, 0.20627348124980927, 0.20427873730659485, 0.19761604070663452, 0.17969323694705963]}}
{"id": "WebQTest-1192", "question": "where did george michael go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george michael go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.016009865328669548, -0.019006051123142242, -0.025311660021543503, -0.030500352382659912, -0.05336730554699898], "norm_scores": [0.20256467163562775, 0.20195864140987396, 0.2006891965866089, 0.1996505707502365, 0.19513697922229767]}}
{"id": "WebQTest-1194", "question": "what club does cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what club does cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.02029261365532875, -0.02185242623090744, -0.026875637471675873, -0.03326154500246048, -0.03695424273610115], "norm_scores": [0.20151251554489136, 0.20119844377040863, 0.20019030570983887, 0.1989159733057022, 0.19818279147148132]}}
{"id": "WebQTest-1195", "question": "what time zone is in the bahamas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.country.form_of_government"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-8.15111070551211e-06, -0.15861418843269348, -0.18566448986530304, -0.18750815093517303, -0.19285613298416138], "norm_scores": [0.23054605722427368, 0.19673243165016174, 0.1914820820093155, 0.1911294013261795, 0.1901099532842636]}}
{"id": "WebQTest-1197", "question": "what language do people speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "</PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0005907469312660396, -0.07430504262447357, -0.0841919481754303, -0.10258021950721741, -0.1873743236064911], "norm_scores": [0.21827402710914612, 0.20276282727718353, 0.20076800882816315, 0.19710996747016907, 0.18108521401882172]}}
{"id": "WebQTest-1198", "question": "who influenced samuel taylor coleridge", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced samuel taylor coleridge [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.006951925344765186, -0.018700124695897102, -0.09031832218170166, -0.09449164569377899, -0.10200084745883942], "norm_scores": [0.21124473214149475, 0.2087775319814682, 0.19434812664985657, 0.19353872537612915, 0.19209085404872894]}}
{"id": "WebQTest-1199", "question": "what does kim kardashian work as", "prediction": [["people.person.profession"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"], ["people.person.profession", "people.profession.specializations"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does kim kardashian work as [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>"], "scores": [-0.0009613500442355871, -0.07500221580266953, -0.08776132762432098, -0.0888361856341362, -0.09002000838518143], "norm_scores": [0.21385060250759125, 0.19858887791633606, 0.19607116281986237, 0.19586053490638733, 0.1956288069486618]}}
{"id": "WebQTest-1200", "question": "where did the mississippi river end", "prediction": [["geography.river.mouth"], ["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.mouth"], ["location.location.partially_contains"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the mississippi river end [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.mouth </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.001412808895111084, -0.060619764029979706, -0.08991048485040665, -0.09867310523986816, -0.11398744583129883], "norm_scores": [0.21465365588665009, 0.20231357216835022, 0.1964736133813858, 0.1947595179080963, 0.19179962575435638]}}
{"id": "WebQTest-1201", "question": "who played carlton lassiter on psych", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carlton lassiter on psych [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.01986639015376568, -0.026767851784825325, -0.031440213322639465, -0.03940064087510109, -0.04260179027915001], "norm_scores": [0.20243768393993378, 0.20104537904262543, 0.20010821521282196, 0.19852158427238464, 0.1978871077299118]}}
{"id": "WebQTest-1203", "question": "who is president of israel", "prediction": [["people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is president of israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>"], "scores": [-0.01929585263133049, -0.019980570301413536, -0.024868588894605637, -0.03724708408117294, -0.03898859769105911], "norm_scores": [0.20175661146640778, 0.2016185075044632, 0.2006353884935379, 0.1981671303510666, 0.19782233238220215]}}
{"id": "WebQTest-1204", "question": "where did the anasazi indians live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.contains"], ["location.location.containedby"], ["location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the anasazi indians live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0007493912125937641, -0.08215894550085068, -0.09508948028087616, -0.09788874536752701, -0.10138064622879028], "norm_scores": [0.21535548567771912, 0.19851815700531006, 0.19596773386001587, 0.19541993737220764, 0.1947387307882309]}}
{"id": "WebQTest-1205", "question": "who plays harley quinn", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays harley quinn [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.01628565602004528, -0.031862031668424606, -0.03933826833963394, -0.04086586833000183, -0.04299968108534813], "norm_scores": [0.20361976325511932, 0.2004726678133011, 0.1989794820547104, 0.1986757516860962, 0.19825226068496704]}}
{"id": "WebQTest-1206", "question": "what was francis bacon contributions", "prediction": [["people.person.profession"], ["law.invention.inventor"], ["law.inventor.inventions"], ["base.argumentmaps.innovator.original_ideas"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was francis bacon contributions [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0060399798676371574, -0.04323170706629753, -0.04449838027358055, -0.07237742096185684, -0.08649908751249313], "norm_scores": [0.209018275141716, 0.20138731598854065, 0.2011323720216751, 0.1956024467945099, 0.19285960495471954]}}
{"id": "WebQTest-1209", "question": "when does ziva start on ncis", "prediction": [["tv.tv_series_season.series"], ["tv.tv_program.seasons"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.seasons"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes", "tv.tv_series_season.series"], ["tv.regular_tv_appearance.series", "tv.tv_series_season.series"]], "ground_paths": [["tv.tv_program.seasons"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does ziva start on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_series_season.series </PATH>", "<PATH> tv.tv_program.seasons </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> fictional_universe.fictional_character.appears_in_these_fictional_universes <SEP> tv.tv_series_season.series </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_series_season.series </PATH>"], "scores": [-0.012601534835994244, -0.03595984727144241, -0.045552801340818405, -0.052905309945344925, -0.0555214025080204], "norm_scores": [0.20563505589962006, 0.20088742673397064, 0.1989695429801941, 0.1975119709968567, 0.19699595868587494]}}
{"id": "WebQTest-1210", "question": "what is the name of the national anthem of canada ( 2 words )", "prediction": [["music.composition.language"], ["music.composition.language", "music.composition.language"], ["music.composer.compositions"], ["location.location.containedby", "music.composition.language"], ["music.composition.language", "language.human_language.main_country"]], "ground_paths": [["location.country.official_language", "music.composition.language"], ["location.country.national_anthem", "government.national_anthem.national_anthem_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the national anthem of canada ( 2 words ) [/INST]", "raw_output": {"paths": ["<PATH> music.composition.language </PATH>", "<PATH> music.composition.language <SEP> music.composition.language </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> location.location.containedby <SEP> music.composition.language </PATH>", "<PATH> music.composition.language <SEP> language.human_language.main_country </PATH>"], "scores": [-0.01109367050230503, -0.020101558417081833, -0.059121884405612946, -0.059180449694395065, -0.059614766389131546], "norm_scores": [0.20619289577007294, 0.20434387028217316, 0.19652387499809265, 0.1965123564004898, 0.19642703235149384]}}
{"id": "WebQTest-1212", "question": "what did mark zuckerberg study", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.degree"], ["people.person.profession", "people.profession.part_of_professional_field"]], "ground_paths": [["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.field_of_study.students_majoring"], ["people.person.profession", "base.skills.skill.performed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mark zuckerberg study [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.profession <SEP> people.profession.part_of_professional_field </PATH>"], "scores": [-0.010131558403372765, -0.01943601295351982, -0.03317825496196747, -0.06184154748916626, -0.07392950356006622], "norm_scores": [0.20594123005867004, 0.20403394103050232, 0.20124922692775726, 0.19556266069412231, 0.19321294128894806]}}
{"id": "WebQTest-1213", "question": "which country was michael jackson born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"], ["music.artist.origin", "location.location.containedby"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was michael jackson born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin <SEP> location.location.containedby </PATH>"], "scores": [-0.013426738791167736, -0.017053719609975815, -0.0481441430747509, -0.08062927424907684, -0.1035757064819336], "norm_scores": [0.20785444974899292, 0.20710192620754242, 0.20076210796833038, 0.19434510171413422, 0.18993636965751648]}}
{"id": "WebQTest-1215", "question": "who was stephen r covey", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was stephen r covey [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.001012528082355857, -0.05973857641220093, -0.0995178297162056, -0.129592627286911, -0.13781479001045227], "norm_scores": [0.2173609882593155, 0.204963818192482, 0.19697055220603943, 0.19113487005233765, 0.18956978619098663]}}
{"id": "WebQTest-1216", "question": "what country was stalin in charge of", "prediction": [["people.person.nationality"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.politician.government_positions_held", "government.governmental_jurisdiction.governing_officials"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was stalin in charge of [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.015044106170535088, -0.022381603717803955, -0.043458983302116394, -0.044408030807971954, -0.05005538836121559], "norm_scores": [0.20402617752552032, 0.20253461599349976, 0.19831037521362305, 0.1981222778558731, 0.19700655341148376]}}
{"id": "WebQTest-1217", "question": "what is monta ellis career high points", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.player"], ["basketball.basketball_player.statistics", "basketball.basketball_player_stats.season"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_award.award"], ["award.award_honor.award_winner", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is monta ellis career high points [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> basketball.basketball_player.statistics <SEP> basketball.basketball_player_stats.season </PATH>"], "scores": [-0.011883684433996677, -0.01701273024082184, -0.03984177112579346, -0.04422816261649132, -0.048410564661026], "norm_scores": [0.20409753918647766, 0.20305338501930237, 0.19847038388252258, 0.19760173559188843, 0.19677700102329254]}}
{"id": "WebQTest-1219", "question": "what nationality is rihanna", "prediction": [["people.person.nationality"], ["people.ethnicity.people"], ["music.artist.origin"], ["people.person.ethnicity"], ["music.album.artist"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is rihanna [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.011574914678931236, -0.029484814032912254, -0.041220471262931824, -0.0517100989818573, -0.09736095368862152], "norm_scores": [0.20697550475597382, 0.2033015936613083, 0.200929656624794, 0.19883301854133606, 0.18996016681194305]}}
{"id": "WebQTest-1220", "question": "what has shannen doherty played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.languages", "tv.tv_program.languages"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has shannen doherty played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.016106611117720604, -0.03181976079940796, -0.03441018983721733, -0.03640766814351082, -0.039857830852270126], "norm_scores": [0.20314036309719086, 0.19997332990169525, 0.19945597648620605, 0.19905796647071838, 0.19837237894535065]}}
{"id": "WebQTest-1221", "question": "what college did joakim noah", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did joakim noah [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.0184884462505579, -0.021701432764530182, -0.023837080225348473, -0.025045938789844513, -0.052114203572273254], "norm_scores": [0.2019445151090622, 0.20129671692848206, 0.20086726546287537, 0.20062460005283356, 0.19526687264442444]}}
{"id": "WebQTest-1223", "question": "who plays billy elliot", "prediction": [["film.film.country", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["film.performance.film", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.language", "people.person.languages"], ["media_common.netflix_title.netflix_genres", "people.person.nationality"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays billy elliot [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.030135083943605423, -0.03332993760704994, -0.03439057990908623, -0.0351826548576355, -0.036573562771081924], "norm_scores": [0.2007584273815155, 0.2001180499792099, 0.1999059021472931, 0.19974763691425323, 0.19946999847888947]}}
{"id": "WebQTest-1225", "question": "what kourtney kardashian boyfriends name", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kourtney kardashian boyfriends name [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.019969996064901352, -0.023495856672525406, -0.024702655151486397, -0.03732109069824219, -0.042134642601013184], "norm_scores": [0.20191265642642975, 0.20120199024677277, 0.20095932483673096, 0.19843944907188416, 0.19748656451702118]}}
{"id": "WebQTest-1226", "question": "what form of government does australia use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.007412806618958712, -0.018422992900013924, -0.1397469937801361, -0.14109660685062408, -0.14581656455993652], "norm_scores": [0.2168862372636795, 0.21451137959957123, 0.19000281393527985, 0.18974654376506805, 0.18885305523872375]}}
{"id": "WebQTest-1227", "question": "what team did howie long", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sport.team_coaches", "sports.sports_team.sport"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did howie long [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sport.team_coaches <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.011783488094806671, -0.013021744787693024, -0.0504717193543911, -0.061956897377967834, -0.06466656923294067], "norm_scores": [0.20574571192264557, 0.2054910957813263, 0.19793778657913208, 0.19567742943763733, 0.19514793157577515]}}
{"id": "WebQTest-1228", "question": "where did the greek alphabet come from", "prediction": [["language.human_language.language_family"], ["language.language_family.languages"], ["language.human_language.writing_system"], ["base.rosetta.languoid.parent"], ["language.language_writing_system.parent_writing_systems"]], "ground_paths": [["language.language_writing_system.child_writing_systems"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the greek alphabet come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> language.human_language.writing_system </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> language.language_writing_system.parent_writing_systems </PATH>"], "scores": [-0.020918510854244232, -0.021531375125050545, -0.04545747861266136, -0.070464126765728, -0.07209865748882294], "norm_scores": [0.2050476223230362, 0.20492200553417206, 0.20007719099521637, 0.19513598084449768, 0.19481727480888367]}}
{"id": "WebQTest-1229", "question": "what timezone is elizabethtown kentucky", "prediction": [["location.location.time_zones"], ["location.location.time_Zones"], ["location.city.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is elizabethtown kentucky [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.city.time_zones </PATH>"], "scores": [-9.278236575482879e-06, -0.17690660059452057, -0.1806044727563858, -0.1975538432598114, -0.20510733127593994], "norm_scores": [0.23212896287441254, 0.19449295103549957, 0.19377504289150238, 0.1905183643102646, 0.1890847086906433]}}
{"id": "WebQTest-1230", "question": "what to see in christchurch new zealand", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["organization.organization.geographic_scope"], ["film.film_festival.location"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in christchurch new zealand [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film_festival.location </PATH>"], "scores": [-0.012580562382936478, -0.012630491517484188, -0.04584202542901039, -0.11994580924510956, -0.13716869056224823], "norm_scores": [0.21060195565223694, 0.21059143543243408, 0.20371225476264954, 0.18916217982769012, 0.18593212962150574]}}
{"id": "WebQTest-1231", "question": "where is chris paul from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.country"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.country"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris paul from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.country </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.country </PATH>"], "scores": [-0.01066128071397543, -0.017030322924256325, -0.06896893680095673, -0.08631860464811325, -0.09952378273010254], "norm_scores": [0.20924371480941772, 0.207915261387825, 0.19739209115505219, 0.19399693608283997, 0.1914520114660263]}}
{"id": "WebQTest-1232", "question": "what year was albert pujols rookie year", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.pro_athlete.teams", "sports.sports_league_season.teams"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.starting_season"], ["sports.sports_award.award_winner", "sports.sports_league_season.awards"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was albert pujols rookie year [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_season.teams </PATH>"], "scores": [-0.013008506037294865, -0.013190153054893017, -0.038515299558639526, -0.050819627940654755, -0.052315499633550644], "norm_scores": [0.20412394404411316, 0.20408685505390167, 0.19898322224617004, 0.1965498924255371, 0.1962561011314392]}}
{"id": "WebQTest-1233", "question": "what are the colors of the ny giants", "prediction": [["sports.sports_team.colors"], ["sports.professional_sports_team.owner_s", "sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.professional_sports_team.colors"], ["sports.sports_team. Colors"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the colors of the ny giants [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.colors </PATH>", "<PATH> sports.sports_team. Colors </PATH>"], "scores": [-4.341404201113619e-05, -0.10725963115692139, -0.12944984436035156, -0.14059439301490784, -0.14976543188095093], "norm_scores": [0.22188815474510193, 0.19932910799980164, 0.19495466351509094, 0.1927940547466278, 0.1910340040922165]}}
{"id": "WebQTest-1234", "question": "what business did andrew carnegie dominate", "prediction": [["people.person.profession"], ["book.written_work.subjects"], ["book.written_work.author"], ["book.author.works_written"], ["organization.organization.founders"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what business did andrew carnegie dominate [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.010482337325811386, -0.04335300996899605, -0.049672339111566544, -0.056329745799303055, -0.06167849153280258], "norm_scores": [0.20684611797332764, 0.20015747845172882, 0.1988966166973114, 0.1975768804550171, 0.19652289152145386]}}
{"id": "WebQTest-1235", "question": "what was sir isaac newton 's inventions", "prediction": [["law.invention.inventor"], ["law.inventor.inventions"], ["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was sir isaac newton 's inventions [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>"], "scores": [-0.004314177669584751, -0.03151681274175644, -0.061449356377124786, -0.06430255621671677, -0.0922589972615242], "norm_scores": [0.209414541721344, 0.2037947028875351, 0.19778500497341156, 0.19722148776054382, 0.19178423285484314]}}
{"id": "WebQTest-1237", "question": "who did shaq first play for", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did shaq first play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.01894804835319519, -0.020592447370290756, -0.022070853039622307, -0.03855927288532257, -0.04623950272798538], "norm_scores": [0.20206525921821594, 0.20173326134681702, 0.20143522322177887, 0.19814111292362213, 0.19662517309188843]}}
{"id": "WebQTest-1238", "question": "who was the artist of mickey mouse", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.character_created_by", "people.person.profession"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the artist of mickey mouse [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by <SEP> people.person.profession </PATH>"], "scores": [-0.0009684527758508921, -0.07055049389600754, -0.07156804203987122, -0.07537293434143066, -0.09534325450658798], "norm_scores": [0.21263673901557922, 0.19834406673908234, 0.19814234972000122, 0.1973898708820343, 0.1934870332479477]}}
{"id": "WebQTest-1239", "question": "what is the zip code for concord", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.us_county.hud_county_place"], ["location.postal_code.citytown"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for concord [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.postal_code.citytown </PATH>"], "scores": [-0.0001412837445968762, -0.1019449383020401, -0.1382015347480774, -0.1522684246301651, -0.16938847303390503], "norm_scores": [0.22334229946136475, 0.20172430574893951, 0.19454145431518555, 0.1918240338563919, 0.18856793642044067]}}
{"id": "WebQTest-1240", "question": "what football team is michael vick on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team is michael vick on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.018961645662784576, -0.02334068901836872, -0.03505615144968033, -0.03732965886592865, -0.039270397275686264], "norm_scores": [0.2023734152317047, 0.2014891654253006, 0.19914239645004272, 0.19869014620780945, 0.1983049213886261]}}
{"id": "WebQTest-1241", "question": "where does the columbia river start", "prediction": [["geography.lake.outflow"], ["geography.river.origin"], [], ["location.location.containedby"], ["geography.lake.outflow", "geography.lake.outflow"]], "ground_paths": [["geography.lake.outflow"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the columbia river start [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.outflow </PATH>", "<PATH> geography.river.origin </PATH>", "<PATH> </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.lake.outflow <SEP> geography.lake.outflow </PATH>"], "scores": [-0.007130471523851156, -0.02932092174887657, -0.06926599889993668, -0.07589210569858551, -0.10528495907783508], "norm_scores": [0.2101781815290451, 0.20556560158729553, 0.19751611351966858, 0.19621168076992035, 0.19052837789058685]}}
{"id": "WebQTest-1242", "question": "what are all the names of the harry potter books", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["book.book_character.appears_in_book"], ["fictional_universe.fictional_universe.works_set_here"], ["book.book.genre"], ["book.written_work.subjects"]], "ground_paths": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["book.book_character.appears_in_book"], ["fictional_universe.fictional_universe.works_set_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the names of the harry potter books [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> book.book_character.appears_in_book </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>", "<PATH> book.book.genre </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.008194608613848686, -0.026132788509130478, -0.031493254005908966, -0.043521832674741745, -0.07727313786745071], "norm_scores": [0.20585724711418152, 0.20219746232032776, 0.20111650228500366, 0.1987118422985077, 0.19211696088314056]}}
{"id": "WebQTest-1244", "question": "who was sir walter raleigh and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["people.profession.people_with_this_profession"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was sir walter raleigh and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.0008446915890090168, -0.060389939695596695, -0.07653400301933289, -0.08534033596515656, -0.09226803481578827], "norm_scores": [0.21272504329681396, 0.20042802393436432, 0.19721828401088715, 0.19548913836479187, 0.1941395401954651]}}
{"id": "WebQTest-1245", "question": "what countries are in the euro dollar", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.coinsdaily.coin_type.country", "olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.coinsdaily.coin_type.country", "base.locations.countries.planet", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the euro dollar [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> base.locations.countries.planet <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.01156794372946024, -0.01863928884267807, -0.04867421090602875, -0.057140279561281204, -0.0622464083135128], "norm_scores": [0.2056528925895691, 0.2042037844657898, 0.19816173613071442, 0.19649115204811096, 0.19549041986465454]}}
{"id": "WebQTest-1246", "question": "what currency can you use in barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["location.country.form_of_government"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.011846606619656086, -0.01395240519195795, -0.12768582999706268, -0.13524076342582703, -0.1383180022239685], "norm_scores": [0.21488603949546814, 0.21443401277065277, 0.19138146936893463, 0.18994103372097015, 0.1893574446439743]}}
{"id": "WebQTest-1247", "question": "who is angelina jolie", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.person.nationality"], ["people.person.parents"], ["common.topic.subjects"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> common.topic.subjects </PATH>"], "scores": [-0.0019561657682061195, -0.05191349610686302, -0.09337910264730453, -0.10030380636453629, -0.10986699908971786], "norm_scores": [0.2142271250486374, 0.20378783345222473, 0.19551046192646027, 0.1941612809896469, 0.1923133283853531]}}
{"id": "WebQTest-1248", "question": "where is nineveh city", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is nineveh city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.0007828532252460718, -0.07373411953449249, -0.10208401829004288, -0.11796172708272934, -0.12211289256811142], "norm_scores": [0.21699152886867523, 0.2017253339290619, 0.19608676433563232, 0.19299793243408203, 0.19219842553138733]}}
{"id": "WebQTest-1249", "question": "what is the state flower of new mexico", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of new mexico [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.013477740809321404, -0.016825757920742035, -0.02236596681177616, -0.02515910379588604, -0.043552499264478683], "norm_scores": [0.20216034352779388, 0.20148463547229767, 0.20037145912647247, 0.19981259107589722, 0.19617094099521637]}}
{"id": "WebQTest-1250", "question": "where is the national ffa foundation located today", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the national ffa foundation located today [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.007715755607932806, -0.0355425663292408, -0.04131707549095154, -0.04597644507884979, -0.047516100108623505], "norm_scores": [0.20563620328903198, 0.19999288022518158, 0.19884136319160461, 0.19791702926158905, 0.19761253893375397]}}
{"id": "WebQTest-1251", "question": "which countries speak germanic languages", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.language_family"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak germanic languages [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.015172610990703106, -0.03037651628255844, -0.03510931506752968, -0.03842861205339432, -0.04845196753740311], "norm_scores": [0.20368869602680206, 0.20061525702476501, 0.1996680349111557, 0.19900637865066528, 0.19702161848545074]}}
{"id": "WebQTest-1252", "question": "who plays eowyn in lord of the rings", "prediction": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["film.film.starring", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays eowyn in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.033451374620199203, -0.03401497006416321, -0.034360043704509735, -0.03438599035143852, -0.03481043130159378], "norm_scores": [0.20015068352222443, 0.2000379115343094, 0.19996888935565948, 0.19996371865272522, 0.19987884163856506]}}
{"id": "WebQTest-1254", "question": "where is augusta masters 2012", "prediction": [["time.event.locations"], ["time.recurring_event.instances"], ["location.location.events"], ["time.recurring_event.current_frequency"], ["sports.sports_championship_event.champion"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is augusta masters 2012 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.002161057898774743, -0.051961034536361694, -0.06839577108621597, -0.08018486946821213, -0.08665044605731964], "norm_scores": [0.21136042475700378, 0.20109246671199799, 0.19781458377838135, 0.19549623131752014, 0.19423629343509674]}}
{"id": "WebQTest-1255", "question": "what type of cancer did eva peron die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.cause_of_death.parent_cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.cause_of_death.parent_cause_of_death </PATH>"], "scores": [-0.00805542804300785, -0.02041120082139969, -0.03735176473855972, -0.08430986851453781, -0.09257518500089645], "norm_scores": [0.20814327895641327, 0.20558732748031616, 0.202133908867836, 0.19286149740219116, 0.1912739872932434]}}
{"id": "WebQTest-1256", "question": "who did terrell owens play for in 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did terrell owens play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.01866021566092968, -0.02198120579123497, -0.022006580606102943, -0.027351852506399155, -0.027845891192555428], "norm_scores": [0.2009829580783844, 0.20031659305095673, 0.20031151175498962, 0.1992436647415161, 0.19914524257183075]}}
{"id": "WebQTest-1257", "question": "where is denmark south carolina", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"]], "ground_paths": [["location.us_county.hud_county_place"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark south carolina [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.013546095229685307, -0.020962409675121307, -0.051328837871551514, -0.05327565595507622, -0.06629451364278793], "norm_scores": [0.20554135739803314, 0.2040226310491562, 0.1979203075170517, 0.19753539562225342, 0.19498036801815033]}}
{"id": "WebQTest-1259", "question": "what books did ernest hemingway write", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"]], "ground_paths": [["film.film_story_contributor.film_story_credits"], ["book.book_edition.author_editor"], ["film.film.story_by"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did ernest hemingway write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.01664857380092144, -0.02244049496948719, -0.027390513569116592, -0.06932877004146576, -0.06976962834596634], "norm_scores": [0.20489750802516937, 0.20371419191360474, 0.20270828902721405, 0.19438284635543823, 0.1942971795797348]}}
{"id": "WebQTest-1260", "question": "what year did baltimore ravens win super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did baltimore ravens win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.012623515911400318, -0.013932829722762108, -0.039333805441856384, -0.07367700338363647, -0.08985701948404312], "norm_scores": [0.2066636085510254, 0.20639319717884064, 0.20121662318706512, 0.19442352652549744, 0.1913030594587326]}}
{"id": "WebQTest-1261", "question": "where did the battle of passchendaele happen", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "time.event.locations"], ["event.disaster.areas_affected"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of passchendaele happen [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> time.event.locations </PATH>", "<PATH> event.disaster.areas_affected </PATH>"], "scores": [-0.01315363496541977, -0.013194434344768524, -0.09101204574108124, -0.10240423679351807, -0.11343582719564438], "norm_scores": [0.21078133583068848, 0.21077273786067963, 0.19499284029006958, 0.19278405606746674, 0.1906690150499344]}}
{"id": "WebQTest-1262", "question": "what part of the country is ohio considered", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.partially_contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.01236794050782919, -0.015936046838760376, -0.08080250769853592, -0.08818811178207397, -0.09073898196220398], "norm_scores": [0.20912203192710876, 0.2083771824836731, 0.19528956711292267, 0.19385254383087158, 0.19335868954658508]}}
{"id": "WebQTest-1263", "question": "what team is cristiano ronaldo play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is cristiano ronaldo play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.021220635622739792, -0.02492987923324108, -0.025165822356939316, -0.03878634795546532, -0.039932239800691605], "norm_scores": [0.20175892114639282, 0.20101194083690643, 0.20096451044082642, 0.1982458233833313, 0.19801878929138184]}}
{"id": "WebQTest-1264", "question": "what standard time is michigan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is michigan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_Zones </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.5694098692620173e-05, -0.14180630445480347, -0.18874116241931915, -0.19663105905056, -0.19808554649353027], "norm_scores": [0.23054032027721405, 0.2000635713338852, 0.1908905804157257, 0.1893904060125351, 0.18911512196063995]}}
{"id": "WebQTest-1265", "question": "what was john tyler", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was john tyler [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.0033449798356741667, -0.042073704302310944, -0.04933207854628563, -0.05266828462481499, -0.05652228370308876], "norm_scores": [0.20759157836437225, 0.19970552623271942, 0.19826124608516693, 0.19760088622570038, 0.1968407928943634]}}
{"id": "WebQTest-1266", "question": "what did romo do", "prediction": [["american_football.football_player.position_s"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.position"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.position"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.position_s"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did romo do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.position </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.position </PATH>", "<PATH> american_football.player_passing_statistics.player <SEP> american_football.player_passing_statistics.position_s </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.01396319828927517, -0.03889119625091553, -0.04984474927186966, -0.05119089037179947, -0.05136129632592201], "norm_scores": [0.2054702639579773, 0.2004116177558899, 0.1982283741235733, 0.1979617327451706, 0.19792798161506653]}}
{"id": "WebQTest-1267", "question": "what currency can you use in jamaica", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in jamaica [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_used <PATH>"], "scores": [-0.0006629700656048954, -0.061717260628938675, -0.16217878460884094, -0.16702048480510712, -0.18337228894233704], "norm_scores": [0.2236446589231491, 0.21039867401123047, 0.1902887523174286, 0.18936964869499207, 0.18629828095436096]}}
{"id": "WebQTest-1268", "question": "where did jay moriarty die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jay moriarty die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.0002654877316672355, -0.0980389192700386, -0.10693755000829697, -0.11263418197631836, -0.12313389778137207], "norm_scores": [0.21816106140613556, 0.19784030318260193, 0.19608761370182037, 0.19497375190258026, 0.19293728470802307]}}
{"id": "WebQTest-1269", "question": "where was the boston tea party 1773", "prediction": [["time.event.locations"], ["location.location.events"], ["event.disaster.areas_affected"], ["base.culturalevent.event.entity_involved"], ["book.written_work.subjects"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the boston tea party 1773 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.006603787187486887, -0.0253591425716877, -0.0640501007437706, -0.07596128433942795, -0.10509104281663895], "norm_scores": [0.20987273752689362, 0.20597317814826965, 0.19815608859062195, 0.19580979645252228, 0.1901881992816925]}}
{"id": "WebQTest-1270", "question": "what drugs lindsay lohan abuse", "prediction": [["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["base.popstra.rehab_stay.patient", "base.popstra.rehab_stay.substance_abused"], ["people.person.substance_abuse", "people.substance_abuse.substance"], ["base.popstra.substance_abuse_problem.celebrity", "base.popstra.substance_abuse_problem.substance"]], "ground_paths": [["base.popstra.rehab_stay.patient", "base.popstra.rehab_stay.substance_abused"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"], ["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drugs lindsay lohan abuse [/INST]", "raw_output": {"paths": ["<PATH> base.popstra.substance_abuse.abuser <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> base.popstra.celebrity.substance_abuse <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> base.popstra.rehab_stay.patient <SEP> base.popstra.rehab_stay.substance_abused </PATH>", "<PATH> people.person.substance_abuse <SEP> people.substance_abuse.substance </PATH>", "<PATH> base.popstra.substance_abuse_problem.celebrity <SEP> base.popstra.substance_abuse_problem.substance </PATH>"], "scores": [-0.006919041741639376, -0.025234738364815712, -0.04689081758260727, -0.053905513137578964, -0.06376263499259949], "norm_scores": [0.20654714107513428, 0.2027985155582428, 0.19845391809940338, 0.19706669449806213, 0.1951337307691574]}}
{"id": "WebQTest-1271", "question": "when did carolina panthers go to superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["sports.sports_championship_event.runner_up"], ["time.recurring_event.instances"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did carolina panthers go to superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> time.recurring_event.instances </PATH>"], "scores": [-0.009469438344240189, -0.0182711873203516, -0.03868825361132622, -0.07014277577400208, -0.09637793153524399], "norm_scores": [0.20745456218719482, 0.2056366205215454, 0.2014806866645813, 0.19524185359477997, 0.1901862472295761]}}
{"id": "WebQTest-1272", "question": "when did the burma cyclone happen", "prediction": [["meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones"], ["time.event.locations"], ["meteorology.tropical_cyclone_season.tropical_cyclones"], ["meteorology.cyclone_affected_area.cyclones", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the burma cyclone happen [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> meteorology.tropical_cyclone_season.tropical_cyclones </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones <SEP> time.event.locations </PATH>"], "scores": [-0.008376006036996841, -0.020726237446069717, -0.049758508801460266, -0.06531524658203125, -0.102791428565979], "norm_scores": [0.20825766026973724, 0.20570145547389984, 0.19981533288955688, 0.19673092663288116, 0.18949463963508606]}}
{"id": "WebQTest-1274", "question": "who won the governor election in texas", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.administrative_division.country", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"]], "ground_paths": [["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.political_appointer.appointees"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["film.film_location.featured_in_films", "film.film.subjects"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.us_state.capital", "people.deceased_person.place_of_death"], ["location.administrative_division.country", "people.person.nationality"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "symbols.name_source.namesakes"], ["people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the governor election in texas [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.02433795854449272, -0.024462062865495682, -0.024919800460338593, -0.02808481641113758, -0.02995985746383667], "norm_scores": [0.2004028856754303, 0.20037801563739777, 0.20028631389141083, 0.19965340197086334, 0.19927938282489777]}}
{"id": "WebQTest-1275", "question": "what does helen flanagan do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types", "type.type.properties"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does helen flanagan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.properties </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.0010700745042413473, -0.058102797716856, -0.09820308536291122, -0.11956541985273361, -0.14183776080608368], "norm_scores": [0.21696937084197998, 0.2049412727355957, 0.19688567519187927, 0.192724347114563, 0.18847937881946564]}}
{"id": "WebQTest-1276", "question": "when are the summer and winter olympics held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.current_frequency"], ["time.event.instance_of_recurrence"], ["olympics.olympic_games.host_city"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when are the summer and winter olympics held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.event.instance_of_recurrence </PATH>", "<PATH> olympics.olympic_games.host_city </PATH>"], "scores": [-0.008738595992326736, -0.017256811261177063, -0.08656463772058487, -0.10998398810625076, -0.11335960775613785], "norm_scores": [0.2118186056613922, 0.2100219428539276, 0.19595876336097717, 0.19142286479473114, 0.19077777862548828]}}
{"id": "WebQTest-1277", "question": "where is tennessee river", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.river.basin_countries"], ["location.location.containedby"], ["location.administrative_division.country"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tennessee river [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.004700486082583666, -0.03426394611597061, -0.07510407269001007, -0.09138889610767365, -0.14363369345664978], "norm_scores": [0.21321342885494232, 0.20700238645076752, 0.19871866703033447, 0.19550877809524536, 0.18555670976638794]}}
{"id": "WebQTest-1278", "question": "what are some major events in venezuela", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.events"], ["government.form_of_government.countries"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"]], "ground_paths": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some major events in venezuela [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>"], "scores": [-0.0163594800978899, -0.02236495539546013, -0.024878162890672684, -0.07248387485742569, -0.09145442396402359], "norm_scores": [0.20582029223442078, 0.2045879364013672, 0.204074427485466, 0.19458693265914917, 0.1909303069114685]}}
{"id": "WebQTest-1279", "question": "what guitar does corey taylor play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"], ["music.musical_group.member", "music.group_membership.group"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar does corey taylor play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.group </PATH>"], "scores": [-0.007828428409993649, -0.032066330313682556, -0.05171983316540718, -0.06829731166362762, -0.07310906797647476], "norm_scores": [0.20784665644168854, 0.2028694599866867, 0.19892127811908722, 0.1956508606672287, 0.19471168518066406]}}
{"id": "WebQTest-1280", "question": "who was president after franklin d. roosevelt", "prediction": [["government.us_president.vice_president"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["government.us_vice_president.to_president"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after franklin d. roosevelt [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.01929706148803234, -0.02372138574719429, -0.03257747367024422, -0.036061424762010574, -0.048755284398794174], "norm_scores": [0.2025628536939621, 0.2016686350107193, 0.19989052414894104, 0.19919531047344208, 0.19668273627758026]}}
{"id": "WebQTest-1281", "question": "where did kate gosselin grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["tv.tv_personality.tv_regular_appearances", "tv.tv_regular_personal_appearance.place_of_birth"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kate gosselin grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> tv.tv_personality.tv_regular_appearances <SEP> tv.tv_regular_personal_appearance.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0022396876011043787, -0.039953943341970444, -0.10029368102550507, -0.13213947415351868, -0.13601505756378174], "norm_scores": [0.21633043885231018, 0.2083236277103424, 0.19612516462802887, 0.18997779488563538, 0.18924295902252197]}}
{"id": "WebQTest-1282", "question": "under what political system does the united states government operate", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["government.form_of_government.countries", "location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["base.locations.countries.planet", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: under what political system does the united states government operate [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.planet <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.0047507518902421, -0.0232615415006876, -0.12757176160812378, -0.12807875871658325, -0.1440630704164505], "norm_scores": [0.21644969284534454, 0.2124798744916916, 0.19143284857273102, 0.19133582711219788, 0.18830177187919617]}}
{"id": "WebQTest-1284", "question": "what to do in paris in 1 day", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["organization.organization.geographic_scope"]], "ground_paths": [["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in paris in 1 day [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> organization.organization.geographic_scope </PATH>"], "scores": [-0.008438891731202602, -0.020457984879612923, -0.03911944106221199, -0.10795198380947113, -0.11917917430400848], "norm_scores": [0.21015964448451996, 0.20764882862567902, 0.2038097381591797, 0.19025291502475739, 0.18812887370586395]}}
{"id": "WebQTest-1285", "question": "what city is mt lassen in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_range"], ["geography.lake.outflow"], ["geography.lake.basin_county"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is mt lassen in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> geography.lake.outflow </PATH>", "<PATH> geography.lake.basin_county </PATH>"], "scores": [-0.005496202036738396, -0.030267387628555298, -0.10756351053714752, -0.11761216074228287, -0.12177705019712448], "norm_scores": [0.2144695222377777, 0.20922212302684784, 0.19365927577018738, 0.1917230188846588, 0.19092616438865662]}}
{"id": "WebQTest-1287", "question": "what is the state motto of ohio", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state motto of ohio [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>"], "scores": [-0.017158981412649155, -0.01934238150715828, -0.021694201976060867, -0.023751970380544662, -0.025821654126048088], "norm_scores": [0.20087996125221252, 0.20044183731079102, 0.19997099041938782, 0.19955991208553314, 0.1991473138332367]}}
{"id": "WebQTest-1289", "question": "what type of music did mozart composed", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.composition.composer"], ["book.author.works_written"], ["music.composer.genre"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did mozart composed [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.composer.genre </PATH>"], "scores": [-0.005495880730450153, -0.030639391392469406, -0.06740061938762665, -0.09014610946178436, -0.10740210860967636], "norm_scores": [0.21110005676746368, 0.20585843920707703, 0.19842824339866638, 0.1939658373594284, 0.19064746797084808]}}
{"id": "WebQTest-1291", "question": "how many kids were there in the kennedy family", "prediction": [["people.person.parents"], ["people.person.children"], ["people.family.members"], ["people.person.nationality", "people.person.nationality"], ["people.family_member.family"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many kids were there in the kennedy family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.family.members </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.family_member.family </PATH>"], "scores": [-0.00952441431581974, -0.017223695293068886, -0.08571276813745499, -0.12960268557071686, -0.13831274211406708], "norm_scores": [0.2134479284286499, 0.21181084215641022, 0.1977897584438324, 0.18929651379585266, 0.18765489757061005]}}
{"id": "WebQTest-1292", "question": "which barcelona airport is closest to the port", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transport_terminus.travel_destinations_served", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which barcelona airport is closest to the port [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transport_terminus.travel_destinations_served <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>"], "scores": [-0.006155120674520731, -0.02903626672923565, -0.04983782395720482, -0.06009124219417572, -0.06218653544783592], "norm_scores": [0.20714060962200165, 0.2024548053741455, 0.19828693568706512, 0.19626420736312866, 0.19585339725017548]}}
{"id": "WebQTest-1293", "question": "where has the english language evolved from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"], ["language.language_family.languages"], ["common.topic.notable_types"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where has the english language evolved from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.006043113302439451, -0.02585466019809246, -0.06349235773086548, -0.08866381645202637, -0.1069248840212822], "norm_scores": [0.21055763959884644, 0.2064272165298462, 0.1988021731376648, 0.1938604712486267, 0.19035251438617706]}}
{"id": "WebQTest-1294", "question": "what country did hitler invade that started ww2", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["military.military_command.military_conflict", "military.military_command.military_combatant"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did hitler invade that started ww2 [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts <SEP> military.military_person.participated_in_conflicts </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> military.military_command.military_conflict <SEP> military.military_command.military_combatant </PATH>"], "scores": [-0.02892024628818035, -0.03133276477456093, -0.034531429409980774, -0.03692194074392319, -0.03861801326274872], "norm_scores": [0.20103031396865845, 0.20054590702056885, 0.19990544021129608, 0.19942815601825714, 0.19909018278121948]}}
{"id": "WebQTest-1295", "question": "what does matt damon play in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["music.recording.featured_artists", "music.recording.releases"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["tv.tv_program.program_creator"], ["film.film.written_by"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.producer.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.languages", "tv.tv_program.languages"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does matt damon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.02236979641020298, -0.02245629020035267, -0.04496502876281738, -0.045131176710128784, -0.04649294540286064], "norm_scores": [0.2027890384197235, 0.20277149975299835, 0.19825835525989532, 0.19822542369365692, 0.1979556679725647]}}
{"id": "WebQTest-1296", "question": "what was scottie pippen known for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position_s"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen known for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position_s </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.015520809218287468, -0.02701692469418049, -0.03302969038486481, -0.0508481003344059, -0.05384507402777672], "norm_scores": [0.20412729680538177, 0.20179405808448792, 0.20058436691761017, 0.19704192876815796, 0.1964522898197174]}}
{"id": "WebQTest-1297", "question": "what continent does canada belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.contains"], ["base.locations.countries.countries_within"]], "ground_paths": [["base.locations.continents.countries_within"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does canada belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.countries_within </PATH>"], "scores": [-0.016723845154047012, -0.019521789625287056, -0.027407050132751465, -0.08320504426956177, -0.13703018426895142], "norm_scores": [0.20794740319252014, 0.20736637711524963, 0.2057376652956009, 0.1945723295211792, 0.1843762993812561]}}
{"id": "WebQTest-1298", "question": "what does george wilson do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["fictional_universe.fictional_character.occupation", "common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does george wilson do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> common.topic.notable_types </PATH>"], "scores": [-0.0048522790893912315, -0.03169257938861847, -0.06605663150548935, -0.066960908472538, -0.0734369084239006], "norm_scores": [0.2088710516691208, 0.20333945751190186, 0.19647057354450226, 0.19629301130771637, 0.19502592086791992]}}
{"id": "WebQTest-1300", "question": "what city does fulham play in", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["location.location.containedby"], ["sports.sports_team.location", "location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does fulham play in [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> sports.sports_team.location <SEP> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.023586580529808998, -0.023728176951408386, -0.02846572734415531, -0.03116801381111145, -0.08240080624818802], "norm_scores": [0.2028265744447708, 0.20279787480831146, 0.2018393576145172, 0.2012946903705597, 0.19124151766300201]}}
{"id": "WebQTest-1301", "question": "what are fun things to do in sacramento ca", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["film.film_festival.location"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are fun things to do in sacramento ca [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> film.film_festival.location </PATH>"], "scores": [-0.014956463128328323, -0.016455737873911858, -0.026126472279429436, -0.10856457054615021, -0.1610288918018341], "norm_scores": [0.20999011397361755, 0.20967552065849304, 0.2076575756072998, 0.1912253051996231, 0.1814514398574829]}}
{"id": "WebQTest-1303", "question": "what other movies is josh hutcherson in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies is josh hutcherson in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.017637060955166817, -0.03475159406661987, -0.0360475517809391, -0.037915538996458054, -0.04084409400820732], "norm_scores": [0.20317873358726501, 0.1997310072183609, 0.1994723379611969, 0.199100062251091, 0.19851785898208618]}}
{"id": "WebQTest-1304", "question": "what was hitler the leader of", "prediction": [["organization.organization_founder.organizations_founded"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["organization.organization.founders"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was hitler the leader of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.017463715746998787, -0.03521884232759476, -0.04117691144347191, -0.042554598301649094, -0.04479880630970001], "norm_scores": [0.20378121733665466, 0.20019498467445374, 0.19900575280189514, 0.19873178005218506, 0.1982862800359726]}}
{"id": "WebQTest-1305", "question": "what awards did marilyn monroe won", "prediction": [["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_honor.award_winner", "award.award_category.winners"]], "ground_paths": [["people.person.profession", "award.award_discipline.awards_in_this_discipline"], ["award.award_nomination.award_nominee", "award.award_category.nominees"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nominee.award_nominations", "award.award_category.nominees"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_honor.award_winner", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards did marilyn monroe won [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_category.winners </PATH>"], "scores": [-0.019636021926999092, -0.02367236465215683, -0.026330266147851944, -0.02881780080497265, -0.031760796904563904], "norm_scores": [0.20128384232521057, 0.20047302544116974, 0.19994090497493744, 0.19944415986537933, 0.19885805249214172]}}
{"id": "WebQTest-1306", "question": "what kind of government does iran have after 1979", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["people.person.nationality", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does iran have after 1979 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.004230400547385216, -0.023086149245500565, -0.13178065419197083, -0.13803766667842865, -0.15332895517349243], "norm_scores": [0.21749469637870789, 0.21343210339546204, 0.19144953787326813, 0.19025537371635437, 0.1873682588338852]}}
{"id": "WebQTest-1307", "question": "where did harper lee attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harper lee attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.015754196792840958, -0.021570740267634392, -0.021869627758860588, -0.03043152578175068, -0.051407571882009506], "norm_scores": [0.20249029994010925, 0.20131593942642212, 0.20125578343868256, 0.19954000413417816, 0.1953980177640915]}}
{"id": "WebQTest-1308", "question": "what college did jrr tolkien go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.employment_history", "business.employment_tenure.company"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did jrr tolkien go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015567058697342873, -0.022454509511590004, -0.030834360048174858, -0.0319339893758297, -0.03815537691116333], "norm_scores": [0.20245309174060822, 0.20106348395347595, 0.1993856579065323, 0.19916652143001556, 0.19793125987052917]}}
{"id": "WebQTest-1309", "question": "what art did wassily kandinsky do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"], ["book.author.school_or_movement"], ["book.school_or_movement.associated_authors"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art did wassily kandinsky do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> book.author.school_or_movement </PATH>", "<PATH> book.school_or_movement.associated_authors </PATH>"], "scores": [-0.0080083217471838, -0.027712199836969376, -0.04284901171922684, -0.046871356666088104, -0.06992439925670624], "norm_scores": [0.20626668632030487, 0.20224221050739288, 0.1992039680480957, 0.19840429723262787, 0.1938828080892563]}}
{"id": "WebQTest-1310", "question": "who won the league cup in 2002", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship.events", "sports.sports_team.championships"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the league cup in 2002 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> time.event.instance_of_recurring_event <SEP> sports.sports_championship_event.champion </PATH>"], "scores": [-0.021576615050435066, -0.02313190884888172, -0.02553989179432392, -0.028989305719733238, -0.032762560993433], "norm_scores": [0.20096537470817566, 0.2006530612707138, 0.20017045736312866, 0.19948118925094604, 0.19872990250587463]}}
{"id": "WebQTest-1311", "question": "what was antoni gaudi inspired by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["visual_art.visual_artist.art_forms"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was antoni gaudi inspired by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.010382396169006824, -0.013638869859278202, -0.07535756379365921, -0.09770433604717255, -0.10219798982143402], "norm_scores": [0.20997397601604462, 0.20929130911827087, 0.19676466286182404, 0.19241636991500854, 0.19155366718769073]}}
{"id": "WebQTest-1312", "question": "in what country do people speak danish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["language.human_language.region"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in what country do people speak danish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.region </PATH>"], "scores": [-0.01036804262548685, -0.019261779263615608, -0.04704328626394272, -0.0872345119714737, -0.11703267693519592], "norm_scores": [0.20920610427856445, 0.20735371112823486, 0.20167240500450134, 0.19372765719890594, 0.1880401074886322]}}
{"id": "WebQTest-1313", "question": "who were demeter 's siblings", "prediction": [["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.sibling_relationship.sibling", "fictional_universe.sibling_relationship.sibling"]], "ground_paths": [["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "fictional_universe.fictional_character.species"], ["fictional_universe.romantic_involvement.partner", "fictional_universe.fictional_character.romantically_involved_with"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were demeter 's siblings [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.fictional_character.siblings </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.sibling_relationship_of_fictional_characters.siblings </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.sibling_relationship.sibling <SEP> fictional_universe.sibling_relationship.sibling </PATH>"], "scores": [-0.021347153931856155, -0.02454383857548237, -0.025782134383916855, -0.027613360434770584, -0.034719571471214294], "norm_scores": [0.20109179615974426, 0.20045000314712524, 0.20020192861557007, 0.1998356580734253, 0.19842061400413513]}}
{"id": "WebQTest-1314", "question": "where should a family stay in paris", "prediction": [["travel.travel_destination.accommodation"], ["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.accommodation.accommodation"], ["travel.tourist_attraction.accommodation"]], "ground_paths": [["travel.travel_destination.accommodation"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where should a family stay in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.accommodation.accommodation </PATH>", "<PATH> travel.tourist_attraction.accommodation </PATH>"], "scores": [-0.003629903309047222, -0.03307509422302246, -0.09733083099126816, -0.1490612030029297, -0.15342555940151215], "norm_scores": [0.21705758571624756, 0.2107594609260559, 0.19764287769794464, 0.18767869472503662, 0.18686136603355408]}}
{"id": "WebQTest-1316", "question": "what was the name of benjamin franklin wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.children"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of benjamin franklin wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02700353041291237, -0.03381746634840965, -0.04006785899400711, -0.04061315581202507, -0.043034810572862625], "norm_scores": [0.20198719203472137, 0.2006155252456665, 0.1993655264377594, 0.19925683736801147, 0.19877488911151886]}}
{"id": "WebQTest-1317", "question": "what the largest city in spain", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.capital"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the largest city in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>"], "scores": [-0.020491525530815125, -0.026898473501205444, -0.04206205531954765, -0.047199513763189316, -0.047825880348682404], "norm_scores": [0.20329520106315613, 0.20199686288833618, 0.19895696640014648, 0.19793745875358582, 0.19781352579593658]}}
{"id": "WebQTest-1319", "question": "who has pudge rodriguez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has pudge rodriguez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.011659309267997742, -0.017178663983941078, -0.029424963518977165, -0.03282914683222771, -0.06877394020557404], "norm_scores": [0.20406392216682434, 0.2029407024383545, 0.20047059655189514, 0.19978931546211243, 0.1927354782819748]}}
{"id": "WebQTest-1320", "question": "what is the milwaukee brewers stadium called", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], [], ["sports.sports_team.arena_stadium", "sports.sports_facility.teams"], ["sports.sports_facility.teams", "sports.sports_facility.teams"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers stadium called [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> </PATH>", "<PATH> sports.sports_team.arena_stadium <SEP> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_facility.teams </PATH>"], "scores": [-0.0029304942581802607, -0.030664660036563873, -0.11443173885345459, -0.11797110736370087, -0.12143126875162125], "norm_scores": [0.21520565450191498, 0.2093191146850586, 0.19249936938285828, 0.19181925058364868, 0.19115667045116425]}}
{"id": "WebQTest-1321", "question": "where did the gallipoli war take place", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["book.written_work.subjects"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gallipoli war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.007938174530863762, -0.021879645064473152, -0.09091367572546005, -0.1195925697684288, -0.1259520798921585], "norm_scores": [0.21323934197425842, 0.21028709411621094, 0.19625987112522125, 0.19071131944656372, 0.18950232863426208]}}
{"id": "WebQTest-1322", "question": "what other movies has tom hardy played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has tom hardy played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.022683629766106606, -0.035125378519296646, -0.038411058485507965, -0.039550960063934326, -0.04089956730604172], "norm_scores": [0.20254173874855042, 0.2000373750925064, 0.19938118755817413, 0.1991540491580963, 0.19888564944267273]}}
{"id": "WebQTest-1323", "question": "what to visit in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> organization.organization.geographic_scope </PATH>"], "scores": [-0.015345347113907337, -0.018518002703785896, -0.02661150135099888, -0.11725424230098724, -0.12866230309009552], "norm_scores": [0.20913463830947876, 0.20847219228744507, 0.2067917138338089, 0.18887197971343994, 0.1867295503616333]}}
{"id": "WebQTest-1324", "question": "what college did john nash go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.education", "education.education.institution"], ["book.written_work.author", "education.dissertation.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.author", "education.dissertation.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john nash go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> book.written_work.author <SEP> education.dissertation.institution </PATH>"], "scores": [-0.01736355759203434, -0.017440954223275185, -0.035471025854349136, -0.03987220302224159, -0.04104014113545418], "norm_scores": [0.20257997512817383, 0.2025642991065979, 0.19894476234912872, 0.19807110726833344, 0.1978399157524109]}}
{"id": "WebQTest-1325", "question": "what language do maltese speak", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do maltese speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.015405325219035149, -0.029854947701096535, -0.03335085138678551, -0.033478688448667526, -0.0718836635351181], "norm_scores": [0.20428824424743652, 0.20135757327079773, 0.20065487921237946, 0.20062921941280365, 0.1930701583623886]}}
{"id": "WebQTest-1326", "question": "what did thomas edison do for the world", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["people.person.profession"], ["law.inventor.us_patents"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas edison do for the world [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> law.inventor.us_patents </PATH>"], "scores": [-0.013524905778467655, -0.02061440795660019, -0.036462362855672836, -0.05935528874397278, -0.07469968497753143], "norm_scores": [0.2055024653673172, 0.2040506899356842, 0.20084241032600403, 0.19629676640033722, 0.19330769777297974]}}
{"id": "WebQTest-1327", "question": "where is shakira from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"], ["people.person.ethnicity"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is shakira from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.011544917710125446, -0.018556401133537292, -0.05984450876712799, -0.08542541414499283, -0.13030050694942474], "norm_scores": [0.20996631681919098, 0.20849929749965668, 0.20006604492664337, 0.19501309096813202, 0.18645530939102173]}}
{"id": "WebQTest-1328", "question": "what did hayes do", "prediction": [["people.person.profession"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["common.topic.notable_types"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hayes do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.006124710198491812, -0.04661392793059349, -0.0477311946451664, -0.05215294286608696, -0.056308381259441376], "norm_scores": [0.20722652971744537, 0.1990036815404892, 0.19878147542476654, 0.19790445268154144, 0.19708378612995148]}}
{"id": "WebQTest-1329", "question": "who played bilbo in the fellowship of the ring", "prediction": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.starring", "film.actor.film"], ["film.performance.film", "film.actor.film"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film.starring", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in the fellowship of the ring [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>"], "scores": [-0.03490542620420456, -0.03513140603899956, -0.03542482852935791, -0.03811004012823105, -0.04401075467467308], "norm_scores": [0.20052170753479004, 0.20047640800476074, 0.20041759312152863, 0.19988013803958893, 0.19870416820049286]}}
{"id": "WebQTest-1330", "question": "what is the most populated state in united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.locations.states_and_provences.country"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.administrative_division.first_level_division_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most populated state in united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.locations.states_and_provences.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.008641047403216362, -0.02699539251625538, -0.03506168723106384, -0.068740613758564, -0.0694466382265091], "norm_scores": [0.20667923986911774, 0.20292039215564728, 0.20129016041755676, 0.19462381303310394, 0.19448645412921906]}}
{"id": "WebQTest-1331", "question": "who is my state senator arizona", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "people.person.nationality"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["base.aareas.schema.administrative_area.capital", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.capital", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my state senator arizona [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>"], "scores": [-0.02323043905198574, -0.026657313108444214, -0.02959643118083477, -0.0299933273345232, -0.0302431657910347], "norm_scores": [0.20094424486160278, 0.200256809592247, 0.19966909289360046, 0.19958986341953278, 0.19954000413417816]}}
{"id": "WebQTest-1332", "question": "when does the vietnam war end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["time.event.locations"], ["book.written_work.subjects"], ["military.military_conflict.casualties", "military.military_conflict.casualties"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does the vietnam war end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>"], "scores": [-0.015309267677366734, -0.03275556489825249, -0.03313928470015526, -0.05856261029839516, -0.07687617093324661], "norm_scores": [0.2056347280740738, 0.20207826793193817, 0.20200073719024658, 0.1969299614429474, 0.19335630536079407]}}
{"id": "WebQTest-1333", "question": "where does the tennessee river go", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"], ["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.partially_contains"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the tennessee river go [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.012492251582443714, -0.03336361423134804, -0.03728128597140312, -0.04516825079917908, -0.06500406563282013], "norm_scores": [0.20527322590351105, 0.20103329420089722, 0.20024725794792175, 0.19867411255836487, 0.19477209448814392]}}
{"id": "WebQTest-1334", "question": "who did tom hanks play in apollo 13", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.film.country", "people.person.nationality"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["spaceflight.astronaut.missions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom hanks play in apollo 13 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.023388830944895744, -0.02836536429822445, -0.031938280910253525, -0.04030008241534233, -0.04281026870012283], "norm_scores": [0.2019989937543869, 0.20099623501300812, 0.2002793848514557, 0.19861166179180145, 0.19811373949050903]}}
{"id": "WebQTest-1335", "question": "what language do they in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.005423163529485464, -0.03530411794781685, -0.04327336326241493, -0.08766656368970871, -0.18140999972820282], "norm_scores": [0.21307899057865143, 0.20680616796016693, 0.20516464114189148, 0.1962559074163437, 0.1786942183971405]}}
{"id": "WebQTest-1336", "question": "what is utah 's state capitol", "prediction": [["base.aareas.schema.administrative_area.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.state"], ["location.us_state.capital"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"]], "ground_paths": [["base.biblioness.bibs_location.city"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is utah 's state capitol [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.us_state.capital </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.district_represented </PATH>"], "scores": [-0.01998530887067318, -0.025370197370648384, -0.029738185927271843, -0.036931511014699936, -0.039670929312705994], "norm_scores": [0.20207622647285461, 0.20099100470542908, 0.20011498034000397, 0.19868065416812897, 0.19813713431358337]}}
{"id": "WebQTest-1337", "question": "what all did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["law.inventor.us_patents"], ["symbols.namesake.named_after"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.006580105517059565, -0.025505755096673965, -0.07705514878034592, -0.0783395990729332, -0.08746127039194107], "norm_scores": [0.20980806648731232, 0.20587465167045593, 0.19553083181381226, 0.19527983665466309, 0.19350665807724]}}
{"id": "WebQTest-1339", "question": "what team does jeremy lin play for 2013", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jeremy lin play for 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.014612192288041115, -0.018115248531103134, -0.022192424163222313, -0.03017236664891243, -0.03361484408378601], "norm_scores": [0.20182903110980988, 0.20112325251102448, 0.20030489563941956, 0.19871284067630768, 0.19802995026111603]}}
{"id": "WebQTest-1340", "question": "what is steve nash nationality", "prediction": [["people.person.nationality"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.country"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.country"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is steve nash nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.country </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.country </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>"], "scores": [-0.0021585985086858273, -0.0567702017724514, -0.059590160846710205, -0.0620448999106884, -0.07355556637048721], "norm_scores": [0.20990750193595886, 0.19875149428844452, 0.1981918215751648, 0.1977059245109558, 0.19544322788715363]}}
{"id": "WebQTest-1341", "question": "who are the u s senators of pennsylvania", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"], ["government.political_district.representatives", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_burial"], ["location.location.containedby", "people.deceased_person.place_of_burial"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the u s senators of pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.023709114640951157, -0.028117289766669273, -0.0281724501401186, -0.032360658049583435, -0.03492722660303116], "norm_scores": [0.20115146040916443, 0.20026670396327972, 0.20025566220283508, 0.19941869378089905, 0.1989075243473053]}}
{"id": "WebQTest-1342", "question": "what kind of language does france speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does france speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.014033465646207333, -0.02197786420583725, -0.027270222082734108, -0.07858137786388397, -0.16378569602966309], "norm_scores": [0.209321066737175, 0.20766472816467285, 0.20656859874725342, 0.19623667001724243, 0.1802089661359787]}}
{"id": "WebQTest-1343", "question": "what degree did martin luther king earn", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"], ["education.education.degree_recipient", "education.education.degree"], ["common.topic.notable_types"]], "ground_paths": [["people.person.education", "education.education.degree"], ["education.education.student", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degree did martin luther king earn [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.degree_recipient <SEP> education.education.degree </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.004890329670161009, -0.023178130388259888, -0.052302468568086624, -0.09237401187419891, -0.10611394792795181], "norm_scores": [0.21028117835521698, 0.2064705342054367, 0.20054394006729126, 0.19266670942306519, 0.1900375932455063]}}
{"id": "WebQTest-1344", "question": "what two states is st louis in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.time_zones"], ["base.biblioness.bibs_location.state"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two states is st louis in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.time_zones </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.003600758034735918, -0.05739978328347206, -0.061049673706293106, -0.06483636796474457, -0.07295673340559006], "norm_scores": [0.20984645187854767, 0.1988552361726761, 0.1981307566165924, 0.19738192856311798, 0.19578559696674347]}}
{"id": "WebQTest-1348", "question": "what team did peyton manning 's dad play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did peyton manning 's dad play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.008518407121300697, -0.01620503142476082, -0.0382620133459568, -0.041476406157016754, -0.05745551362633705], "norm_scores": [0.2047981172800064, 0.20322994887828827, 0.19879639148712158, 0.198158398270607, 0.19501717388629913]}}
{"id": "WebQTest-1350", "question": "what did jesse owens won", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_medal_honor.event"], ["people.person.nationality", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"], ["people.person.nationality", "olympics.olympic_games.participating_countries"]], "ground_paths": [["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesse owens won [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.01896384172141552, -0.02058217115700245, -0.03518997132778168, -0.03870408236980438, -0.043819185346364975], "norm_scores": [0.2025032490491867, 0.2021757960319519, 0.1992439329624176, 0.19854497909545898, 0.19753199815750122]}}
{"id": "WebQTest-1351", "question": "what is the name of the currency used in peru", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_formerly_used"], ["common.topic.notable_types"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in peru [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.009200884029269218, -0.016304055228829384, -0.11731230467557907, -0.1339927315711975, -0.15044395625591278], "norm_scores": [0.21545183658599854, 0.21392688155174255, 0.19337396323680878, 0.19017517566680908, 0.18707214295864105]}}
{"id": "WebQTest-1353", "question": "what are the four nations of the united kingdom", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.first_level_division_of"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four nations of the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.011199453845620155, -0.022903114557266235, -0.02439139597117901, -0.07085847854614258, -0.09667409211397171], "norm_scores": [0.2068071961402893, 0.2044009119272232, 0.2040969282388687, 0.19483011960983276, 0.18986481428146362]}}
{"id": "WebQTest-1354", "question": "who is judy garland father", "prediction": [["people.person.parents"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.gender"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is judy garland father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.has_value <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.007421135436743498, -0.02415549010038376, -0.0981413945555687, -0.1432361900806427, -0.14387914538383484], "norm_scores": [0.21541857719421387, 0.21184368431568146, 0.19673602283000946, 0.18806129693984985, 0.18794041872024536]}}
{"id": "WebQTest-1355", "question": "what school did robert f kennedy go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did robert f kennedy go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.015321093611419201, -0.021475575864315033, -0.025605633854866028, -0.02712317556142807, -0.04134378209710121], "norm_scores": [0.20217488706111908, 0.20093442499637604, 0.20010626316070557, 0.19980283081531525, 0.19698160886764526]}}
{"id": "WebQTest-1356", "question": "what drug killed marilyn monroe", "prediction": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"], ["medicine.disease.parent_disease"], ["medicine.disease.notable_people_with_this_condition"], ["music.artist.track", "music.artist.track"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drug killed marilyn monroe [/INST]", "raw_output": {"paths": ["<PATH> people.cause_of_death.people </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.parent_disease </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> music.artist.track <SEP> music.artist.track </PATH>"], "scores": [-0.007029134314507246, -0.023717155680060387, -0.05711447075009346, -0.059211619198322296, -0.09478741884231567], "norm_scores": [0.20834477245807648, 0.20489676296710968, 0.19816677272319794, 0.19775161147117615, 0.19084011018276215]}}
{"id": "WebQTest-1357", "question": "where is the fukushima daiichi nuclear power station", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.contains"], ["location.location.street_address", "location.mailing_address.citytown"], ["location.location.street_address", "location.mailing_address.state_province_region"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear power station [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.005131660494953394, -0.041610993444919586, -0.04461798444390297, -0.04902831092476845, -0.07348336279392242], "norm_scores": [0.2076219767332077, 0.20018455386161804, 0.199583500623703, 0.19870521128177643, 0.1939048022031784]}}
{"id": "WebQTest-1358", "question": "what is los angeles california time zone", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is los angeles california time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-1.1330394045216963e-05, -0.16354478895664215, -0.16973116993904114, -0.18353407084941864, -0.1842452436685562], "norm_scores": [0.22950975596904755, 0.1948854774236679, 0.19368356466293335, 0.19102852046489716, 0.19089271128177643]}}
{"id": "WebQTest-1359", "question": "what did nicolas cage name his son", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.nationality", "people.person.nationality"], ["film.actor.film", "film.performance.character"], ["people.person.gender"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicolas cage name his son [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.011456791311502457, -0.015582393854856491, -0.10955759137868881, -0.11515604704618454, -0.11896644532680511], "norm_scores": [0.21267525851726532, 0.21179965138435364, 0.19280235469341278, 0.1917259693145752, 0.19099681079387665]}}
{"id": "WebQTest-1361", "question": "what currency should take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.tour_operator.travel_destinations </PATH>"], "scores": [-0.007751890923827887, -0.03255524858832359, -0.049359019845724106, -0.05523708462715149, -0.057286035269498825], "norm_scores": [0.20660963654518127, 0.2015480399131775, 0.19818958640098572, 0.1970280110836029, 0.1966247260570526]}}
{"id": "WebQTest-1362", "question": "who is the state governor of florida", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of florida [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.014486296102404594, -0.02285344898700714, -0.024800004437565804, -0.027516549453139305, -0.036453139036893845], "norm_scores": [0.20215357840061188, 0.20046919584274292, 0.20007935166358948, 0.19953656196594238, 0.19776132702827454]}}
{"id": "WebQTest-1363", "question": "who was jessica simpson married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.infidelity_participant", "base.popstra.infidelity.victim"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["base.popstra.celebrity.infidelity_participant", "base.popstra.celebrity.infidelity_victim"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"], ["music.artist.genre", "music.artist.genre"], ["base.saturdaynightlive.snl_episode.host", "base.saturdaynightlive.snl_host.episodes_hosted"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["broadcast.artist.content", "broadcast.artist.content"], ["people.person.languages", "people.person.languages"], ["music.artist.album", "music.album.featured_artists"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was jessica simpson married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.infidelity_participant <SEP> base.popstra.infidelity.victim </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.02322808839380741, -0.03236134350299835, -0.03809685632586479, -0.04344341903924942, -0.043781034648418427], "norm_scores": [0.20260165631771088, 0.2007596641778946, 0.19961149990558624, 0.1985471099615097, 0.19848008453845978]}}
{"id": "WebQTest-1364", "question": "what do they speak in scotland", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.014801244251430035, -0.015569854527711868, -0.0383712537586689, -0.08037973940372467, -0.16732075810432434], "norm_scores": [0.20959946513175964, 0.20943842828273773, 0.20471696555614471, 0.19629523158073425, 0.17994998395442963]}}
{"id": "WebQTest-1365", "question": "what did john dalton study", "prediction": [["people.person.profession"], ["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.profession", "people.profession.part_of_professional_field"], ["people.person.education", "education.education.field_of_study"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john dalton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.profession <SEP> people.profession.part_of_professional_field </PATH>", "<PATH> people.person.education <SEP> education.education.field_of_study </PATH>"], "scores": [-0.0128819290548563, -0.023952286690473557, -0.03444468230009079, -0.06127128377556801, -0.06350430101156235], "norm_scores": [0.2052941769361496, 0.20303401350975037, 0.20091485977172852, 0.19559665024280548, 0.19516035914421082]}}
{"id": "WebQTest-1366", "question": "what songs do zac brown band sing", "prediction": [["music.recording.artist"], ["music.artist.album"], ["music.composer.compositions"], ["music.album.artist"], ["music.composition.composer"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["music.featured_artist.recordings"], ["music.music_video.artist"], ["music.artist.album"], ["music.album.artist"], ["music.recording.artist"], ["music.recording.featured_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs do zac brown band sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.014177287928760052, -0.029842371121048927, -0.05188563093543053, -0.05202080309391022, -0.06474180519580841], "norm_scores": [0.20571868121623993, 0.2025212049484253, 0.1981058120727539, 0.19807903468608856, 0.19557522237300873]}}
{"id": "WebQTest-1367", "question": "where is glastonbury england", "prediction": [["location.location.containedby"], ["location.location.contains"], ["time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.events"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is glastonbury england [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.011211196891963482, -0.01728525198996067, -0.09345616400241852, -0.11884799599647522, -0.12010127305984497], "norm_scores": [0.21232445538043976, 0.21103869378566742, 0.19556064903736115, 0.19065751135349274, 0.19041873514652252]}}
{"id": "WebQTest-1368", "question": "who formed the american federation of labor", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "people.person.nationality"], ["business.employment_tenure.company", "business.employment_tenure.person"], ["organization.organization.spun_off_from"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who formed the american federation of labor [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>", "<PATH> business.employment_tenure.company <SEP> business.employment_tenure.person </PATH>", "<PATH> organization.organization.spun_off_from </PATH>"], "scores": [-0.0015441437717527151, -0.04992275685071945, -0.07776235789060593, -0.09308044612407684, -0.10100046545267105], "norm_scores": [0.21289075911045074, 0.20283657312393188, 0.1972675621509552, 0.1942688226699829, 0.19273628294467926]}}
{"id": "WebQTest-1369", "question": "what to visit in denver", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["organization.organization.geographic_scope"], ["film.film_festival.location"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in denver [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film_festival.location </PATH>"], "scores": [-0.01348244957625866, -0.02395169623196125, -0.024624492973089218, -0.13710539042949677, -0.1445593684911728], "norm_scores": [0.21099857985973358, 0.20880110561847687, 0.20866066217422485, 0.1864621937274933, 0.1850774586200714]}}
{"id": "WebQTest-1370", "question": "where did thomas hobbes live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas hobbes live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.026093605905771255, -0.03164154663681984, -0.03196268901228905, -0.033630359917879105, -0.04223121330142021], "norm_scores": [0.2014058530330658, 0.20029155910015106, 0.2002272605895996, 0.19989360868930817, 0.19818171858787537]}}
{"id": "WebQTest-1373", "question": "where was john f kennedy when he got shot", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john f kennedy when he got shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.004174144938588142, -0.051309436559677124, -0.05229271948337555, -0.06023057550191879, -0.0640968531370163], "norm_scores": [0.20858094096183777, 0.198977530002594, 0.19878196716308594, 0.19721031188964844, 0.19644932448863983]}}
{"id": "WebQTest-1374", "question": "what type of government does italy have 2011", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does italy have 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.00891838688403368, -0.013712858781218529, -0.14235343039035797, -0.14236915111541748, -0.1429281383752823], "norm_scores": [0.21645238995552063, 0.21541710197925568, 0.1894140988588333, 0.18941113352775574, 0.18930527567863464]}}
{"id": "WebQTest-1376", "question": "what province is montreal canada located", "prediction": [["location.province.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.state"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.first_level_division_of"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is montreal canada located [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.024103395640850067, -0.028452830389142036, -0.030578574165701866, -0.03503004461526871, -0.04379687085747719], "norm_scores": [0.2016601711511612, 0.20078496634960175, 0.20035859942436218, 0.1994686871767044, 0.19772762060165405]}}
{"id": "WebQTest-1378", "question": "who does kellan lutz play in prom night", "prediction": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["film.actor.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kellan lutz play in prom night [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.023701118305325508, -0.02459174580872059, -0.029109859839081764, -0.03041067160665989, -0.030419982969760895], "norm_scores": [0.20078980922698975, 0.20061105489730835, 0.19970671832561493, 0.19944709539413452, 0.19944524765014648]}}
{"id": "WebQTest-1379", "question": "what did franz liszt do to earn a living", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["music.artist.genre"], ["influence.influence_node.influenced_by", "people.person.profession"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did franz liszt do to earn a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>"], "scores": [-0.0010258191032335162, -0.060270119458436966, -0.08587579429149628, -0.09496224671602249, -0.09570043534040451], "norm_scores": [0.21362335979938507, 0.20133499801158905, 0.19624511897563934, 0.19447003304958344, 0.1943265199661255]}}
{"id": "WebQTest-1380", "question": "what was nikola tesla known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["law.inventor.inventions"], ["people.profession.people_with_this_profession"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0022705893497914076, -0.04990016296505928, -0.06810791045427322, -0.0858907401561737, -0.08610644191503525], "norm_scores": [0.21145527064800262, 0.20161983370780945, 0.19798201322555542, 0.19449244439601898, 0.1944504976272583]}}
{"id": "WebQTest-1381", "question": "what is the zip code for princeton nj", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.town.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for princeton nj [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.town.postal_codes </PATH>", "<PATH> location.citytown.postal_codes <PATH>"], "scores": [-0.0003166592214256525, -0.07845940440893173, -0.1178334578871727, -0.16737397015094757, -0.17198239266872406], "norm_scores": [0.2221052497625351, 0.20541013777256012, 0.1974794715642929, 0.18793460726737976, 0.18707051873207092]}}
{"id": "WebQTest-1382", "question": "what other countries border argentina", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_contains"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"]], "ground_paths": [["time.event.locations", "location.location.events"], ["language.human_language.main_country", "language.language_family.geographic_distribution"], ["location.country.form_of_government", "location.country.form_of_government"], ["geography.river.basin_countries", "location.location.partially_contains"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["film.film.featured_film_locations", "media_common.netflix_title.netflix_genres"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.country.languages_spoken", "location.country.official_language"], ["location.location.partially_containedby", "location.location.containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["book.book_subject.works", "book.book_subject.works"], ["film.film.featured_film_locations", "film.film.country"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.locations.continents.counties_within", "base.locations.counties_parishes_and_boroughs.coninent"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.primarily_containedby", "location.location.containedby"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["time.event.locations", "time.event.locations"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.contains", "location.location.contains"], ["location.location.contains", "location.location.containedby"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries border argentina [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.03780283406376839, -0.039940834045410156, -0.04012658819556236, -0.04223742336034775, -0.04515454173088074], "norm_scores": [0.20065036416053772, 0.20022182166576385, 0.2001846432685852, 0.1997625231742859, 0.19918064773082733]}}
{"id": "WebQTest-1383", "question": "what language does people in iceland speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.human_language.language_family"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does people in iceland speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.01611294597387314, -0.017218805849552155, -0.041611429303884506, -0.04260307922959328, -0.1483824998140335], "norm_scores": [0.20731055736541748, 0.20708143711090088, 0.2020912766456604, 0.2018909752368927, 0.18162578344345093]}}
{"id": "WebQTest-1384", "question": "what is the predominant religion in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"], ["location.country.administrative_divisions"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"]], "ground_paths": [["people.person.quotations", "people.person.religion"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["people.person.nationality", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["symbols.namesake.named_after", "religion.religion.notable_figures"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the predominant religion in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> location.country.administrative_divisions <SEP> religion.religion.sacred_sites </PATH>"], "scores": [-0.004024926107376814, -0.028050921857357025, -0.051041409373283386, -0.08978790789842606, -0.09941621869802475], "norm_scores": [0.21020939946174622, 0.2052190899848938, 0.20055484771728516, 0.19293266534805298, 0.19108396768569946]}}
{"id": "WebQTest-1385", "question": "who has coached the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.historical_coaching_staff", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team.sport", "sports.sport.team_coaches"], ["american_football.football_team.historical_coaching_staff", "american_football.football_coach.coaching_history"]], "ground_paths": [["american_football.football_team.historical_coaching_staff", "american_football.football_coach.coaching_history"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has coached the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.team_coaches </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_coach.coaching_history </PATH>"], "scores": [-0.006649901159107685, -0.03272990882396698, -0.034442175179719925, -0.03868325054645538, -0.04011479392647743], "norm_scores": [0.20481684803962708, 0.1995442658662796, 0.19920289516448975, 0.19835984706878662, 0.19807608425617218]}}
{"id": "WebQTest-1387", "question": "what time zone is toronto gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["base.locations.countries.continent"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is toronto gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.languages_spoken </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.00021589608513750136, -0.07528499513864517, -0.1539015918970108, -0.16689205169677734, -0.1671266406774521], "norm_scores": [0.22331549227237701, 0.2071651667356491, 0.19150230288505554, 0.1890306919813156, 0.18898634612560272]}}
{"id": "WebQTest-1388", "question": "what country did toussaint l'ouverture help free", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved"], ["base.activism.activist.area_of_activism"], ["book.written_work.subjects"], ["film.film_subject.films"]], "ground_paths": [["military.military_service.military_person", "military.armed_force.personnel"], ["military.military_service.military_person", "military.military_service.military_force"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did toussaint l'ouverture help free [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> film.film_subject.films </PATH>"], "scores": [-0.018079375848174095, -0.025955088436603546, -0.04952618107199669, -0.05345942825078964, -0.05568262189626694], "norm_scores": [0.20451858639717102, 0.20291417837142944, 0.1981872022151947, 0.19740921258926392, 0.1969708353281021]}}
{"id": "WebQTest-1389", "question": "what jobs did robert burns have", "prediction": [["people.person.profession"], ["influence.influence_node.influenced", "people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"], ["common.topic.notable_types", "type.type.expected_by"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did robert burns have [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>"], "scores": [-0.002406761981546879, -0.055566780269145966, -0.06398773938417435, -0.06671097129583359, -0.07504313439130783], "norm_scores": [0.2102534919977188, 0.19936829805374146, 0.1976964771747589, 0.1971588432788849, 0.19552291929721832]}}
{"id": "WebQTest-1390", "question": "who won utah attorney general", "prediction": [["base.aareas.schema.administrative_area.administrative_children", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["base.biblioness.bibs_location.city", "people.person.place_of_birth"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won utah attorney general [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.015811219811439514, -0.025760971009731293, -0.028904369100928307, -0.0310942605137825, -0.03270592540502548], "norm_scores": [0.20221742987632751, 0.2002153843641281, 0.19958703219890594, 0.19915041327476501, 0.19882972538471222]}}
{"id": "WebQTest-1392", "question": "what the language spoken in indonesia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the language spoken in indonesia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.015061565674841404, -0.023533441126346588, -0.026434076949954033, -0.05308179184794426, -0.14868585765361786], "norm_scores": [0.2075607031583786, 0.20580968260765076, 0.20521357655525208, 0.19981732964515686, 0.1815987527370453]}}
{"id": "WebQTest-1394", "question": "who coaches the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team.sport", "sports.sport.team_coaches"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who coaches the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.team_coaches </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>"], "scores": [-0.006289561279118061, -0.036863863468170166, -0.03690541535615921, -0.037270791828632355, -0.041868336498737335], "norm_scores": [0.20515865087509155, 0.19898097217082977, 0.19897271692752838, 0.198900043964386, 0.19798767566680908]}}
{"id": "WebQTest-1395", "question": "what other countries does south africa share borders with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["time.event.locations", "location.location.events"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.languages_spoken", "language.human_language.main_country"], ["location.location.containedby", "base.locations.continents.countries_within"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["location.country.official_language", "location.country.official_language"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.location.containedby", "location.location.containedby"], ["location.country.currency_used", "location.country.currency_used"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_contains"], ["film.film.featured_film_locations", "film.film.subjects"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.containedby", "base.locations.countries.continent"], ["government.form_of_government.countries", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.location.partially_contains", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["time.event.locations", "time.event.locations"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["common.topic.notable_types", "common.topic.notable_types"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.country.official_language", "language.human_language.main_country"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries does south africa share borders with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03285235911607742, -0.03399507328867912, -0.04389647766947746, -0.043996330350637436, -0.04673895984888077], "norm_scores": [0.20149096846580505, 0.20126084983348846, 0.19927792251110077, 0.1992580145597458, 0.1987122744321823]}}
{"id": "WebQTest-1396", "question": "who played darth vader at the end of return of the jedi", "prediction": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.country", "people.person.nationality"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader at the end of return of the jedi [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.02253084070980549, -0.02666044794023037, -0.02753204107284546, -0.032321035861968994, -0.03361952677369118], "norm_scores": [0.2012023627758026, 0.20037318766117096, 0.20019862055778503, 0.19924215972423553, 0.19898362457752228]}}
{"id": "WebQTest-1398", "question": "what movies did joan crawford play in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.producer.film"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.deceased_person.place_of_death", "film.film.featured_film_locations"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did joan crawford play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.023882778361439705, -0.028313636779785156, -0.034849394112825394, -0.0379672609269619, -0.04126695543527603], "norm_scores": [0.20187939703464508, 0.20098689198493958, 0.19967757165431976, 0.19905593991279602, 0.19840021431446075]}}
{"id": "WebQTest-1399", "question": "what products does the united states export", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products does the united states export [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.02329057827591896, -0.026134204119443893, -0.030323872342705727, -0.03651292249560356, -0.04235871881246567], "norm_scores": [0.2016889899969101, 0.20111627876758575, 0.20027542114257812, 0.1990397423505783, 0.1978795826435089]}}
{"id": "WebQTest-1402", "question": "what is the kentucky state bird", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.Kind_of_symbol", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the kentucky state bird [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.Kind_of_symbol <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.014079900458455086, -0.022446483373641968, -0.028382088989019394, -0.028470182791352272, -0.037646178156137466], "norm_scores": [0.20243366062641144, 0.2007470577955246, 0.1995590180158615, 0.19954144954681396, 0.19771882891654968]}}
{"id": "WebQTest-1403", "question": "who was canadian prime minister in 1993", "prediction": [["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["base.firsts.first_achievement.achievement", "base.firsts.first_achievement.first"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was canadian prime minister in 1993 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.0076817902736365795, -0.01675095222890377, -0.02831588312983513, -0.03339746594429016, -0.0417671799659729], "norm_scores": [0.2035975456237793, 0.2017594426870346, 0.19943954050540924, 0.19842863082885742, 0.19677478075027466]}}
{"id": "WebQTest-1405", "question": "what there to do in salt lake city utah", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["location.location.contains"], ["film.film_festival.location"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what there to do in salt lake city utah [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> film.film_festival.location </PATH>"], "scores": [-0.014320539310574532, -0.014802652411162853, -0.030988531187176704, -0.10790557414293289, -0.137441024184227], "norm_scores": [0.20930108428001404, 0.2092002034187317, 0.20584136247634888, 0.19060224294662476, 0.18505503237247467]}}
{"id": "WebQTest-1408", "question": "who is the current ohio state senator", "prediction": [["government.political_district.representatives", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["government.governmental_body.jurisdiction", "people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current ohio state senator [/INST]", "raw_output": {"paths": ["<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.country.first_level_divisions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.person.place_of_birth </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>"], "scores": [-0.023583443835377693, -0.027068093419075012, -0.027313832193613052, -0.031302645802497864, -0.03175735101103783], "norm_scores": [0.2009255439043045, 0.20022661983966827, 0.2001774162054062, 0.1993805319070816, 0.19928988814353943]}}
{"id": "WebQTest-1409", "question": "what are the major languages spoken in the united kingdom", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.first_level_divisions"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.first_level_divisions </PATH>"], "scores": [-0.015778375789523125, -0.02240116521716118, -0.023176688700914383, -0.04891820624470711, -0.12685437500476837], "norm_scores": [0.20625779032707214, 0.20489628612995148, 0.20473745465278625, 0.19953446090221405, 0.18457405269145966]}}
{"id": "WebQTest-1410", "question": "what did st. nicholas do in his life", "prediction": [["people.person.profession"], ["book.written_work.subjects"], ["symbols.namesake.named_after"], ["base.saints.saint.venerated_in"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st. nicholas do in his life [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.saints.saint.venerated_in </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.017484501004219055, -0.03294125944375992, -0.03656444698572159, -0.038370754569768906, -0.04902913421392441], "norm_scores": [0.2034984976053238, 0.20037727057933807, 0.1996525526046753, 0.19929225742816925, 0.1971794068813324]}}
{"id": "WebQTest-1414", "question": "what happened to pope john paul ii", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["deceased_person.cause_of_death"], ["people.person.deceased_people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to pope john paul ii [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_people </PATH>"], "scores": [-0.0006403936422429979, -0.06558622419834137, -0.11365480720996857, -0.113836370408535, -0.1150694489479065], "norm_scores": [0.21667969226837158, 0.2030544877052307, 0.19352483749389648, 0.1934897005558014, 0.19325125217437744]}}
{"id": "WebQTest-1416", "question": "what was franklin pierce famous for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["common.topic.notable_types"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin pierce famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.011518643237650394, -0.031442493200302124, -0.03834196925163269, -0.040847282856702805, -0.04209306091070175], "norm_scores": [0.20429876446723938, 0.20026862621307373, 0.19889162480831146, 0.19839398562908173, 0.1981469839811325]}}
{"id": "WebQTest-1417", "question": "when was the last time the boston bruins went to the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the boston bruins went to the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.01114475168287754, -0.015055828727781773, -0.029399683699011803, -0.08331260085105896, -0.10031629353761673], "norm_scores": [0.20733679831027985, 0.2065274715423584, 0.20358622074127197, 0.19290092587471008, 0.18964862823486328]}}
{"id": "WebQTest-1418", "question": "who is meredith gray married to in real life", "prediction": [["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"], ["fictional_universe.marriage_of_fictional_characters.spouses", "fictional_universe.marriage_of_fictional_characters.spouses"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is meredith gray married to in real life [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>", "<PATH> fictional_universe.marriage_of_fictional_characters.spouses <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.02075209654867649, -0.026823561638593674, -0.029381386935710907, -0.03465922921895981, -0.036985404789447784], "norm_scores": [0.20179836452007294, 0.20057687163352966, 0.20006448030471802, 0.19901135563850403, 0.19854895770549774]}}
{"id": "WebQTest-1419", "question": "what inspired michelangelo buonarroti", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["symbols.namesake.named_after"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michelangelo buonarroti [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.005573509261012077, -0.02003510482609272, -0.09723522514104843, -0.09877044707536697, -0.12531778216362], "norm_scores": [0.21293732523918152, 0.209880068898201, 0.19428694248199463, 0.19398891925811768, 0.18890675902366638]}}
{"id": "WebQTest-1420", "question": "what does monsanto own", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization.child"], ["business.business_operation.industry"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"]], "ground_paths": [["business.business_operation.industry"], ["business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does monsanto own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>", "<PATH> business.business_operation.industry </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization_relationship.parent </PATH>", "<PATH> organization.organization_relationship.child <SEP> organization.organization_relationship.parent </PATH>"], "scores": [-0.03390335291624069, -0.039690978825092316, -0.0411941334605217, -0.047673266381025314, -0.05100308358669281], "norm_scores": [0.20176200568675995, 0.20059765875339508, 0.2002963423728943, 0.19900278747081757, 0.1983412504196167]}}
{"id": "WebQTest-1421", "question": "who did woody harrelson play on cheers", "prediction": [["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["people.person.profession", "fictional_universe.fictional_character.occupation"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did woody harrelson play on cheers [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.character </PATH>"], "scores": [-0.02486342191696167, -0.02576335147023201, -0.025936227291822433, -0.030864402651786804, -0.036495618522167206], "norm_scores": [0.200783833861351, 0.20060323178768158, 0.2005685418844223, 0.1995825469493866, 0.19846181571483612]}}
{"id": "WebQTest-1422", "question": "what language do they speak in sweden yahoo", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.currency_used"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in sweden yahoo [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.012955324724316597, -0.019320009276270866, -0.0333615243434906, -0.056692976504564285, -0.18455789983272552], "norm_scores": [0.20951293408870697, 0.2081836760044098, 0.20528090000152588, 0.20054684579372406, 0.1764756590127945]}}
{"id": "WebQTest-1423", "question": "what language did they speak in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did they speak in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.004399220924824476, -0.03738267719745636, -0.04754910618066788, -0.07906763255596161, -0.17728444933891296], "norm_scores": [0.21301034092903137, 0.20609912276268005, 0.20401445031166077, 0.19768448173999786, 0.17919158935546875]}}
{"id": "WebQTest-1424", "question": "what form of government do we have in the philippines", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["form.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government do we have in the philippines [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> form.form_of_government.countries </PATH>"], "scores": [-0.008599803782999516, -0.014145647175610065, -0.1347583532333374, -0.14620177447795868, -0.15597490966320038], "norm_scores": [0.21690279245376587, 0.21570321917533875, 0.19119440019130707, 0.18901897966861725, 0.18718065321445465]}}
{"id": "WebQTest-1425", "question": "what colleges did mitt romney attend", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["common.topic.notable_types"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.education", "education.education.institution"], ["people.person.religion", "religion.religion.organizations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did mitt romney attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.017991088330745697, -0.023366378620266914, -0.024978147819638252, -0.038039740175008774, -0.04394758492708206], "norm_scores": [0.20233885943889618, 0.20125414431095123, 0.20093002915382385, 0.19832263886928558, 0.19715441763401031]}}
{"id": "WebQTest-1426", "question": "where is ibrahim afellay from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["sports.sports_team_roster.player", "sports.sports_team_roster.place_of_birth"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ibrahim afellay from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.place_of_birth </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.00491096917539835, -0.037847891449928284, -0.04911923035979271, -0.06555963307619095, -0.07119513303041458], "norm_scores": [0.2082737535238266, 0.20152559876441956, 0.19926689565181732, 0.19601763784885406, 0.19491609930992126]}}
{"id": "WebQTest-1427", "question": "who was the secretary of state when andrew jackson was president", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.appointed_by", "government.government_position_held.office_holder"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["people.person.gender", "people.person.gender"], ["government.government_office_category.officeholders", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["government.us_vice_president.to_president"], ["people.person.profession", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["government.political_appointer.appointees", "government.government_position_held.office_holder"], ["people.person.nationality", "people.person.nationality"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["government.government_position_held.appointed_by", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the secretary of state when andrew jackson was president [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.politician.government_positions_held </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> government.government_office_category.officeholders <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.02213534526526928, -0.024606764316558838, -0.029201190918684006, -0.029626209288835526, -0.03597184643149376], "norm_scores": [0.20123612880706787, 0.20073941349983215, 0.19981923699378967, 0.19973434507846832, 0.19847090542316437]}}
{"id": "WebQTest-1428", "question": "who does ron stoppable 's voice", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ron stoppable 's voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02158460207283497, -0.030100753530859947, -0.03367264196276665, -0.038132768124341965, -0.03894858434796333], "norm_scores": [0.20218855142593384, 0.20047399401664734, 0.19975920021533966, 0.1988702267408371, 0.19870805740356445]}}
{"id": "WebQTest-1429", "question": "what currency is accepted in new zealand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is accepted in new zealand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.008039373904466629, -0.02035813219845295, -0.11974126100540161, -0.12085486948490143, -0.1215210035443306], "norm_scores": [0.21421973407268524, 0.2115969955921173, 0.19157902896404266, 0.1913658082485199, 0.1912383735179901]}}
{"id": "WebQTest-1432", "question": "what places are in the west midlands", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.first_level_division_of"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.contains"], ["location.location.containedby"], ["location.location.containedby", "common.topic.image"], [], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places are in the west midlands [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.005454234778881073, -0.036923013627529144, -0.04826291278004646, -0.0626671239733696, -0.07419006526470184], "norm_scores": [0.20811298489570618, 0.20166589319705963, 0.1993919312953949, 0.1965404450893402, 0.1942887157201767]}}
{"id": "WebQTest-1433", "question": "what to do with kids in asheville nc", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"], ["film.film_festival.location"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in asheville nc [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> film.film_festival.location </PATH>"], "scores": [-0.008018800988793373, -0.020156439393758774, -0.03913840651512146, -0.07502376288175583, -0.15034255385398865], "norm_scores": [0.2100917100906372, 0.20755711197853088, 0.2036544233560562, 0.1964757740497589, 0.18222104012966156]}}
{"id": "WebQTest-1434", "question": "who is in the american league in baseball", "prediction": [["baseball.baseball_team.league"], ["baseball.baseball_league.teams"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["sports.sports_position.sport", "baseball.baseball_player.position_s"], ["sports.sports_position.sport", "sports.pro_athlete.teams"]], "ground_paths": [["sports.sports_league.sport", "sports.sports_team.sport"], ["sports.sports_league_participation.league", "sports.sports_team.league"], ["baseball.baseball_league.teams", "sports.sports_team.previously_known_as"], ["baseball.baseball_league.teams", "sports.defunct_sports_team.later_known_as"], ["sports.sports_league.sport", "sports.defunct_sports_team.sport"], ["sports.sports_league.teams", "sports.sports_team.league"], ["sports.sports_league.teams", "sports.sports_league_participation.team"], ["sports.sports_league_participation.league", "sports.sports_league_participation.team"], ["baseball.baseball_team.league", "sports.sports_team.previously_known_as"], ["baseball.baseball_team.league"], ["baseball.baseball_team.league", "sports.defunct_sports_team.later_known_as"], ["baseball.baseball_league.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in the american league in baseball [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.league </PATH>", "<PATH> baseball.baseball_league.teams </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> sports.sports_position.sport <SEP> baseball.baseball_player.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.pro_athlete.teams </PATH>"], "scores": [-0.011052770540118217, -0.026898792013525963, -0.04371313378214836, -0.050693660974502563, -0.060106050223112106], "norm_scores": [0.2055324912071228, 0.2023012787103653, 0.19892816245555878, 0.1975443810224533, 0.1956937313079834]}}
{"id": "WebQTest-1435", "question": "who did kurt warner play 4", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.passing", "american_football.player_passing_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kurt warner play 4 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_passing_statistics.player <SEP> american_football.player_passing_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.024645110592246056, -0.025653503835201263, -0.028992891311645508, -0.030735978856682777, -0.033327870070934296], "norm_scores": [0.20080578327178955, 0.2006034106016159, 0.19993463158607483, 0.199586421251297, 0.1990697830915451]}}
{"id": "WebQTest-1436", "question": "who is dirk nowitzki wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dirk nowitzki wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03077000565826893, -0.03363380208611488, -0.03977062553167343, -0.040110938251018524, -0.04156532883644104], "norm_scores": [0.2012823522090912, 0.20070675015449524, 0.19947881996631622, 0.19941094517707825, 0.1991211473941803]}}
{"id": "WebQTest-1441", "question": "where all did picasso live", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.nationality", "location.administrative_division.country"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["influence.influence_node.influenced_by", "people.person.nationality"], ["people.person.parents", "people.person.nationality"], ["symbols.namesake.named_after", "film.film.country"], ["influence.influence_node.influenced", "people.person.nationality"], ["book.written_work.subjects", "book.written_work.subjects"], ["symbols.name_source.namesakes", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["film.film_subject.films", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where all did picasso live [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>"], "scores": [-0.024721402674913406, -0.027316709980368614, -0.031044067814946175, -0.04064512252807617, -0.050131071358919144], "norm_scores": [0.20201130211353302, 0.2014877051115036, 0.2007380723953247, 0.19881999492645264, 0.19694292545318604]}}
{"id": "WebQTest-1442", "question": "what currency shall i take to croatia", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency shall i take to croatia [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00851775798946619, -0.01973085105419159, -0.0802130252122879, -0.1486283391714096, -0.1522964984178543], "norm_scores": [0.21482251584529877, 0.21242715418338776, 0.1999599188566208, 0.1867370754480362, 0.1860533505678177]}}
{"id": "WebQTest-1443", "question": "what team is raul ibanez on", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is raul ibanez on [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.015616229735314846, -0.0199546180665493, -0.024586999788880348, -0.02665308490395546, -0.0507269985973835], "norm_scores": [0.20237742364406586, 0.20150133967399597, 0.20057006180286407, 0.2001560926437378, 0.1953950971364975]}}
{"id": "WebQTest-1445", "question": "what tv series did mark harmon star in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.languages", "tv.tv_program.languages"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.languages", "tv.tv_program.languages"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv series did mark harmon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.languages <SEP> tv.tv_program.languages </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>"], "scores": [-0.02058708854019642, -0.02484903298318386, -0.03206542506814003, -0.038097042590379715, -0.039517246186733246], "norm_scores": [0.20209266245365143, 0.20123319327831268, 0.1997862458229065, 0.19858483970165253, 0.1983029991388321]}}
{"id": "WebQTest-1446", "question": "what year did jimmy the rev sullivan die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"], ["people.person.deceased_place_of_death"], ["people.person.nationality", "location.location.events"], ["people.person.deceased_people"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did jimmy the rev sullivan die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.events </PATH>", "<PATH> people.person.deceased_people </PATH>"], "scores": [-0.0007658031536266208, -0.06449098140001297, -0.07635894417762756, -0.09060871601104736, -0.10242491215467453], "norm_scores": [0.2135443240404129, 0.2003607153892517, 0.19799688458442688, 0.1951954960823059, 0.1929025948047638]}}
{"id": "WebQTest-1447", "question": "who plays anakin skywalker in star wars 1", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["film.film.starring", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film.language", "people.person.languages"], ["film.performance.character", "film.performance.actor"], ["film.film.country", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays anakin skywalker in star wars 1 [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.01597045361995697, -0.02262359857559204, -0.02429208718240261, -0.04192148894071579, -0.0449882447719574], "norm_scores": [0.20280422270298004, 0.20145940780639648, 0.20112356543540955, 0.19760893285274506, 0.19700385630130768]}}
{"id": "WebQTest-1448", "question": "where does michelle pfeiffer live now", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth", "location.us_county.county_seat"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does michelle pfeiffer live now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.01590045914053917, -0.03150087594985962, -0.032280221581459045, -0.03304268419742584, -0.03820627182722092], "norm_scores": [0.2028718888759613, 0.19973155856132507, 0.19957596063613892, 0.19942384958267212, 0.19839675724506378]}}
{"id": "WebQTest-1449", "question": "what was laura ingalls wilder famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was laura ingalls wilder famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0034747053869068623, -0.04657524824142456, -0.053266458213329315, -0.08344559371471405, -0.08887308835983276], "norm_scores": [0.2105027288198471, 0.20162267982959747, 0.20027808845043182, 0.19432416558265686, 0.19327233731746674]}}
{"id": "WebQTest-1452", "question": "where is dwight howard now", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwight howard now [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.019856467843055725, -0.021777531132102013, -0.023561391979455948, -0.02938452549278736, -0.05355124920606613], "norm_scores": [0.2019481360912323, 0.20156057178974152, 0.20120131969451904, 0.20003308355808258, 0.19525690376758575]}}
{"id": "WebQTest-1453", "question": "who is nick lachey married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"], ["people.person.languages", "people.person.languages"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nick lachey married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.020695066079497337, -0.03554666042327881, -0.03813307359814644, -0.043131273239851, -0.04732510447502136], "norm_scores": [0.20327241718769073, 0.20027580857276917, 0.19975847005844116, 0.19876253604888916, 0.197930708527565]}}
{"id": "WebQTest-1454", "question": "where did madonna grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"], ["influence.influence_node.influenced_by"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did madonna grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.007812231779098511, -0.03453496843576431, -0.037688400596380234, -0.10415703803300858, -0.12448421865701675], "norm_scores": [0.21087148785591125, 0.20531105995178223, 0.20466463267803192, 0.19150310754776, 0.1876497119665146]}}
{"id": "WebQTest-1455", "question": "what capital city of brazil", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.second_level_division_of"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital city of brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.second_level_division_of </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.004029667470604181, -0.042737074196338654, -0.06848292052745819, -0.07774161547422409, -0.07975369691848755], "norm_scores": [0.21027737855911255, 0.20229360461235046, 0.19715185463428497, 0.19533489644527435, 0.19494226574897766]}}
{"id": "WebQTest-1456", "question": "what college did john stockton go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john stockton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.012780863791704178, -0.021427102386951447, -0.02549920417368412, -0.04593871906399727, -0.06272571533918381], "norm_scores": [0.20418910682201385, 0.20243124663829803, 0.20160861313343048, 0.1975296586751938, 0.19424138963222504]}}
{"id": "WebQTest-1457", "question": "what kind of currency does mexico use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.languages_spoken"], ["film.film.featured_film_locations", "film.film.country"], ["film.film_location.featured_in_films", "finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does mexico use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> film.film.featured_film_locations <SEP> film.film.country </PATH>", "<PATH> film.film_location.featured_in_films <SEP> finance.currency.countries_used </PATH>"], "scores": [-0.0025225740391761065, -0.04480462148785591, -0.0718877762556076, -0.10456358641386032, -0.1053297370672226], "norm_scores": [0.2129073441028595, 0.2040928453207016, 0.19863954186439514, 0.1922537386417389, 0.19210651516914368]}}
{"id": "WebQTest-1458", "question": "who owns chrysler corporation 2011", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["organization.organization.parent", "organization.organization.child"], ["business.employer.employees", "business.employment_tenure.person"], ["organization.organization.parent", "organization.organization_relationship.parent"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["organization.organization.parent", "organization.organization.child"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns chrysler corporation 2011 [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization_relationship.child <SEP> organization.organization_relationship.parent </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>", "<PATH> business.employer.employees <SEP> business.employment_tenure.person </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization_relationship.parent </PATH>"], "scores": [-0.01667547971010208, -0.022687258198857307, -0.02798399142920971, -0.03595246002078056, -0.038962069898843765], "norm_scores": [0.20236241817474365, 0.20114952325820923, 0.20008690655231476, 0.19849884510040283, 0.19790233671665192]}}
{"id": "WebQTest-1459", "question": "what are some books that mark twain wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"]], "ground_paths": [["book.book_subject.works"], ["film.film.story_by"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["film.film_story_contributor.film_story_credits"], ["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some books that mark twain wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.011471015401184559, -0.024725651368498802, -0.03373461589217186, -0.07745478302240372, -0.09772234410047531], "norm_scores": [0.20754100382328033, 0.20480826497077942, 0.2029714435338974, 0.1942887008190155, 0.19039058685302734]}}
{"id": "WebQTest-1460", "question": "what was gregor mendel contribution", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["law.invention.inventor"], ["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was gregor mendel contribution [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.008922656998038292, -0.04501771554350853, -0.05465373024344444, -0.05614857375621796, -0.059770822525024414], "norm_scores": [0.20729072391986847, 0.1999419927597046, 0.19802458584308624, 0.19772878289222717, 0.19701386988162994]}}
{"id": "WebQTest-1461", "question": "who has ray allen dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has ray allen dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> base.popstra.dated.participant <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.018801013007760048, -0.031168410554528236, -0.03288135677576065, -0.04374997690320015, -0.0490887276828289], "norm_scores": [0.20328287780284882, 0.20078428089618683, 0.2004406601190567, 0.1982739269733429, 0.19721822440624237]}}
{"id": "WebQTest-1462", "question": "who is running for vice president with barack obama 2012", "prediction": [["government.election_campaign.election", "government.election_campaign.candidate"], ["government.election_campaign.election", "government.politician.election_campaigns"], ["government.election_campaign.candidate", "government.election_campaign.candidate"], ["government.election.campaigns", "government.election_campaign.candidate"], ["time.event.locations", "people.person.nationality"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is running for vice president with barack obama 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.election <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election_campaign.election <SEP> government.politician.election_campaigns </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election.campaigns <SEP> government.election_campaign.candidate </PATH>", "<PATH> time.event.locations <SEP> people.person.nationality </PATH>"], "scores": [-0.016023246571421623, -0.030375324189662933, -0.031064053997397423, -0.03145277872681618, -0.0324275940656662], "norm_scores": [0.20246027410030365, 0.1995753049850464, 0.19943790137767792, 0.19936038553714752, 0.19916614890098572]}}
{"id": "WebQTest-1463", "question": "what language do people from the uk speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from the uk speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.01552023459225893, -0.01729101873934269, -0.04031803086400032, -0.04647187888622284, -0.14881126582622528], "norm_scores": [0.20753471553325653, 0.20716753602027893, 0.20245160162448883, 0.20120957493782043, 0.18163654208183289]}}
{"id": "WebQTest-1464", "question": "what to do with my kids in toronto", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["organization.organization.geographic_scope"], ["film.film_festival.location"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["film.film_festival.location"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with my kids in toronto [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> film.film_festival.location </PATH>"], "scores": [-0.013871846720576286, -0.01414845697581768, -0.03378766402602196, -0.12227250635623932, -0.1338573843240738], "norm_scores": [0.20989856123924255, 0.20984052121639252, 0.20575961470603943, 0.18833526968955994, 0.18616600334644318]}}
{"id": "WebQTest-1465", "question": "what kind of government is south korea", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is south korea [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.009752701036632061, -0.014378486201167107, -0.13478246331214905, -0.1362156718969345, -0.14146125316619873], "norm_scores": [0.215719074010849, 0.21472351253032684, 0.1903657466173172, 0.19009310007095337, 0.1890985667705536]}}
{"id": "WebQTest-1467", "question": "who did ben franklin marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.children"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben franklin marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.027772914618253708, -0.03219500929117203, -0.03278007358312607, -0.04185527190566063, -0.04455815255641937], "norm_scores": [0.2016143500804901, 0.2007247805595398, 0.20060735940933228, 0.19879505038261414, 0.1982584446668625]}}
{"id": "WebQTest-1468", "question": "what sri lanka capital", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sri lanka capital [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.009165636263787746, -0.027250930666923523, -0.04242599010467529, -0.10683872550725937, -0.11307050287723541], "norm_scores": [0.2101891189813614, 0.20642194151878357, 0.20331312716007233, 0.19063003361225128, 0.18944576382637024]}}
{"id": "WebQTest-1470", "question": "what kind of government does poland have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["locations.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does poland have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> locations.country.form_of_government </PATH>"], "scores": [-0.010808264836668968, -0.013435493223369122, -0.14649587869644165, -0.14697730541229248, -0.17394757270812988], "norm_scores": [0.21773876249790192, 0.217167466878891, 0.19011105597019196, 0.19001954793930054, 0.1849631816148758]}}
{"id": "WebQTest-1471", "question": "what famous people came from delaware", "prediction": [["people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "ground_paths": [["location.location.people_born_here"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people came from delaware [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.013205557130277157, -0.04010513424873352, -0.042652860283851624, -0.046733904629945755, -0.0467512309551239], "norm_scores": [0.20498192310333252, 0.19954149425029755, 0.19903376698493958, 0.19822315871715546, 0.19821971654891968]}}
{"id": "WebQTest-1472", "question": "when did romney become governor", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did romney become governor [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.020573899149894714, -0.031312961131334305, -0.03227902203798294, -0.03315528482198715, -0.04357479140162468], "norm_scores": [0.20232918858528137, 0.20016798377037048, 0.19997471570968628, 0.1997995525598526, 0.19772854447364807]}}
{"id": "WebQTest-1473", "question": "what currency does cyprus use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does cyprus use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.002786077791824937, -0.03931668773293495, -0.11061742156744003, -0.17084576189517975, -0.18036164343357086], "norm_scores": [0.2200457751750946, 0.21215243637561798, 0.19755247235298157, 0.18600542843341827, 0.1842438280582428]}}
{"id": "WebQTest-1475", "question": "what type of guitar does johnny depp play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"], ["people.person.nationality", "music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of guitar does johnny depp play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> people.person.nationality <SEP> music.group_member.instruments_played </PATH>"], "scores": [-0.01355162262916565, -0.03150603920221329, -0.035417720675468445, -0.03586443141102791, -0.05522019416093826], "norm_scores": [0.20417751371860504, 0.2005443423986435, 0.19976142048835754, 0.19967219233512878, 0.19584454596042633]}}
{"id": "WebQTest-1477", "question": "what episode of how i met your mother is carrie underwood on", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what episode of how i met your mother is carrie underwood on [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.series </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>"], "scores": [-0.01798410899937153, -0.02721009962260723, -0.02982136979699135, -0.03752262145280838, -0.053741104900836945], "norm_scores": [0.2030632197856903, 0.20119836926460266, 0.2006736844778061, 0.19913417100906372, 0.1959305703639984]}}
{"id": "WebQTest-1478", "question": "who are the 9 justices of the supreme court 2011", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"], ["book.written_work.subjects", "book.written_work.author"], ["people.appointment.appointed_role", "people.appointee.position"], ["people.appointed_role.appointment", "people.appointee.position"]], "ground_paths": [["book.written_work.subjects", "book.author.works_written"], ["law.court.jurisdiction", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the 9 justices of the supreme court 2011 [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.author.works_written </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>", "<PATH> people.appointment.appointed_role <SEP> people.appointee.position </PATH>", "<PATH> people.appointed_role.appointment <SEP> people.appointee.position </PATH>"], "scores": [-0.007922973483800888, -0.032902706414461136, -0.032925862818956375, -0.03379419445991516, -0.04168746992945671], "norm_scores": [0.20441970229148865, 0.19937661290168762, 0.19937199354171753, 0.19919894635677338, 0.1976328045129776]}}
{"id": "WebQTest-1479", "question": "who is dustin johnson the golfer", "prediction": [["people.person.profession"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft_pick.player", "sports.drafted_athlete.drafted"], ["sports.pro_athlete.sports_played_professionally", "sports.pro_sports_played.athlete"], ["sports.pro_athlete.sports_played_professionally", "sports.pro_athlete.sports_played"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dustin johnson the golfer [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.pro_athlete.sports_played_professionally <SEP> sports.pro_sports_played.athlete </PATH>", "<PATH> sports.pro_athlete.sports_played_professionally <SEP> sports.pro_athlete.sports_played </PATH>"], "scores": [-0.002349045593291521, -0.060408126562833786, -0.06610587984323502, -0.06900516897439957, -0.07574570178985596], "norm_scores": [0.21067817509174347, 0.19879470765590668, 0.1976652443408966, 0.1970929652452469, 0.19576893746852875]}}
{"id": "WebQTest-1480", "question": "who did cher have a son with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["music.artist.genre", "music.artist.genre"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["music.composer.compositions", "music.composer.compositions"], ["music.artist.label", "music.artist.label"], ["base.americancomedy.double_act.comic", "base.americancomedy.double_act.straight_man"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["people.person.profession", "people.person.profession"], ["award.award_nomination.award_nominee", "award.award_nominee.award_nominations"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.album.artist", "music.album.artist"], ["people.person.spouse_s", "people.person.spouse_s"], ["music.artist.concert_tours", "music.concert_tour.artist"], ["music.artist.genre", "music.artist.genre"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"], ["people.marriage.spouse", "people.marriage.spouse"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cher have a son with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.02160114049911499, -0.032885804772377014, -0.03985508158802986, -0.048821061849594116, -0.05462591350078583], "norm_scores": [0.2036099135875702, 0.20132514834403992, 0.19992695748806, 0.1981424242258072, 0.1969955563545227]}}
{"id": "WebQTest-1481", "question": "what city is the university of oregon state in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is the university of oregon state in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.contains </PATH>"], "scores": [-0.0026468550786376, -0.05412463843822479, -0.056568488478660583, -0.056916285306215286, -0.07859151065349579], "norm_scores": [0.20958316326141357, 0.19906727969646454, 0.19858138263225555, 0.19851231575012207, 0.19425581395626068]}}
{"id": "WebQTest-1482", "question": "who played carmen cortez in spy kids", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carmen cortez in spy kids [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.025631392374634743, -0.029191330075263977, -0.03290019556879997, -0.03411484509706497, -0.03572516888380051], "norm_scores": [0.20117835700511932, 0.2004634588956833, 0.1997213214635849, 0.199478879570961, 0.1991579234600067]}}
{"id": "WebQTest-1483", "question": "what is the political system in germany", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["government.form_of_government.countries", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system in germany [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>"], "scores": [-0.010025243274867535, -0.014469203539192677, -0.12843695282936096, -0.1356971561908722, -0.15677322447299957], "norm_scores": [0.21601484715938568, 0.21505700051784515, 0.19189251959323883, 0.19050437211990356, 0.18653132021427155]}}
{"id": "WebQTest-1484", "question": "what prompted shakespeare to write poetry", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what prompted shakespeare to write poetry [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.01654171198606491, -0.026082977652549744, -0.02831711806356907, -0.03229596093297005, -0.03696843609213829], "norm_scores": [0.20230844616889954, 0.20038734376430511, 0.19994014501571655, 0.19914621114730835, 0.19821785390377045]}}
{"id": "WebQTest-1485", "question": "what is the currency in turkey called", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used", "location.country.currency_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in turkey called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used <SEP> location.country.currency_used </PATH>"], "scores": [-0.016382161527872086, -0.01963723823428154, -0.03272724896669388, -0.046568144112825394, -0.13975803554058075], "norm_scores": [0.20683659613132477, 0.2061644196510315, 0.20348329842090607, 0.20068632066249847, 0.182829350233078]}}
{"id": "WebQTest-1486", "question": "what school did mark zuckerberg attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did mark zuckerberg attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.015488266944885254, -0.018190991133451462, -0.022316567599773407, -0.04809322953224182, -0.06615803390741348], "norm_scores": [0.20370708405971527, 0.20315726101398468, 0.20232084393501282, 0.19717232882976532, 0.19364242255687714]}}
{"id": "WebQTest-1487", "question": "where does kate middleton live 2012", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kate middleton live 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.01120191439986229, -0.021507935598492622, -0.03418111428618431, -0.05034203454852104, -0.05295855551958084], "norm_scores": [0.20459316670894623, 0.20249545574188232, 0.1999453902244568, 0.196740061044693, 0.19622595608234406]}}
{"id": "WebQTest-1488", "question": "what type of art does marc chagall do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.associated_artist.associated_periods_or_movements"], ["visual_art.visual_art_form.associated_artists"], ["book.school_or_movement.associated_authors"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["book.school_or_movement.associated_authors"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does marc chagall do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.associated_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_art_form.associated_artists </PATH>", "<PATH> book.school_or_movement.associated_authors </PATH>"], "scores": [-0.00159060163423419, -0.03819581866264343, -0.10714736580848694, -0.11148691922426224, -0.12099761515855789], "norm_scores": [0.21518085896968842, 0.20744654536247253, 0.19362476468086243, 0.1927863508462906, 0.19096150994300842]}}
{"id": "WebQTest-1489", "question": "what character did billy d williams play in star wars", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["film.performance.actor", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what character did billy d williams play in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.02168823592364788, -0.022298423573374748, -0.028471915051341057, -0.029405687004327774, -0.0352928452193737], "norm_scores": [0.20114941895008087, 0.20102670788764954, 0.19978949427604675, 0.19960302114486694, 0.19843138754367828]}}
{"id": "WebQTest-1490", "question": "what language is spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.language_family.geographic_distribution"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>"], "scores": [-0.007448538672178984, -0.028399094939231873, -0.04852510988712311, -0.05747201293706894, -0.14617903530597687], "norm_scores": [0.2100534737110138, 0.20569851994514465, 0.20160000026226044, 0.19980435073375702, 0.18284368515014648]}}
{"id": "WebQTest-1493", "question": "what school did wilfred owen go to", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did wilfred owen go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.015136495232582092, -0.02431553788483143, -0.02469092234969139, -0.028832238167524338, -0.05045443773269653], "norm_scores": [0.20271433889865875, 0.2008621245622635, 0.2007867395877838, 0.19995693862438202, 0.19567984342575073]}}
{"id": "WebQTest-1494", "question": "who is kevin jonas married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is kevin jonas married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.028413662686944008, -0.03459816053509712, -0.039826828986406326, -0.04228735715150833, -0.04487873613834381], "norm_scores": [0.2019231915473938, 0.2006782591342926, 0.1996317058801651, 0.19914110004901886, 0.19862571358680725]}}
{"id": "WebQTest-1495", "question": "where does airtran airways fly", "prediction": [["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airline_airport_presence.airline", "aviation.airline_airport_presence.airport"], ["aviation.airport.hub_for", "aviation.airport.serves"], ["aviation.airline.hubs", "aviation.airport.serves"], ["aviation.airline.hubs", "aviation.airline_airport_presence.airport"]], "ground_paths": [["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airline.focus_cities"], ["aviation.airline_airport_presence.airline", "aviation.airport.airlines"], ["aviation.airport.focus_city_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does airtran airways fly [/INST]", "raw_output": {"paths": ["<PATH> aviation.airline.airports_served <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airline_airport_presence.airline <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airport.hub_for <SEP> aviation.airport.serves </PATH>", "<PATH> aviation.airline.hubs <SEP> aviation.airport.serves </PATH>", "<PATH> aviation.airline.hubs <SEP> aviation.airline_airport_presence.airport </PATH>"], "scores": [-0.019066372886300087, -0.024474650621414185, -0.038530390709638596, -0.04556512087583542, -0.05162888765335083], "norm_scores": [0.20337016880512238, 0.20227326452732086, 0.1994500458240509, 0.19805189967155457, 0.1968545913696289]}}
{"id": "WebQTest-1496", "question": "where does the city of paris get its name", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["location.location.containedby"], [], ["location.location.contains"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the city of paris get its name [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.011056109331548214, -0.01496925763785839, -0.07767200469970703, -0.12504200637340546, -0.12916921079158783], "norm_scores": [0.21220092475414276, 0.21137218177318573, 0.198525533080101, 0.18934063613414764, 0.18856079876422882]}}
{"id": "WebQTest-1499", "question": "who did sandra bullock marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["base.popstra.infidelity.participant", "base.popstra.infidelity.perpetrator"], ["base.popstra.infidelity.victim", "base.popstra.celebrity.infidelity_perpetrator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sandra bullock marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.018139101564884186, -0.037391550838947296, -0.039617039263248444, -0.04419410601258278, -0.04847021400928497], "norm_scores": [0.20391149818897247, 0.20002324879169464, 0.19957858324050903, 0.19866719841957092, 0.19781948626041412]}}
{"id": "WebQTest-1500", "question": "where is olympic national park located", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.011327036656439304, -0.024464812129735947, -0.02900419384241104, -0.09816024452447891, -0.10915210843086243], "norm_scores": [0.20863482356071472, 0.2059117704629898, 0.20497915148735046, 0.19128267467021942, 0.18919163942337036]}}
{"id": "WebQTest-1501", "question": "where is the un based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded", "base.locations.continents.countries_within"]], "ground_paths": [["organization.organization_founder.organizations_founded", "location.location.containedby"], ["book.book_subject.works", "base.reviews.review.mentions"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the un based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.locations.continents.countries_within </PATH>"], "scores": [-0.013229820877313614, -0.04172256588935852, -0.04196789860725403, -0.046846985816955566, -0.047117527574300766], "norm_scores": [0.20503559708595276, 0.19927600026130676, 0.19922712445259094, 0.1982574462890625, 0.19820381700992584]}}
{"id": "WebQTest-1502", "question": "what city and state is yale located in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city and state is yale located in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>"], "scores": [-0.0017390446737408638, -0.05883913114666939, -0.06578150391578674, -0.06705226004123688, -0.08127877861261368], "norm_scores": [0.21084681153297424, 0.19914472103118896, 0.197766974568367, 0.1975158154964447, 0.19472575187683105]}}
{"id": "WebQTest-1503", "question": "where did thomas paine die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people_deceased_here"], ["location.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas paine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>"], "scores": [-0.000380319805117324, -0.0949220135807991, -0.10284736007452011, -0.11161427944898605, -0.11598533391952515], "norm_scores": [0.21748805046081543, 0.19786840677261353, 0.19630643725395203, 0.19459295272827148, 0.1937442272901535]}}
{"id": "WebQTest-1504", "question": "where was barack hussein obama sr born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["people.person.parents"], ["people.person.children"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was barack hussein obama sr born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.0007528195274062455, -0.06363001465797424, -0.10188431292772293, -0.10382699966430664, -0.12868016958236694], "norm_scores": [0.21622321009635925, 0.2030463069677353, 0.1954256147146225, 0.1950463205575943, 0.19025854766368866]}}
{"id": "WebQTest-1506", "question": "where is the home stadium of the new england patriots", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_facility.teams", "sports.sports_team.arena_stadium"], ["location.location.containedby"], ["sports.sports_facility.teams", "sports.sports_facility.teams"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the home stadium of the new england patriots [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_team.arena_stadium </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> sports.sports_facility.teams <SEP> sports.sports_facility.teams </PATH>"], "scores": [-0.0034817655105143785, -0.02745981328189373, -0.1003221794962883, -0.10553557425737381, -0.1106472834944725], "norm_scores": [0.2134305089712143, 0.20837374031543732, 0.19373105466365814, 0.19272369146347046, 0.19174106419086456]}}
{"id": "WebQTest-1507", "question": "who was kate chopin", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kate chopin [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.0004969682195223868, -0.07913113385438919, -0.09776816517114639, -0.13217946887016296, -0.13795219361782074], "norm_scores": [0.21834629774093628, 0.2018345296382904, 0.1981077641248703, 0.19140657782554626, 0.19030481576919556]}}
{"id": "WebQTest-1508", "question": "when did bill clinton go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["common.topic.notable_types"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did bill clinton go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.014917287975549698, -0.020379742607474327, -0.021972868591547012, -0.056498847901821136, -0.05742436647415161], "norm_scores": [0.20386625826358795, 0.20275567471981049, 0.20243293046951294, 0.19556300342082977, 0.19538208842277527]}}
{"id": "WebQTest-1510", "question": "what are staffordshire terriers", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_of_breed.breed_of"], ["biology.breed_breed.breed_of"], ["biology.breed_origin.breeds_originating_here"], ["biology.breed_of.breed_of"]], "ground_paths": [["biology.animal_breed.breed_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are staffordshire terriers [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_of_breed.breed_of </PATH>", "<PATH> biology.breed_breed.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.breed_of.breed_of </PATH>"], "scores": [-0.007227880414575338, -0.03928316384553909, -0.05010638386011124, -0.05985380709171295, -0.06610659509897232], "norm_scores": [0.20755340158939362, 0.2010057270526886, 0.19884192943572998, 0.19691313803195953, 0.1956857442855835]}}
{"id": "WebQTest-1513", "question": "what to do downtown sacramento", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.neighborhood.neighborhood_of"], ["location.location.contains"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do downtown sacramento [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.neighborhood.neighborhood_of </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01706778071820736, -0.019389327615499496, -0.02214624732732773, -0.0821981355547905, -0.10869040340185165], "norm_scores": [0.2065258026123047, 0.20604689419269562, 0.20547962188720703, 0.19350337982177734, 0.1884443461894989]}}
{"id": "WebQTest-1515", "question": "who is michael j fox parents", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.01187985297292471, -0.015868278220295906, -0.14639851450920105, -0.1487591564655304, -0.1690957248210907], "norm_scores": [0.21754494309425354, 0.21667902171611786, 0.1901639997959137, 0.18971562385559082, 0.18589641153812408]}}
{"id": "WebQTest-1516", "question": "where did sanjay gupta go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.honorary_degree_recipient.honorary_degrees", "education.honorary_degree.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.honorary_degree_recipient.honorary_degrees", "education.honorary_degree.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did sanjay gupta go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.honorary_degree_recipient.honorary_degrees <SEP> education.honorary_degree.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.016958270221948624, -0.019512657076120377, -0.02394571341574192, -0.0292695052921772, -0.0398530550301075], "norm_scores": [0.20179130136966705, 0.201276496052742, 0.2003861963748932, 0.19932222366333008, 0.19722381234169006]}}
{"id": "WebQTest-1518", "question": "what countries are involved in the nafta", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["common.topic.notable_types"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.membership_organization.members", "organization.organization_membership.member"]], "ground_paths": [["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_member.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are involved in the nafta [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_membership.member </PATH>"], "scores": [-0.01149125024676323, -0.024983057752251625, -0.04155326262116432, -0.04188452661037445, -0.05542263388633728], "norm_scores": [0.20474737882614136, 0.20200350880622864, 0.19868384301662445, 0.19861803948879242, 0.19594725966453552]}}
{"id": "WebQTest-1519", "question": "what continent is cayman islands located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["common.topic.notable_types"], ["geography.island.island_group"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent is cayman islands located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> geography.island.island_group </PATH>"], "scores": [-0.009037257172167301, -0.021582337096333504, -0.07809998840093613, -0.09086459875106812, -0.09762553125619888], "norm_scores": [0.21019642055034637, 0.2075759768486023, 0.1961696296930313, 0.1936815083026886, 0.19237644970417023]}}
{"id": "WebQTest-1520", "question": "what language speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "</PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0010917262407019734, -0.0669836699962616, -0.07586181908845901, -0.09371571987867355, -0.19534140825271606], "norm_scores": [0.21743018925189972, 0.2035651057958603, 0.20176582038402557, 0.1981954723596573, 0.17904338240623474]}}
{"id": "WebQTest-1522", "question": "who does kroy biermann play football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kroy biermann play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.020049316808581352, -0.023082101717591286, -0.027468672022223473, -0.028883717954158783, -0.029558956623077393], "norm_scores": [0.20115381479263306, 0.20054467022418976, 0.19966690242290497, 0.19938457012176514, 0.1992499828338623]}}
{"id": "WebQTest-1523", "question": "when is the last time the pittsburgh steelers won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.recurring_event.instances"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the pittsburgh steelers won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.01058740820735693, -0.015865352004766464, -0.03449859842658043, -0.059653278440237045, -0.07836862653493881], "norm_scores": [0.20585909485816956, 0.20477543771266937, 0.20099513232707977, 0.19600224494934082, 0.19236810505390167]}}
{"id": "WebQTest-1524", "question": "who invented islamic religion", "prediction": [["religion.religion.notable_figures"], ["religion.religion.founding_figures"], ["religion.belief.belief_of"], ["people.person.religion"], ["symbols.namesake.named_after"]], "ground_paths": [["religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who invented islamic religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>", "<PATH> religion.belief.belief_of </PATH>", "<PATH> people.person.religion </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0025014160200953484, -0.04361490160226822, -0.06788554787635803, -0.09670386463403702, -0.09817153960466385], "norm_scores": [0.21207602322101593, 0.20353364944458008, 0.1986532360315323, 0.1930100917816162, 0.1927269995212555]}}
{"id": "WebQTest-1525", "question": "who were we fighting in the gulf war", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["time.event.locations"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["military.casualties.military_conflict", "military.casualties.combatant"]], "ground_paths": [["film.film_genre.films_in_this_genre", "film.film.country"], ["time.event.locations"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "tv.tv_program.country_of_origin"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["film.film.genre", "film.film.country"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["film.film.subjects", "film.film.country"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "film.film.country"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were we fighting in the gulf war [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>", "<PATH> military.casualties.military_conflict <SEP> military.military_combatant.casualties </PATH>", "<PATH> military.casualties.military_conflict <SEP> military.casualties.combatant </PATH>"], "scores": [-0.024636797606945038, -0.027343548834323883, -0.02924901619553566, -0.03412121534347534, -0.041653312742710114], "norm_scores": [0.2013537883758545, 0.200809508562088, 0.20042723417282104, 0.19945307075977325, 0.19795642793178558]}}
{"id": "WebQTest-1526", "question": "how many languages do they speak in switzerland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in switzerland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.015119360759854317, -0.02245471067726612, -0.023449214175343513, -0.0750257670879364, -0.15600307285785675], "norm_scores": [0.20855756103992462, 0.20703332126140594, 0.20682750642299652, 0.196430504322052, 0.18115107715129852]}}
{"id": "WebQTest-1527", "question": "what political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.election_campaigns", "government.election_campaign.party"]], "ground_paths": [["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.party </PATH>", "<PATH> government.politician.election_campaigns <SEP> government.election_campaign.party </PATH>"], "scores": [-0.019081829115748405, -0.02098921313881874, -0.023358728736639023, -0.03462137281894684, -0.04028044641017914], "norm_scores": [0.201717346906662, 0.2013329714536667, 0.20085646212100983, 0.19860698282718658, 0.19748623669147491]}}
{"id": "WebQTest-1528", "question": "what movies does logan lerman play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does logan lerman play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.016604941338300705, -0.03632347285747528, -0.03658529743552208, -0.03781859204173088, -0.03824072331190109], "norm_scores": [0.20332233607769012, 0.19935238361358643, 0.19930018484592438, 0.1990545690059662, 0.19897054135799408]}}
{"id": "WebQTest-1529", "question": "where did emperor constantine die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["location.location.people_born_here"], ["location.location.people_born_here", "location.location.containedby"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor constantine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.0006750886677764356, -0.07461051642894745, -0.08242306858301163, -0.09838535636663437, -0.10094120353460312], "norm_scores": [0.21451199054718018, 0.1992240697145462, 0.19767367839813232, 0.19454340636730194, 0.19404684007167816]}}
{"id": "WebQTest-1531", "question": "where is mali located", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mali located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.016919860616326332, -0.022289030253887177, -0.040923941880464554, -0.051389120519161224, -0.06303448975086212], "norm_scores": [0.20441631972789764, 0.20332172513008118, 0.19956791400909424, 0.19749030470848083, 0.19520379602909088]}}
{"id": "WebQTest-1533", "question": "what planet did luke skywalker came from", "prediction": [["fictional_universe.fictional_setting.characters_that_have_lived_here"], ["fictional_universe.fictional_character.place_of_birth"], ["fictional_universe.fictional_character.places_lived"], ["fictional_universe.fictional_character.places_lived", "fictional_universe.fictional_setting.fictional_characters_born_here"], ["fictional_universe.fictional_character.place_of_birth", "fictional_universe.fictional_setting.fictional_characters_born_here"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what planet did luke skywalker came from [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> fictional_universe.fictional_character.places_lived </PATH>", "<PATH> fictional_universe.fictional_character.places_lived <SEP> fictional_universe.fictional_setting.fictional_characters_born_here </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth <SEP> fictional_universe.fictional_setting.fictional_characters_born_here </PATH>"], "scores": [-0.012264879420399666, -0.018685517832636833, -0.020148644223809242, -0.05928917974233627, -0.06941866874694824], "norm_scores": [0.2047393023967743, 0.2034289538860321, 0.2031315267086029, 0.19533443450927734, 0.19336579740047455]}}
{"id": "WebQTest-1534", "question": "who are the coaches for the oakland raiders", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.current_head_coach"], ["american_football.football_coach.current_team_head_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the coaches for the oakland raiders [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>", "<PATH> american_football.football_coach.current_team_head_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.01595100201666355, -0.02159965969622135, -0.02545628696680069, -0.035934679210186005, -0.0369013212621212], "norm_scores": [0.2022494375705719, 0.20111022889614105, 0.20033611357212067, 0.19824786484241486, 0.19805634021759033]}}
{"id": "WebQTest-1535", "question": "where was the first microsoft headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["location.location.contains"], ["symbols.namesake.named_after", "location.location.containedby"], ["organization.organization.founders"]], "ground_paths": [["base.engineering1.rfid_company.location", "base.biblioness.bibs_location.country"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first microsoft headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.017928743734955788, -0.03406984359025955, -0.04245711863040924, -0.048436589539051056, -0.05020884424448013], "norm_scores": [0.2041672021150589, 0.2008981704711914, 0.1992202252149582, 0.19803255796432495, 0.19768188893795013]}}
{"id": "WebQTest-1536", "question": "where did john steinbeck go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john steinbeck go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.016495468094944954, -0.018152806907892227, -0.018416093662381172, -0.04894568771123886, -0.08545658737421036], "norm_scores": [0.2041708528995514, 0.203832745552063, 0.20377911627292633, 0.19765181839466095, 0.19056551158428192]}}
{"id": "WebQTest-1537", "question": "what did niels bohr discover about the atomic structure", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["base.nobelprizes.nobel_honor.nobel_prize_winner"], ["base.nobelprizes.nobel_honor.nobel_prize_winner", "base.nobelprizes.nobel_subject_area.nobel_awards"], ["base.nobelprizes.nobel_prize_winner.nobel_honor"]], "ground_paths": [["base.nobelprizes.nobel_honor.nobel_prize_winner", "base.nobelprizes.nobel_honor.subject_area"], ["education.academic.advisees", "book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did niels bohr discover about the atomic structure [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.nobelprizes.nobel_honor.nobel_prize_winner </PATH>", "<PATH> base.nobelprizes.nobel_honor.nobel_prize_winner <SEP> base.nobelprizes.nobel_subject_area.nobel_awards </PATH>", "<PATH> base.nobelprizes.nobel_prize_winner.nobel_honor </PATH>"], "scores": [-0.012314355000853539, -0.024923212826251984, -0.03441367298364639, -0.04595304653048515, -0.047787245362997055], "norm_scores": [0.20417816936969757, 0.20161989331245422, 0.19971546530723572, 0.19742412865161896, 0.19706234335899353]}}
{"id": "WebQTest-1538", "question": "what books did louis sachar wrote", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"], ["book.author.book_editions_published"], ["film.film.story_by"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did louis sachar wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.012108225375413895, -0.025594647973775864, -0.029560383409261703, -0.07474660128355026, -0.07700838893651962], "norm_scores": [0.20636653900146484, 0.20360206067562103, 0.20279623568058014, 0.19383658468723297, 0.19339866936206818]}}
{"id": "WebQTest-1539", "question": "who is ryan reynolds married to 2012", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is ryan reynolds married to 2012 [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.person.spouse_s </PATH>"], "scores": [-0.017514409497380257, -0.03013991378247738, -0.03180087357759476, -0.042231038212776184, -0.045799437910318375], "norm_scores": [0.20321212708950043, 0.20066259801387787, 0.20032958686351776, 0.19825096428394318, 0.19754479825496674]}}
{"id": "WebQTest-1540", "question": "what is newt gingrich 's wife 's name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.person.profession", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["film.film.executive_produced_by", "film.film.executive_produced_by"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.religion", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is newt gingrich 's wife 's name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.023939713835716248, -0.0327267125248909, -0.03629732131958008, -0.039410438388586044, -0.04011867940425873], "norm_scores": [0.20211942493915558, 0.20035119354724884, 0.19963708519935608, 0.19901655614376068, 0.19887566566467285]}}
{"id": "WebQTest-1541", "question": "what type of music did antonio vivaldi play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["book.written_work.author"], ["music.composer.compositions"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did antonio vivaldi play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> music.composer.compositions </PATH>"], "scores": [-0.0025223789270967245, -0.03742826730012894, -0.11031343042850494, -0.1209702417254448, -0.1348281055688858], "norm_scores": [0.21608152985572815, 0.20866914093494415, 0.19400128722190857, 0.19194482266902924, 0.1893032193183899]}}
{"id": "WebQTest-1542", "question": "what school does james franco go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school does james franco go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.014820277690887451, -0.016286354511976242, -0.023996196687221527, -0.053857870399951935, -0.0715872198343277], "norm_scores": [0.2042512744665146, 0.20395205914974213, 0.20238564908504486, 0.19643142819404602, 0.19297951459884644]}}
{"id": "WebQTest-1543", "question": "what is samuel langhorne clemens well known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"], ["people.profession.people_with_this_profession"], ["book.written_work.author"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is samuel langhorne clemens well known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.0014760142657905817, -0.046419233083724976, -0.08996684849262238, -0.09955994039773941, -0.10877841711044312], "norm_scores": [0.2138492912054062, 0.20445099472999573, 0.19573871791362762, 0.193869948387146, 0.1920909732580185]}}
{"id": "WebQTest-1544", "question": "who was president after william henry harrison", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.gender", "people.person.gender"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after william henry harrison [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.015158739872276783, -0.022562485188245773, -0.04375329241156578, -0.05485980585217476, -0.057251136749982834], "norm_scores": [0.20473797619342804, 0.20322774350643158, 0.19896650314331055, 0.1967688947916031, 0.19629892706871033]}}
{"id": "WebQTest-1546", "question": "where did rick santorum attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rick santorum attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.016030199825763702, -0.02223624847829342, -0.02778969146311283, -0.03188115358352661, -0.035236772149801254], "norm_scores": [0.20212745666503906, 0.20087692141532898, 0.1997644603252411, 0.19894880056381226, 0.19828233122825623]}}
{"id": "WebQTest-1547", "question": "what is the title of george bush 's new book", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["book.written_work.subjects"], ["book.author.book_editions_published"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the title of george bush 's new book [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.book_editions_published </PATH>"], "scores": [-0.0096049215644598, -0.02395934797823429, -0.04573480039834976, -0.05502193048596382, -0.06799013167619705], "norm_scores": [0.20622175931930542, 0.20328271389007568, 0.19890400767326355, 0.1970652937889099, 0.19452621042728424]}}
{"id": "WebQTest-1548", "question": "where did antoine lavoisier die", "prediction": [["people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"], ["people.place_of_death.people", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did antoine lavoisier die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.00023411960864905268, -0.0930163636803627, -0.10376930981874466, -0.10484050959348679, -0.10821833461523056], "norm_scores": [0.21685591340065002, 0.19764073193073273, 0.19552689790725708, 0.1953175663948059, 0.19465893507003784]}}
{"id": "WebQTest-1550", "question": "what is the name of king george vi wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["royalty.monarch.kingdom", "people.person.nationality"], ["common.image.appears_in_topic_gallery", "common.topic.image"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.spouse_s", "people.person.spouse_s"], ["film.film.subjects", "film.film_subject.films"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.family.members", "people.family.members"], ["people.person.religion", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of king george vi wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.nationality </PATH>", "<PATH> common.image.appears_in_topic_gallery <SEP> common.topic.image </PATH>"], "scores": [-0.036556098610162735, -0.036899399012327194, -0.039114177227020264, -0.04156764596700668, -0.049262892454862595], "norm_scores": [0.2008243203163147, 0.20075538754463196, 0.20031125843524933, 0.199820414185524, 0.19828864932060242]}}
{"id": "WebQTest-1551", "question": "what year did seattle seahawks go to the super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did seattle seahawks go to the super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>"], "scores": [-0.018026866018772125, -0.021756090223789215, -0.025105813518166542, -0.034405872225761414, -0.039855197072029114], "norm_scores": [0.2019636332988739, 0.2012118697166443, 0.2005389928817749, 0.19868260622024536, 0.19760288298130035]}}
{"id": "WebQTest-1553", "question": "where does sami yusuf live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sami yusuf live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.024526173248887062, -0.02871505357325077, -0.03268773853778839, -0.03914494812488556, -0.04116428643465042], "norm_scores": [0.20174801349639893, 0.20090468227863312, 0.2001081258058548, 0.19882014393806458, 0.19841907918453217]}}
{"id": "WebQTest-1554", "question": "what is the official language of brazil now", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the official language of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "</PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0005169865908101201, -0.07373283803462982, -0.09072424471378326, -0.09930972754955292, -0.18496620655059814], "norm_scores": [0.21831035614013672, 0.20289769768714905, 0.19947929680347443, 0.1977740079164505, 0.1815386414527893]}}
{"id": "WebQTest-1555", "question": "what is the parliament of nepal called", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.governing_officials", "government.governmental_body.members"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.governmental_body"], ["location.location.containedby", "government.governmental_jurisdiction.government_bodies"], ["location.location.containedby", "government.governmental_body.jurisdiction"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the parliament of nepal called [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_body.members </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.governmental_body </PATH>", "<PATH> location.location.containedby <SEP> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> location.location.containedby <SEP> government.governmental_body.jurisdiction </PATH>"], "scores": [-0.016712432727217674, -0.018418824300169945, -0.02690831944346428, -0.029707038775086403, -0.052095960825681686], "norm_scores": [0.20240966975688934, 0.20206458866596222, 0.2003563940525055, 0.19979645311832428, 0.19537292420864105]}}
{"id": "WebQTest-1556", "question": "when 's the last time the mets won the world series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the mets won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.010962061583995819, -0.011655863374471664, -0.07782719284296036, -0.08668455481529236, -0.11391516029834747], "norm_scores": [0.20991335809230804, 0.20976777374744415, 0.19633644819259644, 0.19460511207580566, 0.18937738239765167]}}
{"id": "WebQTest-1557", "question": "where do florida panthers play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do florida panthers play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team_roster.team </PATH>"], "scores": [-0.007017402444034815, -0.024143161252141, -0.06397511065006256, -0.06561197340488434, -0.12740656733512878], "norm_scores": [0.21020245552062988, 0.20663321018218994, 0.19856438040733337, 0.19823963940143585, 0.18636031448841095]}}
{"id": "WebQTest-1558", "question": "who is john cena married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is john cena married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>"], "scores": [-0.019967516884207726, -0.03367365896701813, -0.03515838459134102, -0.044989313930273056, -0.046898405998945236], "norm_scores": [0.20325084030628204, 0.20048405230045319, 0.200186625123024, 0.19822824001312256, 0.19785018265247345]}}
{"id": "WebQTest-1559", "question": "what to do san jose ca", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["location.location.contains"], ["base.usnationalparks.us_national_park.state"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["location.neighborhood.neighborhood_of"], ["film.film_festival.location"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do san jose ca [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.usnationalparks.us_national_park.state </PATH>"], "scores": [-0.016360066831111908, -0.016370926052331924, -0.028266772627830505, -0.10850479453802109, -0.12826204299926758], "norm_scores": [0.20858284831047058, 0.20858058333396912, 0.2061140388250351, 0.19022196531295776, 0.18650057911872864]}}
{"id": "WebQTest-1560", "question": "who founded new york university", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["education.educational_institution.parent_institution", "people.person.parents"], ["education.educational_institution.parent_institution", "people.person.education"], ["education.educational_institution.parent_institution"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded new york university [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> people.person.parents </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> people.person.education </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>"], "scores": [-0.0262211374938488, -0.03158292919397354, -0.035866670310497284, -0.03808467835187912, -0.04344865307211876], "norm_scores": [0.20176829397678375, 0.20068934559822083, 0.19983148574829102, 0.1993887573480606, 0.1983221024274826]}}
{"id": "WebQTest-1561", "question": "who created arthur", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.program_creator"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program.program_creator"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who created arthur [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.program_creator </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.007058948744088411, -0.03737016022205353, -0.04710210859775543, -0.051976531744003296, -0.07708316296339035], "norm_scores": [0.20749734342098236, 0.20130221545696259, 0.19935265183448792, 0.19838328659534454, 0.19346454739570618]}}
{"id": "WebQTest-1562", "question": "what countries constitute the eu", "prediction": [["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization.geographic_scope", "location.location.containedby"], ["organization.organization.geographic_scope", "base.locations.countries.continent"], ["organization.membership_organization.members", "organization.organization_member.member_of"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["government.governmental_jurisdiction.governing_officials", "government.political_district.representatives"], ["organization.organization_membership.organization", "organization.organization_membership.member"], ["base.industrystandards.standard_organization.standards", "base.aareas.coding_scheme.geo_scope"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.district_represented"], ["government.government_position_held.jurisdiction_of_office", "government.political_district.representatives"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization.geographic_scope", "location.location.containedby"], ["organization.organization.geographic_scope", "location.location.primarily_containedby"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization.geographic_scope", "base.locations.countries.continent"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization.founders"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries constitute the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.continents.countries_within </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.countries.continent </PATH>", "<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>"], "scores": [-0.029466811567544937, -0.03215986490249634, -0.033436212688684464, -0.035881783813238144, -0.03868561610579491], "norm_scores": [0.2008928507566452, 0.20035256445407867, 0.200096994638443, 0.1996082365512848, 0.19904936850070953]}}
{"id": "WebQTest-1563", "question": "what are all the countries in north africa", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["common.topic.notable_types"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the countries in north africa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0051166280172765255, -0.03323099762201309, -0.06901104748249054, -0.07115014642477036, -0.08127465844154358], "norm_scores": [0.20950523018836975, 0.20369715988636017, 0.19653770327568054, 0.19611774384975433, 0.1941421627998352]}}
{"id": "WebQTest-1564", "question": "what did bobby darin die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bobby darin die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>"], "scores": [-0.0014589186757802963, -0.04433247447013855, -0.1105155199766159, -0.11556347459554672, -0.15162540972232819], "norm_scores": [0.21704019606113434, 0.20793156325817108, 0.19461554288864136, 0.19363559782505035, 0.18677711486816406]}}
{"id": "WebQTest-1565", "question": "where does houston dynamo play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"], ["travel.travel_destination.tourist_attractions"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does houston dynamo play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.011954357847571373, -0.013476438820362091, -0.08322842419147491, -0.08346127718687057, -0.09632818400859833], "norm_scores": [0.20921538770198822, 0.2088971883058548, 0.1948227733373642, 0.19477739930152893, 0.19228728115558624]}}
{"id": "WebQTest-1566", "question": "what kind of guitar john mayer", "prediction": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"], ["music.instrument.family"], ["music.group_member.instruments_played"], ["music.artist.guitars_played"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar john mayer [/INST]", "raw_output": {"paths": ["<PATH> music.guitar.guitarists </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.guitars_played </PATH>"], "scores": [-0.014537966810166836, -0.01591595448553562, -0.05241145193576813, -0.057084184139966965, -0.11001031845808029], "norm_scores": [0.2070930451154709, 0.20680786669254303, 0.19939638674259186, 0.19846683740615845, 0.18823589384555817]}}
{"id": "WebQTest-1567", "question": "what cancer did patrick swayze", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.cancer_patient.cancer"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did patrick swayze [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.cancer_patient.cancer </PATH>"], "scores": [-0.001650485210120678, -0.04603775963187218, -0.05852574110031128, -0.11361018568277359, -0.11922655254602432], "norm_scores": [0.21347229182720184, 0.20420405268669128, 0.2016698122024536, 0.19086137413978577, 0.1897924244403839]}}
{"id": "WebQTest-1568", "question": "what university did obama graduated from", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.014080621302127838, -0.01614624634385109, -0.02639928087592125, -0.08636331558227539, -0.09961924701929092], "norm_scores": [0.20686893165111542, 0.20644205808639526, 0.20433621108531952, 0.19244351983070374, 0.18990933895111084]}}
{"id": "WebQTest-1569", "question": "what jamaican money called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.currency_used", "location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jamaican money called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.country.currency_used <SEP> location.country.currency_used </PATH>"], "scores": [-0.0005292757414281368, -0.07230530679225922, -0.1352464109659195, -0.14120197296142578, -0.14560213685035706], "norm_scores": [0.22034025192260742, 0.2050793319940567, 0.19256925582885742, 0.19142580032348633, 0.19058534502983093]}}
{"id": "WebQTest-1570", "question": "where is the arizona cardinals stadium located", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"], ["location.location.containedby"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the arizona cardinals stadium located [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.006087771151214838, -0.025793200358748436, -0.052980344742536545, -0.07301024347543716, -0.09138909727334976], "norm_scores": [0.20884758234024048, 0.2047724425792694, 0.1992802619934082, 0.19532839953899384, 0.19177128374576569]}}
{"id": "WebQTest-1571", "question": "what is sir isaac newton theory", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["people.person.profession"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is sir isaac newton theory [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.007520895451307297, -0.02315588667988777, -0.06980343908071518, -0.08346325904130936, -0.09555990248918533], "norm_scores": [0.20978879928588867, 0.2065342515707016, 0.19712120294570923, 0.194446861743927, 0.1921088844537735]}}
{"id": "WebQTest-1572", "question": "what should i do today in san francisco", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["metropolitan_transit.transit_system.area_served"], ["film.film_festival.location"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.neighborhood.neighborhood_of"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what should i do today in san francisco [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> film.film_festival.location </PATH>"], "scores": [-0.015381012111902237, -0.019301997497677803, -0.02197178266942501, -0.10308171063661575, -0.11962274461984634], "norm_scores": [0.2080489993095398, 0.20723482966423035, 0.20668230950832367, 0.19058015942573547, 0.18745370209217072]}}
{"id": "WebQTest-1573", "question": "who influenced picasso art", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["visual_art.visual_artist.art_forms"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced picasso art [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.01203923486173153, -0.012998182326555252, -0.11077098548412323, -0.11561602354049683, -0.11885405331850052], "norm_scores": [0.2125251293182373, 0.2123214155435562, 0.19254472851753235, 0.1916140764951706, 0.19099463522434235]}}
{"id": "WebQTest-1574", "question": "what is my timezone in pennsylvania", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_zones", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <SEP> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-1.4764250408916268e-05, -0.131673201918602, -0.1803390085697174, -0.18906252086162567, -0.19363710284233093], "norm_scores": [0.2291814088821411, 0.20090968906879425, 0.19136634469032288, 0.18970422446727753, 0.18883837759494781]}}
{"id": "WebQTest-1575", "question": "what products and\\/or services does google offer customers", "prediction": [["organization.organization.sectors"], ["business.business_operation.industry"], ["organization.organization_founder.organizations_founded"], ["business.brand.products"], ["organization.organization.sectors", "organization.organization.sectors"]], "ground_paths": [["projects.project_participation.participant", "projects.project_participation.project"], ["internet.website.owner"], ["business.consumer_company.products"], ["computer.computer_manufacturer_brand.computer_models"], ["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"], ["computer.software_developer.software"], ["business.consumer_company.products", "business.consumer_product.company"], ["computer.software.developer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products and\\/or services does google offer customers [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> business.business_operation.industry </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> business.brand.products </PATH>", "<PATH> organization.organization.sectors <SEP> organization.organization.sectors </PATH>"], "scores": [-0.04368605092167854, -0.04808720201253891, -0.050152815878391266, -0.052907947450876236, -0.055493053048849106], "norm_scores": [0.20127829909324646, 0.2003943920135498, 0.19998088479042053, 0.1994306594133377, 0.1989157795906067]}}
{"id": "WebQTest-1577", "question": "who did cliff lee play for last year", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cliff lee play for last year [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.014331415295600891, -0.014729762449860573, -0.02980632148683071, -0.033352892845869064, -0.05940554291009903], "norm_scores": [0.20319706201553345, 0.20311614871025085, 0.20007680356502533, 0.19936847686767578, 0.1942414939403534]}}
{"id": "WebQTest-1580", "question": "what type of democracy is the united states", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["form.form_of_government.countries"], ["base.locations.countries.planet"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of democracy is the united states [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> base.locations.countries.planet </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.008717958815395832, -0.015363939106464386, -0.15436111390590668, -0.16206930577754974, -0.16560833156108856], "norm_scores": [0.21879595518112183, 0.2173466682434082, 0.18914170563220978, 0.18768936395645142, 0.18702629208564758]}}
{"id": "WebQTest-1582", "question": "what time in china hong kong", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.schemastaging.disputed_location_claimant.disputed_territories", "location.location.time_zones"], ["base.locations.countries.continent"], ["location.location.containedby"]], "ground_paths": [["time.time_zone.locations_in_this_time_zone"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in china hong kong [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.schemastaging.disputed_location_claimant.disputed_territories <SEP> location.location.time_zones </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0054021007381379604, -0.024587562307715416, -0.08502243459224701, -0.08525986969470978, -0.10468733310699463], "norm_scores": [0.21127372980117798, 0.20725896954536438, 0.19510428607463837, 0.1950579732656479, 0.19130507111549377]}}
{"id": "WebQTest-1583", "question": "what all movies has taylor lautner been in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all movies has taylor lautner been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.024042081087827682, -0.029356420040130615, -0.029842263087630272, -0.030954068526625633, -0.03113880194723606], "norm_scores": [0.20100678503513336, 0.19994139671325684, 0.19984428584575653, 0.19962219893932343, 0.19958533346652985]}}
{"id": "WebQTest-1584", "question": "what disease did bernie mac die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"], ["medicine.notable_person_with_medical_condition.condition"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did bernie mac die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.00456855446100235, -0.03465873748064041, -0.03999040275812149, -0.06123088672757149, -0.10432026535272598], "norm_scores": [0.20896320044994354, 0.20276911556720734, 0.2016908973455429, 0.19745208323001862, 0.1891246736049652]}}
{"id": "WebQTest-1585", "question": "who plays spike spiegel", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["base.schemastaging.tv_character_extra.regular_dubbing_performances", "base.schemastaging.tv_star_dubbing_performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays spike spiegel [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.016744717955589294, -0.03511600196361542, -0.03540729731321335, -0.039682142436504364, -0.04141833260655403], "norm_scores": [0.2034067064523697, 0.19970397651195526, 0.19964580237865448, 0.1987941712141037, 0.19844934344291687]}}
{"id": "WebQTest-1586", "question": "who was queen isabella 's mother", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen isabella 's mother [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.008783835917711258, -0.019986269995570183, -0.12587910890579224, -0.1298048347234726, -0.14530305564403534], "norm_scores": [0.21566757559776306, 0.2132650762796402, 0.1918364018201828, 0.19108478724956512, 0.18814614415168762]}}
{"id": "WebQTest-1587", "question": "who does chris hemsworth have a baby with", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"], ["people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.friendship.friend", "celebrities.friendship.friend"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does chris hemsworth have a baby with [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.021647673100233078, -0.024852272123098373, -0.02686188369989395, -0.031644172966480255, -0.05701647326350212], "norm_scores": [0.2021467089653015, 0.20149993896484375, 0.20109540224075317, 0.2001360058784485, 0.19512197375297546]}}
{"id": "WebQTest-1589", "question": "where is the mts iceplex in winnipeg", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"], ["business.asset_ownership.owner", "business.asset_ownership.owned_asset"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the mts iceplex in winnipeg [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> business.asset_ownership.owner <SEP> business.asset_ownership.owned_asset </PATH>"], "scores": [-0.013927816413342953, -0.026010435074567795, -0.029220540076494217, -0.0303433109074831, -0.10756213217973709], "norm_scores": [0.20545914769172668, 0.20299158990383148, 0.2023410201072693, 0.20211395621299744, 0.1870943307876587]}}
{"id": "WebQTest-1590", "question": "what are the most popular religions in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["symbols.namesake.named_after", "religion.religion.notable_figures"]], "ground_paths": [["people.person.quotations", "people.person.religion"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["people.person.nationality", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["symbols.namesake.named_after", "religion.religion.notable_figures"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the most popular religions in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>", "<PATH> location.country.administrative_divisions <SEP> religion.religion.sacred_sites </PATH>", "<PATH> symbols.namesake.named_after <SEP> religion.religion.notable_figures </PATH>"], "scores": [-0.003092133905738592, -0.030323075130581856, -0.0590156614780426, -0.07705600559711456, -0.08582176268100739], "norm_scores": [0.20972920954227448, 0.20409514009952545, 0.1983223408460617, 0.19477662444114685, 0.1930767148733139]}}
{"id": "WebQTest-1591", "question": "what was obama 's former occupation", "prediction": [["people.person.profession"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.profession", "people.profession.specializations"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["people.person.profession", "common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was obama 's former occupation [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>"], "scores": [-0.007719783112406731, -0.03528295457363129, -0.04001981019973755, -0.04563352093100548, -0.06108717992901802], "norm_scores": [0.20610664784908295, 0.20050327479839325, 0.19955576956272125, 0.19843865931034088, 0.19539561867713928]}}
{"id": "WebQTest-1592", "question": "what are all the songs justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"], ["music.composition.composer"], ["music.lyricist.lyrics_written"], ["music.artist.album"]], "ground_paths": [["music.featured_artist.recordings"], ["music.featured_artist.albums"], ["music.artist.album"], ["music.recording.artist"], ["music.album.artist"], ["music.recording.featured_artists"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["music.composer.compositions"], ["music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the songs justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>", "<PATH> music.artist.album </PATH>"], "scores": [-0.017549922689795494, -0.022036010399460793, -0.03340226411819458, -0.050043392926454544, -0.0761106014251709], "norm_scores": [0.20445941388607025, 0.2035442292690277, 0.2012437880039215, 0.19792257249355316, 0.19282998144626617]}}
{"id": "WebQTest-1593", "question": "where is the thames river source", "prediction": [["geography.river.origin"], ["geography.river.basin_countries"], ["geography.river.mouth"], ["location.location.containedby"], ["geography.lake.basin_countries"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the thames river source [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.lake.basin_countries </PATH>"], "scores": [-0.004996508825570345, -0.052191682159900665, -0.05343784764409065, -0.05615731701254845, -0.09997636079788208], "norm_scores": [0.20981377363204956, 0.20014160871505737, 0.19989235699176788, 0.19934947788715363, 0.19080279767513275]}}
{"id": "WebQTest-1595", "question": "what new movies is robert pattinson in", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["music.artist.track", "music.recording.releases"], ["music.recording.artist", "music.recording.releases"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.producer.films_executive_produced"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what new movies is robert pattinson in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>"], "scores": [-0.02849286049604416, -0.030359230935573578, -0.032316289842128754, -0.032767415046691895, -0.0348270945250988], "norm_scores": [0.20065252482891083, 0.2002784013748169, 0.19988681375980377, 0.199796661734581, 0.19938556849956512]}}
{"id": "WebQTest-1596", "question": "what language do they use in pakistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they use in pakistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in <PATH>"], "scores": [-0.015978559851646423, -0.017830712720751762, -0.03408724069595337, -0.05304165557026863, -0.1879882663488388], "norm_scores": [0.2089519053697586, 0.20856526494026184, 0.2052021026611328, 0.20134925842285156, 0.1759314388036728]}}
{"id": "WebQTest-1597", "question": "what kind of products does nike sell", "prediction": [["business.business_operation.industry"], ["business.industry.companies"], ["business.industry.parent_industry"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["business.brand.products"]], "ground_paths": [["business.business_operation.industry"], ["business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of products does nike sell [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> business.industry.parent_industry </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> business.brand.products </PATH>"], "scores": [-0.011604242958128452, -0.02904798649251461, -0.05356196314096451, -0.057137176394462585, -0.07520583271980286], "norm_scores": [0.20680448412895203, 0.20322832465171814, 0.19830696284770966, 0.19759923219680786, 0.19406093657016754]}}
{"id": "WebQTest-1598", "question": "who did lbj marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["book.written_work.subjects", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.profession", "common.topic.notable_types"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did lbj marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.026605753228068352, -0.04059824347496033, -0.04113312438130379, -0.04599408805370331, -0.048196014016866684], "norm_scores": [0.20279359817504883, 0.19997577369213104, 0.199868842959404, 0.19889962673187256, 0.19846215844154358]}}
{"id": "WebQTest-1599", "question": "where did kim jong il die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim jong il die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.0003712364996317774, -0.09621553868055344, -0.09665478765964508, -0.10974883288145065, -0.11050917208194733], "norm_scores": [0.21697083115577698, 0.1971409022808075, 0.19705432653427124, 0.19449090957641602, 0.19434307515621185]}}
{"id": "WebQTest-1603", "question": "what is the sacred text of daoism", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"], ["religion.religion.sacred_sites"], ["religion.religion.religious_texts"], ["common.topic.notable_types"]], "ground_paths": [["book.written_work.subjects"], ["religion.religion.texts"], ["religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the sacred text of daoism [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> religion.religion.sacred_sites </PATH>", "<PATH> religion.religion.religious_texts </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.009297645650804043, -0.014652548357844353, -0.11552350223064423, -0.12654969096183777, -0.13219939172267914], "norm_scores": [0.21424320340156555, 0.21309903264045715, 0.19265210628509521, 0.19053955376148224, 0.18946610391139984]}}
{"id": "WebQTest-1604", "question": "what countries speak english natively", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak english natively [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.016523418948054314, -0.021450858563184738, -0.025168996304273605, -0.0770275667309761, -0.15632694959640503], "norm_scores": [0.2084507942199707, 0.20742620527744293, 0.20665638148784637, 0.19621261954307556, 0.18125402927398682]}}
{"id": "WebQTest-1607", "question": "who won fedex cup 2012", "prediction": [["award.competition.winner", "people.person.profession"], ["award.competition.winner", "people.person.gender", "people.person.gender"], ["award.competitor.competitions", "people.person.profession"], ["award.competitor.competitions", "people.person.gender", "people.person.gender"], ["award.competition.winner"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fedex cup 2012 [/INST]", "raw_output": {"paths": ["<PATH> award.competition.winner <SEP> people.person.profession </PATH>", "<PATH> award.competition.winner <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> award.competitor.competitions <SEP> people.person.profession </PATH>", "<PATH> award.competitor.competitions <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> award.competition.winner </PATH>"], "scores": [-0.017888709902763367, -0.027781382203102112, -0.029985250905156136, -0.04048096761107445, -0.04354942589998245], "norm_scores": [0.20282085239887238, 0.2008243054151535, 0.20038220286369324, 0.19829005002975464, 0.19768254458904266]}}
{"id": "WebQTest-1608", "question": "what language is mainly spoken in egypt", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["base.culturalevent.event.entity_involved", "location.location.events", "language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is mainly spoken in egypt [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.022689566016197205, -0.023555852472782135, -0.02673199400305748, -0.029295166954398155, -0.11055527627468109], "norm_scores": [0.20389854907989502, 0.2037220001220703, 0.20307596027851105, 0.20255610346794128, 0.18674738705158234]}}
{"id": "WebQTest-1609", "question": "who played alf on tv show", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played alf on tv show [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.017579400911927223, -0.020724637433886528, -0.04767845943570137, -0.048131003975868225, -0.048714760690927505], "norm_scores": [0.20381276309490204, 0.2031727284193039, 0.19776959717273712, 0.19768011569976807, 0.1975647360086441]}}
{"id": "WebQTest-1610", "question": "who was gerald ford vp", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.politician.government_positions_held", "government.politician.government_positions_held"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was gerald ford vp [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.politician.government_positions_held </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.005344569217413664, -0.025027425959706306, -0.07793795317411423, -0.09040865302085876, -0.09278473258018494], "norm_scores": [0.21073858439922333, 0.20663119852542877, 0.1959824413061142, 0.19355358183383942, 0.19309422373771667]}}
{"id": "WebQTest-1611", "question": "what is armenian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is armenian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.006626528687775135, -0.025945696979761124, -0.07909290492534637, -0.09272322058677673, -0.13936415314674377], "norm_scores": [0.21257764101028442, 0.20851022005081177, 0.1977178305387497, 0.19504114985466003, 0.18615314364433289]}}
{"id": "WebQTest-1612", "question": "what type of art did raphael sanzio create", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"], ["art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did raphael sanzio create [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> art.visual_artist.art_forms </PATH>"], "scores": [-0.004673222079873085, -0.02234635315835476, -0.05904456973075867, -0.10234177112579346, -0.13148710131645203], "norm_scores": [0.21198074519634247, 0.20826730132102966, 0.2007628083229065, 0.19225582480430603, 0.18673333525657654]}}
{"id": "WebQTest-1613", "question": "what language do they speak in malta", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in malta [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.018245186656713486, -0.02140302211046219, -0.02745596319437027, -0.04208539053797722, -0.16785216331481934], "norm_scores": [0.20724672079086304, 0.20659330487251282, 0.20534658432006836, 0.20236435532569885, 0.17844903469085693]}}
{"id": "WebQTest-1614", "question": "what religion was king louis xiv", "prediction": [["people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["religion.religion.notable_figures"], ["royalty.kingdom.rulers", "people.person.religion"], ["people.person.nationality", "people.person.religion"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion was king louis xiv [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>"], "scores": [-0.00172037270385772, -0.05304248258471489, -0.0658954307436943, -0.07199731469154358, -0.08912061899900436], "norm_scores": [0.2111370861530304, 0.20057444274425507, 0.19801297783851624, 0.19680839776992798, 0.19346706569194794]}}
{"id": "WebQTest-1615", "question": "what football team does mike wright play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["people.person.nationality", "sports.sports_team_location.teams"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team does mike wright play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>"], "scores": [-0.015582825988531113, -0.01658046804368496, -0.0303952693939209, -0.05655568838119507, -0.05684215575456619], "norm_scores": [0.2039261907339096, 0.20372284948825836, 0.20092780888080597, 0.19573961198329926, 0.1956835389137268]}}
{"id": "WebQTest-1616", "question": "what did shannon hoon die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["people.people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shannon hoon die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.people.deceased_person.cause_of_death </PATH>"], "scores": [-0.002461111405864358, -0.03612872585654259, -0.12222854793071747, -0.12511833012104034, -0.13394978642463684], "norm_scores": [0.21666710078716278, 0.20949384570121765, 0.19221116602420807, 0.19165651500225067, 0.18997137248516083]}}
{"id": "WebQTest-1617", "question": "what did nicholas copernicus discover", "prediction": [["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"], ["base.argumentmaps.innovator.original_ideas"], ["astronomy.astronomer.astronomical_objects_discovered"], ["symbols.name_source.namesakes"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"], ["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicholas copernicus discover [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> astronomy.astronomer.astronomical_objects_discovered </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.015567893162369728, -0.02317182347178459, -0.03539801388978958, -0.044531166553497314, -0.05492139607667923], "norm_scores": [0.20384645462036133, 0.20230229198932648, 0.19984397292137146, 0.19802707433700562, 0.19598019123077393]}}
{"id": "WebQTest-1618", "question": "what teams has manny ramirez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_coach.teams_coached", "sports.sports_team.coaches"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what teams has manny ramirez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.01861133612692356, -0.021883759647607803, -0.021902456879615784, -0.023717936128377914, -0.050560589879751205], "norm_scores": [0.2017386108636856, 0.20107951760292053, 0.20107576251029968, 0.20071104168891907, 0.1953950822353363]}}
{"id": "WebQTest-1619", "question": "where do the appalachian mountains run", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.mountain_range.mountains"], ["geography.river.basin_countries"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.00831474456936121, -0.02048586495220661, -0.08177608251571655, -0.09477835893630981, -0.09718767553567886], "norm_scores": [0.21056152880191803, 0.20801429450511932, 0.19564789533615112, 0.19312050938606262, 0.1926557570695877]}}
{"id": "WebQTest-1620", "question": "when was president wilson in office", "prediction": [["book.written_work.subjects", "base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "location.location.events"], ["government.government_position_held.office_holder", "government.government_position_held.office_term"], ["book.written_work.subjects", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was president wilson in office [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location.events </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_term </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.events </PATH>"], "scores": [-0.03130827844142914, -0.03337579965591431, -0.033418476581573486, -0.04139171168208122, -0.04366498067975044], "norm_scores": [0.2010650932788849, 0.20064982771873474, 0.20064127445220947, 0.19904786348342896, 0.19859588146209717]}}
{"id": "WebQTest-1621", "question": "what political party is jerry brown", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"]], "ground_paths": [["government.election_campaign.candidate", "government.election_campaign.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party is jerry brown [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> base.politicalconventions.political_convention.presidential_nominee <SEP> base.politicalconventions.political_convention.party </PATH>"], "scores": [-0.013722313567996025, -0.018626466393470764, -0.02737143449485302, -0.03869586065411568, -0.04954207316040993], "norm_scores": [0.20318177342414856, 0.20218777656555176, 0.2004273533821106, 0.19817043840885162, 0.19603265821933746]}}
{"id": "WebQTest-1622", "question": "what ocean is around hawaii", "prediction": [["location.location.contains", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "location.location.contains"], ["geography.island.island_group"], ["geography.island.island_group", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what ocean is around hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>", "<PATH> geography.island.island_group <SEP> location.location.containedby </PATH>"], "scores": [-0.027449507266283035, -0.029114719480276108, -0.03919493034482002, -0.04295829311013222, -0.04505371302366257], "norm_scores": [0.20186440646648407, 0.20152854919433594, 0.19950729608535767, 0.19875788688659668, 0.19834184646606445]}}
{"id": "WebQTest-1623", "question": "what is the most common language spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.language_family.geographic_distribution"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>"], "scores": [-0.007209335453808308, -0.02822865918278694, -0.04558975622057915, -0.04881821572780609, -0.12584346532821655], "norm_scores": [0.20881514251232147, 0.20447179675102234, 0.20095258951187134, 0.20030486583709717, 0.1854555606842041]}}
{"id": "WebQTest-1624", "question": "what language tunisia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language tunisia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.016839630901813507, -0.0170498788356781, -0.04595988243818283, -0.048739317804574966, -0.14602842926979065], "norm_scores": [0.20753340423107147, 0.2074897736310959, 0.20157712697982788, 0.20101763308048248, 0.1823820322751999]}}
{"id": "WebQTest-1625", "question": "what are some inventions that leonardo da vinci invented", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some inventions that leonardo da vinci invented [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>"], "scores": [-0.007196925114840269, -0.018693896010518074, -0.07566855847835541, -0.08513016253709793, -0.0986928641796112], "norm_scores": [0.21008490025997162, 0.20768339931964874, 0.19618147611618042, 0.19433404505252838, 0.19171613454818726]}}
{"id": "WebQTest-1626", "question": "who wrote st trinians", "prediction": [["film.film.written_by"], ["fictional_universe.fictional_universe.locations"], ["film.film_story_contributor.film_story_credits"], ["film.film.directed_by"], ["fictional_universe.fictional_universe.works_set_here"]], "ground_paths": [["fictional_universe.fictional_universe.created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote st trinians [/INST]", "raw_output": {"paths": ["<PATH> film.film.written_by </PATH>", "<PATH> fictional_universe.fictional_universe.locations </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.directed_by </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>"], "scores": [-0.022324388846755028, -0.034860607236623764, -0.04285016655921936, -0.054668642580509186, -0.07200901210308075], "norm_scores": [0.20462752878665924, 0.20207828283309937, 0.2004702091217041, 0.19811490178108215, 0.19470912218093872]}}
{"id": "WebQTest-1627", "question": "what are malaysian coins called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["location.country.coins_used"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are malaysian coins called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> location.country.coins_used </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.006585501600056887, -0.027206428349018097, -0.0638294443488121, -0.07821065187454224, -0.09647425264120102], "norm_scores": [0.20969343185424805, 0.20541363954544067, 0.1980268657207489, 0.1951993703842163, 0.19166669249534607]}}
{"id": "WebQTest-1628", "question": "what type government does mexico have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["film.film.featured_film_locations", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type government does mexico have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> film.film.featured_film_locations <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.0059588137082755566, -0.022478973492980003, -0.14283952116966248, -0.14326758682727814, -0.14377549290657043], "norm_scores": [0.2174551635980606, 0.21389228105545044, 0.18963706493377686, 0.18955589830875397, 0.1894596368074417]}}
{"id": "WebQTest-1629", "question": "what currency does dominican republic take", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"], ["travel.tourist_attraction.near_travel_destination"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does dominican republic take [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.00044287365744821727, -0.07351754605770111, -0.1336919516324997, -0.1454123556613922, -0.15174846351146698], "norm_scores": [0.2207791954278946, 0.20522119104862213, 0.1932363361120224, 0.19098477065563202, 0.18977847695350647]}}
{"id": "WebQTest-1630", "question": "what inspired michael jackson to become a singer", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.profession"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michael jackson to become a singer [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.006204237695783377, -0.019870610907673836, -0.09141740947961807, -0.10073202848434448, -0.10373741388320923], "norm_scores": [0.21179181337356567, 0.20891709625720978, 0.1944919377565384, 0.1926887184381485, 0.19211046397686005]}}
{"id": "WebQTest-1631", "question": "who is the texas state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.location.events", "military.military_person.participated_in_conflicts"], ["location.administrative_division.country", "people.person.nationality"], ["location.us_state.capital", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.political_district.elections", "government.election.winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the texas state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.016881367191672325, -0.025733578950166702, -0.028626762330532074, -0.03176150470972061, -0.03655174374580383], "norm_scores": [0.20221376419067383, 0.20043160021305084, 0.1998525708913803, 0.19922704994678497, 0.19827499985694885]}}
{"id": "WebQTest-1632", "question": "what to visit in london city", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["organization.organization.geographic_scope"], ["transportation.bridge.locale"]], "ground_paths": [["organization.organization.geographic_scope"], ["transportation.bridge.locale"], ["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["metropolitan_transit.transit_system.area_served"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in london city [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> transportation.bridge.locale </PATH>"], "scores": [-0.012214191257953644, -0.020489804446697235, -0.031231282278895378, -0.07791173458099365, -0.09701099246740341], "norm_scores": [0.20712362229824066, 0.20541663467884064, 0.20322194695472717, 0.1939534842967987, 0.19028426706790924]}}
{"id": "WebQTest-1633", "question": "what genre of art is the mona lisa", "prediction": [["visual_art.artwork.art_form"], ["visual_art.visual_artist.art_forms"], ["visual_art.art_subject.artwork_on_the_subject"], ["visual_art.artwork.genre"], ["visual_art.art_subject.artworks"]], "ground_paths": [["visual_art.artwork.art_genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what genre of art is the mona lisa [/INST]", "raw_output": {"paths": ["<PATH> visual_art.artwork.art_form </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_subject.artwork_on_the_subject </PATH>", "<PATH> visual_art.artwork.genre </PATH>", "<PATH> visual_art.art_subject.artworks </PATH>"], "scores": [-0.018758757039904594, -0.02819954790174961, -0.03969183191657066, -0.04204219952225685, -0.05932497978210449], "norm_scores": [0.20378555357456207, 0.20187070965766907, 0.19956402480602264, 0.19909553229808807, 0.19568417966365814]}}
{"id": "WebQTest-1634", "question": "who does peter griffin voice", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peter griffin voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.017738202586770058, -0.03340907767415047, -0.03362426534295082, -0.039346665143966675, -0.040977805852890015], "norm_scores": [0.20307281613349915, 0.19991528987884521, 0.19987225532531738, 0.19873179495334625, 0.19840788841247559]}}
{"id": "WebQTest-1635", "question": "when did tony romo got drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"], ["sports.sports_league_draft.picks", "sports.sports_league_draft_pick.draft"], ["sports.sports_league_draft.picks", "sports.sports_league_draft.picks"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did tony romo got drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_league_draft.picks <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_league_draft.picks <SEP> sports.sports_league_draft.picks </PATH>"], "scores": [-0.01288466714322567, -0.014806963503360748, -0.026001842692494392, -0.035418927669525146, -0.05613414943218231], "norm_scores": [0.20323392748832703, 0.20284363627433777, 0.2005854696035385, 0.19870541989803314, 0.19463153183460236]}}
{"id": "WebQTest-1637", "question": "what are the catholic holy days", "prediction": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"], ["time.recurring_event.holidays"], ["time.recurring_event.current_frequency"], ["time.recurring_event.instances"]], "ground_paths": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the catholic holy days [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> time.recurring_event.holidays </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.recurring_event.instances </PATH>"], "scores": [-0.011512687429785728, -0.014511899091303349, -0.09201503545045853, -0.09478064626455307, -0.11959816515445709], "norm_scores": [0.2110901176929474, 0.2104579657316208, 0.19476287066936493, 0.1942249834537506, 0.18946412205696106]}}
{"id": "WebQTest-1638", "question": "what language is most commonly spoken in belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["country.languages_spoken.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is most commonly spoken in belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> country.languages_spoken.countries_spoken_in </PATH>"], "scores": [-0.011136717163026333, -0.01947667822241783, -0.035487230867147446, -0.0830887109041214, -0.142081156373024], "norm_scores": [0.20940302312374115, 0.2076638787984848, 0.204365536570549, 0.19486533105373383, 0.1837022453546524]}}
{"id": "WebQTest-1639", "question": "where did frank baum die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"], ["people.place_of_death.person", "people.place_of_death.location"], ["people.place_of_death.people", "people.place_of_death.location"], ["people.place_of_death.people_deceased_person"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did frank baum die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_of_death.person <SEP> people.place_of_death.location </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>", "<PATH> people.place_of_death.people_deceased_person </PATH>"], "scores": [-0.00034199643414467573, -0.10130557417869568, -0.10620201379060745, -0.10778360813856125, -0.11760922521352768], "norm_scores": [0.21781779825687408, 0.196899875998497, 0.1959381252527237, 0.19562847912311554, 0.19371573626995087]}}
{"id": "WebQTest-1640", "question": "where is tunis on a map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.containedby"], ["base.locations.countries.continent"], ["location.administrative_division.country"], []], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tunis on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.013676583766937256, -0.03566935658454895, -0.038863405585289, -0.043855294585227966, -0.05171327292919159], "norm_scores": [0.2046527862548828, 0.20020104944705963, 0.19956260919570923, 0.1985688954591751, 0.19701465964317322]}}
{"id": "WebQTest-1641", "question": "who is nicolas cage married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nicolas cage married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.01741321012377739, -0.03154951333999634, -0.042506225407123566, -0.042977672070264816, -0.0497727207839489], "norm_scores": [0.20391099154949188, 0.20104871690273285, 0.19885791838169098, 0.19876419007778168, 0.19741815328598022]}}
{"id": "WebQTest-1643", "question": "who does the voice for chucky in child 's play", "prediction": [["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film.country", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.film.starring", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for chucky in child 's play [/INST]", "raw_output": {"paths": ["<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.02948908321559429, -0.029911663383245468, -0.03070797771215439, -0.03253328055143356, -0.040903348475694656], "norm_scores": [0.20064324140548706, 0.20055848360061646, 0.20039881765842438, 0.20003336668014526, 0.19836606085300446]}}
{"id": "WebQTest-1644", "question": "what did mary wollstonecraft fight for", "prediction": [["people.person.profession"], ["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"], ["influence.influence_node.influenced_by", "base.activism.activism_issue.activists"], ["book.written_work.author"]], "ground_paths": [["base.activism.activist.area_of_activism"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mary wollstonecraft fight for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> base.activism.activism_issue.activists </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.0184292271733284, -0.02211744897067547, -0.042501937597990036, -0.05275164917111397, -0.055087823420763016], "norm_scores": [0.20396514236927032, 0.20321425795555115, 0.19911377131938934, 0.19708333909511566, 0.19662345945835114]}}
{"id": "WebQTest-1646", "question": "what year did the phillies begin", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_team.previously_known"], ["sports.sports_team.previously_known_as"], ["sports.sports_team_roster.team"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the phillies begin [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.previously_known </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>", "<PATH> sports.sports_team_roster.team </PATH>"], "scores": [-0.0039559053257107735, -0.03550773859024048, -0.058941252529621124, -0.07336676865816116, -0.09278304129838943], "norm_scores": [0.20993466675281525, 0.20341424643993378, 0.19870294630527496, 0.19585713744163513, 0.19209101796150208]}}
{"id": "WebQTest-1648", "question": "what language does iceland speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["language.human_language.language_family"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does iceland speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.014642984606325626, -0.018162930384278297, -0.047099512070417404, -0.04900616407394409, -0.1400522142648697], "norm_scores": [0.20777462422847748, 0.2070445716381073, 0.20113925635814667, 0.20075611770153046, 0.1832854449748993]}}
{"id": "WebQTest-1649", "question": "what was dr seuss real name", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by", "people.person.nationality"], ["book.book_edition.author_editor"], ["book.book.interior_illustrations_by"], ["book.written_work.author"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss real name [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.book.interior_illustrations_by </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.025361334905028343, -0.02780085615813732, -0.03850116953253746, -0.04549190774559975, -0.045688334852457047], "norm_scores": [0.20224662125110626, 0.20175382494926453, 0.1996065080165863, 0.19821597635746002, 0.1981770545244217]}}
{"id": "WebQTest-1650", "question": "what did gloria steinem founded", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.founder.organizations_founded"], ["base.activism.activist.area_of_activism"], ["book.written_work.author"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gloria steinem founded [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.founder.organizations_founded </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.005529929418116808, -0.02429184317588806, -0.10645066201686859, -0.11466564238071442, -0.11854000389575958], "norm_scores": [0.21389605104923248, 0.20992036163806915, 0.19336305558681488, 0.19178105890750885, 0.19103948771953583]}}
{"id": "WebQTest-1652", "question": "what language they speak in scotland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.first_level_divisions"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.first_level_divisions </PATH>"], "scores": [-0.016375089064240456, -0.016939016059041023, -0.030821045860648155, -0.07603710144758224, -0.17906804382801056], "norm_scores": [0.20933513343334198, 0.2092171162366867, 0.20633281767368317, 0.19721105694770813, 0.1779039055109024]}}
{"id": "WebQTest-1653", "question": "what boarding school did mark zuckerberg go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what boarding school did mark zuckerberg go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.014409211464226246, -0.01986156776547432, -0.02552417665719986, -0.029511915519833565, -0.05272775515913963], "norm_scores": [0.2028016448020935, 0.201698899269104, 0.20055998861789703, 0.19976182281970978, 0.1951775997877121]}}
{"id": "WebQTest-1654", "question": "what are bigos", "prediction": [["food.dish.cuisine"], ["dining.cuisine.dishes"], ["food.diet_based_dish.diet"], ["food.diet_and_soup.diet"], ["food.diet_based_dish.dishes"]], "ground_paths": [["food.type_of_dish.dishes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are bigos [/INST]", "raw_output": {"paths": ["<PATH> food.dish.cuisine </PATH>", "<PATH> dining.cuisine.dishes </PATH>", "<PATH> food.diet_based_dish.diet </PATH>", "<PATH> food.diet_and_soup.diet </PATH>", "<PATH> food.diet_based_dish.dishes </PATH>"], "scores": [-0.03280897065997124, -0.060825008898973465, -0.08157406002283096, -0.08838199079036713, -0.09317249804735184], "norm_scores": [0.20780764520168304, 0.20206649601459503, 0.19791701436042786, 0.19657418131828308, 0.19563473761081696]}}
{"id": "WebQTest-1655", "question": "where are ike and tina turner 's children", "prediction": [["people.person.children"], ["people.person.parents"], ["music.artist.genre", "music.artist.genre"], ["people.person.place_of_birth"], ["music.artist.origin", "people.person.place_of_birth"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are ike and tina turner 's children [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.008147846907377243, -0.020219188183546066, -0.0816028043627739, -0.08972626179456711, -0.09738826006650925], "norm_scores": [0.21037285029888153, 0.20784862339496613, 0.1954738199710846, 0.19389232993125916, 0.19241240620613098]}}
{"id": "WebQTest-1656", "question": "what are some of leonardo da vinci most famous inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some of leonardo da vinci most famous inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>"], "scores": [-0.006805113051086664, -0.019618825986981392, -0.07560145854949951, -0.08262931555509567, -0.0949009358882904], "norm_scores": [0.20993243157863617, 0.20725959539413452, 0.19597545266151428, 0.19460299611091614, 0.1922294944524765]}}
{"id": "WebQTest-1657", "question": "when did shaq come into the nba", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shaq come into the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_league_season.awards </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>"], "scores": [-0.015621423721313477, -0.021321581676602364, -0.02164168283343315, -0.031354617327451706, -0.03958839550614357], "norm_scores": [0.20206013321876526, 0.20091162621974945, 0.200847327709198, 0.19890595972537994, 0.19727493822574615]}}
{"id": "WebQTest-1658", "question": "where do they speak turkish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak turkish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.013907610438764095, -0.022168666124343872, -0.028005141764879227, -0.10369507223367691, -0.15972836315631866], "norm_scores": [0.2102513164281845, 0.20852157473564148, 0.2073080986738205, 0.1921960860490799, 0.18172286450862885]}}
{"id": "WebQTest-1659", "question": "what is the zip code for walnut creek", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"], ["location.Citytown.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for walnut creek [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes <PATH>", "<PATH> location.Citytown.postal_codes </PATH>"], "scores": [-7.61209157644771e-05, -0.10678175836801529, -0.1604539304971695, -0.17076529562473297, -0.1721934974193573], "norm_scores": [0.22544986009597778, 0.2026321291923523, 0.19204312562942505, 0.19007308781147003, 0.18980179727077484]}}
{"id": "WebQTest-1661", "question": "who was the voice of darth vader in the original star wars movie", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the voice of darth vader in the original star wars movie [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.0175612922757864, -0.02634095773100853, -0.02938196249306202, -0.030772395431995392, -0.038909606635570526], "norm_scores": [0.20221377909183502, 0.20044617354869843, 0.1998375505208969, 0.19955988228321075, 0.1979426145553589]}}
{"id": "WebQTest-1662", "question": "where is english spoken around the world", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken around the world [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.012621065601706505, -0.025364072993397713, -0.028495971113443375, -0.058983590453863144, -0.14311900734901428], "norm_scores": [0.2081623524427414, 0.20552656054496765, 0.2048838883638382, 0.19873172044754028, 0.1826954185962677]}}
{"id": "WebQTest-1663", "question": "what music did louis armstrong play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["book.written_work.author"], ["influence.influence_node.influenced"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.0017083212733268738, -0.04961815103888512, -0.09248067438602448, -0.11876848340034485, -0.13513919711112976], "norm_scores": [0.21593303978443146, 0.2058316320180893, 0.19719557464122772, 0.19207929074764252, 0.18896040320396423]}}
{"id": "WebQTest-1664", "question": "what form of currency does germany use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does germany use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.005690301302820444, -0.025728847831487656, -0.11970088630914688, -0.14495447278022766, -0.15614187717437744], "norm_scores": [0.21726250648498535, 0.21295221149921417, 0.19385215640068054, 0.18901799619197845, 0.18691514432430267]}}
{"id": "WebQTest-1665", "question": "where is the latin language from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"], ["language.language_family.languages"], ["location.country.official_language"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the latin language from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.001945960451848805, -0.052531275898218155, -0.06874378770589828, -0.0943748727440834, -0.1425514966249466], "norm_scores": [0.2142886370420456, 0.2037183791399002, 0.20044222474098206, 0.19536994397640228, 0.18618081510066986]}}
{"id": "WebQTest-1666", "question": "what are the top tourist attractions in rome", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["base.arthist.int_zm_nyek.helynev"], ["base.arthist.helynevek.intezmeny"]], "ground_paths": [["base.arthist.int_zm_nyek.helynev"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the top tourist attractions in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> base.arthist.int_zm_nyek.helynev </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>"], "scores": [-0.017415298148989677, -0.018649477511644363, -0.01909102126955986, -0.0991644561290741, -0.10180901736021042], "norm_scores": [0.2067113071680069, 0.20645634829998016, 0.2063652127981186, 0.19048510491847992, 0.18998202681541443]}}
{"id": "WebQTest-1667", "question": "who were the children of king solomon", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"], ["fictional_universe.fictional_character.parents"], ["people.family.members"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were the children of king solomon [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.family.members </PATH>"], "scores": [-0.013553132303059101, -0.014137161895632744, -0.1351444572210312, -0.14627960324287415, -0.1595657765865326], "norm_scores": [0.21622660756111145, 0.21610036492347717, 0.19147086143493652, 0.18935063481330872, 0.18685151636600494]}}
{"id": "WebQTest-1668", "question": "what did andy irons die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did andy irons die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death <PATH>"], "scores": [-0.0019711260683834553, -0.040068067610263824, -0.1248958632349968, -0.13162186741828918, -0.15789559483528137], "norm_scores": [0.21829411387443542, 0.21013420820236206, 0.19304408133029938, 0.19175001978874207, 0.18677763640880585]}}
{"id": "WebQTest-1669", "question": "where is auburn university located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is auburn university located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.0023781098425388336, -0.04521021991968155, -0.11738276481628418, -0.12755724787712097, -0.1307903528213501], "norm_scores": [0.21686029434204102, 0.20776782929897308, 0.1933010220527649, 0.1913442462682724, 0.1907266229391098]}}
{"id": "WebQTest-1670", "question": "who is tatyana ali parents", "prediction": [["people.person.children"], ["people.person.parents"], ["music.artist.album", "music.album.artist"], ["people.person.parent"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is tatyana ali parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> music.artist.album <SEP> music.album.artist </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.012090303003787994, -0.016222475096583366, -0.15457139909267426, -0.15564389526844025, -0.16173584759235382], "norm_scores": [0.21784746646881104, 0.21694913506507874, 0.18891820311546326, 0.18871568143367767, 0.1875695288181305]}}
{"id": "WebQTest-1671", "question": "where did leif ericson grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["symbols.name_source.namesakes"], ["symbols.namesake.named_after"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did leif ericson grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0016878645401448011, -0.048816751688718796, -0.08574234694242477, -0.11458806693553925, -0.1150118038058281], "norm_scores": [0.2146177589893341, 0.20473770797252655, 0.197315514087677, 0.1917051374912262, 0.19162389636039734]}}
{"id": "WebQTest-1672", "question": "what language do the maasai speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["language.human_language.region"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.region </PATH>"], "scores": [-0.0028613272588700056, -0.049508001655340195, -0.06401296705007553, -0.06745724380016327, -0.08600746840238571], "norm_scores": [0.21040372550487518, 0.20081448554992676, 0.1979226917028427, 0.19724218547344208, 0.19361700117588043]}}
{"id": "WebQTest-1673", "question": "what political party was hitler in", "prediction": [["base.activism.activist.area_of_activism", "government.political_party.ideology"], ["organization.organization_founder.organizations_founded"], ["base.activism.activism_issue.supporting_political_parties"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["base.activism.activist.area_of_activism", "government.political_party.ideology"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party was hitler in [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism <SEP> government.political_party.ideology </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activism_issue.supporting_political_parties </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>"], "scores": [-0.01801508106291294, -0.020502286031842232, -0.03021770529448986, -0.03045744076371193, -0.03688616678118706], "norm_scores": [0.2018437385559082, 0.20134234428405762, 0.19939568638801575, 0.19934788346290588, 0.1980704367160797]}}
{"id": "WebQTest-1674", "question": "who is the leader of north korea today", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the leader of north korea today [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.004282326437532902, -0.029567355290055275, -0.06751268357038498, -0.07376080006361008, -0.08172445744276047], "norm_scores": [0.20955047011375427, 0.20431840419769287, 0.1967107355594635, 0.19548548758029938, 0.19393488764762878]}}
{"id": "WebQTest-1675", "question": "where do the appalachian mountains run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.mountain_range.mountains"], ["geography.river.basin_countries"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.mountain_range.mountains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.007679365109652281, -0.02017396129667759, -0.09579623490571976, -0.09585380554199219, -0.09800149500370026], "norm_scores": [0.2113059163093567, 0.20868216454982758, 0.19348308444023132, 0.19347193837165833, 0.1930568665266037]}}
{"id": "WebQTest-1676", "question": "what money does spain use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"], ["finance.currency.countries_formerly_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does spain use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.0061330306343734264, -0.026066193357110023, -0.13830554485321045, -0.15267448127269745, -0.1625460833311081], "norm_scores": [0.2185625284910202, 0.21424902975559235, 0.19150225818157196, 0.18877024948596954, 0.18691594898700714]}}
{"id": "WebQTest-1677", "question": "where was terry waite born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was terry waite born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.009581553749740124, -0.01730266399681568, -0.125493124127388, -0.12563827633857727, -0.13383851945400238], "norm_scores": [0.2147563248872757, 0.21310454607009888, 0.19125209748744965, 0.1912243515253067, 0.18966266512870789]}}
{"id": "WebQTest-1678", "question": "what country did the battle of san juan hill take place", "prediction": [["location.location.events"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.combatant"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "location.location.containedby"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did the battle of san juan hill take place [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.combatant </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.containedby </PATH>"], "scores": [-0.006332311779260635, -0.02730858512222767, -0.05512421950697899, -0.059813935309648514, -0.06347112357616425], "norm_scores": [0.20729650557041168, 0.2029934823513031, 0.19742490351200104, 0.19650119543075562, 0.19578386843204498]}}
{"id": "WebQTest-1679", "question": "what kind of government does the united states have currently", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"], ["form.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does the united states have currently [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007673404645174742, -0.01653973199427128, -0.15748505294322968, -0.1578826904296875, -0.16537094116210938], "norm_scores": [0.21897751092910767, 0.21704457700252533, 0.1885111927986145, 0.18843623995780945, 0.18703044950962067]}}
{"id": "WebQTest-1680", "question": "when was the last time the orioles won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"], ["sports.sports_league_season.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the orioles won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_league_season.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.00917612574994564, -0.013419614173471928, -0.09870161861181259, -0.12778107821941376, -0.1496163159608841], "norm_scores": [0.21425825357437134, 0.21335096657276154, 0.19591024518013, 0.19029532372951508, 0.18618519604206085]}}
{"id": "WebQTest-1682", "question": "who did ben stiller date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"]], "ground_paths": [["people.person.place_of_birth", "people.person.place_of_birth"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["people.person.languages", "people.person.languages"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben stiller date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>", "<PATH> base.popstra.dated.participant <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.015382516197860241, -0.030004167929291725, -0.03480991721153259, -0.04821974039077759, -0.051616959273815155], "norm_scores": [0.20415017008781433, 0.2011868804693222, 0.20022232830524445, 0.1975553184747696, 0.19688531756401062]}}
{"id": "WebQTest-1683", "question": "what is the most common language in norway", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.region"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.region </PATH>"], "scores": [-0.015025006607174873, -0.022303380072116852, -0.032648876309394836, -0.03898158669471741, -0.12683282792568207], "norm_scores": [0.20636330544948578, 0.20486676692962646, 0.20275823771953583, 0.2014782875776291, 0.18453338742256165]}}
{"id": "WebQTest-1684", "question": "what influenced william shakespeare to start writing", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["people.person.profession"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced william shakespeare to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.009740443900227547, -0.013444260694086552, -0.10085326433181763, -0.10567205399274826, -0.10758845508098602], "norm_scores": [0.211661234498024, 0.21087872982025146, 0.19322866201400757, 0.1922997683286667, 0.1919315904378891]}}
{"id": "WebQTest-1685", "question": "where did benjamin franklin went to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.013255211524665356, -0.022964082658290863, -0.02406928688287735, -0.03171858564019203, -0.05474041774868965], "norm_scores": [0.20322510600090027, 0.20126156508922577, 0.20103925466537476, 0.19950731098651886, 0.19496674835681915]}}
{"id": "WebQTest-1686", "question": "who is the governor of arizona 2009", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of arizona 2009 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.020358705893158913, -0.02109270915389061, -0.025965237990021706, -0.026408126577734947, -0.028178732842206955], "norm_scores": [0.2008090764284134, 0.20066174864768982, 0.19968637824058533, 0.19959795475006104, 0.19924485683441162]}}
{"id": "WebQTest-1687", "question": "what movies has spike lee made", "prediction": [["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"], ["film.producer.film"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"], ["film.producer.film"], ["film.writer.film"], ["film.film.executive_produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has spike lee made [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.director.film </PATH>", "<PATH> film.producer.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.02750088833272457, -0.03214970976114273, -0.03361948952078819, -0.03802994266152382, -0.06031397357583046], "norm_scores": [0.20216284692287445, 0.20122520625591278, 0.200929656624794, 0.20004543662071228, 0.19563691318035126]}}
{"id": "WebQTest-1688", "question": "what did bernie mac died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["deceased.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bernie mac died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> deceased.deceased_person.cause_of_death </PATH>"], "scores": [-0.0015366864390671253, -0.04434717446565628, -0.12665723264217377, -0.12718363106250763, -0.13038313388824463], "norm_scores": [0.2173195630311966, 0.20821234583854675, 0.19176071882247925, 0.1916598081588745, 0.1910475790500641]}}
{"id": "WebQTest-1690", "question": "where is electrolux based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.place_founded", "location.location.containedby"], ["organization.organization.place_founded"]], "ground_paths": [["organization.organization.founders", "people.deceased_person.place_of_death"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is electrolux based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.place_founded </PATH>"], "scores": [-0.0046724844723939896, -0.03492940589785576, -0.05500495806336403, -0.057744350284338, -0.06043360009789467], "norm_scores": [0.20767627656459808, 0.201486736536026, 0.19748210906982422, 0.19694186747074127, 0.19641295075416565]}}
{"id": "WebQTest-1691", "question": "what is houston timezone", "prediction": [["location.location.time_zones"], ["location.location.time_Zones"], ["location.location.time_zonessymbol"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is houston timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> location.location.time_zonessymbol </PATH>"], "scores": [-9.589511137164664e-06, -0.1947859227657318, -0.19708412885665894, -0.20956839621067047, -0.2324734777212143], "norm_scores": [0.2354232519865036, 0.19375774264335632, 0.19331297278404236, 0.1909146010875702, 0.18659138679504395]}}
{"id": "WebQTest-1694", "question": "what was shakespeare 's wife called", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.children", "people.person.parents"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["book.book_subject.works", "book.book_subject.works"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was shakespeare 's wife called [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.025562848895788193, -0.03687604144215584, -0.0377969853579998, -0.04055183753371239, -0.043781232088804245], "norm_scores": [0.20227926969528198, 0.2000037282705307, 0.1998196393251419, 0.19926992058753967, 0.19862742722034454]}}
{"id": "WebQTest-1695", "question": "when did truman become president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "location.location.containedby", "time.event.locations"], ["event.speech_or_presentation.speaker_s", "event.speech_or_presentation.event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did truman become president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> location.location.containedby <SEP> time.event.locations </PATH>", "<PATH> event.speech_or_presentation.speaker_s <SEP> event.speech_or_presentation.event </PATH>"], "scores": [-0.00725566316395998, -0.027779949828982353, -0.06141951307654381, -0.06659539043903351, -0.07363703101873398], "norm_scores": [0.2081112414598465, 0.20388345420360565, 0.1971389800310135, 0.1961212456226349, 0.19474507868289948]}}
{"id": "WebQTest-1696", "question": "what did ferdinand magellan do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"], ["base.culturalevent.event.entity_involved"], ["base.schemastaging.disaster_extra.disasters"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did ferdinand magellan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.schemastaging.disaster_extra.disasters </PATH>"], "scores": [-0.0016554051544517279, -0.06128869205713272, -0.06570388376712799, -0.10636424273252487, -0.16245731711387634], "norm_scores": [0.2158840149641037, 0.20338650047779083, 0.20249047875404358, 0.19442228972911835, 0.18381676077842712]}}
{"id": "WebQTest-1698", "question": "where do burmese cats originate from", "prediction": [["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.breed_of"], ["biology.breed_origin.breeds_originating_here", "base.breed_origin.breeds_originating_here"], ["biology.breed_origin.breeds_originating_here", "base.breeds.breed_origin.breeds_originating_here"]], "ground_paths": [["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do burmese cats originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.place_of_origin </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> base.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.breed_origin.breeds_originating_here <SEP> base.breeds.breed_origin.breeds_originating_here </PATH>"], "scores": [-0.003949639853090048, -0.026857996359467506, -0.0628754124045372, -0.13236035406589508, -0.1362306773662567], "norm_scores": [0.21387101709842682, 0.2090272605419159, 0.20163261890411377, 0.18809784948825836, 0.18737126886844635]}}
{"id": "WebQTest-1699", "question": "who played kirk", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played kirk [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.02342717908322811, -0.0317499004304409, -0.03283914551138878, -0.03448069468140602, -0.03660528361797333], "norm_scores": [0.2016836702823639, 0.20001207292079926, 0.19979433715343475, 0.19946663081645966, 0.19904330372810364]}}
{"id": "WebQTest-1700", "question": "where is mt st helens located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"], ["geography.mountain.mountain_range"], ["geography.mountain_range.mountains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mt st helens located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>", "<PATH> geography.mountain.mountain_range </PATH>", "<PATH> geography.mountain_range.mountains </PATH>"], "scores": [-0.008048292249441147, -0.022119252011179924, -0.09131673723459244, -0.12062093615531921, -0.1606869101524353], "norm_scores": [0.21467962861061096, 0.21168002486228943, 0.19752760231494904, 0.19182321429252625, 0.18428955972194672]}}
{"id": "WebQTest-1701", "question": "what county is st paul minnesota in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"], ["location.location.contains"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul minnesota in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.014695664867758751, -0.018291445448994637, -0.03589240089058876, -0.05289551615715027, -0.07938453555107117], "norm_scores": [0.20511488616466522, 0.2043786644935608, 0.20081287622451782, 0.19742728769779205, 0.1922663003206253]}}
{"id": "WebQTest-1702", "question": "who was king henry viii son", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"], ["fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.children"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king henry viii son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>"], "scores": [-0.013698914088308811, -0.015882832929491997, -0.06370625644922256, -0.10454430431127548, -0.11739450693130493], "norm_scores": [0.20992088317871094, 0.2094629406929016, 0.19968146085739136, 0.19169111549854279, 0.1892436146736145]}}
{"id": "WebQTest-1705", "question": "where was franz ferdinand from", "prediction": [["people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["people.person.nationality"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was franz ferdinand from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0009512441465631127, -0.07217922806739807, -0.07735669612884521, -0.08388042449951172, -0.12083911150693893], "norm_scores": [0.2143564671278, 0.1996193677186966, 0.1985885053873062, 0.19729718565940857, 0.19013844430446625]}}
{"id": "WebQTest-1706", "question": "what was benedict arnold most famous for", "prediction": [["base.honouriam.dishonoured_person.dishonor_bestowed"], ["people.person.profession"], ["base.culturalevent.event.entity_involved"], ["base.honouriam.dishonoured_person.dishonor_bestowed", "base.honouri.dishonor.dishonor_bestowed"], ["base.activism.activist.area_of_activism"]], "ground_paths": [["base.honouriam.dishonoured_person.dishonor_bestowed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was benedict arnold most famous for [/INST]", "raw_output": {"paths": ["<PATH> base.honouriam.dishonoured_person.dishonor_bestowed </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.honouriam.dishonoured_person.dishonor_bestowed <SEP> base.honouri.dishonor.dishonor_bestowed </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.02149198390543461, -0.02512931451201439, -0.05455467849969864, -0.06016644090414047, -0.06053588166832924], "norm_scores": [0.2045985758304596, 0.2038557380437851, 0.19794459640979767, 0.1968369036912918, 0.19676420092582703]}}
{"id": "WebQTest-1707", "question": "who plays lex luthor on smallville", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.tv_program.languages", "people.person.languages"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays lex luthor on smallville [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.022145353257656097, -0.024007922038435936, -0.027645384892821312, -0.030096735805273056, -0.030097322538495064], "norm_scores": [0.20093174278736115, 0.2005578577518463, 0.19982965290546417, 0.19934040307998657, 0.19934026896953583]}}
{"id": "WebQTest-1708", "question": "what are dallas cowboys colors", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.sports_team.color"], ["sports.sports_team_color.team"], ["sports.sports_team_colors.team"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are dallas cowboys colors [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.color </PATH>", "<PATH> sports.sports_team_color.team </PATH>", "<PATH> sports.sports_team_colors.team </PATH>"], "scores": [-8.335871098097414e-05, -0.11494899541139603, -0.12230509519577026, -0.1332562118768692, -0.13878022134304047], "norm_scores": [0.2211291790008545, 0.19713355600833893, 0.19568872451782227, 0.19355742633342743, 0.19249115884304047]}}
{"id": "WebQTest-1710", "question": "what is the currency of the czech republic", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"], ["language.human_language.countries_spoken_in"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of the czech republic [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.0022752475924789906, -0.03995572403073311, -0.12017937749624252, -0.13579639792442322, -0.1552162617444992], "norm_scores": [0.21810291707515717, 0.21003760397434235, 0.19384577870368958, 0.19084201753139496, 0.18717163801193237]}}
{"id": "WebQTest-1711", "question": "when was the last time the la kings won a playoff series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the la kings won a playoff series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>"], "scores": [-0.00676516629755497, -0.01740260049700737, -0.06339813023805618, -0.08828418701887131, -0.11988115310668945], "norm_scores": [0.2105652242898941, 0.2083372175693512, 0.198971688747406, 0.1940811723470688, 0.18804466724395752]}}
{"id": "WebQTest-1712", "question": "where did spencer pratt go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did spencer pratt go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.016187531873583794, -0.025585196912288666, -0.029353225603699684, -0.030517585575580597, -0.03055071085691452], "norm_scores": [0.20205780863761902, 0.20016783475875854, 0.19941501319408417, 0.19918297231197357, 0.1991763710975647]}}
{"id": "WebQTest-1713", "question": "what disease did abraham lincoln had", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.medical_condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did abraham lincoln had [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.0030703728552907705, -0.036519549787044525, -0.04708044230937958, -0.10647614300251007, -0.11640696972608566], "norm_scores": [0.21192459762096405, 0.2049531489610672, 0.20280005037784576, 0.19110533595085144, 0.18921689689159393]}}
{"id": "WebQTest-1714", "question": "who is vancouver canucks coach", "prediction": [["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["ice_hockey.hockey_coach.current_team"]], "ground_paths": [["ice_hockey.hockey_coach.current_team"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is vancouver canucks coach [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>", "<PATH> ice_hockey.hockey_coach.current_team </PATH>"], "scores": [-0.012049439363181591, -0.02121376432478428, -0.030507439747452736, -0.033759038895368576, -0.04299871623516083], "norm_scores": [0.20322567224502563, 0.20137174427509308, 0.199508935213089, 0.1988612860441208, 0.19703231751918793]}}
{"id": "WebQTest-1715", "question": "where did cs lewis wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did cs lewis wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.021214714273810387, -0.028367972001433372, -0.03622221201658249, -0.04192023351788521, -0.047121524810791016], "norm_scores": [0.2027612030506134, 0.2013159841299057, 0.19974097609519958, 0.19860608875751495, 0.19757576286792755]}}
{"id": "WebQTest-1716", "question": "what is carlos pena jr nationality", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.ethnicity"], ["people.ethnicity.people"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is carlos pena jr nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.0011181100271642208, -0.06706388294696808, -0.07690411061048508, -0.08361240476369858, -0.09456325322389603], "norm_scores": [0.2130015790462494, 0.1994081735610962, 0.19745557010173798, 0.19613540172576904, 0.1939992904663086]}}
{"id": "WebQTest-1719", "question": "who was married to jane wyman", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was married to jane wyman [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.020247705280780792, -0.036695487797260284, -0.039565309882164, -0.04376903548836708, -0.04690336808562279], "norm_scores": [0.20345863699913025, 0.20013955235481262, 0.19956602156162262, 0.19872887432575226, 0.19810695946216583]}}
{"id": "WebQTest-1721", "question": "what language does peru speak", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does peru speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.main_country </PATH>"], "scores": [-0.010674471966922283, -0.029879266396164894, -0.03957932069897652, -0.04472729191184044, -0.1821689009666443], "norm_scores": [0.2100217044353485, 0.20602674782276154, 0.20403794944286346, 0.20299027860164642, 0.17692331969738007]}}
{"id": "WebQTest-1723", "question": "where is english spoken in the world map", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken in the world map [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.0130116892978549, -0.023813843727111816, -0.02830592170357704, -0.05571892112493515, -0.14370635151863098], "norm_scores": [0.20791076123714447, 0.20567697286605835, 0.20475512742996216, 0.19921840727329254, 0.1824387013912201]}}
{"id": "WebQTest-1724", "question": "when did arsenal won the league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["soccer.football_team.matches_played", "soccer.football_player_match_participation.season"], ["soccer.football_team.matches_played", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did arsenal won the league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> soccer.football_team.matches_played <SEP> soccer.football_player_match_participation.season </PATH>", "<PATH> soccer.football_team.matches_played <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.011067165061831474, -0.013764728792011738, -0.08797283470630646, -0.09688315540552139, -0.10275377333164215], "norm_scores": [0.21037407219409943, 0.20980733633041382, 0.19480159878730774, 0.1930735558271408, 0.19194342195987701]}}
{"id": "WebQTest-1725", "question": "what timezone is phoenix az in right now", "prediction": [["location.location.time_zones"], ["location.location.time_Zones"], ["locations.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is phoenix az in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>", "<PATH> locations.location.time_zones </PATH>"], "scores": [-9.340575161331799e-06, -0.17728205025196075, -0.1853790581226349, -0.20114558935165405, -0.2221083790063858], "norm_scores": [0.23326414823532104, 0.19537070393562317, 0.19379517436027527, 0.1907636523246765, 0.1868063509464264]}}
{"id": "WebQTest-1726", "question": "where is south portland", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.location.contains"], ["location.citytown.postal_codes"], ["location.hud_county_place.county"]], "ground_paths": [["location.hud_county_place.county"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is south portland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.007486199028789997, -0.03993532806634903, -0.0503205731511116, -0.06854800879955292, -0.07147599756717682], "norm_scores": [0.2081199586391449, 0.20147505402565002, 0.19939349591732025, 0.19579198956489563, 0.19521956145763397]}}
{"id": "WebQTest-1727", "question": "what countries speak french as a first language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["base.rosetta.local_name.locale"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["base.rosetta.local_name.locale"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak french as a first language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.01184145174920559, -0.02412573993206024, -0.027303479611873627, -0.10352589935064316, -0.12133422493934631], "norm_scores": [0.2091556042432785, 0.20660199224948883, 0.20594650506973267, 0.19083212316036224, 0.18746379017829895]}}
{"id": "WebQTest-1728", "question": "who was ptolemy and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["common.topic.image"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was ptolemy and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.image </PATH>"], "scores": [-0.0017490142490714788, -0.05127629637718201, -0.07124688476324081, -0.0759282261133194, -0.10343611985445023], "norm_scores": [0.21202798187732697, 0.20178262889385223, 0.1977928727865219, 0.1968691051006317, 0.19152745604515076]}}
{"id": "WebQTest-1729", "question": "what was the name of henry viii first wife", "prediction": [["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["book.written_work.subjects", "theater.play.characters"], ["people.person.parents", "people.person.parents"], ["people.family_member.family", "people.family.members"], ["people.person.religion", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of henry viii first wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.03435206413269043, -0.03492255136370659, -0.03508085757493973, -0.03853503242135048, -0.038674354553222656], "norm_scores": [0.2003922164440155, 0.2002779245376587, 0.20024622976779938, 0.19955573976039886, 0.19952793419361115]}}
{"id": "WebQTest-1730", "question": "where does the parana river flow", "prediction": [["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.river.origin"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the parana river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.origin </PATH>"], "scores": [-0.019621234387159348, -0.02271769940853119, -0.02721257694065571, -0.05680735409259796, -0.07734781503677368], "norm_scores": [0.2042170614004135, 0.2035856992006302, 0.20267266035079956, 0.19676250219345093, 0.1927621215581894]}}
{"id": "WebQTest-1731", "question": "what is president nixon 's first name", "prediction": [["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.gender", "people.person.gender"], ["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is president nixon 's first name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.023554885759949684, -0.03497980535030365, -0.04170657694339752, -0.05336929112672806, -0.059287216514348984], "norm_scores": [0.2038247138261795, 0.2015092670917511, 0.20015831291675568, 0.1978374868631363, 0.19667017459869385]}}
{"id": "WebQTest-1732", "question": "where did theodor schwann get his education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann get his education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.016409534960985184, -0.02021755650639534, -0.02118798904120922, -0.04765339568257332, -0.048958566039800644], "norm_scores": [0.20289547741413116, 0.20212431252002716, 0.2019282579421997, 0.19665424525737762, 0.19639775156974792]}}
{"id": "WebQTest-1733", "question": "who or what influenced frank lloyd wright", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["people.person.profession"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced frank lloyd wright [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.009944237768650055, -0.013512185774743557, -0.09341251850128174, -0.09421533346176147, -0.09435085952281952], "norm_scores": [0.21032269299030304, 0.2095736265182495, 0.19348010420799255, 0.19332484900951385, 0.19329865276813507]}}
{"id": "WebQTest-1734", "question": "what year did houston rockets win their first championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.defunct_sports_team.later_known_as"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did houston rockets win their first championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.defunct_sports_team.later_known_as </PATH>"], "scores": [-0.006984182167798281, -0.017102032899856567, -0.1237579956650734, -0.12871582806110382, -0.14322589337825775], "norm_scores": [0.21562239527702332, 0.21345174312591553, 0.19185787439346313, 0.1909090280532837, 0.18815894424915314]}}
{"id": "WebQTest-1735", "question": "where do most of the people live in russia", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["location.country.administrative_divisions"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do most of the people live in russia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.021903757005929947, -0.026503926143050194, -0.034622881561517715, -0.03942896053195, -0.04162110388278961], "norm_scores": [0.20218868553638458, 0.2012607306241989, 0.19963330030441284, 0.19867615401744843, 0.19824109971523285]}}
{"id": "WebQTest-1736", "question": "where is holy roman empire located", "prediction": [["location.location.contains"], ["location.location.containedby"], ["fictional_universe.fictional_setting.languages", "language.human_language.main_country"], ["fictional_universe.fictional_setting.languages", "language.human_language.countries_spoken_in"], ["fictional_universe.fictional_setting.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is holy roman empire located [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.main_country </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> fictional_universe.fictional_setting.contains </PATH>"], "scores": [-0.01833311840891838, -0.020800095051527023, -0.04296581447124481, -0.04596320912241936, -0.055919550359249115], "norm_scores": [0.2037048488855362, 0.20320293307304382, 0.19874833524227142, 0.1981535106897354, 0.19619041681289673]}}
{"id": "WebQTest-1737", "question": "what movies has liam hemsworth played in", "prediction": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has liam hemsworth played in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.02649359218776226, -0.03017324022948742, -0.031924858689308167, -0.03675210848450661, -0.03684883192181587], "norm_scores": [0.20119093358516693, 0.20045198500156403, 0.20010118186473846, 0.19913756847381592, 0.19911831617355347]}}
{"id": "WebQTest-1738", "question": "who plays nicholas newman on the young and the restless", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays nicholas newman on the young and the restless [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.02163388580083847, -0.026648148894309998, -0.02723049372434616, -0.036629315465688705, -0.037289075553417206], "norm_scores": [0.20165354013442993, 0.20064492523670197, 0.2005281001329422, 0.1986522078514099, 0.1985211819410324]}}
{"id": "WebQTest-1739", "question": "who is the voice of eric cartman on south park", "prediction": [["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.special_performance_type", "film.actor.film"], ["tv.regular_tv_appearance.special_performance_type", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of eric cartman on south park [/INST]", "raw_output": {"paths": ["<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.special_performance_type <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.01802969165146351, -0.028015296906232834, -0.030442003160715103, -0.04470466822385788, -0.04513821750879288], "norm_scores": [0.20305964350700378, 0.2010420560836792, 0.200554758310318, 0.1977146416902542, 0.1976289302110672]}}
{"id": "WebQTest-1740", "question": "who is angelina jolies brother", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.place_of_birth"], ["people.person.children", "people.person.children"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.children"]], "ground_paths": [["film.producer.films_executive_produced", "film.producer.films_executive_produced"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.place_of_birth", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolies brother [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.021387778222560883, -0.03380121290683746, -0.03991393744945526, -0.040897663682699203, -0.04821782559156418], "norm_scores": [0.20310699939727783, 0.20060132443904877, 0.1993788331747055, 0.19918279349803925, 0.19773006439208984]}}
{"id": "WebQTest-1741", "question": "where is the headquarters of bank of america", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "ground_paths": [["common.image.appears_in_topic_gallery", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the headquarters of bank of america [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.0051300120539963245, -0.03631536290049553, -0.05112306773662567, -0.05369863286614418, -0.0740865096449852], "norm_scores": [0.20788708329200745, 0.20150408148765564, 0.1985422521829605, 0.19803155958652496, 0.19403499364852905]}}
{"id": "WebQTest-1742", "question": "what type of government japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.010511048138141632, -0.014276999048888683, -0.12935662269592285, -0.13951964676380157, -0.14836308360099792], "norm_scores": [0.21577860414981842, 0.2149675041437149, 0.191599503159523, 0.1896621435880661, 0.18799225986003876]}}
{"id": "WebQTest-1744", "question": "what team does josh hamilton play for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does josh hamilton play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.01395157165825367, -0.01676582358777523, -0.0298802237957716, -0.0335325188934803, -0.05572519823908806], "norm_scores": [0.20320728421211243, 0.2026362121105194, 0.19999611377716064, 0.19926698505878448, 0.19489344954490662]}}
{"id": "WebQTest-1746", "question": "where is burundi country", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.river.basin_countries"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is burundi country [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.0032446279656141996, -0.05188484489917755, -0.06725981831550598, -0.07337765395641327, -0.09129733592271805], "norm_scores": [0.21103739738464355, 0.20101812481880188, 0.19795112311840057, 0.19674378633499146, 0.19324959814548492]}}
{"id": "WebQTest-1748", "question": "through which countries of the sahel does the niger river flow", "prediction": [["location.location.partially_containedby"], ["geography.river.basin_countries"], ["location.location.partially_contains"], ["common.topic.notable_types"], ["location.location.containedby"]], "ground_paths": [["geography.river.basin_countries"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: through which countries of the sahel does the niger river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01709805056452751, -0.018431752920150757, -0.031426798552274704, -0.03925316408276558, -0.06575077772140503], "norm_scores": [0.20345710217952728, 0.20318593084812164, 0.20056259632110596, 0.1989990472793579, 0.193795308470726]}}
{"id": "WebQTest-1750", "question": "what city does paul ryan live in", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does paul ryan live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.020375603809952736, -0.021559249609708786, -0.034753765910863876, -0.03610144928097725, -0.03823831304907799], "norm_scores": [0.20196981728076935, 0.2017309069633484, 0.19908663630485535, 0.19881850481033325, 0.19839411973953247]}}
{"id": "WebQTest-1751", "question": "what city is acadia university in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["organization.organization.headquarters", "location.mailing_address.citytown"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is acadia university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>"], "scores": [-0.006856794469058514, -0.04382811859250069, -0.047003552317619324, -0.0500452034175396, -0.059640221297740936], "norm_scores": [0.2070106863975525, 0.19949696958065033, 0.19886450469493866, 0.19826054573059082, 0.1963673233985901]}}
{"id": "WebQTest-1752", "question": "what did f. scott fitzgerald do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"], ["book.written_work.author"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did f. scott fitzgerald do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.0005907139275223017, -0.07943671941757202, -0.0804545059800148, -0.09208168089389801, -0.10033248364925385], "norm_scores": [0.21435929834842682, 0.1981070339679718, 0.1979055255651474, 0.19561776518821716, 0.1940104067325592]}}
{"id": "WebQTest-1753", "question": "what countries does tomtom western europe include", "prediction": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["common.topic.notable_types"], ["location.location.containedby"], ["location.location.contains"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["organization.organization.phone_number", "common.phone_number.service_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does tomtom western europe include [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.009141426533460617, -0.021643612533807755, -0.04456515610218048, -0.05804620310664177, -0.08999897539615631], "norm_scores": [0.20715218782424927, 0.2045784592628479, 0.19994254410266876, 0.19726517796516418, 0.19106164574623108]}}
{"id": "WebQTest-1754", "question": "what is the national language of iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.language_family"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national language of iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.01764450967311859, -0.02192843332886696, -0.02788720838725567, -0.041811227798461914, -0.15523718297481537], "norm_scores": [0.2069060355424881, 0.20602157711982727, 0.20479758083820343, 0.20196571946144104, 0.1803090125322342]}}
{"id": "WebQTest-1755", "question": "where did the second battle of ypres occur", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["event.disaster.areas_affected"], ["military.military_conflict.casualties"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the second battle of ypres occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> military.military_conflict.casualties </PATH>"], "scores": [-0.009444860741496086, -0.020033234730362892, -0.0678703784942627, -0.10683518648147583, -0.11102345585823059], "norm_scores": [0.2108224332332611, 0.2086019515991211, 0.19885794818401337, 0.191258504986763, 0.19045914709568024]}}
{"id": "WebQTest-1756", "question": "what languages do people speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do people speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.013986743986606598, -0.023810338228940964, -0.026632526889443398, -0.06318965554237366, -0.14058706164360046], "norm_scores": [0.2078685760498047, 0.20583656430244446, 0.20525647699832916, 0.19788837432861328, 0.1831500381231308]}}
{"id": "WebQTest-1757", "question": "what did michael crabtree do", "prediction": [["american_football.football_player.games", "american_football.football_player_game_statistics.position_s"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.position"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.role"], ["american_football.football_player.games", "american_football.football_player.position_s"], ["american_football.football_player.receiving", "american_football.football_player_receiving_statistics.position"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"], ["american_football.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did michael crabtree do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.football_player_game_statistics.position_s </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.position </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.role </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.football_player.position_s </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.football_player_receiving_statistics.position </PATH>"], "scores": [-0.0296461284160614, -0.03214703127741814, -0.03414059802889824, -0.036925774067640305, -0.04025353118777275], "norm_scores": [0.20099641382694244, 0.20049436390399933, 0.200095072388649, 0.19953854382038116, 0.19887563586235046]}}
{"id": "WebQTest-1758", "question": "where do they speak afrikaans", "prediction": [["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.region"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak afrikaans [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.region </PATH>"], "scores": [-0.0045525324530899525, -0.04453907534480095, -0.049155715852975845, -0.05467063561081886, -0.09282738715410233], "norm_scores": [0.20903858542442322, 0.20084474980831146, 0.1999196708202362, 0.19882015883922577, 0.19137674570083618]}}
{"id": "WebQTest-1759", "question": "what 3 states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_containedby"]], "ground_paths": [["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.containedby", "location.location.contains"], ["location.location.contains", "location.location.contains"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 3 states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.02670969069004059, -0.03624602407217026, -0.03758068382740021, -0.03911600634455681, -0.03931211680173874], "norm_scores": [0.20182272791862488, 0.19990721344947815, 0.19964058697223663, 0.1993343085050583, 0.19929523766040802]}}
{"id": "WebQTest-1760", "question": "who plays ziva david on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ziva david on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.021250445395708084, -0.030604807659983635, -0.03268575295805931, -0.036260444670915604, -0.04433861002326012], "norm_scores": [0.20236371457576752, 0.20047955214977264, 0.2000627964735031, 0.19934892654418945, 0.19774502515792847]}}
{"id": "WebQTest-1761", "question": "who ruled after king henry viii died", "prediction": [["people.person.parents"], ["people.person.children"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.religion", "people.person.religion"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who ruled after king henry viii died [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.deceased_person.place_of_burial <SEP> people.deceased_person.place_of_burial </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.02097075805068016, -0.03724914416670799, -0.04137437045574188, -0.04523172974586487, -0.04973674938082695], "norm_scores": [0.20361079275608063, 0.20032314956188202, 0.1994984894990921, 0.1987304389476776, 0.19783715903759003]}}
{"id": "WebQTest-1763", "question": "who are the st louis cardinals coaches", "prediction": [["baseball.baseball_team.current_coaches"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"], ["sports.sports_team.coaches", "baseball.baseball_team.current_manager"]], "ground_paths": [["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the st louis cardinals coaches [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> baseball.baseball_team.current_coaches <SEP> baseball.current_coaching_tenure.baseball_coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> baseball.baseball_team.current_manager </PATH>"], "scores": [-0.016167093068361282, -0.03333064541220665, -0.039316799491643906, -0.0406109094619751, -0.04219803959131241], "norm_scores": [0.20365534722805023, 0.2001897245645523, 0.1989949345588684, 0.19873759150505066, 0.1984224170446396]}}
{"id": "WebQTest-1764", "question": "where is daud ibrahim", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["people.deceased_person.place_of_death"], ["location.location.people_born_here"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is daud ibrahim [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.016000710427761078, -0.022281497716903687, -0.040173280984163284, -0.0423487164080143, -0.08056016266345978], "norm_scores": [0.2048621028661728, 0.20357942581176758, 0.19996941089630127, 0.19953489303588867, 0.19205419719219208]}}
{"id": "WebQTest-1765", "question": "what region is turkey considered", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"], ["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region is turkey considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.012447151355445385, -0.027989035472273827, -0.04776319861412048, -0.055204395204782486, -0.06717398017644882], "norm_scores": [0.20598313212394714, 0.2028065323829651, 0.1988355815410614, 0.19736149907112122, 0.19501323997974396]}}
{"id": "WebQTest-1766", "question": "what language do russians speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.language_family"]], "ground_paths": [["people.ethnicity.included_in_group", "people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["people.ethnicity.geographic_distribution", "language.human_language.countries_spoken_in"], ["people.ethnicity.geographic_distribution", "location.country.languages_spoken"], ["language.human_language.main_country"], ["people.ethnicity.geographic_distribution", "language.human_language.main_country"], ["people.ethnicity.geographic_distribution", "location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do russians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.014641011133790016, -0.020167861133813858, -0.028709225356578827, -0.12661731243133545, -0.14104130864143372], "norm_scores": [0.21026752889156342, 0.2091086208820343, 0.2073301374912262, 0.18799294531345367, 0.1853007823228836]}}
{"id": "WebQTest-1767", "question": "where was martin luther king jr buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"], ["symbols.name_source.namesakes"], ["people.person.place_of_burial"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> people.person.place_of_burial </PATH>"], "scores": [-0.003214651020243764, -0.03163057193160057, -0.08658336102962494, -0.09466402232646942, -0.09588623046875], "norm_scores": [0.2120397686958313, 0.206099271774292, 0.19507910311222076, 0.19350910186767578, 0.19327270984649658]}}
{"id": "WebQTest-1768", "question": "what are the official languages of argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.language_family.geographic_distribution"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>"], "scores": [-0.008173149079084396, -0.025198720395565033, -0.047317761927843094, -0.05844508484005928, -0.1326494961977005], "norm_scores": [0.2092636674642563, 0.20573098957538605, 0.20123039186000824, 0.1990036517381668, 0.18477126955986023]}}
{"id": "WebQTest-1769", "question": "where is the city wales", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the city wales [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.018458237871527672, -0.021149160340428352, -0.036677125841379166, -0.04760049283504486, -0.06675668060779572], "norm_scores": [0.20394070446491241, 0.2033926546573639, 0.2002587616443634, 0.19808316230773926, 0.19432476162910461]}}
{"id": "WebQTest-1770", "question": "what country does rafael nadal play for", "prediction": [["people.person.nationality"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does rafael nadal play for [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.017631515860557556, -0.021286696195602417, -0.029490480199456215, -0.0391789935529232, -0.048587389290332794], "norm_scores": [0.2027260959148407, 0.20198644697666168, 0.20033615827560425, 0.19840459525585175, 0.19654665887355804]}}
{"id": "WebQTest-1771", "question": "what is the islamic belief", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"], ["belief.belief.belief_of"], ["symbols.namesake.named_after"], ["common.topic.notable_types"]], "ground_paths": [["religion.belief.belief_of"], ["religion.religion.beliefs"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the islamic belief [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>", "<PATH> belief.belief.belief_of </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.008320488967001438, -0.01883726753294468, -0.11354470252990723, -0.12250777333974838, -0.1351207196712494], "norm_scores": [0.214469775557518, 0.21222607791423798, 0.1930491179227829, 0.1913265436887741, 0.1889284998178482]}}
{"id": "WebQTest-1772", "question": "who is the prime minister of spain 2011", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of spain 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>"], "scores": [-0.008932044729590416, -0.019364630803465843, -0.029533620923757553, -0.033480387181043625, -0.052652426064014435], "norm_scores": [0.2039899379014969, 0.20187285542488098, 0.19983041286468506, 0.19904328882694244, 0.1952635794878006]}}
{"id": "WebQTest-1773", "question": "who was the son of king leonidas", "prediction": [["people.person.gender"], ["people.person.children"], ["people.person.parents"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the son of king leonidas [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.008969067595899105, -0.03037724457681179, -0.04216473549604416, -0.06649613380432129, -0.0858168751001358], "norm_scores": [0.20762844383716583, 0.20323075354099274, 0.20084922015666962, 0.19602127373218536, 0.19227033853530884]}}
{"id": "WebQTest-1774", "question": "who was judi dench married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was judi dench married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.02109512872993946, -0.027171680703759193, -0.0401584729552269, -0.04655158892273903, -0.047007668763399124], "norm_scores": [0.20307272672653198, 0.20184248685836792, 0.1992381513118744, 0.1979684680700302, 0.19787819683551788]}}
{"id": "WebQTest-1775", "question": "how many languages do they speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.016563018783926964, -0.017514951527118683, -0.03278378024697304, -0.048334021121263504, -0.146607905626297], "norm_scores": [0.20705033838748932, 0.20685333013534546, 0.20371891558170319, 0.2005755454301834, 0.18180179595947266]}}
{"id": "WebQTest-1776", "question": "where are the sahara desert located", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"], ["location.location.partially_contains"], ["geography.river.basin_countries"]], "ground_paths": [["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the sahara desert located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.0024168924428522587, -0.053794119507074356, -0.07141464948654175, -0.08140335977077484, -0.08915958553552628], "norm_scores": [0.21167540550231934, 0.2010747492313385, 0.19756275415420532, 0.19559918344020844, 0.1940879225730896]}}
{"id": "WebQTest-1777", "question": "what is rihanna mum called", "prediction": [["people.person.parents"], ["people.person.children"], ["music.artist.origin"], ["people.person.parent"], ["music.album.artist"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is rihanna mum called [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.012645678594708443, -0.015087444335222244, -0.12673747539520264, -0.14478467404842377, -0.17187024652957916], "norm_scores": [0.21650825440883636, 0.21598023176193237, 0.19316348433494568, 0.18970869481563568, 0.18463928997516632]}}
{"id": "WebQTest-1779", "question": "when did president theodore roosevelt take office", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.us_president.vice_president"], ["business.employment_tenure.person", "business.employment_tenure.start_date"], ["base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did president theodore roosevelt take office [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.start_date </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.027864649891853333, -0.03452732414007187, -0.03470052033662796, -0.03530455380678177, -0.044489528983831406], "norm_scores": [0.2015053629875183, 0.20016726851463318, 0.2001325935125351, 0.20001175999641418, 0.19818304479122162]}}
{"id": "WebQTest-1780", "question": "who does michael oher play for in 2010", "prediction": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael oher play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.016812188550829887, -0.018883297219872475, -0.018944386392831802, -0.028844991698861122, -0.037605758756399155], "norm_scores": [0.2014804184436798, 0.20106355845928192, 0.201051265001297, 0.19907055795192719, 0.1973341703414917]}}
{"id": "WebQTest-1781", "question": "who or what influenced albert einstein", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced albert einstein [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.011205684393644333, -0.012539933435618877, -0.10256098955869675, -0.10496479272842407, -0.11974872648715973], "norm_scores": [0.2119092047214508, 0.21162664890289307, 0.19340813159942627, 0.1929437667131424, 0.19011227786540985]}}
{"id": "WebQTest-1782", "question": "where does egypt export to", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.statistical_region.places_imported_from", "location.imports_and_exports.imported_from"]], "ground_paths": [["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["location.location.contains", "location.location.contains"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.events", "time.event.locations"], ["location.location.events", "location.location.events"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does egypt export to [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> location.statistical_region.places_imported_from <SEP> location.imports_and_exports.imported_from </PATH>"], "scores": [-0.026306070387363434, -0.029157182201743126, -0.03215453028678894, -0.04255830869078636, -0.04599570855498314], "norm_scores": [0.20178769528865814, 0.20121319591999054, 0.20061099529266357, 0.19853468239307404, 0.1978534311056137]}}
{"id": "WebQTest-1784", "question": "where is the seat of the african union", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization_founder.organizations_founded"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seat of the african union [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.009816857986152172, -0.03538046404719353, -0.038834117352962494, -0.04337506368756294, -0.04911880940198898], "norm_scores": [0.2051442414522171, 0.19996647536754608, 0.19927704334259033, 0.1983742117881775, 0.19723805785179138]}}
{"id": "WebQTest-1785", "question": "what was kim richards in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was kim richards in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.01780870370566845, -0.03683392331004143, -0.0377279557287693, -0.03893916681408882, -0.04299427941441536], "norm_scores": [0.20343180000782013, 0.1995980590581894, 0.19941969215869904, 0.1991783082485199, 0.1983722448348999]}}
{"id": "WebQTest-1788", "question": "what artistic style did henri rousseau work in", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"], ["visual_art.artist.associated_periods_or_movements"], ["film.film_genre.films_in_this_genre"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic style did henri rousseau work in [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.artist.associated_periods_or_movements </PATH>", "<PATH> film.film_genre.films_in_this_genre </PATH>"], "scores": [-0.0020631959196180105, -0.03939567506313324, -0.051505912095308304, -0.09403534978628159, -0.10705304890871048], "norm_scores": [0.21152472496032715, 0.2037735879421234, 0.20132070779800415, 0.1929381787776947, 0.19044283032417297]}}
{"id": "WebQTest-1789", "question": "what is john edwards indicted for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.election_campaign.candidate", "government.election_campaign.offense"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["people.person.profession", "people.profession.specializations"]], "ground_paths": [["base.crime.acquitted_person.acquittal", "base.crime.acquittal.crime_type"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is john edwards indicted for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.offense </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>"], "scores": [-0.022404786199331284, -0.025626633316278458, -0.042818982154130936, -0.05191480368375778, -0.05340180918574333], "norm_scores": [0.20337703824043274, 0.20272283256053925, 0.19926735758781433, 0.19746306538581848, 0.19716964662075043]}}
{"id": "WebQTest-1790", "question": "where was huey newton killed", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death", "base.biblioness.bibs_location.city"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was huey newton killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> base.biblioness.bibs_location.city </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.0003552007838152349, -0.0768432542681694, -0.10514996200799942, -0.11691146343946457, -0.1182684674859047], "norm_scores": [0.21712572872638702, 0.2011374533176422, 0.19552373886108398, 0.1932375580072403, 0.1929755061864853]}}
{"id": "WebQTest-1791", "question": "when was the printing press invented by gutenberg", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["book.book_subject.works"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the printing press invented by gutenberg [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.014883967116475105, -0.02657468058168888, -0.030215207487344742, -0.05257002264261246, -0.0640956312417984], "norm_scores": [0.2045760452747345, 0.202198326587677, 0.20146356523036957, 0.1970098465681076, 0.19475221633911133]}}
{"id": "WebQTest-1792", "question": "where is midlothian scotland", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.country.first_level_divisions"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.second_level_division_of"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is midlothian scotland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.country.first_level_divisions </PATH>"], "scores": [-0.0027241341304033995, -0.04878810793161392, -0.062492914497852325, -0.0737915113568306, -0.08293615281581879], "norm_scores": [0.21046948432922363, 0.2009943276643753, 0.19825853407382965, 0.1960310935974121, 0.19424663484096527]}}
{"id": "WebQTest-1793", "question": "where does marta play soccer", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["people.person.nationality", "sports.sports_team_location.teams"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does marta play soccer [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>"], "scores": [-0.026437342166900635, -0.033135902136564255, -0.03343499079346657, -0.038484472781419754, -0.03944297507405281], "norm_scores": [0.2015538066625595, 0.20020818710327148, 0.2001483291387558, 0.19914022088050842, 0.1989494413137436]}}
{"id": "WebQTest-1794", "question": "where is mitt romney 's family from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["people.person.parents"], ["people.person.children"], ["people.family.members"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mitt romney 's family from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.family.members </PATH>"], "scores": [-0.013396374881267548, -0.02359631098806858, -0.044780079275369644, -0.06853781640529633, -0.07132090628147125], "norm_scores": [0.2062268853187561, 0.20413407683372498, 0.199855238199234, 0.1951630860567093, 0.19462068378925323]}}
{"id": "WebQTest-1795", "question": "what are john terry 's children called", "prediction": [["people.person.children"], ["people.person.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.parent"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are john terry 's children called [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.parent </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.00909452885389328, -0.018660010769963264, -0.13860353827476501, -0.14145216345787048, -0.1548156440258026], "norm_scores": [0.21694640815258026, 0.21488109230995178, 0.19059322774410248, 0.19005106389522552, 0.18752820789813995]}}
{"id": "WebQTest-1796", "question": "what city was michael jackson born in", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["music.artist.origin"], ["symbols.namesake.named_after"], ["music.artist.origin", "location.location.containedby"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city was michael jackson born in [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> music.artist.origin <SEP> location.location.containedby </PATH>"], "scores": [-0.011662343516945839, -0.02018439583480358, -0.03941291198134422, -0.10063879936933517, -0.10434816777706146], "norm_scores": [0.20874682068824768, 0.20697543025016785, 0.20303362607955933, 0.190975621342659, 0.19026851654052734]}}
{"id": "WebQTest-1797", "question": "who fought in the battle of vicksburg", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"], ["military.military_conflict.casualties", "military.military_person.casualties"], ["time.event.locations", "people.person.place_of_birth"]], "ground_paths": [["base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the battle of vicksburg [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_person.casualties </PATH>", "<PATH> time.event.locations <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.016373421996831894, -0.030919551849365234, -0.04528447240591049, -0.046111494302749634, -0.046192001551389694], "norm_scores": [0.20414899289608002, 0.2012009173631668, 0.1983313262462616, 0.19816738367080688, 0.19815142452716827]}}
{"id": "WebQTest-1799", "question": "what kind of writing is nikolai gogol famous for", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.author.school_or_movement"], ["book.book_edition.author_editor"], ["book.book_subject.works"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of writing is nikolai gogol famous for [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.school_or_movement </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.0156271792948246, -0.024155735969543457, -0.029910502955317497, -0.04396400973200798, -0.04865621402859688], "norm_scores": [0.203380286693573, 0.2016531080007553, 0.20049598813056946, 0.19769802689552307, 0.19677257537841797]}}
{"id": "WebQTest-1800", "question": "what year was first world series", "prediction": [["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"], ["time.event.locations"], ["sports.sports_team.championships"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was first world series [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.011842693202197552, -0.01810993067920208, -0.05381656810641289, -0.06032177805900574, -0.08340496569871902], "norm_scores": [0.20677123963832855, 0.20547938346862793, 0.19827185571193695, 0.19698624312877655, 0.19249126315116882]}}
{"id": "WebQTest-1801", "question": "who plays simon birch", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["award.award_honor.honored_for", "award.award_winner.awards_won"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays simon birch [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.021502884104847908, -0.02815600112080574, -0.04307054355740547, -0.043974701315164566, -0.045857928693294525], "norm_scores": [0.20301476120948792, 0.20166854560375214, 0.19868309795856476, 0.1985035240650177, 0.1981300562620163]}}
{"id": "WebQTest-1802", "question": "where are the timezones in the usa", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.time_zones.locations_in_this_time_zone"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the timezones in the usa [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.time_zones.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-2.483389289409388e-05, -0.121805839240551, -0.13875249028205872, -0.15219643712043762, -0.18148790299892426], "norm_scores": [0.22478491067886353, 0.19901156425476074, 0.19566743075847626, 0.19305448234081268, 0.18748164176940918]}}
{"id": "WebQTest-1803", "question": "what does gm make", "prediction": [["automotive.company.make_s"], ["automotive.make.parent_company"], ["common.topic.subjects"], ["common.topic.subject_of"], ["symbols.namesake.named_after"]], "ground_paths": [["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does gm make [/INST]", "raw_output": {"paths": ["<PATH> automotive.company.make_s </PATH>", "<PATH> automotive.make.parent_company </PATH>", "<PATH> common.topic.subjects </PATH>", "<PATH> common.topic.subject_of </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.012348426505923271, -0.018137251958251, -0.07273990660905838, -0.09965915977954865, -0.12049455940723419], "norm_scores": [0.21054792404174805, 0.20933261513710022, 0.19820895791053772, 0.1929444968700409, 0.18896599113941193]}}
{"id": "WebQTest-1804", "question": "what do people speak in uk", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.form_of_government"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.013231738470494747, -0.022544141858816147, -0.036163005977869034, -0.055828627198934555, -0.15467365086078644], "norm_scores": [0.20857387781143188, 0.2066405713558197, 0.20384544134140015, 0.1998758614063263, 0.1810641586780548]}}
{"id": "WebQTest-1805", "question": "what do people in jamaica speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in jamaica speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.01110902614891529, -0.019367869943380356, -0.04522234946489334, -0.06275812536478043, -0.14175941050052643], "norm_scores": [0.2089679092168808, 0.20724919438362122, 0.2019595354795456, 0.19844889640808105, 0.18337446451187134]}}
{"id": "WebQTest-1806", "question": "what position did lebron james play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"], ["sports.sports_position.sport", "sports.sports_team.sport"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position_s"], ["sports.sports_position.sport", "sports.sports_position.sport", "basketball.basketball_player.position_s"]], "ground_paths": [["basketball.basketball_player.position_s"], ["sports.pro_athlete.teams", "sports.sports_team_roster.position"], ["basketball.basketball_position.players"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did lebron james play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_position.sport <SEP> basketball.basketball_player.position_s </PATH>"], "scores": [-0.008289115503430367, -0.033557210117578506, -0.04643828794360161, -0.04923304542899132, -0.052412498742341995], "norm_scores": [0.2060013711452484, 0.20086131989955902, 0.1982906013727188, 0.1977372020483017, 0.19710950553417206]}}
{"id": "WebQTest-1807", "question": "what was wayne gretzky 's first team", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["ice_hockey.hockey_player.hockey_position", "ice.hockey.hockey_team.captain"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was wayne gretzky 's first team [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> ice_hockey.hockey_player.hockey_position <SEP> ice.hockey.hockey_team.captain </PATH>"], "scores": [-0.008175963535904884, -0.020300187170505524, -0.03523709625005722, -0.04574531316757202, -0.05435759946703911], "norm_scores": [0.20494966208934784, 0.20247982442378998, 0.19947786629199982, 0.19739270210266113, 0.19569998979568481]}}
{"id": "WebQTest-1808", "question": "who is the president of peru now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of peru now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.003997519612312317, -0.036750033497810364, -0.04339464381337166, -0.044451188296079636, -0.05121103674173355], "norm_scores": [0.20646719634532928, 0.19981442391872406, 0.1984911561012268, 0.19828154146671295, 0.1969456970691681]}}
{"id": "WebQTest-1809", "question": "what kind of guitar did jerry garcia play", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"], ["music.instrument.family"], ["music.musical_group.member", "music.group_membership.role"]], "ground_paths": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did jerry garcia play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>"], "scores": [-0.01292924489825964, -0.026357214897871017, -0.03304510563611984, -0.03476887196302414, -0.07965724915266037], "norm_scores": [0.204893097281456, 0.20216019451618195, 0.2008126825094223, 0.2004668265581131, 0.19166716933250427]}}
{"id": "WebQTest-1810", "question": "what type of art leonardo da vinci do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"], ["influence.influence_node.influenced_by"], ["visual_art.art_form.artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art leonardo da vinci do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.art_form.artists </PATH>"], "scores": [-0.00022429921955335885, -0.08887742459774017, -0.09966687113046646, -0.12139056622982025, -0.12674853205680847], "norm_scores": [0.21798153221607208, 0.19948863983154297, 0.19734783470630646, 0.19310693442821503, 0.19207504391670227]}}
{"id": "WebQTest-1811", "question": "who are the senators of new jersey now", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["base.biblioness.bibs_location.country", "people.person.nationality"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of new jersey now [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> people.person.nationality </PATH>"], "scores": [-0.017910704016685486, -0.01800951361656189, -0.02168741077184677, -0.03661215305328369, -0.03902556002140045], "norm_scores": [0.20174670219421387, 0.2017267793416977, 0.20098620653152466, 0.19800880551338196, 0.19753150641918182]}}
{"id": "WebQTest-1812", "question": "where was rihanna raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"], ["people.person.place_of_birth"], ["music.album.artist"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.019680749624967575, -0.021140163764357567, -0.02585742250084877, -0.09926748275756836, -0.11144346743822098], "norm_scores": [0.20711636543273926, 0.20681433379650116, 0.20584100484848022, 0.19127152860164642, 0.18895673751831055]}}
{"id": "WebQTest-1813", "question": "what team does messi play for 2011", "prediction": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"]], "ground_paths": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does messi play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players <SEP> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team </PATH>"], "scores": [-0.02528209052979946, -0.025774016976356506, -0.026045093312859535, -0.03286319971084595, -0.035528991371393204], "norm_scores": [0.20076295733451843, 0.200664222240448, 0.20060983300209045, 0.19924670457839966, 0.19871626794338226]}}
{"id": "WebQTest-1815", "question": "where does lil wayne go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does lil wayne go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.016337724402546883, -0.016338570043444633, -0.028165990486741066, -0.03586550056934357, -0.04464747756719589], "norm_scores": [0.20238861441612244, 0.2023884356021881, 0.2000088095664978, 0.19847474992275238, 0.19673936069011688]}}
{"id": "WebQTest-1817", "question": "where did jrr tolkien go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jrr tolkien go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.011545589193701744, -0.019542593508958817, -0.033582497388124466, -0.04456332325935364, -0.04494313523173332], "norm_scores": [0.20387715101242065, 0.20225325226783752, 0.19943347573280334, 0.19725550711154938, 0.1971806138753891]}}
{"id": "WebQTest-1818", "question": "what state is saint louis university in", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is saint louis university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>"], "scores": [-0.006052803713828325, -0.034052446484565735, -0.04033011943101883, -0.05555126443505287, -0.05948270112276077], "norm_scores": [0.20668111741542816, 0.20097438991069794, 0.1997167021036148, 0.19669978320598602, 0.19592799246311188]}}
{"id": "WebQTest-1819", "question": "what religion does canada follow", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical.subjects"], ["book.newspaper.circulation_areas", "book.periodical_subject.periodicals"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical_subject.periodicals"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.monarch.kingdom", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does canada follow [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.subjects </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical_subject.periodicals </PATH>"], "scores": [-0.012868824414908886, -0.021763013675808907, -0.02684558741748333, -0.04787676781415939, -0.07971684634685516], "norm_scores": [0.2049936205148697, 0.20317845046520233, 0.2021484076976776, 0.197941392660141, 0.19173818826675415]}}
{"id": "WebQTest-1821", "question": "which countries have spanish as their main language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["base.rosetta.local_name.locale"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries have spanish as their main language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>"], "scores": [-0.008752560243010521, -0.027911117300391197, -0.03213811665773392, -0.09441810846328735, -0.11534759402275085], "norm_scores": [0.20943723618984222, 0.20546290278434753, 0.20459625124931335, 0.19224269688129425, 0.18826095759868622]}}
{"id": "WebQTest-1822", "question": "who is in paul ryan 's family", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in paul ryan 's family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02029419131577015, -0.025509746745228767, -0.03403854742646217, -0.04403476417064667, -0.05787816271185875], "norm_scores": [0.2032189816236496, 0.20216183364391327, 0.20044498145580292, 0.19845128059387207, 0.19572295248508453]}}
{"id": "WebQTest-1823", "question": "what government does iraq use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["people.person.nationality", "government.form_of_government.countries"], ["base.locations.countries.continent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does iraq use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.005956609267741442, -0.022477105259895325, -0.1304640769958496, -0.15793335437774658, -0.16409751772880554], "norm_scores": [0.21837471425533295, 0.21479667723178864, 0.19280995428562164, 0.18758568167686462, 0.18643292784690857]}}
{"id": "WebQTest-1824", "question": "what type of currency do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.003516486380249262, -0.03200441971421242, -0.13125169277191162, -0.1499479115009308, -0.16957437992095947], "norm_scores": [0.21916450560092926, 0.2130090445280075, 0.1928836852312088, 0.18931099772453308, 0.18563172221183777]}}
{"id": "WebQTest-1825", "question": "what did lincoln steffens wrote", "prediction": [["book.author.works_written"], ["book.written_work.author"], ["book.book_edition.author_editor"], ["book.book_subject.works"], ["film.film.story_by"]], "ground_paths": [["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did lincoln steffens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.book_subject.works </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.012717614881694317, -0.016031846404075623, -0.07274001091718674, -0.10007262229919434, -0.10784383118152618], "norm_scores": [0.20990566909313202, 0.20921114087104797, 0.19767729938030243, 0.1923474222421646, 0.19085845351219177]}}
{"id": "WebQTest-1826", "question": "what schools did anders celsius attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did anders celsius attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.0185028575360775, -0.019395247101783752, -0.0227187592536211, -0.03230486437678337, -0.04900052398443222], "norm_scores": [0.20197300612926483, 0.2017928510904312, 0.20112331211566925, 0.19920451939105988, 0.19590628147125244]}}
{"id": "WebQTest-1827", "question": "what can do in new york", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["metropolitan_transit.transit_system.area_served"], ["organization.organization.geographic_scope"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.neighborhood.neighborhood_of"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what can do in new york [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> metropolitan_transit.transit_system.area_served </PATH>", "<PATH> organization.organization.geographic_scope </PATH>"], "scores": [-0.013758174143731594, -0.016639597713947296, -0.034524764865636826, -0.0997137650847435, -0.11765474826097488], "norm_scores": [0.2085278034210205, 0.20792780816555023, 0.2042420357465744, 0.19135241210460663, 0.18794995546340942]}}
{"id": "WebQTest-1828", "question": "what currency does france accept", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["language.human_language.countries_spoken_in"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does france accept [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.008374632336199284, -0.0214570090174675, -0.09598715603351593, -0.11565512418746948, -0.1225433200597763], "norm_scores": [0.2130606323480606, 0.21029144525527954, 0.19518820941448212, 0.19138675928115845, 0.19007298350334167]}}
{"id": "WebQTest-1829", "question": "what type of religion did massachusetts have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["religion.religion.states"], ["religion.religion.organizations"], ["religion.religion.main_worship"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion did massachusetts have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> religion.religion.states </PATH>", "<PATH> religion.religion.organizations </PATH>", "<PATH> religion.religion.main_worship </PATH>"], "scores": [-0.003440265776589513, -0.038523729890584946, -0.07973065972328186, -0.09552657604217529, -0.11243076622486115], "norm_scores": [0.21272806823253632, 0.20539425313472748, 0.19710257649421692, 0.19401362538337708, 0.190761536359787]}}
{"id": "WebQTest-1830", "question": "what language do they speak in colombia south america", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.location.containedby"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in colombia south america [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.008272198960185051, -0.018610823899507523, -0.057211413979530334, -0.06299862265586853, -0.14310286939144135], "norm_scores": [0.20997102558612823, 0.2078114151954651, 0.19994260370731354, 0.1987888365983963, 0.18348610401153564]}}
{"id": "WebQTest-1833", "question": "what kind of government does brazil has", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["people.person.nationality", "government.form_of_government.countries"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does brazil has [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> people.person.nationality <SEP> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.003727469826117158, -0.029040148481726646, -0.16112913191318512, -0.16382679343223572, -0.17215968668460846], "norm_scores": [0.2209254652261734, 0.21540343761444092, 0.18875005841255188, 0.18824155628681183, 0.18667948246002197]}}
{"id": "WebQTest-1835", "question": "what country did gregor mendel live in", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did gregor mendel live in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.016008269041776657, -0.016156688332557678, -0.047195810824632645, -0.05550772324204445, -0.06085255369544029], "norm_scores": [0.20464284718036652, 0.20461247861385345, 0.19835902750492096, 0.19671711325645447, 0.19566850364208221]}}
{"id": "WebQTest-1836", "question": "where laura ingalls wilder lived", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.parents", "people.deceased_person.place_of_death"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["people.person.nationality", "location.administrative_division.first_level_division_of"], ["influence.influence_node.influenced", "location.location.people_born_here"], ["people.deceased_person.place_of_death"], ["book.author.works_written", "fictional_universe.work_of_fiction.setting"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.parents", "people.deceased_person.place_of_death"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where laura ingalls wilder lived [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.parents <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.025215527042746544, -0.028676830232143402, -0.02994520030915737, -0.03171585127711296, -0.033238667994737625], "norm_scores": [0.20090988278388977, 0.2002156674861908, 0.19996188580989838, 0.19960814714431763, 0.19930440187454224]}}
{"id": "WebQTest-1838", "question": "what sports are popular in germany", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["time.event.locations"], ["cricket.cricket_team.country"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_team_location.teams"], ["cricket.cricket_team.country"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports are popular in germany [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> cricket.cricket_team.country </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.010551925748586655, -0.015285946428775787, -0.13728994131088257, -0.14096368849277496, -0.15066295862197876], "norm_scores": [0.2162986397743225, 0.21527709066867828, 0.19055141508579254, 0.18985265493392944, 0.18802012503147125]}}
{"id": "WebQTest-1839", "question": "what 's my timezone in oklahoma", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's my timezone in oklahoma [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-9.436873369850218e-06, -0.16734907031059265, -0.18913088738918304, -0.19149024784564972, -0.21197891235351562], "norm_scores": [0.23210887610912323, 0.19634373486042023, 0.1921132653951645, 0.19166052341461182, 0.18777363002300262]}}
{"id": "WebQTest-1840", "question": "what do they speak in norway", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"], ["location.country.language_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.language_spoken </PATH>"], "scores": [-0.017171356827020645, -0.018705544993281364, -0.03282640501856804, -0.05018745735287666, -0.17368167638778687], "norm_scores": [0.20809143781661987, 0.20777244865894318, 0.20485912263393402, 0.20133325457572937, 0.17794370651245117]}}
{"id": "WebQTest-1842", "question": "what currency does singapore use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does singapore use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.00402344623580575, -0.0318758562207222, -0.13784101605415344, -0.1624755561351776, -0.17856864631175995], "norm_scores": [0.22023595869541168, 0.21418648958206177, 0.19265133142471313, 0.18796342611312866, 0.18496271967887878]}}
{"id": "WebQTest-1843", "question": "what did the blackfoot indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken", "language.human_language.language_family", "language.human_language.language_family"], ["people.ethnicity.languages_spoken", "language.human_language.region", "language.human_language.region"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the blackfoot indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.language_family <SEP> language.human_language.language_family </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.region <SEP> language.human_language.region </PATH>"], "scores": [-0.0011618557618930936, -0.06737657636404037, -0.06938669085502625, -0.07588955014944077, -0.07926493138074875], "norm_scores": [0.2117367535829544, 0.1981707662343979, 0.197772815823555, 0.19649089872837067, 0.19582878053188324]}}
{"id": "WebQTest-1844", "question": "who plays robin hood in prince of thieves", "prediction": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["people.person.gender", "people.person.gender"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film.starring", "film.performance.actor"]], "ground_paths": [["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays robin hood in prince of thieves [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>"], "scores": [-0.012318416498601437, -0.03715626522898674, -0.04324968159198761, -0.04688107222318649, -0.04986300691962242], "norm_scores": [0.20516231656074524, 0.20012928545475006, 0.19891352951526642, 0.1981925070285797, 0.19760240614414215]}}
{"id": "WebQTest-1845", "question": "what standard time is texas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is texas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_Zones </PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.691857869445812e-05, -0.13673529028892517, -0.18598510324954987, -0.19709376990795135, -0.19770380854606628], "norm_scores": [0.2301882654428482, 0.20077385008335114, 0.1911253184080124, 0.18901392817497253, 0.18889865279197693]}}
{"id": "WebQTest-1847", "question": "where does kurdish people live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"], ["location.country.official_language"], ["people.ethnicity.languages_spoken"], ["location.location.contains"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kurdish people live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00117387599311769, -0.061354897916316986, -0.11440113186836243, -0.11716753244400024, -0.11777298152446747], "norm_scores": [0.21668636798858643, 0.2040306180715561, 0.19348961114883423, 0.19295506179332733, 0.19283828139305115]}}
{"id": "WebQTest-1848", "question": "what instrument does louis armstrong", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.musical_group.member", "music.group_membership.role"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_member.instruments_played", "music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument does louis armstrong [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>"], "scores": [-0.0002565228787716478, -0.07979058474302292, -0.11104755103588104, -0.12315903604030609, -0.12387510389089584], "norm_scores": [0.21801981329917908, 0.20135144889354706, 0.19515515863895416, 0.19280579686164856, 0.19266779720783234]}}
{"id": "WebQTest-1849", "question": "who does alyson stoner play in camp rock", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.film.starring", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does alyson stoner play in camp rock [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.024939285591244698, -0.025066858157515526, -0.025552362203598022, -0.038780566304922104, -0.04508016258478165], "norm_scores": [0.20138657093048096, 0.20136088132858276, 0.20126314461231232, 0.1986183375120163, 0.19737105071544647]}}
{"id": "WebQTest-1850", "question": "what religion does madonna practice", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"], ["religion.religion.practices"], ["symbols.namesake.named_after"], ["religion.religion.founding_figures"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does madonna practice [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.practices </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> religion.religion.founding_figures </PATH>"], "scores": [-0.002649304922670126, -0.04042402654886246, -0.10305190831422806, -0.11307266354560852, -0.12164769321680069], "norm_scores": [0.21502290666103363, 0.20705197751522064, 0.1944824606180191, 0.19254334270954132, 0.1908993422985077]}}
{"id": "WebQTest-1851", "question": "what countries are located near egypt", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.events", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_contains"], ["time.event.locations", "time.event.locations"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["location.location.partially_containedby", "location.location.contains"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.contains_major_portion_of", "location.location.containedby"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.containedby"], ["location.location.partially_contains", "location.location.partially_contains"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["government.form_of_government.countries", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["time.event.locations", "time.event.locations"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.contains", "location.location.contains"], ["location.location.contains", "location.location.containedby"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are located near egypt [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.events <SEP> time.event.locations </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>"], "scores": [-0.03733571246266365, -0.03789251297712326, -0.04289393872022629, -0.05140746012330055, -0.05244162306189537], "norm_scores": [0.20141251385211945, 0.20130039751529694, 0.20029611885547638, 0.19859814643859863, 0.198392853140831]}}
{"id": "WebQTest-1853", "question": "what made richard hammond famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what made richard hammond famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.0017818001797422767, -0.04815923050045967, -0.11548394709825516, -0.11879744380712509, -0.11931350827217102], "norm_scores": [0.2161746770143509, 0.20637798309326172, 0.19294103980064392, 0.19230279326438904, 0.1922035664319992]}}
{"id": "WebQTest-1854", "question": "who originally voiced lois griffin", "prediction": [["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced lois griffin [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.027502363547682762, -0.02816261723637581, -0.03226865082979202, -0.03365820273756981, -0.04073513299226761], "norm_scores": [0.20099282264709473, 0.2008601427078247, 0.20003710687160492, 0.1997593492269516, 0.19835063815116882]}}
{"id": "WebQTest-1855", "question": "what airport do you fly into in paris", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"], ["location.location.containedby"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"], ["travel.transportation.mode_of_transportation", "travel.transportation.transport_terminus"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into in paris [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transportation.transport_terminus </PATH>"], "scores": [-0.006814833264797926, -0.024935008957982063, -0.05945650115609169, -0.07064733654260635, -0.09062442183494568], "norm_scores": [0.2088322937488556, 0.20508229732513428, 0.19812336564064026, 0.19591856002807617, 0.1920434981584549]}}
{"id": "WebQTest-1857", "question": "what rihanna new album called 2012", "prediction": [["music.album.artist"], ["music.artist.album"], ["music.recording.artist"], ["music.composition.composer"], ["music.composition.lyricist"]], "ground_paths": [["music.album.artist"], ["music.artist.album"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rihanna new album called 2012 [/INST]", "raw_output": {"paths": ["<PATH> music.album.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>"], "scores": [-0.009757817722856998, -0.021781858056783676, -0.05612081289291382, -0.06792937219142914, -0.07852306216955185], "norm_scores": [0.20747846364974976, 0.2049986720085144, 0.1980787217617035, 0.19575345516204834, 0.19369064271450043]}}
{"id": "WebQTest-1858", "question": "where does dave ramsey live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["location.location.people_born_here"], ["people.person.place_of_birth"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does dave ramsey live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.025549380108714104, -0.026475971564650536, -0.02756057120859623, -0.030675746500492096, -0.04386698827147484], "norm_scores": [0.20105350017547607, 0.20086729526519775, 0.20064954459667206, 0.20002543926239014, 0.1974041908979416]}}
{"id": "WebQTest-1860", "question": "what discovery did galileo make", "prediction": [["astronomy.astronomer.astronomical_objects_discovered"], ["law.invention.inventor"], ["symbols.namesake.named_after"], ["astronomy.astronomical_discovery.discoverer"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["astronomy.astronomical_discovery.discoverer"], ["astronomy.astronomer.astronomical_objects_discovered"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what discovery did galileo make [/INST]", "raw_output": {"paths": ["<PATH> astronomy.astronomer.astronomical_objects_discovered </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> astronomy.astronomical_discovery.discoverer </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.01256527565419674, -0.03565751761198044, -0.03994026407599449, -0.04878739267587662, -0.05743147432804108], "norm_scores": [0.2053084522485733, 0.20062175393104553, 0.19976437091827393, 0.1980048269033432, 0.19630064070224762]}}
{"id": "WebQTest-1861", "question": "what is the government system of saudi arabia", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["location.location.contains", "location.country.form_of_government"], ["government.governmental_jurisdiction.government"], ["location.location.contains", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of saudi arabia [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> location.location.contains <SEP> location.country.form_of_government </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>", "<PATH> location.location.contains <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.010758867487311363, -0.012742189690470695, -0.09607887268066406, -0.11310968548059464, -0.1209208071231842], "norm_scores": [0.21210533380508423, 0.21168507635593414, 0.19475902616977692, 0.1914701908826828, 0.1899804323911667]}}
{"id": "WebQTest-1864", "question": "who plays the role of tony dinozzo on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the role of tony dinozzo on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.01806488260626793, -0.030461503192782402, -0.031224338337779045, -0.033047571778297424, -0.04157397896051407], "norm_scores": [0.20257264375686646, 0.20007693767547607, 0.19992436468601227, 0.19956019520759583, 0.19786588847637177]}}
{"id": "WebQTest-1865", "question": "what did jesus do for living", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["base.activism.activist.area_of_activism"], ["common.topic.image"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesus do for living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> common.topic.image </PATH>"], "scores": [-0.0010061574866995215, -0.07627065479755402, -0.07847675681114197, -0.09634831547737122, -0.11482327431440353], "norm_scores": [0.2148488461971283, 0.19927190244197845, 0.19883278012275696, 0.19531089067459106, 0.1917356550693512]}}
{"id": "WebQTest-1866", "question": "what is the state name of new york city", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state name of new york city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.01124905701726675, -0.047841381281614304, -0.05076197534799576, -0.05077451467514038, -0.05290165916085243], "norm_scores": [0.20636533200740814, 0.19895043969154358, 0.19837023317813873, 0.19836775958538055, 0.19794625043869019]}}
{"id": "WebQTest-1868", "question": "who plays knox overstreet", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays knox overstreet [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02705531008541584, -0.03375673666596413, -0.03687876835465431, -0.03905760496854782, -0.03909280523657799], "norm_scores": [0.20162713527679443, 0.20028045773506165, 0.199656143784523, 0.19922161102294922, 0.1992146074771881]}}
{"id": "WebQTest-1870", "question": "what movies did david carradine play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.languages", "tv.tv_program.languages"], ["film.director.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["people.person.languages", "film.film.language"], ["film.film.edited_by"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did david carradine play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.016206564381718636, -0.03453778848052025, -0.03477736935019493, -0.036419421434402466, -0.03697296231985092], "norm_scores": [0.20313335955142975, 0.19944359362125397, 0.1993958055973053, 0.19906866550445557, 0.19895851612091064]}}
{"id": "WebQTest-1871", "question": "what is the stanley cup named after", "prediction": [["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team_owner.teams_owned"], ["sports.sports_championship_event.championship", "sports.sports_team_owner.teams_owned"], ["sports.sports_team.championships"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the stanley cup named after [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team_owner.teams_owned </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team_owner.teams_owned </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.01478396076709032, -0.017044853419065475, -0.04759184643626213, -0.0556001141667366, -0.0606238916516304], "norm_scores": [0.20489004254341125, 0.2044273316860199, 0.19827710092067719, 0.19669558107852936, 0.19570991396903992]}}
{"id": "WebQTest-1873", "question": "where is traditional chinese used", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["language.human_language.region"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is traditional chinese used [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.region </PATH>"], "scores": [-0.018094222992658615, -0.025496747344732285, -0.02814607508480549, -0.03992674872279167, -0.12871387600898743], "norm_scores": [0.20591780543327332, 0.20439912378787994, 0.20385831594467163, 0.20147082209587097, 0.18435393273830414]}}
{"id": "WebQTest-1874", "question": "who does portugal trade with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"]], "ground_paths": [["military.military_combatant.belongs_to_combatant_group", "military.military_combatant.includes_allies"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.time_zones", "location.location.time_zones"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.person.nationality", "location.location.people_born_here"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["organization.organization.founders", "organization.organization.founders"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["book.written_work.subjects", "book.written_work.subjects"], ["government.form_of_government.countries", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does portugal trade with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> location.imports_and_exports.imported_from <SEP> location.statistical_region.places_imported_from </PATH>"], "scores": [-0.02907951921224594, -0.03095555305480957, -0.033870674669742584, -0.03462865576148033, -0.03789924457669258], "norm_scores": [0.20084227621555328, 0.2004658430814743, 0.19988229870796204, 0.19973085820674896, 0.19907869398593903]}}
{"id": "WebQTest-1875", "question": "what type of political system is brazil", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of political system is brazil [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <PATH>", "</PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.004714875016361475, -0.025335799902677536, -0.17188912630081177, -0.1765860915184021, -0.1778467744588852], "norm_scores": [0.2217906415462494, 0.21726393699645996, 0.18764644861221313, 0.18676713109016418, 0.18653184175491333]}}
{"id": "WebQTest-1876", "question": "what language did the ancient babylonians speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the ancient babylonians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.007756451144814491, -0.030101317912340164, -0.035880353301763535, -0.05709054693579674, -0.0740954801440239], "norm_scores": [0.20670351386070251, 0.20213596522808075, 0.20097118616104126, 0.19675344228744507, 0.193435937166214]}}
{"id": "WebQTest-1877", "question": "what kind of painter was wassily kandinsky", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["book.author.school_or_movement"], ["visual_art.visual_artist.associated_periods_or_movements"], ["book.school_or_movement.associated_authors"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of painter was wassily kandinsky [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> book.author.school_or_movement </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> book.school_or_movement.associated_authors </PATH>"], "scores": [-0.0033369823358953, -0.03776570409536362, -0.0592598095536232, -0.07100958377122879, -0.07595077157020569], "norm_scores": [0.2093672752380371, 0.20228169858455658, 0.19798022508621216, 0.19566760957241058, 0.19470316171646118]}}
{"id": "WebQTest-1878", "question": "what form of government does the united states use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.form_of_government"], ["form.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.007059115916490555, -0.018200116232037544, -0.14850039780139923, -0.14866873621940613, -0.1674715131521225], "norm_scores": [0.21849425137043, 0.21607351303100586, 0.1896762102842331, 0.18964427709579468, 0.18611174821853638]}}
{"id": "WebQTest-1879", "question": "what money does cuba use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does cuba use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.012113201431930065, -0.015246938914060593, -0.141565203666687, -0.14435531198978424, -0.16145965456962585], "norm_scores": [0.21678584814071655, 0.2161075621843338, 0.19046300649642944, 0.18993233144283295, 0.18671129643917084]}}
{"id": "WebQTest-1880", "question": "who is my representative in illinois house", "prediction": [["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"]], "ground_paths": [["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["government.governmental_body.members", "government.government_position_held.office_holder"], ["government.governmental_body.members", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my representative in illinois house [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>"], "scores": [-0.022576674818992615, -0.030292972922325134, -0.03092053532600403, -0.03133096173405647, -0.03262331336736679], "norm_scores": [0.20139802992343903, 0.1998499631881714, 0.19972458481788635, 0.1996426284313202, 0.19938477873802185]}}
{"id": "WebQTest-1881", "question": "what timezone is sweden gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"], ["base.locations.countries.continent"], ["common.topic.notable_types"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0029866178520023823, -0.032546207308769226, -0.13362732529640198, -0.16077271103858948, -0.1628759801387787], "norm_scores": [0.21955455839633942, 0.21315960586071014, 0.19266636669635773, 0.18750672042369843, 0.1871127486228943]}}
{"id": "WebQTest-1882", "question": "what was franklin d roosevelt 's job before president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "base.aareas.schema.administrative_area.administrative_parent", "government.governmental_jurisdiction.government_positions"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "government.governmental_jurisdiction.government_positions"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "base.onephylogeny.type_of_thing.things_of_this_type"], ["common.topic.subject_of", "people.person.nationality", "government.governmental_jurisdiction.government_positions"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.appointed_by", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin d roosevelt 's job before president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> base.aareas.schema.administrative_area.administrative_parent <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> common.topic.subject_of <SEP> people.person.nationality <SEP> government.governmental_jurisdiction.government_positions </PATH>"], "scores": [-0.019044315442442894, -0.025159845128655434, -0.027804696932435036, -0.029863882809877396, -0.034545619040727615], "norm_scores": [0.20165202021598816, 0.20042257010936737, 0.19989319145679474, 0.1994819939136505, 0.1985502541065216]}}
{"id": "WebQTest-1883", "question": "where did robert downey jr go to jail", "prediction": [["people.person.nationality", "location.location.containedby"], ["base.prison.imprisonment.prisoner", "base.prison.prison.prisoners"], ["people.person.languages", "location.country.official_language"], ["base.prison.prisoner.imprisoned", "base.prison.prison.prisoners"], ["people.person.languages", "location.country.languages_spoken"]], "ground_paths": [["celebrities.rehab.celebrity", "celebrities.rehab.rehab_facility"], ["base.prison.imprisonment.prisoner", "base.prison.imprisonment.prison"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert downey jr go to jail [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> base.prison.imprisonment.prisoner <SEP> base.prison.prison.prisoners </PATH>", "<PATH> people.person.languages <SEP> location.country.official_language </PATH>", "<PATH> base.prison.prisoner.imprisoned <SEP> base.prison.prison.prisoners </PATH>", "<PATH> people.person.languages <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.006918556056916714, -0.027423890307545662, -0.04821218550205231, -0.05686508119106293, -0.06670679897069931], "norm_scores": [0.2069324553012848, 0.20273244380950928, 0.19856148958206177, 0.1968507617712021, 0.19492292404174805]}}
{"id": "WebQTest-1884", "question": "what degrees did barack obama get", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["education.education.student", "education.education.major_field_of_study"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did barack obama get [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.01110176369547844, -0.019633861258625984, -0.02730550244450569, -0.09509870409965515, -0.1043543666601181], "norm_scores": [0.20808066427707672, 0.2063128650188446, 0.2047361433506012, 0.19131645560264587, 0.1895538717508316]}}
{"id": "WebQTest-1885", "question": "what does god shiva represent", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["common.topic.notable_types"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"]], "ground_paths": [["religion.deity.deity_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does god shiva represent [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.010788967832922935, -0.02561909332871437, -0.053237274289131165, -0.054346952587366104, -0.07816888391971588], "norm_scores": [0.2067851722240448, 0.2037411630153656, 0.19819119572639465, 0.197971373796463, 0.19331105053424835]}}
{"id": "WebQTest-1886", "question": "what inventions were made by ben franklin", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"], ["law.inventor.us_patents"], ["symbols.namesake.named_after"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions were made by ben franklin [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> law.inventor.us_patents </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.007684561423957348, -0.020965376868844032, -0.07284312695264816, -0.08595015108585358, -0.08836852014064789], "norm_scores": [0.2096027284860611, 0.20683744549751282, 0.19638076424598694, 0.19382359087467194, 0.19335541129112244]}}
{"id": "WebQTest-1887", "question": "what is the name of the currency used in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"], ["base.coinsdaily.coin_type.country"], ["common.topic.notable_types"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.004797125235199928, -0.025733372196555138, -0.14870235323905945, -0.15761703252792358, -0.1622917652130127], "norm_scores": [0.21940328180789948, 0.2148575484752655, 0.1899966299533844, 0.1883103847503662, 0.1874321550130844]}}
{"id": "WebQTest-1888", "question": "what year did the milwaukee brewers go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"], ["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.championship"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the milwaukee brewers go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.005448236130177975, -0.021876445040106773, -0.04754965007305145, -0.08466115593910217, -0.09783699363470078], "norm_scores": [0.20928941667079926, 0.20587924122810364, 0.20066094398498535, 0.19335061311721802, 0.19081975519657135]}}
{"id": "WebQTest-1889", "question": "who was vice president with bill clinton", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president with bill clinton [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.01833638735115528, -0.021927086636424065, -0.03239560127258301, -0.03727075830101967, -0.05365213006734848], "norm_scores": [0.20288094878196716, 0.2021537721157074, 0.2000485509634018, 0.19907566905021667, 0.19584110379219055]}}
{"id": "WebQTest-1890", "question": "what songs did franz liszt compose", "prediction": [["music.composition.composer"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["music.composer.compositions"], ["music.recording.artist"]], "ground_paths": [["music.composition.composer"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>", "<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.0030993414111435413, -0.06143098324537277, -0.06150034815073013, -0.06731632351875305, -0.07507359981536865], "norm_scores": [0.21030649542808533, 0.1983899027109146, 0.1983761489391327, 0.19722576439380646, 0.1957017332315445]}}
{"id": "WebQTest-1891", "question": "where did the russian japanese war happen", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"], ["book.written_work.subjects"], ["military.military_conflict.casualties"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the russian japanese war happen [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> military.military_conflict.casualties </PATH>"], "scores": [-0.011344320140779018, -0.018430717289447784, -0.060860954225063324, -0.07450418174266815, -0.09886899590492249], "norm_scores": [0.20834992825984955, 0.2068786919116974, 0.19828438758850098, 0.19559752941131592, 0.1908894181251526]}}
{"id": "WebQTest-1892", "question": "what year did the la kings win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_club.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the la kings win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_club.championships </PATH>"], "scores": [-0.008205896243453026, -0.016044754534959793, -0.11783841252326965, -0.12875553965568542, -0.1592063307762146], "norm_scores": [0.21576668322086334, 0.21408191323280334, 0.19336220622062683, 0.19126273691654205, 0.18552641570568085]}}
{"id": "WebQTest-1893", "question": "what kind of money does egypt have", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"], ["finance.currency.countries_used", "location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money does egypt have [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used <SEP> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.002584540518000722, -0.04179537296295166, -0.08460495620965958, -0.09767290204763412, -0.09916766732931137], "norm_scores": [0.2127646654844284, 0.20458343625068665, 0.19601012766361237, 0.19346533715724945, 0.19317637383937836]}}
{"id": "WebQTest-1894", "question": "what baseball team was jackie robinson on", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what baseball team was jackie robinson on [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.014485266990959644, -0.0147249735891819, -0.032068803906440735, -0.03583740070462227, -0.06182199344038963], "norm_scores": [0.20346000790596008, 0.20341123640537262, 0.19991372525691986, 0.19916175305843353, 0.1940532624721527]}}
{"id": "WebQTest-1895", "question": "what body of water does st lawrence river flow into", "prediction": [["geography.river.mouth"], ["geography.body_of_water.mouth"], ["location.location.containedby", "geography.lake.basin_countries"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what body of water does st lawrence river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> geography.body_of_water.mouth </PATH>", "<PATH> location.location.containedby <SEP> geography.lake.basin_countries </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.002796184504404664, -0.05132726952433586, -0.07086014002561569, -0.07874302566051483, -0.08171998709440231], "norm_scores": [0.21106822788715363, 0.2010694444179535, 0.19718009233474731, 0.19563186168670654, 0.19505034387111664]}}
{"id": "WebQTest-1896", "question": "what form of government does the united states have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.form_of_government"], ["form.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>", "<PATH> form.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.007239732425659895, -0.017851606011390686, -0.14714093506336212, -0.14824214577674866, -0.1680363565683365], "norm_scores": [0.2183958888053894, 0.21609055995941162, 0.1898830235004425, 0.18967404961585999, 0.18595649302005768]}}
{"id": "WebQTest-1897", "question": "what are the names of ariel 's six sisters", "prediction": [["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.fictional_character.siblings"]], "ground_paths": [["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of ariel 's six sisters [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> fictional_universe.sibling_relationship_of_fictional_characters.siblings <SEP> fictional_universe.sibling_relationship_of_fictional_characters.siblings </PATH>", "<PATH> fictional_universe.sibling_relationship_of_fictional_characters.siblings <SEP> fictional_universe.fictional_character.siblings </PATH>"], "scores": [-0.01664627529680729, -0.021318374201655388, -0.027918357402086258, -0.0329878069460392, -0.03466169163584709], "norm_scores": [0.20201744139194489, 0.20107580721378326, 0.199753075838089, 0.19874298572540283, 0.19841060042381287]}}
{"id": "WebQTest-1898", "question": "what voices does seth macfarlane play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what voices does seth macfarlane play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_universe.characters </PATH>", "<PATH> tv.tv_program_creator.programs_created <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.019042860716581345, -0.021598869934678078, -0.023209819570183754, -0.024436183273792267, -0.02666529454290867], "norm_scores": [0.2007904201745987, 0.20027786493301392, 0.19995547831058502, 0.199710413813591, 0.1992657333612442]}}
{"id": "WebQTest-1899", "question": "who was william mckinley married to", "prediction": [["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["people.person.parents", "people.person.parents"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"], ["people.person.parents", "people.person.children"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"]], "ground_paths": [["people.person.children", "people.person.parents"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["people.person.profession", "common.topic.notable_types"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was william mckinley married to [/INST]", "raw_output": {"paths": ["<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_owner.pets_owned </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_ownership.owner </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> base.famouspets.pet_owner.pets_owned <SEP> base.famouspets.pet_owner.pets_owned </PATH>"], "scores": [-0.03768445551395416, -0.03816992789506912, -0.03846919536590576, -0.039346881210803986, -0.03953078016638756], "norm_scores": [0.2001911997795105, 0.2000940442085266, 0.20003417134284973, 0.1998586803674698, 0.19982191920280457]}}
{"id": "WebQTest-1900", "question": "who played darth vader in the first movie", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader in the first movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.019614238291978836, -0.019734174013137817, -0.02407943643629551, -0.02819962427020073, -0.05264397710561752], "norm_scores": [0.20184136927127838, 0.20181716978549957, 0.20094211399555206, 0.20011590421199799, 0.19528348743915558]}}
{"id": "WebQTest-1901", "question": "where did kevin james go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin james go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.01323735062032938, -0.017517337575554848, -0.027135714888572693, -0.04389544948935509, -0.06554516404867172], "norm_scores": [0.20404954254627228, 0.20317807793617249, 0.20123320817947388, 0.19788868725299835, 0.1936504989862442]}}
{"id": "WebQTest-1902", "question": "what year was hitler was born", "prediction": [["people.person.place_of_birth"], ["military.military_person.participated_in_conflicts"], ["base.activism.activist.area_of_activism", "time.event.locations"], ["base.activism.activist.area_of_activism"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was hitler was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> time.event.locations </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.01139913871884346, -0.03749806806445122, -0.043869491666555405, -0.05489572882652283, -0.05887722596526146], "norm_scores": [0.20604293048381805, 0.20073500275611877, 0.19946011900901794, 0.1972728818655014, 0.19648900628089905]}}
{"id": "WebQTest-1904", "question": "what 's there to see in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["organization.organization.geographic_scope"], ["geography.lake.cities"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's there to see in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization.geographic_scope </PATH>", "<PATH> geography.lake.cities </PATH>"], "scores": [-0.01591106317937374, -0.017174972221255302, -0.02365642413496971, -0.11866762489080429, -0.12899412214756012], "norm_scores": [0.20892333984375, 0.20865945518016815, 0.20731139183044434, 0.18852126598358154, 0.18658453226089478]}}
{"id": "WebQTest-1905", "question": "where was auschwitz concentration camp located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["time.event.locations"], ["base.locations.continents.countries_within"], ["location.location.events"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was auschwitz concentration camp located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.002009026240557432, -0.06489510089159012, -0.08822114765644073, -0.09461335837841034, -0.10055370628833771], "norm_scores": [0.21394219994544983, 0.2009025365114212, 0.19627051055431366, 0.1950199007987976, 0.1938648521900177]}}
{"id": "WebQTest-1907", "question": "what kind of currency does dominican republic use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"], ["location.location.containedby"], ["location.country. currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does dominican republic use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country. currency_used </PATH>"], "scores": [-8.153462113114074e-05, -0.10546283423900604, -0.13138794898986816, -0.17817777395248413, -0.18492856621742249], "norm_scores": [0.22497068345546722, 0.20246942341327667, 0.19728784263134003, 0.18826939165592194, 0.18700271844863892]}}
{"id": "WebQTest-1909", "question": "what is spoken in czech republic", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["film.film.country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is spoken in czech republic [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> film.film.country </PATH>"], "scores": [-0.012301353737711906, -0.018311161547899246, -0.04420414939522743, -0.0713072195649147, -0.17494304478168488], "norm_scores": [0.21029582619667053, 0.20903578400611877, 0.20369268953800201, 0.19824615120887756, 0.17872950434684753]}}
{"id": "WebQTest-1910", "question": "where was shakespeare born at", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was shakespeare born at [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.012729015201330185, -0.014009351842105389, -0.08583346754312515, -0.10192019492387772, -0.10218849778175354], "norm_scores": [0.21020236611366272, 0.20993341505527496, 0.19538389146327972, 0.19226594269275665, 0.19221436977386475]}}
{"id": "WebQTest-1911", "question": "where did scott fitzgerald go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did scott fitzgerald go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.013706170953810215, -0.015012931078672409, -0.026505514979362488, -0.04773785546422005, -0.08035807311534882], "norm_scores": [0.20458106696605682, 0.20431391894817352, 0.20197924971580505, 0.1977359652519226, 0.19138985872268677]}}
{"id": "WebQTest-1912", "question": "what did joan crawford died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"], ["people.person.cause_of_death"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joan crawford died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.002564574358984828, -0.03539853170514107, -0.12070892006158829, -0.13240791857242584, -0.1571698635816574], "norm_scores": [0.21780546009540558, 0.21077017486095428, 0.19353491067886353, 0.1912839412689209, 0.1866055428981781]}}
{"id": "WebQTest-1913", "question": "where did rob kardashian go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rob kardashian go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.014017692767083645, -0.018472211435437202, -0.027415625751018524, -0.0323907807469368, -0.05765852332115173], "norm_scores": [0.20319673418998718, 0.20229358971118927, 0.2004924714565277, 0.19949746131896973, 0.1945197880268097]}}
{"id": "WebQTest-1914", "question": "what year was allen iverson mvp", "prediction": [["award.award_honor.award_winner", "award.award_honor.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["award.award_nomination.award_nominee", "award.award_nomination.season"], ["award.award_winner.awards", "award.award_winning_work.awards_won"], ["award.award_winner.awards", "award.award_honor.season"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was allen iverson mvp [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.season </PATH>", "<PATH> award.award_winner.awards <SEP> award.award_winning_work.awards_won </PATH>", "<PATH> award.award_winner.awards <SEP> award.award_honor.season </PATH>"], "scores": [-0.028157785534858704, -0.03013010136783123, -0.03080846183001995, -0.03369038552045822, -0.04279503971338272], "norm_scores": [0.20099152624607086, 0.20059548318386078, 0.20045948028564453, 0.19988258183002472, 0.19807100296020508]}}
{"id": "WebQTest-1915", "question": "where did harry s truman go to school", "prediction": [["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harry s truman go to school [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01089611928910017, -0.026561357080936432, -0.033025652170181274, -0.033260446041822433, -0.044459398835897446], "norm_scores": [0.2037719041109085, 0.200604647397995, 0.19931206107139587, 0.199265256524086, 0.19704614579677582]}}
{"id": "WebQTest-1916", "question": "what county is utica ny in", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.us_county.county_seat"], ["location.location.contains"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is utica ny in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.022032620385289192, -0.0240355022251606, -0.024113690480589867, -0.044130589812994, -0.0653415396809578], "norm_scores": [0.2027706503868103, 0.20236492156982422, 0.20234911143779755, 0.19833897054195404, 0.1941763162612915]}}
{"id": "WebQTest-1917", "question": "what did charles babbage discover", "prediction": [["law.invention.inventor"], ["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"], ["law.inventor.inventions"], ["base.argumentmaps.original_idea.innovator"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did charles babbage discover [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>"], "scores": [-0.0030675625894218683, -0.0535324402153492, -0.06043520197272301, -0.062054093927145004, -0.07961422204971313], "norm_scores": [0.20990483462810516, 0.19957485795021057, 0.198201984167099, 0.19788138568401337, 0.19443689286708832]}}
{"id": "WebQTest-1919", "question": "who plays king julian madagascar", "prediction": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays king julian madagascar [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.029604723677039146, -0.03502601012587547, -0.038015685975551605, -0.04046647250652313, -0.040562115609645844], "norm_scores": [0.20142945647239685, 0.2003403902053833, 0.1997423619031906, 0.19925342500209808, 0.19923436641693115]}}
{"id": "WebQTest-1920", "question": "who is rick scott", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_holder"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"], ["people.profession.people_with_this_profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is rick scott [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.0017519256798550487, -0.057785362005233765, -0.0653509870171547, -0.0738023966550827, -0.0779433622956276], "norm_scores": [0.21092544496059418, 0.19943159818649292, 0.19792847335338593, 0.19626273214817047, 0.19545170664787292]}}
{"id": "WebQTest-1921", "question": "where did the crimean war take place", "prediction": [["time.event.locations"], ["location.location.events"], ["book.written_work.subjects"], ["base.culturalevent.event.entity_involved"], ["book.book_subject.works"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the crimean war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.009482483379542828, -0.019985204562544823, -0.093100406229496, -0.10457665473222733, -0.11865353584289551], "norm_scores": [0.21208012104034424, 0.20986436307430267, 0.1950676143169403, 0.1928417682647705, 0.19014616310596466]}}
{"id": "WebQTest-1922", "question": "what offices did theodore roosevelt hold", "prediction": [["business.employment_tenure.person", "business.employment_tenure.title"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["common.topic.notable_types", "type.type.expected_by"], ["government.government_position_held.office_holder", "government.government_position_held.basic_title"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "ground_paths": [["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["government.government_position_held.appointed_by", "government.government_position_held.basic_title"], ["government.government_position_held.office_holder", "government.government_office_category.officeholders"], ["common.topic.notable_types", "type.type.expected_by"], ["business.employment_tenure.person", "business.employment_tenure.title"], ["government.government_position_held.office_holder", "government.government_position_held.basic_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what offices did theodore roosevelt hold [/INST]", "raw_output": {"paths": ["<PATH> business.employment_tenure.person <SEP> business.employment_tenure.title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.basic_title </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.basic_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.016354748979210854, -0.018098343163728714, -0.02742834761738777, -0.030746543779969215, -0.0436444915831089], "norm_scores": [0.2021821141242981, 0.20182989537715912, 0.19995558261871338, 0.19929318130016327, 0.19673922657966614]}}
{"id": "WebQTest-1923", "question": "what movies has miley cyrus been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["music.artist.genre", "music.album.genre"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["music.artist.genre", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["people.person.parents", "film.film.produced_by"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["people.person.children", "film.producer.film"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["people.person.children", "film.film.produced_by"], ["film.film_featured_song.performed_by", "film.film.featured_song"], ["music.recording_contribution.contributor", "music.recording_contribution.album"], ["people.person.children", "film.film.executive_produced_by"], ["music.concert_tour.artist", "music.album.supporting_tours"], ["people.person.languages", "film.film.language"], ["music.artist.genre", "music.album.genre"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["film.film.music"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.languages", "tv.tv_program.languages"], ["film.actor.film", "film.film.starring"], ["music.artist.concert_tours", "music.album.supporting_tours"], ["music.lyricist.lyrics_written", "film.film.featured_song"], ["music.artist.origin", "film.film.featured_film_locations"], ["music.artist.album", "media_common.adaptation.adapted_from"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has miley cyrus been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> music.artist.genre <SEP> music.album.genre </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.027884773910045624, -0.02932059019804001, -0.037369921803474426, -0.04301275312900543, -0.04352499544620514], "norm_scores": [0.20167014002799988, 0.20138077437877655, 0.19976630806922913, 0.19864223897457123, 0.19854050874710083]}}
{"id": "WebQTest-1925", "question": "who did james franco play in milk", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.performance.character"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did james franco play in milk [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>"], "scores": [-0.019102007150650024, -0.027069713920354843, -0.03298521414399147, -0.038223110139369965, -0.041596945375204086], "norm_scores": [0.2025483250617981, 0.20094089210033417, 0.1997557282447815, 0.19871217012405396, 0.1980428695678711]}}
{"id": "WebQTest-1926", "question": "when did the libyan conflict start", "prediction": [["time.event.locations"], ["time.event.includes_event"], ["time.event.included_in_event"], ["base.culturalevent.event.entity_involved"], ["location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the libyan conflict start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.010632426477968693, -0.025993552058935165, -0.03709052875638008, -0.08657708764076233, -0.10193577408790588], "norm_scores": [0.20840919017791748, 0.20523224771022797, 0.20296737551689148, 0.19316770136356354, 0.1902235746383667]}}
{"id": "WebQTest-1928", "question": "what wnba team is diana taurasi on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team.sport"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wnba team is diana taurasi on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team.sport </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.0191657654941082, -0.025340411812067032, -0.028922179713845253, -0.031639039516448975, -0.034629181027412415], "norm_scores": [0.20175953209400177, 0.20051757991313934, 0.19980067014694214, 0.19925856590270996, 0.1986636370420456]}}
{"id": "WebQTest-1929", "question": "what do see in paris", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["film.film_location.featured_in_films"]], "ground_paths": [["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do see in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.arthist.helynevek.intezmeny </PATH>", "<PATH> film.film_location.featured_in_films </PATH>"], "scores": [-0.012661435641348362, -0.016684576869010925, -0.040365323424339294, -0.11023680120706558, -0.11360322684049606], "norm_scores": [0.20921897888183594, 0.20837895572185516, 0.2035023421049118, 0.1897687166929245, 0.189130961894989]}}
{"id": "WebQTest-1932", "question": "who made the laws in canada", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.government"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.governmental_body.jurisdiction"]], "ground_paths": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_body.jurisdiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who made the laws in canada [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_body.jurisdiction </PATH>"], "scores": [-0.015368582680821419, -0.030785439535975456, -0.04069218784570694, -0.043647125363349915, -0.04748062416911125], "norm_scores": [0.20407283306121826, 0.20095080137252808, 0.19896984100341797, 0.19838277995586395, 0.19762374460697174]}}
{"id": "WebQTest-1933", "question": "what year did adam morrison get drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did adam morrison get drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.019594933837652206, -0.022104043513536453, -0.02220100909471512, -0.02299136482179165, -0.03584643825888634], "norm_scores": [0.20098966360092163, 0.2004859894514084, 0.20046654343605042, 0.200308158993721, 0.19774967432022095]}}
{"id": "WebQTest-1934", "question": "where michael jackson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"], ["symbols.namesake.named_after"], ["people.person.nationality"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where michael jackson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.00589648075401783, -0.03517916053533554, -0.04790506139397621, -0.11643403023481369, -0.1205236166715622], "norm_scores": [0.21199668943881989, 0.2058788537979126, 0.20327547192573547, 0.1898118108510971, 0.18903714418411255]}}
{"id": "WebQTest-1935", "question": "what college did maya moore go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did maya moore go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.012919541448354721, -0.020992308855056763, -0.0248006172478199, -0.05473152920603752, -0.05992872267961502], "norm_scores": [0.20436200499534607, 0.20271886885166168, 0.20194832980632782, 0.19599339365959167, 0.19497741758823395]}}
{"id": "WebQTest-1936", "question": "when was the last time the dodgers won a championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.mascot.team"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dodgers won a championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.mascot.team </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.00975592527538538, -0.013608662411570549, -0.0998123362660408, -0.11680006235837936, -0.1219024807214737], "norm_scores": [0.21265584230422974, 0.21183811128139496, 0.19434185326099396, 0.19106830656528473, 0.19009588658809662]}}
{"id": "WebQTest-1937", "question": "who is captain james cook facts", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["fictional_universe.fictional_character.occupation"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is captain james cook facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0017584481975063682, -0.06420186907052994, -0.07656731456518173, -0.08464012295007706, -0.09221231937408447], "norm_scores": [0.21270424127578735, 0.19982846081256866, 0.19737271964550018, 0.1957857608795166, 0.19430884718894958]}}
{"id": "WebQTest-1938", "question": "where did robin williams get married", "prediction": [["people.person.children", "people.person.place_of_birth"], ["people.marriage.spouse", "people.marriage.location_of_ceremony"], ["people.person.nationality", "location.location.containedby"], ["people.person.spouse_s", "people.marriage.location_of_ceremony"], ["people.person.parents", "people.person.place_of_birth"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.spouse_s", "people.marriage.location_of_ceremony"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin williams get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.location_of_ceremony </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.location_of_ceremony </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.02346460334956646, -0.026340952143073082, -0.028365112841129303, -0.03610773757100105, -0.04737956076860428], "norm_scores": [0.20177382230758667, 0.20119430124759674, 0.20078745484352112, 0.1992388367652893, 0.19700565934181213]}}
{"id": "WebQTest-1939", "question": "what form of government does canada", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"], ["form_of_government.countries"], ["base.locations.countries.continent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does canada [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> form_of_government.countries </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.008228611201047897, -0.01750040240585804, -0.16827034950256348, -0.16852813959121704, -0.18036898970603943], "norm_scores": [0.220428004860878, 0.21839368343353271, 0.18782849609851837, 0.18778008222579956, 0.18556973338127136]}}
{"id": "WebQTest-1940", "question": "who do islamic people follow", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["base.argumentmaps.intellectual_dispute.positions", "book.book_subject.works"], ["religion.belief.belief_of"], ["people.person.religion"]], "ground_paths": [["religion.deity.deity_of"], ["religion.religion.deities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people follow [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions <SEP> book.book_subject.works </PATH>", "<PATH> religion.belief.belief_of </PATH>", "<PATH> people.person.religion </PATH>"], "scores": [-0.005579182878136635, -0.031777750700712204, -0.06689982116222382, -0.07149438560009003, -0.07312078773975372], "norm_scores": [0.2089618444442749, 0.20355843007564545, 0.19653314352035522, 0.1956322342157364, 0.19531431794166565]}}
{"id": "WebQTest-1941", "question": "what kind of government does peru", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.main_country", "location.country.form_of_government"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does peru [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.009735633619129658, -0.015380998142063618, -0.13423733413219452, -0.14284242689609528, -0.14899513125419617], "norm_scores": [0.21632467210292816, 0.21510687470436096, 0.1910010129213333, 0.1893644779920578, 0.18820296227931976]}}
{"id": "WebQTest-1942", "question": "who was in the israeli palestinian conflict", "prediction": [["time.event.people_involved"], ["people.person.nationality"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"], ["people.person.ethnicity"]], "ground_paths": [["time.event.people_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was in the israeli palestinian conflict [/INST]", "raw_output": {"paths": ["<PATH> time.event.people_involved </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.0005748701514676213, -0.07248920947313309, -0.09900142252445221, -0.1032942533493042, -0.11198235303163528], "norm_scores": [0.2158043384552002, 0.20082980394363403, 0.19557532668113708, 0.19473755359649658, 0.1930529773235321]}}
{"id": "WebQTest-1943", "question": "who founded the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.spun_off_from"], ["organization.organization.geographic_scope", "people.person.nationality"], ["organization.organization.geographic_scope", "people.person.place_of_birth"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.spun_off_from </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.0014263230841606855, -0.05193806067109108, -0.1060057058930397, -0.11520494520664215, -0.13625718653202057], "norm_scores": [0.21655495464801788, 0.2058880478143692, 0.19505177438259125, 0.1932656615972519, 0.1892395168542862]}}
{"id": "WebQTest-1944", "question": "where did diego velazquez die", "prediction": [["people.deceased_person.place_of_death"], ["symbols.namesake.named_after", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did diego velazquez die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> symbols.namesake.named_after <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.00024405097065027803, -0.09920527786016464, -0.10401375591754913, -0.10975827276706696, -0.11258752644062042], "norm_scores": [0.21752287447452545, 0.19702740013599396, 0.19608227908611298, 0.19495908915996552, 0.19440829753875732]}}
{"id": "WebQTest-1946", "question": "what is the country code for mexico", "prediction": [["location.country.internet_tld"], ["location.location.containedby"], ["location.country.languages_spoken"], ["location.location.contains"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.internet_tld"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the country code for mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.internet_tld </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.01666182279586792, -0.029920756816864014, -0.03406590595841408, -0.04657163470983505, -0.060589779168367386], "norm_scores": [0.20420119166374207, 0.20151157677173615, 0.2006780058145523, 0.19818399846553802, 0.19542522728443146]}}
{"id": "WebQTest-1947", "question": "what currency do they use in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency do they use in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.travel_destination.tour_operators </PATH>", "<PATH> travel.travel_destination.tour_operators <SEP> travel.tour_operator.travel_destinations </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.004480903502553701, -0.029196523129940033, -0.13797539472579956, -0.1565273255109787, -0.15728795528411865], "norm_scores": [0.2189207226037979, 0.2135762870311737, 0.1915627121925354, 0.18804161250591278, 0.1878986358642578]}}
{"id": "WebQTest-1950", "question": "where is president kennedy buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"], ["people.person.place_of_burial"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is president kennedy buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>", "<PATH> people.person.place_of_burial </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.004929060582071543, -0.02888934314250946, -0.07539787143468857, -0.09384950995445251, -0.1025153398513794], "norm_scores": [0.21140649914741516, 0.20640133321285248, 0.1970217227935791, 0.1934196799993515, 0.19175077974796295]}}
{"id": "WebQTest-1952", "question": "where was the prophet mohammed from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"], ["people.person.nationality"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the prophet mohammed from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.004416425246745348, -0.028847483918070793, -0.09462620317935944, -0.10904770344495773, -0.11083880066871643], "norm_scores": [0.21325115859508514, 0.20810432732105255, 0.19485600292682648, 0.19206605851650238, 0.19172236323356628]}}
{"id": "WebQTest-1954", "question": "what language do ukrainian people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do ukrainian people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.008998614735901356, -0.018182506784796715, -0.07277125865221024, -0.08547454327344894, -0.1482493132352829], "norm_scores": [0.2116195261478424, 0.20968492329120636, 0.19854530692100525, 0.1960390955209732, 0.184111088514328]}}
{"id": "WebQTest-1955", "question": "who are the judges appointed in the supreme court", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"], ["book.written_work.subjects", "book.written_work.author"], ["government.governmental_jurisdiction.government_bodies", "people.person.nationality"], ["book.book_subject.works", "book.author.works_written"]], "ground_paths": [["book.written_work.subjects", "book.author.works_written"], ["law.court.jurisdiction", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the judges appointed in the supreme court [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.author.works_written </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>", "<PATH> government.governmental_jurisdiction.government_bodies <SEP> people.person.nationality </PATH>", "<PATH> book.book_subject.works <SEP> book.author.works_written </PATH>"], "scores": [-0.013441719114780426, -0.02184046059846878, -0.029653556644916534, -0.04919173941016197, -0.053189247846603394], "norm_scores": [0.2040204554796219, 0.2023141086101532, 0.20073959231376648, 0.1968555599451065, 0.19607019424438477]}}
{"id": "WebQTest-1956", "question": "which country in europe has the largest land area", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in europe has the largest land area [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.014029904268682003, -0.027657143771648407, -0.028763240203261375, -0.04487740993499756, -0.06813789159059525], "norm_scores": [0.20454946160316467, 0.20178091526031494, 0.20155787467956543, 0.19833596050739288, 0.19377580285072327]}}
{"id": "WebQTest-1957", "question": "when did kaley cuoco join charmed", "prediction": [["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.seasons"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.seasons"], ["tv.regular_tv_appearance.series", "tv.tv_series_season.regular_cast"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kaley cuoco join charmed [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.seasons </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_series_season.regular_cast </PATH>"], "scores": [-0.025828097015619278, -0.026514438912272453, -0.028878062963485718, -0.031175285577774048, -0.03830018639564514], "norm_scores": [0.2008620649576187, 0.200724259018898, 0.20025037229061127, 0.19979088008403778, 0.19837245345115662]}}
{"id": "WebQTest-1958", "question": "where is chris rock from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"], ["music.artist.origin"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris rock from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.008205566555261612, -0.021617306396365166, -0.08028701692819595, -0.11559753865003586, -0.12295587360858917], "norm_scores": [0.21245428919792175, 0.20962391793727875, 0.19767917692661285, 0.1908208131790161, 0.1894218474626541]}}
{"id": "WebQTest-1960", "question": "when was the last time the toronto maple leafs win the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion", "sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs win the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_championship_event.champion </PATH>"], "scores": [-0.009305406361818314, -0.01199938915669918, -0.07283853739500046, -0.08055726438760757, -0.12432263791561127], "norm_scores": [0.21015772223472595, 0.20959234237670898, 0.1972210705280304, 0.1957046091556549, 0.18732425570487976]}}
{"id": "WebQTest-1961", "question": "what kind of language do egyptian speak", "prediction": [["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.main_country"], ["people.ethnicity.languages_spoken", "common.topic.notable_types", "common.topic.notable_types"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do egyptian speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>"], "scores": [-0.034599561244249344, -0.03654882311820984, -0.03668332099914551, -0.038923393934965134, -0.04045595973730087], "norm_scores": [0.20056892931461334, 0.2001783400774002, 0.2001514434814453, 0.19970358908176422, 0.1993977427482605]}}
{"id": "WebQTest-1962", "question": "what language do brazilians use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do brazilians use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.001392557518556714, -0.06144259124994278, -0.07235750555992126, -0.07836437970399857, -0.09460252523422241], "norm_scores": [0.2123083621263504, 0.1999344825744629, 0.1977640837430954, 0.1965796947479248, 0.1934133768081665]}}
{"id": "WebQTest-1963", "question": "what position did vince lombardi play", "prediction": [["american_football.football_player.position_s"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_coach.sports_coached"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_member.position_s"], ["sports.sports_position.sport", "sports.sports_team_coach.sports_coached", "american_football.football_player.position_s"], ["sports.pro_athlete.teams", "sports.sports_team_roster.position"]], "ground_paths": [["american_football.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did vince lombardi play [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_member.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team_coach.sports_coached <SEP> american_football.football_player.position_s </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.010131597518920898, -0.02934974431991577, -0.04553232342004776, -0.04858456179499626, -0.05850180983543396], "norm_scores": [0.20570875704288483, 0.2017931491136551, 0.1985539048910141, 0.19794879853725433, 0.19599539041519165]}}
{"id": "WebQTest-1964", "question": "who played stanley kowalski in a streetcar named desire movie", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["film.performance.film", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["theater.play.country_of_origin", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.performance.character", "film.actor.film"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["film.film.starring", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played stanley kowalski in a streetcar named desire movie [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.02813829481601715, -0.03366848826408386, -0.03491025045514107, -0.035903289914131165, -0.03704558685421944], "norm_scores": [0.20116136968135834, 0.20005197823047638, 0.19980372488498688, 0.19960540533065796, 0.19937752187252045]}}
{"id": "WebQTest-1965", "question": "who did the voice for lola bunny", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice for lola bunny [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.01559398416429758, -0.02758428268134594, -0.030786186456680298, -0.03315456211566925, -0.04296170175075531], "norm_scores": [0.2028973549604416, 0.20047909021377563, 0.19983820617198944, 0.19936546683311462, 0.19741982221603394]}}
{"id": "WebQTest-1966", "question": "what book did charles darwin wrote in 1859", "prediction": [["book.written_work.author"], ["book.written_work.subjects"], ["common.topic.notable_types"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["book.written_work.author"], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did charles darwin wrote in 1859 [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.001536994008347392, -0.06534575670957565, -0.067466139793396, -0.06784460693597794, -0.09031876921653748], "norm_scores": [0.21162812411785126, 0.19854620099067688, 0.1981256604194641, 0.19805069267749786, 0.1936493217945099]}}
{"id": "WebQTest-1967", "question": "what language do serbs speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.person.ethnicity", "people.person.nationality", "location.country.languages_spoken"]], "ground_paths": [["people.ethnicity.languages_spoken"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do serbs speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.008538908325135708, -0.031167849898338318, -0.0479864701628685, -0.051505666226148605, -0.06569428741931915], "norm_scores": [0.20655454695224762, 0.20193292200565338, 0.19856508076190948, 0.1978675276041031, 0.19507989287376404]}}
{"id": "WebQTest-1968", "question": "what was the cause of death for laci peterson", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"], ["people.person.cause_of_death"], ["people.cause_of_death.people", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for laci peterson [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.002471298910677433, -0.03420760855078697, -0.09968079626560211, -0.10150697827339172, -0.10815399140119553], "norm_scores": [0.21360455453395844, 0.2069319784641266, 0.19381749629974365, 0.19346386194229126, 0.19218216836452484]}}
{"id": "WebQTest-1969", "question": "what team is kris humphries play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is kris humphries play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.020907841622829437, -0.020978383719921112, -0.021771172061562538, -0.029586216434836388, -0.03655366599559784], "norm_scores": [0.20100899040699005, 0.2009948194026947, 0.20083554089069366, 0.19927209615707397, 0.19788852334022522]}}
{"id": "WebQTest-1971", "question": "what movies gerard butler has been in", "prediction": [["film.performance.actor", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["film.film.produced_by"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.producer.film"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.film.produced_by", "film.film.sequel"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["music.recording.artist"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.producer.films_executive_produced"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies gerard butler has been in [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.029903285205364227, -0.03128775209188461, -0.032532840967178345, -0.03262315317988396, -0.04088931530714035], "norm_scores": [0.2007085680961609, 0.20043088495731354, 0.2001814842224121, 0.20016340911388397, 0.1985156387090683]}}
{"id": "WebQTest-1972", "question": "where did john franklin live", "prediction": [["people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john franklin live [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.023579539731144905, -0.034331947565078735, -0.037433333694934845, -0.041709430515766144, -0.05140966922044754], "norm_scores": [0.20283424854278564, 0.20066498219966888, 0.20004358887672424, 0.19919002056121826, 0.19726717472076416]}}
{"id": "WebQTest-1975", "question": "what was the last movie sean connery was in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["film.film.produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the last movie sean connery was in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.01581699401140213, -0.03130510821938515, -0.03458894416689873, -0.03476346284151077, -0.03580831363797188], "norm_scores": [0.20294375717639923, 0.1998247504234314, 0.19916965067386627, 0.19913490116596222, 0.19892694056034088]}}
{"id": "WebQTest-1976", "question": "what team did drogba play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["people.person.place_of_birth", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did drogba play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.026341227814555168, -0.03028378263115883, -0.030899303033947945, -0.03207263723015785, -0.03803020715713501], "norm_scores": [0.20103809237480164, 0.20024704933166504, 0.2001238316297531, 0.1998891532421112, 0.19870184361934662]}}
{"id": "WebQTest-1977", "question": "where did crabtree go to college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did crabtree go to college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.017000675201416016, -0.01750500313937664, -0.028262494131922722, -0.03402497619390488, -0.047115083783864975], "norm_scores": [0.20235739648342133, 0.20225538313388824, 0.20009127259254456, 0.19894157350063324, 0.19635435938835144]}}
{"id": "WebQTest-1978", "question": "where does freddie ljungberg come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"], ["people.ethnicity.people"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does freddie ljungberg come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.0047887652181088924, -0.03858214244246483, -0.03864065930247307, -0.09261798113584518, -0.11093976348638535], "norm_scores": [0.21058429777622223, 0.20358684659004211, 0.20357494056224823, 0.19287779927253723, 0.1893761307001114]}}
{"id": "WebQTest-1979", "question": "who is number 22 on miami dolphins", "prediction": [["american_football.football_player.position_s", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["american_football.football_player.position_s"], ["american_football.football_player.position_s", "people.person.gender", "people.person.gender"], ["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is number 22 on miami dolphins [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.player </PATH>", "<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.football_player.position_s <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.drafted_athlete.drafted </PATH>"], "scores": [-0.020638931542634964, -0.026134323328733444, -0.027139030396938324, -0.032675694674253464, -0.035159144550561905], "norm_scores": [0.20154541730880737, 0.2004408836364746, 0.20023959875106812, 0.1991340070962906, 0.19864007830619812]}}
{"id": "WebQTest-1981", "question": "what state did john adams live in", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["location.location.people_born_here", "location.location.containedby"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["people.person.nationality", "location.administrative_division.first_level_division_of"]], "ground_paths": [["people.deceased_person.place_of_burial", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["tv.tv_program.country_of_origin", "base.locations.countries.states_provinces_within"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["people.person.place_of_birth", "location.location.containedby"], ["symbols.name_source.namesakes", "base.usnationalparks.us_national_park.state"], ["symbols.name_source.namesakes", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state did john adams live in [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.02498309314250946, -0.03360319882631302, -0.04093649983406067, -0.042362093925476074, -0.044516436755657196], "norm_scores": [0.20246942341327667, 0.20073160529136658, 0.19926497340202332, 0.19898110628128052, 0.19855289161205292]}}
{"id": "WebQTest-1982", "question": "what type of music did vivaldi compose", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"], ["music.composer.genre"], ["music.album.genre"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did vivaldi compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> music.composer.genre </PATH>", "<PATH> music.album.genre </PATH>"], "scores": [-0.0027057151310145855, -0.039916250854730606, -0.10838060826063156, -0.11192315816879272, -0.12633606791496277], "norm_scores": [0.2153579294681549, 0.20749160647392273, 0.1937612146139145, 0.1930760145187378, 0.1903131902217865]}}
{"id": "WebQTest-1985", "question": "what team does mike fisher play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does mike fisher play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.014773066155612469, -0.017098471522331238, -0.02367113158106804, -0.04860249534249306, -0.04971500113606453], "norm_scores": [0.2032017856836319, 0.20272980630397797, 0.2014017105102539, 0.19644255936145782, 0.1962241381406784]}}
{"id": "WebQTest-1986", "question": "what films has daniel craig been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.film.executive_produced_by"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has daniel craig been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.020502159371972084, -0.03413773700594902, -0.036491867154836655, -0.03831106051802635, -0.03910164162516594], "norm_scores": [0.2026541382074356, 0.19990958273410797, 0.19943952560424805, 0.1990770399570465, 0.19891971349716187]}}
{"id": "WebQTest-1987", "question": "what materials did eduardo paolozzi use in his work", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.art_forms", "visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists", "visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artist", "visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.artworks", "visual_art.artwork.media"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what materials did eduardo paolozzi use in his work [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.art_forms <SEP> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists <SEP> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artist <SEP> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.002484450815245509, -0.034002356231212616, -0.045933257788419724, -0.07300636917352676, -0.07508391886949539], "norm_scores": [0.20884083211421967, 0.20236125588417053, 0.19996124505996704, 0.194620281457901, 0.19421637058258057]}}
{"id": "WebQTest-1988", "question": "what did joe biden study in college", "prediction": [["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"]], "ground_paths": [["education.education.student", "education.education.major_field_of_study"], ["common.topic.subjects", "book.written_work.school_or_movement"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joe biden study in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>"], "scores": [-0.015563249588012695, -0.01679072342813015, -0.028317706659436226, -0.042250096797943115, -0.05922910198569298], "norm_scores": [0.20337438583374023, 0.20312492549419403, 0.2007969468832016, 0.19801874458789825, 0.1946849673986435]}}
{"id": "WebQTest-1990", "question": "who plays timon 's voice in the lion king", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["music.recording.releases", "music.featured_artist.recordings"], ["music.recording.releases", "music.artist.track"], ["film.film.featured_song", "music.recording.artist"], ["film.film.country", "people.person.nationality"], ["film.film.language", "people.person.languages"], ["film.film.starring", "film.actor.film"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays timon 's voice in the lion king [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.028899217024445534, -0.029024001210927963, -0.03083859570324421, -0.036656253039836884, -0.03862457349896431], "norm_scores": [0.20078173279762268, 0.20075668394565582, 0.2003927230834961, 0.19923029839992523, 0.19883853197097778]}}
{"id": "WebQTest-1991", "question": "what part of the country is ohio in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.administrative_division.country"], ["base.locations.states_and_provences.country"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.locations.states_and_provences.country </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.012052745558321476, -0.018367931246757507, -0.0771278515458107, -0.07740257680416107, -0.08084867894649506], "norm_scores": [0.20829173922538757, 0.20698046684265137, 0.19516874849796295, 0.1951151341199875, 0.19444391131401062]}}
{"id": "WebQTest-1992", "question": "which island is kailua on", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.island.island_group"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which island is kailua on [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.008765227161347866, -0.032688748091459274, -0.044716835021972656, -0.06709618866443634, -0.08053780347108841], "norm_scores": [0.20767882466316223, 0.20276936888694763, 0.20034505426883698, 0.19591127336025238, 0.19329550862312317]}}
{"id": "WebQTest-1993", "question": "what language do speak in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["language.language_family.geographic_distribution"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do speak in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.language_family.geographic_distribution </PATH>"], "scores": [-0.00717657245695591, -0.02821023389697075, -0.04979622736573219, -0.06019957736134529, -0.16637583076953888], "norm_scores": [0.2110307216644287, 0.20663833618164062, 0.20222564041614532, 0.20013272762298584, 0.1799725741147995]}}
{"id": "WebQTest-1994", "question": "what did whoopi goldberg won a grammy for", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nominee.award_nominations", "award.award_nomination.award"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_category.nominees"], ["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did whoopi goldberg won a grammy for [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_category.winners </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>"], "scores": [-0.016572749242186546, -0.021748993545770645, -0.0252127293497324, -0.03041590191423893, -0.03567083925008774], "norm_scores": [0.20187462866306305, 0.20083238184452057, 0.2001379430294037, 0.19909930229187012, 0.19805580377578735]}}
{"id": "WebQTest-1996", "question": "who played maggie in himym", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.place_of_birth", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played maggie in himym [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.01752283051609993, -0.03588578477501869, -0.03640422224998474, -0.039253875613212585, -0.04865967854857445], "norm_scores": [0.20362672209739685, 0.19992166757583618, 0.19981802999973297, 0.19924943149089813, 0.19738411903381348]}}
{"id": "WebQTest-1997", "question": "who does owen schmitt play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does owen schmitt play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.01958000659942627, -0.021707067266106606, -0.02769618295133114, -0.02822207286953926, -0.03398267552256584], "norm_scores": [0.20133332908153534, 0.20090554654598236, 0.19970588386058807, 0.19960089027881622, 0.1984543800354004]}}
{"id": "WebQTest-1998", "question": "what party was thomas jefferson affiliated with", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.politician.party", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"]], "ground_paths": [["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was thomas jefferson affiliated with [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>"], "scores": [-0.01871423050761223, -0.019017625600099564, -0.02653186209499836, -0.02995990961790085, -0.0442071408033371], "norm_scores": [0.20179370045661926, 0.20173250138759613, 0.20022229850292206, 0.19953709840774536, 0.19671441614627838]}}
{"id": "WebQTest-1999", "question": "what religions are popular in france", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["royalty.kingdom.rulers", "people.person.religion"], ["religion.religion.countries"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["periodicals.newspaper_circulation_area.newspapers", "book.periodical.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religions are popular in france [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>", "<PATH> religion.religion.countries </PATH>"], "scores": [-0.009582236409187317, -0.013019205071032047, -0.07320377230644226, -0.09121701866388321, -0.10608568787574768], "norm_scores": [0.2098836451768875, 0.20916353166103363, 0.19694644212722778, 0.19343054294586182, 0.1905757635831833]}}
{"id": "WebQTest-2000", "question": "who played berger in sex and the city", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["film.film.country", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played berger in sex and the city [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02329104393720627, -0.02340715378522873, -0.04712998494505882, -0.05290734022855759, -0.05748376622796059], "norm_scores": [0.20351965725421906, 0.2034960389137268, 0.19872532784938812, 0.19758053123950958, 0.19667838513851166]}}
{"id": "WebQTest-2001", "question": "who won the 2000 fa cup final", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["sports.sports_award.award", "sports.sports_award.award_winner"]], "ground_paths": [["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_award.award", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event", "sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the 2000 fa cup final [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_award.award <SEP> sports.sports_award.award_winner </PATH>"], "scores": [-0.019807472825050354, -0.02242347039282322, -0.024554837495088577, -0.029981117695569992, -0.03283318132162094], "norm_scores": [0.20122390985488892, 0.20069819688796997, 0.2002708911895752, 0.19918711483478546, 0.19861982762813568]}}
{"id": "WebQTest-2002", "question": "what are the names of michael jackson 's kids", "prediction": [["people.person.parents"], ["people.person.children"], ["music.recording.artist", "music.recording.artist"], ["music.composition.composer", "people.person.parents"], ["music.composition.composer"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>", "<PATH> music.composition.composer <SEP> people.person.parents </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.008134284056723118, -0.019362254068255424, -0.09258090704679489, -0.0941603034734726, -0.10156500339508057], "norm_scores": [0.2111385315656662, 0.2087811380624771, 0.1940406858921051, 0.19373445212841034, 0.19230520725250244]}}
{"id": "WebQTest-2003", "question": "what movies have robert pattinson in them", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["music.artist.track", "music.recording.releases"], ["music.recording.artist", "music.recording.releases"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.producer.films_executive_produced"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies have robert pattinson in them [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.honored_for </PATH>"], "scores": [-0.028856797143816948, -0.028900036588311195, -0.03098277933895588, -0.03311387449502945, -0.03441312536597252], "norm_scores": [0.2004793882369995, 0.2004707157611847, 0.2000536322593689, 0.19962775707244873, 0.19936855137348175]}}
{"id": "WebQTest-2004", "question": "what the time zone in japan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in", "location.location.time_zones"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in japan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country <SEP> location.location.time_zones </PATH>"], "scores": [-0.0004625064320862293, -0.0639491006731987, -0.1055942252278328, -0.1361667513847351, -0.1601564586162567], "norm_scores": [0.21909654140472412, 0.2056191861629486, 0.19723200798034668, 0.1912933737039566, 0.18675890564918518]}}
{"id": "WebQTest-2005", "question": "who plays bob kelso in scrubs", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program_creator.programs_created"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays bob kelso in scrubs [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>"], "scores": [-0.015841731801629066, -0.027465753257274628, -0.03342251852154732, -0.04011840000748634, -0.05185355991125107], "norm_scores": [0.20359712839126587, 0.20124420523643494, 0.20004902780056, 0.1987139880657196, 0.1963956654071808]}}
{"id": "WebQTest-2006", "question": "what race is vanessa carlton", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality"], ["people.person.nationality", "people.person.nationality"], ["people.person.nationality", "people.ethnicity.people"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what race is vanessa carlton [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> people.ethnicity.people </PATH>"], "scores": [-0.006495282053947449, -0.04184546694159508, -0.05256243422627449, -0.06429030746221542, -0.07129523158073425], "norm_scores": [0.20827502012252808, 0.2010410726070404, 0.19889803230762482, 0.1965790092945099, 0.19520679116249084]}}
{"id": "WebQTest-2007", "question": "what do people in czech republic speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.main_country"], ["base.rosetta.languoid.languoid_class"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in czech republic speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.languoid.languoid_class </PATH>"], "scores": [-0.012672374956309795, -0.01411930751055479, -0.06686202436685562, -0.06916598230600357, -0.15277665853500366], "norm_scores": [0.21007756888866425, 0.20977382361888885, 0.19899648427963257, 0.19853854179382324, 0.18261361122131348]}}
{"id": "WebQTest-2008", "question": "which countries speak arabic language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"], ["base.rosetta.local_name.locale"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak arabic language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> base.rosetta.local_name.locale </PATH>"], "scores": [-0.016856450587511063, -0.017301544547080994, -0.03305831924080849, -0.07962986826896667, -0.16478687524795532], "norm_scores": [0.20898069441318512, 0.20888769626617432, 0.2056220918893814, 0.19626551866531372, 0.18024399876594543]}}
{"id": "WebQTest-2009", "question": "where does tim cook work", "prediction": [["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["organization.leadership.person", "organization.organization.leadership"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim cook work [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.019850607961416245, -0.034853894263505936, -0.0412791408598423, -0.042022302746772766, -0.04415600746870041], "norm_scores": [0.20333601534366608, 0.20030808448791504, 0.19902516901493073, 0.19887731969356537, 0.19845344126224518]}}
{"id": "WebQTest-2010", "question": "what degrees does romney have", "prediction": [["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.degree"], ["common.topic.notable_types", "common.topic.notable_types"], ["common.topic.notable_types"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees does romney have [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.018285000696778297, -0.018482763320207596, -0.019516805186867714, -0.09273531287908554, -0.09338833391666412], "norm_scores": [0.2059956192970276, 0.2059548795223236, 0.2057420164346695, 0.19121617078781128, 0.1910913586616516]}}
{"id": "WebQTest-2011", "question": "where is bob marley buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"], ["people.person.place_of_burial"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is bob marley buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>", "<PATH> people.person.place_of_burial </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0036466519813984632, -0.031932760030031204, -0.08361145108938217, -0.10415367037057877, -0.11119335144758224], "norm_scores": [0.21287137269973755, 0.20693443715572357, 0.19651195406913757, 0.19251635670661926, 0.19116586446762085]}}
{"id": "WebQTest-2012", "question": "what is london uk time zone", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is london uk time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-4.3131829443154857e-05, -0.1035337895154953, -0.18894349038600922, -0.191166952252388, -0.2117176353931427], "norm_scores": [0.22910495102405548, 0.20658037066459656, 0.18966887891292572, 0.18924763798713684, 0.1853981465101242]}}
{"id": "WebQTest-2013", "question": "where to stay in chicago tourist", "prediction": [["location.location.containedby"], ["travel.travel_destination.accommodation"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["travel.travel_destination.tourist_accommodations"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where to stay in chicago tourist [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.travel_destination.tourist_accommodations </PATH>"], "scores": [-0.020908700302243233, -0.022276615723967552, -0.025274207815527916, -0.030125293880701065, -0.11408746987581253], "norm_scores": [0.2042427957057953, 0.20396360754966736, 0.20335310697555542, 0.20236903429031372, 0.18607144057750702]}}
{"id": "WebQTest-2014", "question": "what type of breast cancer did sheryl crow have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.parent_disease"], ["music.artist.genre", "music.artist.genre"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of breast cancer did sheryl crow have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.parent_disease </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.002725090365856886, -0.034535691142082214, -0.05238448455929756, -0.09709911793470383, -0.12450546771287918], "norm_scores": [0.21206527948379517, 0.20542553067207336, 0.201791450381279, 0.19296719133853912, 0.18775047361850739]}}
{"id": "WebQTest-2015", "question": "what type of government does argentina have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"], ["government.form_of_government.countries", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does argentina have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.008954044431447983, -0.015457265079021454, -0.13643386960029602, -0.14272713661193848, -0.1653088480234146], "norm_scores": [0.21720841526985168, 0.2158004343509674, 0.19121100008487701, 0.19001144170761108, 0.185768723487854]}}
{"id": "WebQTest-2016", "question": "where does fabio capello come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"], ["sports.sports_team_coach_tenure.coach", "sports.sports_team_coach_tenure.place_of_birth"], ["sports.sports_team_manager.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does fabio capello come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> sports.sports_team_coach_tenure.coach <SEP> sports.sports_team_coach_tenure.place_of_birth </PATH>", "<PATH> sports.sports_team_manager.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.005608499050140381, -0.03416914492845535, -0.04837660863995552, -0.08587031811475754, -0.08812444657087326], "norm_scores": [0.20948338508605957, 0.20358504354953766, 0.20071305334568024, 0.1933269202709198, 0.19289162755012512]}}
{"id": "WebQTest-2017", "question": "what party was abe lincoln part of", "prediction": [["organization.organization_founder.organizations_founded"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["government.politician.party", "government.political_party_tenure.party"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["influence.influence_node.influenced_by", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was abe lincoln part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party.politicians_in_this_party </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.013753039762377739, -0.020417528226971626, -0.025912292301654816, -0.04143168777227402, -0.04929715394973755], "norm_scores": [0.2032911628484726, 0.20194083452224731, 0.200834259390831, 0.19774149358272552, 0.19619227945804596]}}
{"id": "WebQTest-2018", "question": "what did jack london do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jack london do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.0012679198989644647, -0.05586152896285057, -0.0673101469874382, -0.09567490220069885, -0.11053363978862762], "norm_scores": [0.21324847638607025, 0.20191854238510132, 0.19962003827095032, 0.19403742253780365, 0.19117559492588043]}}
{"id": "WebQTest-2019", "question": "who did morgan freeman marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did morgan freeman marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.01875084638595581, -0.03582433611154556, -0.039970286190509796, -0.04296647384762764, -0.05000222474336624], "norm_scores": [0.2037746161222458, 0.20032501220703125, 0.19949619472026825, 0.19889934360980988, 0.19750486314296722]}}
{"id": "WebQTest-2020", "question": "what are the mountains in northern italy called", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.contains"], ["geography.mountain_range.mountains"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the mountains in northern italy called [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain_range.mountains </PATH>"], "scores": [-0.012902012094855309, -0.02901097573339939, -0.033846668899059296, -0.04313107579946518, -0.08996856212615967], "norm_scores": [0.20578907430171967, 0.202500581741333, 0.20152370631694794, 0.19966134428977966, 0.1905253380537033]}}
{"id": "WebQTest-2021", "question": "what influenced john steinbeck to start writing", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced john steinbeck to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.009609383530914783, -0.013462919741868973, -0.103995181620121, -0.1044808179140091, -0.13150428235530853], "norm_scores": [0.21272897720336914, 0.21191078424453735, 0.19356882572174072, 0.19347484409809113, 0.1883164942264557]}}
{"id": "WebQTest-2022", "question": "where is dwayne johnson ethnicity", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality"], ["people.ethnicity.people_with_this_ethnicity"], ["people.ethnicity.people", "people.ethnicity.people"]], "ground_paths": [["people.person.ethnicity"], ["people.ethnicity.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwayne johnson ethnicity [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.ethnicity.people_with_this_ethnicity </PATH>", "<PATH> people.ethnicity.people <SEP> people.ethnicity.people </PATH>"], "scores": [-0.016140596941113472, -0.016451822593808174, -0.04590171203017235, -0.049275919795036316, -0.05638677999377251], "norm_scores": [0.20415134727954865, 0.20408780872821808, 0.19816508889198303, 0.19749756157398224, 0.19609816372394562]}}
{"id": "WebQTest-2023", "question": "where are the nato headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.founders", "base.biblioness.bibs_location.country"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nato headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.00736817205324769, -0.04741793870925903, -0.05374056473374367, -0.05502496287226677, -0.061343830078840256], "norm_scores": [0.2076263576745987, 0.19947528839111328, 0.19821806252002716, 0.1979636251926422, 0.19671666622161865]}}
{"id": "WebQTest-2024", "question": "in which state was the battle of antietam fought", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["common.image.appears_in_topic_gallery", "time.event.locations"], ["common.image.appears_in_topic_gallery", "location.location.events"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which state was the battle of antietam fought [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> common.image.appears_in_topic_gallery <SEP> time.event.locations </PATH>", "<PATH> common.image.appears_in_topic_gallery <SEP> location.location.events </PATH>"], "scores": [-0.0013137680944055319, -0.05108340457081795, -0.11550875008106232, -0.1277095526456833, -0.1289309710264206], "norm_scores": [0.21715578436851501, 0.20661257207393646, 0.193721204996109, 0.19137199223041534, 0.1911383867263794]}}
{"id": "WebQTest-2025", "question": "which ocean does the zambezi river flow into", "prediction": [["geography.river.mouth"], ["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.containedby", "location.location.containedby"], ["geography.geographical_feature_category.features"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which ocean does the zambezi river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> geography.geographical_feature_category.features </PATH>"], "scores": [-0.00241183303296566, -0.05095556005835533, -0.06748855859041214, -0.07609368115663528, -0.08985167741775513], "norm_scores": [0.2111998349428177, 0.20119227468967438, 0.19789330661296844, 0.19619770348072052, 0.19351691007614136]}}
{"id": "WebQTest-2027", "question": "what team did david beckham play for before la galaxy", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player.loans", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for before la galaxy [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02335958182811737, -0.02444274351000786, -0.026261577382683754, -0.03097648173570633, -0.03206634894013405], "norm_scores": [0.20081277191638947, 0.20059536397457123, 0.20023085176944733, 0.1992889940738678, 0.19907192885875702]}}
{"id": "WebQTest-2028", "question": "who is the current leader of france 2010", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.gender", "people.person.gender"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["fictional_universe.fictional_character.occupation", "fictional_universe.fictional_character.occupation", "people.person.profession"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current leader of france 2010 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>"], "scores": [-0.000448806444182992, -0.07214326411485672, -0.07770726829767227, -0.07820603251457214, -0.08407357335090637], "norm_scores": [0.21270118653774261, 0.19798551499843597, 0.19688697159290314, 0.19678880274295807, 0.195637509226799]}}
{"id": "WebQTest-2029", "question": "where was the palace of knossos located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["location.location.contains"], ["architecture.structure.architect"], ["architecture.building.building_function"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the palace of knossos located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.building.building_function </PATH>"], "scores": [-0.0009235759498551488, -0.07599532604217529, -0.099607914686203, -0.10670160502195358, -0.11226381361484528], "norm_scores": [0.2160777449607849, 0.2004503309726715, 0.19577263295650482, 0.19438879191875458, 0.19331057369709015]}}
{"id": "WebQTest-2030", "question": "where is roswell area 51", "prediction": [["aviation.airport.serves"], ["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.nearby_airports"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is roswell area 51 [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.nearby_airports </PATH>"], "scores": [-0.004442277364432812, -0.03144552558660507, -0.08016017824411392, -0.10932392627000809, -0.11526191234588623], "norm_scores": [0.21294867992401123, 0.20727531611919403, 0.19741997122764587, 0.1917456090450287, 0.1906103938817978]}}
{"id": "WebQTest-2031", "question": "when was father chris riley born", "prediction": [["people.person.place_of_birth"], ["people.person.children"], ["people.person.parents"], ["people.person.place_of_birth", "location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was father chris riley born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0014420137740671635, -0.06650049984455109, -0.08940230309963226, -0.09162326157093048, -0.09882692247629166], "norm_scores": [0.2139594703912735, 0.20048274099826813, 0.19594348967075348, 0.19550879299640656, 0.19410547614097595]}}
