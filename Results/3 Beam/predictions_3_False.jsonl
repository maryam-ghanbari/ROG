{"id": "WebQTest-0", "question": "what does jamaican people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does jamaican people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.00816990528255701, -0.023363839834928513, -0.05129489675164223], "norm_scores": [0.33982253074645996, 0.33469831943511963, 0.3254791796207428]}}
{"id": "WebQTest-1", "question": "what did james k polk do before he was president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did james k polk do before he was president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.nationality <SEP> government.government_office_or_title.jurisdiction </PATH>"], "scores": [-0.019873684272170067, -0.022302331402897835, -0.027906494215130806], "norm_scores": [0.3344958424568176, 0.3336844742298126, 0.33181965351104736]}}
{"id": "WebQTest-3", "question": "who plays ken barlow in coronation street", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ken barlow in coronation street [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.017491154372692108, -0.026540065184235573, -0.03183753415942192], "norm_scores": [0.3359370827674866, 0.3329109251499176, 0.3311519920825958]}}
{"id": "WebQTest-6", "question": "where is jamarcus russell from", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jamarcus russell from [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.00779408635571599, -0.020439546555280685, -0.09174960106611252], "norm_scores": [0.34400832653045654, 0.3396855592727661, 0.31630608439445496]}}
{"id": "WebQTest-7", "question": "where was george washington carver from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was george washington carver from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.005141076631844044, -0.03688138723373413, -0.03965846076607704], "norm_scores": [0.3407352864742279, 0.3300900459289551, 0.32917463779449463]}}
{"id": "WebQTest-8", "question": "what else did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>"], "scores": [-0.0068801692686975, -0.02558380551636219, -0.06954336166381836], "norm_scores": [0.3423801362514496, 0.33603590726852417, 0.32158389687538147]}}
{"id": "WebQTest-9", "question": "who was richard nixon married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["people.person.profession", "common.topic.notable_types"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_ownership.owner"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was richard nixon married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.022445492446422577, -0.033713798969984055, -0.04662007838487625], "norm_scores": [0.3372783660888672, 0.33349913358688354, 0.32922255992889404]}}
{"id": "WebQTest-12", "question": "who is governor of ohio 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"]], "ground_paths": [["base.locations.states_and_provences.country", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is governor of ohio 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>"], "scores": [-0.016765866428613663, -0.02157958224415779, -0.032919175922870636], "norm_scores": [0.33566349744796753, 0.33405157923698425, 0.3302849531173706]}}
{"id": "WebQTest-13", "question": "who was vice president after kennedy died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president after kennedy died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.012162194587290287, -0.013368213549256325, -0.06646322458982468], "norm_scores": [0.33945006132125854, 0.3390409052371979, 0.32150909304618835]}}
{"id": "WebQTest-14", "question": "where is the fukushima daiichi nuclear plant located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.street_address", "location.mailing_address.citytown"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear plant located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.citytown </PATH>"], "scores": [-0.009434275329113007, -0.026383500546216965, -0.03923063725233078], "norm_scores": [0.33854272961616516, 0.3328530490398407, 0.32860419154167175]}}
{"id": "WebQTest-16", "question": "what countries are part of the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.022299576550722122, -0.024500343948602676, -0.03108403831720352], "norm_scores": [0.3345538377761841, 0.33381837606430054, 0.33162781596183777]}}
{"id": "WebQTest-19", "question": "what is my timezone in louisiana", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in louisiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.2840228009736165e-05, -0.15889142453670502, -0.1829555481672287], "norm_scores": [0.3723124563694, 0.3176197409629822, 0.310067743062973]}}
{"id": "WebQTest-20", "question": "what did st augustine do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st augustine do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0018865532474592328, -0.05468723177909851, -0.0672532245516777], "norm_scores": [0.3465852439403534, 0.3287600576877594, 0.3246546983718872]}}
{"id": "WebQTest-21", "question": "who is keyshia cole dad", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is keyshia cole dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.006339045707136393, -0.025842344388365746, -0.16772453486919403], "norm_scores": [0.35315102338790894, 0.3463301360607147, 0.30051881074905396]}}
{"id": "WebQTest-22", "question": "what kind government does egypt have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind government does egypt have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005506176967173815, -0.023704934865236282, -0.1394987553358078], "norm_scores": [0.35007113218307495, 0.34375789761543274, 0.3061710000038147]}}
{"id": "WebQTest-23", "question": "what town was martin luther king assassinated in", "prediction": [["people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.country"], ["location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town was martin luther king assassinated in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0016482652863487601, -0.06914828717708588, -0.07628632336854935], "norm_scores": [0.34930750727653503, 0.32650741934776306, 0.3241851031780243]}}
{"id": "WebQTest-24", "question": "where did edgar allan poe died", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did edgar allan poe died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> deceased_person.place_of_death </PATH>"], "scores": [-0.0002138755371561274, -0.09104185551404953, -0.11143899708986282], "norm_scores": [0.3561365008354187, 0.3252148926258087, 0.31864863634109497]}}
{"id": "WebQTest-26", "question": "what to do today in atlanta with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do today in atlanta with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.015394425019621849, -0.01637457311153412, -0.024271082133054733], "norm_scores": [0.33442771434783936, 0.33410006761550903, 0.331472247838974]}}
{"id": "WebQTest-28", "question": "what electorate does anna bligh represent", "prediction": [["people.person.nationality", "base.biblioness.bibs_location.country"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"]], "ground_paths": [["government.politician.government_positions_held", "government.political_district.representatives"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what electorate does anna bligh represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>"], "scores": [-0.01475333608686924, -0.019724814221262932, -0.02572355791926384], "norm_scores": [0.3351059854030609, 0.33344411849975586, 0.33144986629486084]}}
{"id": "WebQTest-31", "question": "where did andy murray started playing tennis", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_facility.teams"], ["sports.sports_facility.athlete", "sports.sports_facility.location"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did andy murray started playing tennis [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_facility.athlete <SEP> sports.sports_facility.location </PATH>"], "scores": [-0.022943414747714996, -0.03410044685006142, -0.05838454142212868], "norm_scores": [0.3385143578052521, 0.3347585201263428, 0.32672715187072754]}}
{"id": "WebQTest-32", "question": "what is the australian dollar called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_used", "common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the australian dollar called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.007525380235165358, -0.029322417452931404, -0.0605340339243412], "norm_scores": [0.34166884422302246, 0.33430206775665283, 0.3240290582180023]}}
{"id": "WebQTest-33", "question": "what timezone is sweden", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0005943325231783092, -0.0614117830991745, -0.15369154512882233], "norm_scores": [0.3572651743888855, 0.3361847698688507, 0.3065500557422638]}}
{"id": "WebQTest-34", "question": "who did cam newton sign with", "prediction": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cam newton sign with [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.02348175272345543, -0.024982061237096786, -0.025180580094456673], "norm_scores": [0.333688884973526, 0.33318862318992615, 0.33312249183654785]}}
{"id": "WebQTest-35", "question": "what county is frederick md in", "prediction": [["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is frederick md in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01949334144592285, -0.020993834361433983, -0.030071942135691643], "norm_scores": [0.33467450737953186, 0.33417269587516785, 0.3311527967453003]}}
{"id": "WebQTest-36", "question": "what highschool did harper lee go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did harper lee go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.012926492840051651, -0.019604118540883064, -0.02671414613723755], "norm_scores": [0.3356097340583801, 0.33337610960006714, 0.3310142159461975]}}
{"id": "WebQTest-37", "question": "what timezone is utah in", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is utah in [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.3352308087632991e-05, -0.14416825771331787, -0.19609178602695465], "norm_scores": [0.37206512689590454, 0.3221167325973511, 0.305818110704422]}}
{"id": "WebQTest-38", "question": "who did george w. bush run against for the second term", "prediction": [["time.event.locations", "people.person.nationality"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.election.campaigns", "government.election_campaign.candidate"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did george w. bush run against for the second term [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations <SEP> people.person.nationality </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> government.election.campaigns <SEP> government.election_campaign.candidate </PATH>"], "scores": [-0.012818843126296997, -0.02276981994509697, -0.023381900042295456], "norm_scores": [0.3356165587902069, 0.33229339122772217, 0.3320900797843933]}}
{"id": "WebQTest-39", "question": "who is niall ferguson 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is niall ferguson 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.027204668149352074, -0.028980007395148277, -0.03180922195315361], "norm_scores": [0.33404237031936646, 0.3334498405456543, 0.33250778913497925]}}
{"id": "WebQTest-41", "question": "where is the galapagos islands located on a world map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.island.island_group"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the galapagos islands located on a world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>"], "scores": [-0.0002291556738782674, -0.08745638281106949, -0.1067582294344902], "norm_scores": [0.3551870882511139, 0.325517863035202, 0.3192950189113617]}}
{"id": "WebQTest-42", "question": "what are the songs that justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["music.featured_artist.albums"], ["music.artist.album"], ["music.recording.artist"], ["music.album.artist"], ["music.recording.featured_artists"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["music.composer.compositions"], ["music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the songs that justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.018415646627545357, -0.021426549181342125, -0.033111222088336945], "norm_scores": [0.3352998197078705, 0.3342917859554291, 0.3304084241390228]}}
{"id": "WebQTest-43", "question": "what did george clemenceau do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george clemenceau do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.003522406332194805, -0.0479082427918911, -0.05441251024603844], "norm_scores": [0.34400105476379395, 0.3290661871433258, 0.32693275809288025]}}
{"id": "WebQTest-44", "question": "what is the state flower of arizona", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of arizona [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.012948598712682724, -0.016393641009926796, -0.024691540747880936], "norm_scores": [0.33502110838890076, 0.33386892080307007, 0.3311099708080292]}}
{"id": "WebQTest-45", "question": "where was rihanna born and raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna born and raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.007220996543765068, -0.026886219158768654, -0.05506786331534386], "norm_scores": [0.34085407853126526, 0.3342165946960449, 0.32492929697036743]}}
{"id": "WebQTest-46", "question": "what year did president william henry harrison take office", "prediction": [["government.election_campaign.candidate", "government.election_campaign.election"], ["government.election_campaign.candidate", "time.event.instance_of_recurring_event"], ["people.person.nationality", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did president william henry harrison take office [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.candidate <SEP> government.election_campaign.election </PATH>", "<PATH> government.election_campaign.candidate <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> people.person.nationality <SEP> location.location.events </PATH>"], "scores": [-0.019297759979963303, -0.035886429250240326, -0.037177182734012604], "norm_scores": [0.33717402815818787, 0.33162689208984375, 0.33119910955429077]}}
{"id": "WebQTest-47", "question": "where george lopez was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where george lopez was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.00803112518042326, -0.02070295810699463, -0.09294936805963516], "norm_scores": [0.3441161513328552, 0.33978310227394104, 0.31610074639320374]}}
{"id": "WebQTest-48", "question": "where are samsung based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are samsung based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded </PATH>"], "scores": [-0.008792579174041748, -0.03241679444909096, -0.03926841542124748], "norm_scores": [0.3393699824810028, 0.3314466178417206, 0.329183429479599]}}
{"id": "WebQTest-49", "question": "what did the islamic people believe in", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"], ["religion.deity.deity_of"]], "ground_paths": [["religion.belief.belief_of"], ["religion.religion.beliefs"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the islamic people believe in [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>", "<PATH> religion.deity.deity_of </PATH>"], "scores": [-0.008382306434214115, -0.01748630218207836, -0.107707180082798], "norm_scores": [0.34525787830352783, 0.3421289026737213, 0.31261324882507324]}}
{"id": "WebQTest-51", "question": "who will play mr gray in the film", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will play mr gray in the film [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.022104468196630478, -0.022241799160838127, -0.02941267006099224], "norm_scores": [0.3341597020626068, 0.33411383628845215, 0.3317264914512634]}}
{"id": "WebQTest-52", "question": "what did george orwell died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george orwell died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0015872296644374728, -0.04356284812092781, -0.12566480040550232], "norm_scores": [0.3518396019935608, 0.33737656474113464, 0.31078389286994934]}}
{"id": "WebQTest-54", "question": "what country did adolf hitler control", "prediction": [["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["people.deceased_person.place_of_death", "location.country.capital"], ["military.military_commander.military_commands", "military.military_command.military_combatant"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.political_appointer.appointees", "government.governmental_jurisdiction.governing_officials"], ["military.military_command.military_commander", "military.military_combatant.military_commanders"], ["organization.organization.founders", "organization.organization.place_founded"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.crime.crime.victim_s", "time.event.locations"], ["organization.organization_founder.organizations_founded", "organization.organization.place_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did adolf hitler control [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.0267393309623003, -0.0280836820602417, -0.030211705714464188], "norm_scores": [0.3338686227798462, 0.3334200978279114, 0.3327113091945648]}}
{"id": "WebQTest-55", "question": "who is michael buble", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael buble [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.00031902006594464183, -0.08164098858833313, -0.13599203526973724], "norm_scores": [0.3577785789966583, 0.3298349380493164, 0.31238651275634766]}}
{"id": "WebQTest-56", "question": "what county is kansas city kansas", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.hud_county_place.county"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is kansas city kansas [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.017924081534147263, -0.022522758692502975, -0.03789204731583595], "norm_scores": [0.33606192469596863, 0.3345200717449188, 0.32941800355911255]}}
{"id": "WebQTest-58", "question": "who did jackie robinson first play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jackie robinson first play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01378564815968275, -0.017192712053656578, -0.0332571305334568], "norm_scores": [0.335873007774353, 0.3347306549549103, 0.3293963372707367]}}
{"id": "WebQTest-59", "question": "when did annie open", "prediction": [["theater.theater_character.plays_appears_in"], ["theater.play.characters"], ["film.film.directed_by"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did annie open [/INST]", "raw_output": {"paths": ["<PATH> theater.theater_character.plays_appears_in </PATH>", "<PATH> theater.play.characters </PATH>", "<PATH> film.film.directed_by </PATH>"], "scores": [-0.03094557113945484, -0.0429534912109375, -0.0436275377869606], "norm_scores": [0.3360822796821594, 0.3320707380771637, 0.3318469822406769]}}
{"id": "WebQTest-60", "question": "where did eleanor roosevelt die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did eleanor roosevelt die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0014993584481999278, -0.0673714429140091, -0.07968808710575104], "norm_scores": [0.3495231866836548, 0.32724127173423767, 0.32323551177978516]}}
{"id": "WebQTest-61", "question": "what are the religions practiced in indonesia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.kingdom.rulers", "people.person.religion"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["book.book_subject.works", "book.written_work.subjects"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the religions practiced in indonesia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>"], "scores": [-0.0061368923634290695, -0.017192427068948746, -0.07297846674919128], "norm_scores": [0.3419564962387085, 0.3381967842578888, 0.3198467195034027]}}
{"id": "WebQTest-62", "question": "what was jesse james killed with", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse james killed with [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0014995700912550092, -0.05996781960129738, -0.07089803367853165], "norm_scores": [0.34768542647361755, 0.3279397785663605, 0.32437482476234436]}}
{"id": "WebQTest-63", "question": "who was vp for lincoln", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.political_party_tenure.politician", "government.political_party_tenure.politician"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for lincoln [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.politician </PATH>"], "scores": [-0.007777773309499025, -0.018633365631103516, -0.09731921553611755], "norm_scores": [0.34440556168556213, 0.3406870663166046, 0.31490737199783325]}}
{"id": "WebQTest-64", "question": "what other books did charles dickens write", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["book.written_work.author"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did charles dickens write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>"], "scores": [-0.005768067203462124, -0.02911999449133873, -0.04815569519996643], "norm_scores": [0.34066709876060486, 0.3328040540218353, 0.3265288174152374]}}
{"id": "WebQTest-65", "question": "who was the first dictator of the soviet union", "prediction": [["people.person.nationality"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first dictator of the soviet union [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.015821561217308044, -0.01958061382174492, -0.03271225467324257], "norm_scores": [0.33562687039375305, 0.3343676030635834, 0.3300055265426636]}}
{"id": "WebQTest-66", "question": "what is the currency of puerto rico called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of puerto rico called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.0005755942547693849, -0.06194547563791275, -0.13329000771045685], "norm_scores": [0.35508957505226135, 0.33395299315452576, 0.3109573423862457]}}
{"id": "WebQTest-67", "question": "what kind of cancer did carl wilson have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did carl wilson have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.012509253807365894, -0.015852363780140877, -0.03528018295764923], "norm_scores": [0.33623066544532776, 0.3351084887981415, 0.32866087555885315]}}
{"id": "WebQTest-68", "question": "who plays the voice of kitt in knight rider", "prediction": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of kitt in knight rider [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.016008632257580757, -0.02107653021812439, -0.021512774750590324], "norm_scores": [0.334509015083313, 0.33281806111335754, 0.33267292380332947]}}
{"id": "WebQTest-69", "question": "what county is brentwood tennessee in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is brentwood tennessee in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.014443746767938137, -0.0232554879039526, -0.03563913702964783], "norm_scores": [0.33667147159576416, 0.33371782302856445, 0.329610675573349]}}
{"id": "WebQTest-71", "question": "who did michael j fox marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did michael j fox marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.02565835230052471, -0.027392497286200523, -0.040592167526483536], "norm_scores": [0.3351830244064331, 0.3346022963523865, 0.33021464943885803]}}
{"id": "WebQTest-72", "question": "what battles did stonewall jackson fight in", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["people.person.nationality", "book.written_work.subjects"], ["base.culturalevent.event.entity_involved"], ["common.topic.image"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["military.military_conflict.military_personnel_involved"], ["military.military_person.participated_in_conflicts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what battles did stonewall jackson fight in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.011233285069465637, -0.029562948271632195, -0.030234551057219505], "norm_scores": [0.33749401569366455, 0.3313641846179962, 0.33114174008369446]}}
{"id": "WebQTest-73", "question": "what language do the maasai tribe speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai tribe speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.000643103034235537, -0.08105416595935822, -0.09029501676559448], "norm_scores": [0.3524866998195648, 0.325252503156662, 0.3222607374191284]}}
{"id": "WebQTest-74", "question": "who did benjamin franklin get married to", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.children"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did benjamin franklin get married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.028319502249360085, -0.03069384954869747, -0.03688808158040047], "norm_scores": [0.33454927802085876, 0.33375588059425354, 0.33169490098953247]}}
{"id": "WebQTest-75", "question": "what disease did patrick swayze died from", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["medicine.notable_person_with_medical_condition.condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did patrick swayze died from [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>"], "scores": [-0.005158598069101572, -0.027571413666009903, -0.03892442584037781], "norm_scores": [0.33960074186325073, 0.33207398653030396, 0.3283253014087677]}}
{"id": "WebQTest-76", "question": "what works of art did leonardo da vinci produce", "prediction": [["visual_art.visual_artist.art_forms"], ["film.film.subjects"], ["visual_art.art_form.artists"]], "ground_paths": [["visual_art.artwork.artist"], ["visual_art.visual_artist.artworks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what works of art did leonardo da vinci produce [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> visual_art.art_form.artists </PATH>"], "scores": [-0.0008196434937417507, -0.07521196454763412, -0.08428420126438141], "norm_scores": [0.3510950803756714, 0.32592418789863586, 0.32298070192337036]}}
{"id": "WebQTest-77", "question": "what capital of austria", "prediction": [["location.country.capital"], ["location.location.containedby"], ["location.administrative_division.first_level_division_of"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital of austria [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.006874026730656624, -0.038883332163095474, -0.06845089793205261], "norm_scores": [0.34378695487976074, 0.33295685052871704, 0.3232562243938446]}}
{"id": "WebQTest-78", "question": "what type of cancer did eva peron have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.008040562272071838, -0.027310026809573174, -0.038335733115673065], "norm_scores": [0.3388596773147583, 0.33239254355430603, 0.32874780893325806]}}
{"id": "WebQTest-79", "question": "what country did buddha come from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.006731244269758463, -0.03340527042746544, -0.04844677075743675], "norm_scores": [0.34096860885620117, 0.33199381828308105, 0.3270375430583954]}}
{"id": "WebQTest-80", "question": "what county is greeley colorado in", "prediction": [["location.us_county.county_seat"], ["location.us_county.hud_county_place"], ["location.location.containedby"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is greeley colorado in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.017841124907135963, -0.024798952043056488, -0.026631854474544525], "norm_scores": [0.33508536219596863, 0.3327619731426239, 0.3321526348590851]}}
{"id": "WebQTest-82", "question": "what did peter tchaikovsky do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did peter tchaikovsky do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0008842654060572386, -0.0696912407875061, -0.07293109595775604], "norm_scores": [0.3491627275943756, 0.32594579458236694, 0.32489147782325745]}}
{"id": "WebQTest-83", "question": "which countries speak german officially", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak german officially [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.018002867698669434, -0.02100452408194542, -0.023965846747159958], "norm_scores": [0.33432990312576294, 0.33332785964012146, 0.3323422372341156]}}
{"id": "WebQTest-84", "question": "what type of music did john lennon sing", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did john lennon sing [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.001214992138557136, -0.05478212982416153, -0.11377401649951935], "norm_scores": [0.351940780878067, 0.33358436822891235, 0.31447482109069824]}}
{"id": "WebQTest-85", "question": "who are the colorado representatives", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "ground_paths": [["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the colorado representatives [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.02512218803167343, -0.027025600895285606, -0.028354397043585777], "norm_scores": [0.3339041471481323, 0.3332692086696625, 0.3328266441822052]}}
{"id": "WebQTest-86", "question": "which country does greenland belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["location.country.administrative_divisions"]], "ground_paths": [["location.country.administrative_divisions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country does greenland belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.01734977215528488, -0.03385746479034424, -0.038510218262672424], "norm_scores": [0.3375311493873596, 0.33200499415397644, 0.33046388626098633]}}
{"id": "WebQTest-87", "question": "what the zip code for seattle washington", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the zip code for seattle washington [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00016005833458621055, -0.09008322656154633, -0.15424029529094696], "norm_scores": [0.3608538806438446, 0.3298209607601166, 0.30932512879371643]}}
{"id": "WebQTest-88", "question": "who is willow smith mom name", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is willow smith mom name [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.007841096259653568, -0.022687163203954697, -0.12792494893074036], "norm_scores": [0.34817612171173096, 0.34304526448249817, 0.30877864360809326]}}
{"id": "WebQTest-89", "question": "what do you call the chinese writing system", "prediction": [["language.human_language.writing_system"], ["language.language_writing_system.writing_systems"], ["location.country.writing_system"]], "ground_paths": [["language.human_language.writing_system"], ["language.language_writing_system.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call the chinese writing system [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.writing_system </PATH>", "<PATH> language.language_writing_system.writing_systems </PATH>", "<PATH> location.country.writing_system </PATH>"], "scores": [-0.007974469102919102, -0.0445745624601841, -0.053064849227666855], "norm_scores": [0.34246891736984253, 0.3301611542701721, 0.3273698389530182]}}
{"id": "WebQTest-90", "question": "who played on the jeffersons", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played on the jeffersons [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.023960167542099953, -0.02505473792552948, -0.028492316603660583], "norm_scores": [0.3339584767818451, 0.33359313011169434, 0.3324483633041382]}}
{"id": "WebQTest-91", "question": "what is the name of the san francisco newspaper", "prediction": [["book.newspaper.circulation_areas"], ["periodicals.newspaper_circulation_area.newspapers"], ["location.location.containedby"]], "ground_paths": [["periodicals.newspaper_circulation_area.newspapers"], ["book.newspaper.circulation_areas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the san francisco newspaper [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0031240361277014017, -0.029983777552843094, -0.09311039745807648], "norm_scores": [0.3463273346424103, 0.33714890480041504, 0.3165237009525299]}}
{"id": "WebQTest-93", "question": "what continent does armenia belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does armenia belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.014909324236214161, -0.024467170238494873, -0.02899220772087574], "norm_scores": [0.3359646797180176, 0.33276888728141785, 0.33126649260520935]}}
{"id": "WebQTest-94", "question": "what did randy savage died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did randy savage died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0008968079928308725, -0.05426626279950142, -0.1314382553100586], "norm_scores": [0.35390087962150574, 0.3355085253715515, 0.310590535402298]}}
{"id": "WebQTest-95", "question": "what was the title of the book charles darwin wrote", "prediction": [["book.written_work.author"], ["common.topic.notable_types"], ["book.book_edition.author_editor"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["book.written_work.author"], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the title of the book charles darwin wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.001455741818062961, -0.07517626881599426, -0.07577527314424515], "norm_scores": [0.3499799966812134, 0.3251073658466339, 0.3249126672744751]}}
{"id": "WebQTest-96", "question": "where did richard nixon die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard nixon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>"], "scores": [-0.0005524607840925455, -0.07791302353143692, -0.09886259585618973], "norm_scores": [0.35311684012413025, 0.3268294334411621, 0.32005372643470764]}}
{"id": "WebQTest-97", "question": "what songs does smokey robinson sing", "prediction": [["music.recording.artist"], ["music.artist.track"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["music.artist.album"], ["music.producer.releases_produced"], ["music.recording.artist"], ["music.album.artist"], ["music.recording.producer"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["music.recording.featured_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs does smokey robinson sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.014330506324768066, -0.03720298781991005, -0.04739732667803764], "norm_scores": [0.3395746052265167, 0.3318958282470703, 0.32852956652641296]}}
{"id": "WebQTest-98", "question": "who was vice president when jfk was president", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president when jfk was president [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.010315682739019394, -0.01456256303936243, -0.08032584190368652], "norm_scores": [0.3415129780769348, 0.34006568789482117, 0.3184213638305664]}}
{"id": "WebQTest-99", "question": "what is serbian language called", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.language_family"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is serbian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.01783745363354683, -0.025819726288318634, -0.035735830664634705], "norm_scores": [0.3362123966217041, 0.33353933691978455, 0.33024826645851135]}}
{"id": "WebQTest-100", "question": "what language is spoken in haiti today", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in haiti today [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.011784952133893967, -0.01688312739133835, -0.0511055625975132], "norm_scores": [0.3382541239261627, 0.3365340232849121, 0.32521188259124756]}}
{"id": "WebQTest-102", "question": "who played barbara gordon batgirl", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played barbara gordon batgirl [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.011477282270789146, -0.03228534385561943, -0.039313074201345444], "norm_scores": [0.33875855803489685, 0.3317824900150299, 0.32945898175239563]}}
{"id": "WebQTest-103", "question": "what basketball teams has shaq played for", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball teams has shaq played for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.021249856799840927, -0.021569961681962013, -0.026128271594643593], "norm_scores": [0.33391061425209045, 0.33380377292633057, 0.332285612821579]}}
{"id": "WebQTest-104", "question": "where is jay leno from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jay leno from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004610786680132151, -0.03396317735314369, -0.059655554592609406], "norm_scores": [0.3427572250366211, 0.3328426778316498, 0.3244000971317291]}}
{"id": "WebQTest-105", "question": "what language do people from thailand speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from thailand speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.015633322298526764, -0.019285617396235466, -0.02923237346112728], "norm_scores": [0.3352501392364502, 0.3340279459953308, 0.330721914768219]}}
{"id": "WebQTest-106", "question": "who did armie hammer play in the social network", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did armie hammer play in the social network [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.018967535346746445, -0.026982855051755905, -0.02796335145831108], "norm_scores": [0.3352260887622833, 0.33254989981651306, 0.3322240114212036]}}
{"id": "WebQTest-107", "question": "what was thomas jefferson role in the declaration of independence", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["book.author.book_editions_published"], ["book.written_work.subjects"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was thomas jefferson role in the declaration of independence [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.029783135280013084, -0.03266281634569168, -0.03372126817703247], "norm_scores": [0.33409127593040466, 0.3331305682659149, 0.3327781856060028]}}
{"id": "WebQTest-108", "question": "what was the book written by charles darwin", "prediction": [["book.written_work.subjects"], ["book.written_work.author"], ["book.book_subject.works"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["book.written_work.author"], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the book written by charles darwin [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.01362221222370863, -0.01763441413640976, -0.0569901317358017], "norm_scores": [0.3385750651359558, 0.33721932768821716, 0.32420557737350464]}}
{"id": "WebQTest-109", "question": "who did tim tebow play college football for", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tim tebow play college football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.016102418303489685, -0.022354206070303917, -0.027990655973553658], "norm_scores": [0.33535104990005493, 0.33326104283332825, 0.3313879072666168]}}
{"id": "WebQTest-110", "question": "who does peyton manning play football for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peyton manning play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.009652211330831051, -0.02116725966334343, -0.031204741448163986], "norm_scores": [0.337014764547348, 0.3331562578678131, 0.32982897758483887]}}
{"id": "WebQTest-111", "question": "where is the carpathian mountain range located", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the carpathian mountain range located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.01614941470324993, -0.016883786767721176, -0.03747439384460449], "norm_scores": [0.33577701449394226, 0.335530549287796, 0.32869240641593933]}}
{"id": "WebQTest-112", "question": "what was robert burns", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>"], "scores": [-0.003768058493733406, -0.04387376829981804, -0.060761045664548874], "norm_scores": [0.34419986605644226, 0.3306686580181122, 0.32513144612312317]}}
{"id": "WebQTest-114", "question": "who did the voice of darth vader in episode 3", "prediction": [["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice of darth vader in episode 3 [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.027204113081097603, -0.031525224447250366, -0.033781394362449646], "norm_scores": [0.33454522490501404, 0.3331027328968048, 0.33235204219818115]}}
{"id": "WebQTest-115", "question": "what super bowl did peyton manning win", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what super bowl did peyton manning win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.005560195539146662, -0.03774557262659073, -0.04432099685072899], "norm_scores": [0.3412610590457916, 0.3304523527622223, 0.32828661799430847]}}
{"id": "WebQTest-116", "question": "which country was justin bieber born in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was justin bieber born in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0072303153574466705, -0.026592757552862167, -0.05423586070537567], "norm_scores": [0.34072640538215637, 0.33419257402420044, 0.3250809907913208]}}
{"id": "WebQTest-118", "question": "who fought in the gulf war 1991", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"]], "ground_paths": [["film.film_genre.films_in_this_genre", "film.film.country"], ["time.event.locations"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "tv.tv_program.country_of_origin"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["film.film.genre", "film.film.country"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["film.film.subjects", "film.film.country"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "film.film.country"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the gulf war 1991 [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>", "<PATH> time.event.includes_event <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.02023574523627758, -0.029177220538258553, -0.03258073329925537], "norm_scores": [0.3357023596763611, 0.3327140510082245, 0.33158358931541443]}}
{"id": "WebQTest-119", "question": "who plays london tipton in suite life on deck", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.genre", "music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays london tipton in suite life on deck [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>"], "scores": [-0.01534520648419857, -0.02968759462237358, -0.03239470347762108], "norm_scores": [0.336830198764801, 0.3320337235927582, 0.3311360776424408]}}
{"id": "WebQTest-121", "question": "what team did david beckham play for in 2011", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "ground_paths": [["book.written_work.subjects", "book.written_work.subjects"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for in 2011 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.022229483351111412, -0.024897778406739235, -0.028800813481211662], "norm_scores": [0.3343603014945984, 0.33346933126449585, 0.332170307636261]}}
{"id": "WebQTest-122", "question": "where did francisco coronado come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did francisco coronado come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.007555427495390177, -0.02244654856622219, -0.06633494049310684], "norm_scores": [0.3415144383907318, 0.33646658062934875, 0.32201895117759705]}}
{"id": "WebQTest-123", "question": "what all does google now do", "prediction": [["organization.organization.sectors"], ["organization.organization.founders"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.sectors"]], "ground_paths": [["projects.project_participation.participant", "projects.project_participation.project"], ["internet.website.owner"], ["business.consumer_company.products"], ["computer.computer_manufacturer_brand.computer_models"], ["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"], ["computer.software_developer.software"], ["business.consumer_company.products", "business.consumer_product.company"], ["computer.software.developer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all does google now do [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.sectors </PATH>"], "scores": [-0.04291461780667305, -0.04604785516858101, -0.05334505811333656], "norm_scores": [0.3348406255245209, 0.33379313349723816, 0.3313662111759186]}}
{"id": "WebQTest-124", "question": "what language does egyptian people speak", "prediction": [["location.country.languages_spoken"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does egyptian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.036645710468292236, -0.04042776674032211, -0.04052630439400673], "norm_scores": [0.33418527245521545, 0.3329237699508667, 0.33289095759391785]}}
{"id": "WebQTest-125", "question": "what are abraham sons names", "prediction": [["people.person.children"], ["people.person.parents"], ["fictional_universe.fictional_character.parents"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are abraham sons names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>"], "scores": [-0.005857937503606081, -0.030298929661512375, -0.09324494749307632], "norm_scores": [0.3457602262496948, 0.33741194009780884, 0.31682783365249634]}}
{"id": "WebQTest-126", "question": "who wrote the jana gana mana", "prediction": [["music.composition.composer"], ["music.composition.lyricist"], ["music.composer.compositions"]], "ground_paths": [["music.composition.composer"], ["music.lyricist.lyrics_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the jana gana mana [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.composer.compositions </PATH>"], "scores": [-0.00820823386311531, -0.02889382094144821, -0.04320121183991432], "norm_scores": [0.33954253792762756, 0.33259105682373047, 0.32786640524864197]}}
{"id": "WebQTest-127", "question": "what type of cancer did gilda radner die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did gilda radner die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.008225989528000355, -0.026470666751265526, -0.0283323023468256], "norm_scores": [0.3376080095767975, 0.33150428533554077, 0.33088770508766174]}}
{"id": "WebQTest-128", "question": "who plays juni cortez", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays juni cortez [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02057935856282711, -0.029334113001823425, -0.0325351282954216], "norm_scores": [0.335638165473938, 0.3327125608921051, 0.3316492438316345]}}
{"id": "WebQTest-129", "question": "what was lucille ball", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lucille ball [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0015276893973350525, -0.06323476880788803, -0.07350929081439972], "norm_scores": [0.3483463525772095, 0.32750070095062256, 0.32415300607681274]}}
{"id": "WebQTest-131", "question": "who inspired obama", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who inspired obama [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.010130629874765873, -0.015915198251605034, -0.11707381159067154], "norm_scores": [0.3456847667694092, 0.3436909019947052, 0.31062430143356323]}}
{"id": "WebQTest-132", "question": "who is michael j fox wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.022117745131254196, -0.036192312836647034, -0.04501231387257576], "norm_scores": [0.33745142817497253, 0.33273521065711975, 0.3298133909702301]}}
{"id": "WebQTest-133", "question": "where did dolly parton grow up", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did dolly parton grow up [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.008726146072149277, -0.031579937785863876, -0.03265611454844475], "norm_scores": [0.3385515809059143, 0.3309021592140198, 0.33054620027542114]}}
{"id": "WebQTest-134", "question": "what type of books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.010760356672108173, -0.021795613691210747, -0.041099194437265396], "norm_scores": [0.33793577551841736, 0.334227055311203, 0.32783713936805725]}}
{"id": "WebQTest-138", "question": "what are the four main languages spoken in spain", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four main languages spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.017459532245993614, -0.01834498532116413, -0.02492830716073513], "norm_scores": [0.334261029958725, 0.3339652121067047, 0.3317737877368927]}}
{"id": "WebQTest-139", "question": "where is the nra headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the nra headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.0036069462075829506, -0.03819924592971802, -0.051051024347543716], "norm_scores": [0.3425052762031555, 0.33085983991622925, 0.3266349136829376]}}
{"id": "WebQTest-141", "question": "who is gimli 's father in the hobbit", "prediction": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.parents"], ["people.person.gender"]], "ground_paths": [["fictional_universe.fictional_character.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is gimli 's father in the hobbit [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.02937302738428116, -0.030342312529683113, -0.039610449224710464], "norm_scores": [0.3345773220062256, 0.3342531621456146, 0.3311695456504822]}}
{"id": "WebQTest-142", "question": "who developed the tcp ip reference model", "prediction": [["law.invention.inventor"], ["base.biblioness.bibs_location.country", "people.person.nationality"], ["internet.website.owner", "organization.organization_founder.organizations_founded"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who developed the tcp ip reference model [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> people.person.nationality </PATH>", "<PATH> internet.website.owner <SEP> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.01055945921689272, -0.04854031652212143, -0.06799876689910889], "norm_scores": [0.34400781989097595, 0.3311871290206909, 0.32480505108833313]}}
{"id": "WebQTest-145", "question": "what college did albert einstein go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "organization.organization.founders"], ["people.person.education", "education.education.institution"], ["business.employment_tenure.person", "business.employer.employees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did albert einstein go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.02078697271645069, -0.02234788052737713, -0.023284342139959335], "norm_scores": [0.33378440141677856, 0.3332638144493103, 0.3329518437385559]}}
{"id": "WebQTest-146", "question": "what team does jordan own", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team_owner.teams_owned"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jordan own [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team_owner.teams_owned </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.003562582889571786, -0.04383942484855652, -0.04920915886759758], "norm_scores": [0.3429469168186188, 0.32940858602523804, 0.3276444971561432]}}
{"id": "WebQTest-147", "question": "who is the state governor of tennessee", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "ground_paths": [["base.locations.countries.states_provinces_within", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of tennessee [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.020772526040673256, -0.025312865152955055, -0.027918856590986252], "norm_scores": [0.33463290333747864, 0.3331170082092285, 0.33225002884864807]}}
{"id": "WebQTest-149", "question": "what do portuguese people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do portuguese people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013050118461251259, -0.022330215200781822, -0.03352709859609604], "norm_scores": [0.33664435148239136, 0.33353468775749207, 0.3298209607601166]}}
{"id": "WebQTest-150", "question": "what language does cuba speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does cuba speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.007869040593504906, -0.02181832678616047, -0.07542554289102554], "norm_scores": [0.34236928820610046, 0.3376266658306122, 0.3200039863586426]}}
{"id": "WebQTest-153", "question": "what are the sights to see in madrid", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["common.topic.image"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sights to see in madrid [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01703629456460476, -0.019327042624354362, -0.020026374608278275], "norm_scores": [0.33392035961151123, 0.3331562876701355, 0.33292338252067566]}}
{"id": "WebQTest-154", "question": "what instruments did louis armstrong play", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.instrument.family"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.0002656581636983901, -0.08417306840419769, -0.12058662623167038], "norm_scores": [0.3563598692417145, 0.32767874002456665, 0.31596142053604126]}}
{"id": "WebQTest-155", "question": "what year was george w bush elected", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.event.locations", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was george w bush elected [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.event.locations <SEP> location.location.events </PATH>"], "scores": [-0.014910903759300709, -0.03400726243853569, -0.046422019600868225], "norm_scores": [0.33897557854652405, 0.3325637876987457, 0.32846060395240784]}}
{"id": "WebQTest-156", "question": "what time zone am i in california", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.states_provinces_within"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone am i in california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>"], "scores": [-1.7198346540681086e-05, -0.13619178533554077, -0.15842929482460022], "norm_scores": [0.366812527179718, 0.3201137185096741, 0.3130737245082855]}}
{"id": "WebQTest-157", "question": "where is mission san buenaventura located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mission san buenaventura located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.0008677620789967477, -0.06249837577342987, -0.09102026373147964], "norm_scores": [0.3503827154636383, 0.3294404149055481, 0.3201768696308136]}}
{"id": "WebQTest-159", "question": "what time in hilo hawaii", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["time.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in hilo hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> time.location.time_zones </PATH>"], "scores": [-7.659947004867718e-05, -0.09426544606685638, -0.15307746827602386], "norm_scores": [0.36124035716056824, 0.328768789768219, 0.3099908232688904]}}
{"id": "WebQTest-160", "question": "where did rudolf virchow conduct his research", "prediction": [["people.deceased_person.place_of_death", "location.location.containedby"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["business.employment_tenure.person", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rudolf virchow conduct his research [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.012320371344685555, -0.024812379851937294, -0.0335148386657238], "norm_scores": [0.3370846211910248, 0.33289995789527893, 0.33001548051834106]}}
{"id": "WebQTest-161", "question": "what university did gordon brown attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did gordon brown attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.014196385629475117, -0.019626321271061897, -0.022768298164010048], "norm_scores": [0.33489060401916504, 0.3330771028995514, 0.3320322334766388]}}
{"id": "WebQTest-162", "question": "what countries does greece share borders with", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.containedby", "base.locations.continents.countries_within"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.events", "location.location.events"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.location.containedby", "location.location.containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["media_common.netflix_title.netflix_genres", "film.film.country"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["media_common.netflix_genre.titles", "film.film.country"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["organization.organization.founders", "organization.organization.founders"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.containedby", "base.locations.countries.continent"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["time.event.locations", "time.event.locations"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.country.official_language", "location.country.languages_spoken"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does greece share borders with [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.02887718752026558, -0.04258117452263832, -0.047402575612068176], "norm_scores": [0.3369233012199402, 0.3323375880718231, 0.3307391405105591]}}
{"id": "WebQTest-163", "question": "what year did the seahawks win the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the seahawks win the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.011130349710583687, -0.014441253617405891, -0.04620040953159332], "norm_scores": [0.3375832438468933, 0.3364674150943756, 0.32594937086105347]}}
{"id": "WebQTest-164", "question": "what did stephen hawking become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.002132568508386612, -0.050013069063425064, -0.07361317425966263], "norm_scores": [0.34670913219451904, 0.33049967885017395, 0.322791188955307]}}
{"id": "WebQTest-165", "question": "who plays donna noble", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.ethnicity", "people.person.ethnicity"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays donna noble [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.02236173301935196, -0.02800271287560463, -0.03862902149558067], "norm_scores": [0.3357688784599304, 0.33388012647628784, 0.3303510248661041]}}
{"id": "WebQTest-166", "question": "what was dr seuss education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.012480262666940689, -0.013580577448010445, -0.044223275035619736], "norm_scores": [0.33696627616882324, 0.336595743894577, 0.32643795013427734]}}
{"id": "WebQTest-167", "question": "where did queensland get its name from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.armorial_grant.coat_of_arms", "symbols.armorial_grant.armiger"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did queensland get its name from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.armorial_grant.coat_of_arms <SEP> symbols.armorial_grant.armiger </PATH>"], "scores": [-0.006656239740550518, -0.02713969349861145, -0.07681617140769958], "norm_scores": [0.3434101641178131, 0.33644750714302063, 0.32014232873916626]}}
{"id": "WebQTest-168", "question": "what document did james madison write", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["book.book_subject.works"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did james madison write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.01287609338760376, -0.017048675566911697, -0.05652426928281784], "norm_scores": [0.33862414956092834, 0.3372141420841217, 0.32416173815727234]}}
{"id": "WebQTest-169", "question": "what are the three official languages of belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the three official languages of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.009596829302608967, -0.02083355188369751, -0.03903375566005707], "norm_scores": [0.33785855770111084, 0.3340834081172943, 0.32805800437927246]}}
{"id": "WebQTest-170", "question": "who played bilbo in lord of the rings", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.03126632049679756, -0.03439362719655037, -0.03532096743583679], "norm_scores": [0.33413177728652954, 0.3330884575843811, 0.33277973532676697]}}
{"id": "WebQTest-171", "question": "what airport do you fly into to get to destin fl", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into to get to destin fl [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.010903645306825638, -0.017981458455324173, -0.047018863260746], "norm_scores": [0.33812612295150757, 0.33574140071868896, 0.32613250613212585]}}
{"id": "WebQTest-172", "question": "what do you call members of the senate", "prediction": [["government.government_office_or_title.governmental_body_if_any", "common.topic.notable_types", "common.topic.notable_types"], ["government.governmental_body.offices_positions", "common.topic.notable_types", "common.topic.notable_types"], ["government.government_position_held.governmental_body", "government.government_position_held.office_position_or_title"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call members of the senate [/INST]", "raw_output": {"paths": ["<PATH> government.government_office_or_title.governmental_body_if_any <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.governmental_body.offices_positions <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.012277469970285892, -0.0202743299305439, -0.020798129960894585], "norm_scores": [0.3351711332798004, 0.33250150084495544, 0.33232736587524414]}}
{"id": "WebQTest-173", "question": "what countries are part of the baltic", "prediction": [["location.location.containedby"], ["location.location.contains"], ["common.topic.notable_types"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the baltic [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.011249879375100136, -0.019351011142134666, -0.05202696844935417], "norm_scores": [0.338756263256073, 0.3360230326652527, 0.32522064447402954]}}
{"id": "WebQTest-174", "question": "what team does colin kaepernick play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does colin kaepernick play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.013066768646240234, -0.02539980039000511, -0.025997184216976166], "norm_scores": [0.3361462652683258, 0.33202603459358215, 0.33182772994041443]}}
{"id": "WebQTest-175", "question": "where did joe flacco attend college", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did joe flacco attend college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.016425879672169685, -0.020429888740181923, -0.022404465824365616], "norm_scores": [0.3344433307647705, 0.3331069052219391, 0.3324497938156128]}}
{"id": "WebQTest-176", "question": "who is emma stone father", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is emma stone father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.007975446060299873, -0.024092361330986023, -0.12840460240840912], "norm_scores": [0.34836500883102417, 0.3427954614162445, 0.30883949995040894]}}
{"id": "WebQTest-177", "question": "where was theodore roosevelt buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.place_of_interment.interred_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was theodore roosevelt buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.010248328559100628, -0.02367720752954483, -0.06162174418568611], "norm_scores": [0.3405316472053528, 0.33598923683166504, 0.32347914576530457]}}
{"id": "WebQTest-178", "question": "who was the leader of the us during wwii", "prediction": [["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speaker_s"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["military.military_conflict.commanders", "military.military_command.military_commander"]], "ground_paths": [["event.speech_topic.speeches_or_presentations_on_this_topic", "event.speech_or_presentation.speaker_s"], ["military.military_command.military_conflict", "military.military_commander.military_commands"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the us during wwii [/INST]", "raw_output": {"paths": ["<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speaker_s </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> military.military_conflict.commanders <SEP> military.military_command.military_commander </PATH>"], "scores": [-0.016040323302149773, -0.028001822531223297, -0.02812008745968342], "norm_scores": [0.3360099196434021, 0.33201467990875244, 0.33197540044784546]}}
{"id": "WebQTest-179", "question": "what artistic movement did henri matisse belong to", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic movement did henri matisse belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0013342954916879535, -0.041391462087631226, -0.10092902928590775], "norm_scores": [0.3489258289337158, 0.33522507548332214, 0.31584909558296204]}}
{"id": "WebQTest-180", "question": "where the missouri river ends", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"], ["location.location.containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where the missouri river ends [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.003623232478275895, -0.057159047573804855, -0.0589439831674099], "norm_scores": [0.34553608298301697, 0.32752397656440735, 0.3269399106502533]}}
{"id": "WebQTest-181", "question": "what type of currency do they use in thailand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["royalty.monarch.kingdom", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in thailand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> royalty.monarch.kingdom <SEP> location.country.currency_used </PATH>"], "scores": [-0.009797685779631138, -0.016251206398010254, -0.105824775993824], "norm_scores": [0.34458914399147034, 0.34237247705459595, 0.31303834915161133]}}
{"id": "WebQTest-182", "question": "who is khloe kardashian 's husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is khloe kardashian 's husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.019335007295012474, -0.020378252491354942, -0.03569803759455681], "norm_scores": [0.33526360988616943, 0.33491402864456177, 0.3298223316669464]}}
{"id": "WebQTest-184", "question": "what state does romney live in", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.country"]], "ground_paths": [["people.person.nationality", "location.administrative_division.country"], ["people.person.places_lived", "people.place_lived.location"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state does romney live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>"], "scores": [-0.029038934037089348, -0.03049289993941784, -0.0381980836391449], "norm_scores": [0.33451196551322937, 0.33402594923973083, 0.3314621150493622]}}
{"id": "WebQTest-185", "question": "what type of government does france use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does france use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010945438407361507, -0.013301385566592216, -0.13512292504310608], "norm_scores": [0.3471173942089081, 0.34630057215690613, 0.30658209323883057]}}
{"id": "WebQTest-186", "question": "who rules denmark right now", "prediction": [["royalty.monarch.kingdom"], ["people.person.nationality"], ["royalty.kingdom.rulers"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who rules denmark right now [/INST]", "raw_output": {"paths": ["<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> royalty.kingdom.rulers </PATH>"], "scores": [-0.010596992447972298, -0.019626474007964134, -0.0654638335108757], "norm_scores": [0.34041136503219604, 0.3373514413833618, 0.32223719358444214]}}
{"id": "WebQTest-187", "question": "what did stephen hawking study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"]], "ground_paths": [["education.education.student", "education.education.major_field_of_study"], ["people.person.profession", "people.professional_field.professions_in_this_field"], ["book.author.works_written", "book.written_work.subjects"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["book.written_work.author", "book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.019142620265483856, -0.02028101682662964, -0.0253083948045969], "norm_scores": [0.3341447114944458, 0.33376455307006836, 0.3320907652378082]}}
{"id": "WebQTest-188", "question": "where did bristol palin go to school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did bristol palin go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.008782661519944668, -0.02361508645117283, -0.027804657816886902], "norm_scores": [0.3371049761772156, 0.33214181661605835, 0.3307531774044037]}}
{"id": "WebQTest-189", "question": "what region of the world is egypt associated with", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region of the world is egypt associated with [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.018440401181578636, -0.018889402970671654, -0.03673947975039482], "norm_scores": [0.33541083335876465, 0.335260272026062, 0.3293289542198181]}}
{"id": "WebQTest-190", "question": "what did dr. seuss do", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr. seuss do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.008193839341402054, -0.03000985085964203, -0.057681892067193985], "norm_scores": [0.341280996799469, 0.33391624689102173, 0.3248027563095093]}}
{"id": "WebQTest-191", "question": "where did drew brees go to college wikianswers", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did drew brees go to college wikianswers [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.017404187470674515, -0.01847916468977928, -0.02698173187673092], "norm_scores": [0.3345159888267517, 0.3341566026210785, 0.3313274383544922]}}
{"id": "WebQTest-193", "question": "what school did ben roethlisberger go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did ben roethlisberger go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01862913928925991, -0.02221664972603321, -0.022516686469316483], "norm_scores": [0.33416441082954407, 0.33296775817871094, 0.3328678607940674]}}
{"id": "WebQTest-194", "question": "where did the iroquois indians come from", "prediction": [["people.ethnicity.geographic_distribution"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["people.ethnicity.geographic_distribution", "base.locations.states_and_provences.country"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the iroquois indians come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.states_and_provences.country </PATH>"], "scores": [-0.0002030294417636469, -0.0998932421207428, -0.10019473731517792], "norm_scores": [0.3558765947818756, 0.32211026549339294, 0.32201313972473145]}}
{"id": "WebQTest-195", "question": "what kind of guitar did george harrison use", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did george harrison use [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.00919301901012659, -0.03420494124293327, -0.03669973835349083], "norm_scores": [0.3391938805580139, 0.3308151960372925, 0.3299909234046936]}}
{"id": "WebQTest-196", "question": "what language do navajo people speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do navajo people speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0006161394412629306, -0.08114543557167053, -0.08715453743934631], "norm_scores": [0.35214635729789734, 0.3249000310897827, 0.32295355200767517]}}
{"id": "WebQTest-197", "question": "what places make up new england", "prediction": [["location.location.contains"], ["location.location.containedby"], ["book.book_subject.works"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places make up new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.009241601452231407, -0.02021464891731739, -0.12520743906497955], "norm_scores": [0.3472713530063629, 0.34348157048225403, 0.30924710631370544]}}
{"id": "WebQTest-199", "question": "what type of government does iraq have now", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does iraq have now [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005737868137657642, -0.02112662047147751, -0.1296524703502655], "norm_scores": [0.3486526310443878, 0.3433283567428589, 0.3080190122127533]}}
{"id": "WebQTest-200", "question": "where was the vietnam war location", "prediction": [["time.event.locations"], ["location.location.events"], ["book.written_work.subjects"]], "ground_paths": [["book.written_work.subjects"], ["book.book_subject.works"], ["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the vietnam war location [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.00829231459647417, -0.030145050957798958, -0.04802793636918068], "norm_scores": [0.340202271938324, 0.33284854888916016, 0.3269491493701935]}}
{"id": "WebQTest-201", "question": "who did gerald ford select as his vice president when he became president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did gerald ford select as his vice president when he became president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.005817528814077377, -0.020081989467144012, -0.06777206808328629], "norm_scores": [0.3417912423610687, 0.3369503617286682, 0.3212583661079407]}}
{"id": "WebQTest-202", "question": "what is the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.005144914612174034, -0.02695925161242485, -0.1431395411491394], "norm_scores": [0.3509356677532196, 0.34336310625076294, 0.30570125579833984]}}
{"id": "WebQTest-204", "question": "what is the capital of modern egypt", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.010129742324352264, -0.024351248517632484, -0.045764170587062836], "norm_scores": [0.3388828635215759, 0.3340975344181061, 0.327019602060318]}}
{"id": "WebQTest-205", "question": "what school did sir ernest rutherford go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did sir ernest rutherford go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>"], "scores": [-0.013599377125501633, -0.018504835665225983, -0.0293735284358263], "norm_scores": [0.33563172817230225, 0.33398932218551636, 0.3303789496421814]}}
{"id": "WebQTest-206", "question": "what position does brandon roy play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["basketball.basketball_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position does brandon roy play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.009509283117949963, -0.031838998198509216, -0.038995351642370224], "norm_scores": [0.33911386132240295, 0.33162546157836914, 0.3292606770992279]}}
{"id": "WebQTest-207", "question": "where is harvard university", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harvard university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>"], "scores": [-0.003468922572210431, -0.04817919060587883, -0.07668734341859818], "norm_scores": [0.346539705991745, 0.33138707280158997, 0.32207322120666504]}}
{"id": "WebQTest-209", "question": "where is laos in world map", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"]], "ground_paths": [["book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is laos in world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0152413509786129, -0.027537191286683083, -0.03437720611691475], "norm_scores": [0.33683353662490845, 0.33271723985671997, 0.3304492235183716]}}
{"id": "WebQTest-210", "question": "where is tyrese gibson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tyrese gibson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.005991695914417505, -0.02462282031774521, -0.0805334597826004], "norm_scores": [0.3436768352985382, 0.3373330235481262, 0.3189900815486908]}}
{"id": "WebQTest-211", "question": "what countries are near italy", "prediction": [["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["geography.river.basin_countries", "location.location.partially_contains"], ["location.location.containedby", "geography.river.basin_countries"], ["organization.organization.founders", "base.nations.supranational_body.member"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["base.locations.countries.continent", "location.location.containedby"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["location.country.official_language", "location.country.official_language"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["location.country.currency_used", "location.country.currency_used"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["organization.organization.founders", "organization.organization.founders"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.country.official_language", "language.human_language.main_country"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_contains", "geography.river.origin"], ["base.locations.countries.continent", "location.location.contains_major_portion_of"], ["location.country.currency_used", "finance.currency.countries_used"], ["military.military_combatant.includes_allies", "military.military_combatant.includes_allies"], ["location.location.partially_contains", "location.location.partially_containedby"], ["travel.guidebook.travel_destinations_covered", "travel.travel_destination.guidebooks"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.containedby", "base.locations.countries.continent"], ["fictional_universe.fictional_setting.works_set_here", "fictional_universe.work_of_fiction.setting"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are near italy [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03569933399558067, -0.03745919093489647, -0.04019889980554581], "norm_scores": [0.3340289890766144, 0.33344167470932007, 0.33252936601638794]}}
{"id": "WebQTest-212", "question": "which states does the colorado river run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which states does the colorado river run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012339743785560131, -0.019164172932505608, -0.04009154438972473], "norm_scores": [0.3371739089488983, 0.3348807096481323, 0.327945351600647]}}
{"id": "WebQTest-213", "question": "who did scarlett johansson date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.languages", "people.person.languages"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["people.person.children", "people.person.children"], ["film.film.executive_produced_by", "film.producer.films_executive_produced"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.spouse_s", "people.marriage.spouse"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did scarlett johansson date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.017041532322764397, -0.021890947595238686, -0.03382325544953346], "norm_scores": [0.3357371389865875, 0.3341129422187805, 0.33014991879463196]}}
{"id": "WebQTest-214", "question": "which continents were part of the roman empire", "prediction": [["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.location.partiallycontains"], ["location.country.official_language", "language.human_language.region"], ["book.book_subject.works", "book.written_work.subjects"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which continents were part of the roman empire [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.region </PATH>", "<PATH> location.location.partially_contained_by <SEP> location.partial_containment_relationship.partially_contained_by </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.017791450023651123, -0.030761318281292915, -0.03622273728251457], "norm_scores": [0.33683058619499207, 0.33249014616012573, 0.3306792378425598]}}
{"id": "WebQTest-215", "question": "where was elvis costello born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was elvis costello born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.016427310183644295, -0.018102295696735382, -0.033345360308885574], "norm_scores": [0.3353959321975708, 0.33483463525772095, 0.32976940274238586]}}
{"id": "WebQTest-216", "question": "where are the gobi desert located on a map", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the gobi desert located on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0015278159407898784, -0.059035640209913254, -0.0767211765050888], "norm_scores": [0.348228394985199, 0.32876747846603394, 0.3230041563510895]}}
{"id": "WebQTest-217", "question": "what type of government does the us follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does the us follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.0065951417200267315, -0.020342249423265457, -0.1676947921514511], "norm_scores": [0.35241618752479553, 0.3476046025753021, 0.29997920989990234]}}
{"id": "WebQTest-218", "question": "in which continent is germany", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which continent is germany [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00708354078233242, -0.03237629309296608, -0.0556756965816021], "norm_scores": [0.341577410697937, 0.3330463171005249, 0.3253762423992157]}}
{"id": "WebQTest-220", "question": "what language turkey people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language turkey people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007121640257537365, -0.030475173145532608, -0.0524408258497715], "norm_scores": [0.3409932255744934, 0.3331221044063568, 0.3258846402168274]}}
{"id": "WebQTest-221", "question": "who does jeremy lin play for", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.place_of_birth", "sports.sports_team.location"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy lin play for [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.0139534343034029, -0.02561316266655922, -0.033279214054346085], "norm_scores": [0.33678334951400757, 0.33287936449050903, 0.330337256193161]}}
{"id": "WebQTest-222", "question": "where is the seychelles on world map", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.contains"]], "ground_paths": [["base.locations.continents.countries_within"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seychelles on world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.003873815992847085, -0.047813985496759415, -0.053353242576122284], "norm_scores": [0.3437919318675995, 0.32901275157928467, 0.32719531655311584]}}
{"id": "WebQTest-224", "question": "where did william morris go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did william morris go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015433832071721554, -0.015540830790996552, -0.02678559720516205], "norm_scores": [0.3346042335033417, 0.33456844091415405, 0.33082735538482666]}}
{"id": "WebQTest-225", "question": "what state is mount st. helens in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is mount st. helens in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>"], "scores": [-0.003994351718574762, -0.032951150089502335, -0.14001677930355072], "norm_scores": [0.3515826165676117, 0.3415479063987732, 0.3068694472312927]}}
{"id": "WebQTest-226", "question": "who is jimmy savile", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jimmy savile [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0007511028670705855, -0.07335459440946579, -0.07771380245685577], "norm_scores": [0.35015302896499634, 0.32563167810440063, 0.32421526312828064]}}
{"id": "WebQTest-227", "question": "where were the seneca indians located", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the seneca indians located [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0007258269470185041, -0.08427391946315765, -0.10015738755464554], "norm_scores": [0.3539574146270752, 0.32558661699295044, 0.32045602798461914]}}
{"id": "WebQTest-231", "question": "when did florida marlins join mlb", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.championships"], ["sports.sports_league_participation.league", "sports.sports_league_participation.season"]], "ground_paths": [["baseball.baseball_team.team_stats", "baseball.baseball_team_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did florida marlins join mlb [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_league_participation.season </PATH>"], "scores": [-0.015852591022849083, -0.01928277686238289, -0.03872641175985336], "norm_scores": [0.3362518548965454, 0.3351004123687744, 0.32864779233932495]}}
{"id": "WebQTest-232", "question": "what countries have spanish as the national language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have spanish as the national language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.01119261421263218, -0.022920504212379456, -0.030627094209194183], "norm_scores": [0.3368031084537506, 0.33287617564201355, 0.33032071590423584]}}
{"id": "WebQTest-233", "question": "what to see in phoenix arizona", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in phoenix arizona [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013863983564078808, -0.014090340584516525, -0.035880882292985916], "norm_scores": [0.3357961177825928, 0.33572012186050415, 0.3284837305545807]}}
{"id": "WebQTest-234", "question": "what countries does the panama canal go through", "prediction": [["location.location.containedby"], ["geography.body_of_water.bridges", "location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does the panama canal go through [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.body_of_water.bridges <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0010468731634318829, -0.07123111188411713, -0.09636401385068893], "norm_scores": [0.3519507348537445, 0.3280962407588959, 0.31995299458503723]}}
{"id": "WebQTest-235", "question": "what is charlie chaplin famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["film.film.directed_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is charlie chaplin famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> film.film.directed_by </PATH>"], "scores": [-0.0038597355596721172, -0.033809222280979156, -0.13066819310188293], "norm_scores": [0.3507052958011627, 0.34035760164260864, 0.308937132358551]}}
{"id": "WebQTest-237", "question": "what did fred durst do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did fred durst do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.0035011661238968372, -0.05778495967388153, -0.06860148161649704], "norm_scores": [0.3467242121696472, 0.32840442657470703, 0.32487139105796814]}}
{"id": "WebQTest-239", "question": "where do american bulldogs originate from", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.place_of_origin"]], "ground_paths": [["biology.animal_breed.place_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do american bulldogs originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.place_of_origin </PATH>"], "scores": [-0.008112144656479359, -0.016629930585622787, -0.08113960176706314], "norm_scores": [0.3423375189304352, 0.3394339382648468, 0.31822851300239563]}}
{"id": "WebQTest-240", "question": "what colleges did albert einstein teach at", "prediction": [["people.person.employment_history", "business.employer.employees"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.employment_history", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"], ["education.department.academics_old", "education.university.departments"], ["people.person.nationality", "organization.organization.founders"], ["people.person.education", "education.education.institution"], ["business.employment_tenure.person", "business.employer.employees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did albert einstein teach at [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.017314516007900238, -0.028726905584335327, -0.03174512833356857], "norm_scores": [0.3362106680870056, 0.3323955237865448, 0.3313938081264496]}}
{"id": "WebQTest-241", "question": "who plays the voice of brian on family guy", "prediction": [["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of brian on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>"], "scores": [-0.022773979231715202, -0.022864315658807755, -0.030392900109291077], "norm_scores": [0.33418890833854675, 0.334158718585968, 0.3316524028778076]}}
{"id": "WebQTest-243", "question": "where did george w bush live as a child", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george w bush live as a child [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.011001162230968475, -0.025393903255462646, -0.039810676127672195], "norm_scores": [0.3381449282169342, 0.3333129286766052, 0.3285421133041382]}}
{"id": "WebQTest-245", "question": "what year was the first miss america pageant held", "prediction": [["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"], ["time.recurring_event.current_frequency"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was the first miss america pageant held [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.007809631526470184, -0.0240695271641016, -0.052656691521406174], "norm_scores": [0.34013429284095764, 0.334648460149765, 0.32521727681159973]}}
{"id": "WebQTest-246", "question": "what was nikola tesla inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.inventor.inventions"], ["symbols.name_source.namesakes"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.005983551032841206, -0.03920337185263634, -0.054961562156677246], "norm_scores": [0.34252113103866577, 0.3313295543193817, 0.3261493146419525]}}
{"id": "WebQTest-247", "question": "where did flemish people come from", "prediction": [["people.ethnicity.geographic_distribution"], ["base.popstra.ethnicity.geographic_distribution"], ["people.ethnicity.included_in_group"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did flemish people come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> base.popstra.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>"], "scores": [-7.743923197267577e-05, -0.13063284754753113, -0.1308499276638031], "norm_scores": [0.36297309398651123, 0.3185480535030365, 0.31847891211509705]}}
{"id": "WebQTest-248", "question": "what state new york city belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state new york city belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.011672292836010456, -0.04908847063779831, -0.05008493736386299], "norm_scores": [0.34181126952171326, 0.3292583227157593, 0.32893040776252747]}}
{"id": "WebQTest-249", "question": "what was the cause of death for bruce lee", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for bruce lee [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.001078304136171937, -0.04777459427714348, -0.10792271792888641], "norm_scores": [0.35050299763679504, 0.3345120847225189, 0.3149849474430084]}}
{"id": "WebQTest-250", "question": "what type of government does usa follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does usa follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.007876994088292122, -0.01778598316013813, -0.15668387711048126], "norm_scores": [0.350646436214447, 0.3471890389919281, 0.3021644949913025]}}
{"id": "WebQTest-251", "question": "what type of currency do they use in england", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in england [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.002311436925083399, -0.03943929076194763, -0.11633721739053726], "norm_scores": [0.35016605257987976, 0.3374035358428955, 0.31243041157722473]}}
{"id": "WebQTest-252", "question": "where was benjamin franklin educated", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was benjamin franklin educated [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01263184379786253, -0.025404533371329308, -0.027940822765231133], "norm_scores": [0.3364606201648712, 0.33219045400619507, 0.3313489556312561]}}
{"id": "WebQTest-253", "question": "what countries included in the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.02292894385755062, -0.025430571287870407, -0.0304978396743536], "norm_scores": [0.3344525098800659, 0.33361688256263733, 0.33193060755729675]}}
{"id": "WebQTest-254", "question": "what countries in the world speak chinese", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries in the world speak chinese [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.018824920058250427, -0.021344223991036415, -0.031606122851371765], "norm_scores": [0.3350326120853424, 0.33418962359428406, 0.33077773451805115]}}
{"id": "WebQTest-255", "question": "where obama went to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where obama went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.011171533726155758, -0.02866487205028534, -0.029829684644937515], "norm_scores": [0.3373621702194214, 0.33151188492774963, 0.331125944852829]}}
{"id": "WebQTest-256", "question": "what is the primary language of china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the primary language of china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.007092136889696121, -0.03606947511434555, -0.037340402603149414], "norm_scores": [0.33994609117507935, 0.3302367031574249, 0.3298172354698181]}}
{"id": "WebQTest-257", "question": "who was the soviet leader during world war ii", "prediction": [["military.military_person.participated_in_conflicts"], ["people.person.nationality"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the soviet leader during world war ii [/INST]", "raw_output": {"paths": ["<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.010702083818614483, -0.0134000638499856, -0.08934325724840164], "norm_scores": [0.3422691822052002, 0.34134697914123535, 0.31638386845588684]}}
{"id": "WebQTest-258", "question": "which of the following does australia export the most", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which of the following does australia export the most [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.00888390839099884, -0.029984086751937866, -0.0333649218082428], "norm_scores": [0.3384166955947876, 0.33135083317756653, 0.33023250102996826]}}
{"id": "WebQTest-260", "question": "where was the city of david", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the city of david [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.005797425284981728, -0.03458694368600845, -0.08138687908649445], "norm_scores": [0.3449682593345642, 0.3351783752441406, 0.31985345482826233]}}
{"id": "WebQTest-261", "question": "what is cindy sherman known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is cindy sherman known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.006205020006746054, -0.032386381179094315, -0.0511292889714241], "norm_scores": [0.3412703573703766, 0.33245137333869934, 0.32627829909324646]}}
{"id": "WebQTest-262", "question": "what tv shows did shawnee smith play in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "ground_paths": [["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["film.performance.actor", "film.film.starring"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv shows did shawnee smith play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>"], "scores": [-0.02214420586824417, -0.02302638627588749, -0.031924694776535034], "norm_scores": [0.3345169126987457, 0.3342219591140747, 0.33126112818717957]}}
{"id": "WebQTest-264", "question": "what does australia use for currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does australia use for currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.011792833916842937, -0.015166186727583408, -0.12130950391292572], "norm_scores": [0.34567394852638245, 0.3445098400115967, 0.30981624126434326]}}
{"id": "WebQTest-265", "question": "which airport to fly into rome", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0060999696142971516, -0.030030416324734688, -0.060478098690509796], "norm_scores": [0.34206423163414, 0.3339756429195404, 0.3239601254463196]}}
{"id": "WebQTest-266", "question": "what is there to do for fun in kansas city", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.place_with_neighborhoods.neighborhoods"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do for fun in kansas city [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011239754036068916, -0.016923779621720314, -0.03275385871529579], "norm_scores": [0.33635517954826355, 0.33444875478744507, 0.32919609546661377]}}
{"id": "WebQTest-267", "question": "where was martin luther king jr raised", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr raised [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.0006612647557631135, -0.06148388981819153, -0.11169219017028809], "norm_scores": [0.35262158513069153, 0.33181342482566833, 0.31556496024131775]}}
{"id": "WebQTest-268", "question": "what year was lebron james rookie season", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was lebron james rookie season [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.010202724486589432, -0.014783271588385105, -0.03551777824759483], "norm_scores": [0.3366512954235077, 0.3351127505302429, 0.328235924243927]}}
{"id": "WebQTest-269", "question": "where does delaware river start", "prediction": [["geography.river.origin"], ["location.location.containedby"], ["location.location.partial_containedby"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does delaware river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partial_containedby </PATH>"], "scores": [-0.0009585883817635477, -0.0804084837436676, -0.10546811670064926], "norm_scores": [0.35405871272087097, 0.327017217874527, 0.31892409920692444]}}
{"id": "WebQTest-270", "question": "what kind of monarchy does japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["royalty.monarch.kingdom"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of monarchy does japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> royalty.monarch.kingdom </PATH>"], "scores": [-0.007608008570969105, -0.024157414212822914, -0.0457439161837101], "norm_scores": [0.33942386507987976, 0.3338527977466583, 0.3267233371734619]}}
{"id": "WebQTest-271", "question": "where did mitt romney 's parents come from", "prediction": [["people.person.place_of_birth"], ["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mitt romney 's parents come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.01494262833148241, -0.02696005441248417, -0.029303651303052902], "norm_scores": [0.3362705409526825, 0.3322536051273346, 0.3314758539199829]}}
{"id": "WebQTest-273", "question": "when did michael jordan return to the nba", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael jordan return to the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.011644747108221054, -0.02351452223956585, -0.025937717407941818], "norm_scores": [0.33624646067619324, 0.33227887749671936, 0.3314746618270874]}}
{"id": "WebQTest-276", "question": "where was mission san rafael located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mission san rafael located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.0006343463901430368, -0.07562582939863205, -0.10835669189691544], "norm_scores": [0.353903591632843, 0.3283345699310303, 0.3177618384361267]}}
{"id": "WebQTest-277", "question": "who was esther 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["visual_art.art_subject.artwork_on_the_subject", "visual_art.art_subject.artwork_on_the_subject"], ["people.person.spouse_s", "people.person.spouse_s"], ["book.translated_work.translations", "book.book_subject.works"], ["visual_art.art_subject.artwork_on_the_subject", "visual_art.artwork.art_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was esther 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.028603345155715942, -0.03708994388580322, -0.048454027622938156], "norm_scores": [0.33648571372032166, 0.3336421847343445, 0.32987213134765625]}}
{"id": "WebQTest-278", "question": "who 's dating claire danes", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["people.person.children", "people.person.parents"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who 's dating claire danes [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.018030688166618347, -0.027329422533512115, -0.030190201476216316], "norm_scores": [0.33572161197662354, 0.33261430263519287, 0.331664115190506]}}
{"id": "WebQTest-279", "question": "what county is st paul va in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul va in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.018525512889027596, -0.024769065901637077, -0.029823752120137215], "norm_scores": [0.3352845311164856, 0.3331977128982544, 0.3315177261829376]}}
{"id": "WebQTest-281", "question": "what type of sports do japanese play", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of sports do japanese play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.009914499707520008, -0.01505520660430193, -0.13156674802303314], "norm_scores": [0.34718257188796997, 0.34540238976478577, 0.3074149787425995]}}
{"id": "WebQTest-282", "question": "which countries share a border with russia", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["location.location.time_zones", "time.time_zone.locations_in_this_time_zone"], ["base.locations.continents.countries_within", "base.locations.countries.continent"], ["time.event.locations", "location.location.events"], ["geography.river.basin_countries", "location.location.partially_contains"], ["base.rivalries.rival.rivals", "base.rivalries.rival.rivals"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.country.languages_spoken", "language.human_language.main_country"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.partially_containedby", "base.schemastaging.disputed_location.claimed_by"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_containedby", "base.locations.continents.countries_within"], ["location.location.events", "location.location.events"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["location.location.containedby", "location.location.containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.country.languages_spoken", "location.country.official_language"], ["location.location.partially_containedby", "location.location.containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "geography.lake.basin_countries"], ["location.location.partially_containedby", "geography.river.origin"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.rivalries.rival.rivals", "base.rivalries.rivalry.rival"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.locations.continents.countries_within", "base.locations.continents.countries_within"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["organization.organization.founders", "organization.organization.founders"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["language.human_language.main_country", "location.country.languages_spoken"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_contains", "location.location.containedby"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_contains", "geography.river.origin"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["base.locations.continents.countries_within", "location.location.partially_contains"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["location.location.partially_containedby", "base.locations.countries.continent"], ["time.event.locations", "time.event.locations"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["language.human_language.main_country", "language.human_language.countries_spoken_in"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries share a border with russia [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03513781353831291, -0.037725791335105896, -0.041582703590393066], "norm_scores": [0.3343373239040375, 0.33347320556640625, 0.33218950033187866]}}
{"id": "WebQTest-284", "question": "where did the latin language originate from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the latin language originate from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.00471104308962822, -0.03338860347867012, -0.050791431218385696], "norm_scores": [0.3416823446750641, 0.33202290534973145, 0.32629477977752686]}}
{"id": "WebQTest-286", "question": "what school did karl benz go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did karl benz go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.017976878210902214, -0.020793648436665535, -0.020971566438674927], "norm_scores": [0.33397936820983887, 0.3330399692058563, 0.3329806923866272]}}
{"id": "WebQTest-287", "question": "what movies has carmen electra been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.film_character.portrayed_in_films", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has carmen electra been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.02111314982175827, -0.026522867381572723, -0.0380333811044693], "norm_scores": [0.33581534028053284, 0.3340035676956177, 0.3301810622215271]}}
{"id": "WebQTest-289", "question": "when do world war ii end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when do world war ii end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.009958657436072826, -0.035555288195610046, -0.04309828579425812], "norm_scores": [0.33988964557647705, 0.3312999904155731, 0.3288104236125946]}}
{"id": "WebQTest-291", "question": "where did morgan freeman graduate", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did morgan freeman graduate [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012749282643198967, -0.012770889326930046, -0.042568959295749664], "norm_scores": [0.33663246035575867, 0.3366251587867737, 0.32674235105514526]}}
{"id": "WebQTest-294", "question": "who was carolus linnaeus", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was carolus linnaeus [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0008869490120559931, -0.07022672891616821, -0.07838103920221329], "norm_scores": [0.3498408794403076, 0.3264049291610718, 0.3237541615962982]}}
{"id": "WebQTest-295", "question": "what type of government does germany have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does germany have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009550231508910656, -0.014540162868797779, -0.14023321866989136], "norm_scores": [0.3481265604496002, 0.34639376401901245, 0.30547964572906494]}}
{"id": "WebQTest-296", "question": "what kind of government does libya have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does libya have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005805027671158314, -0.02092590741813183, -0.14876607060432434], "norm_scores": [0.3506580889225006, 0.3453957140445709, 0.3039461672306061]}}
{"id": "WebQTest-297", "question": "what time zone is anaheim california", "prediction": [["location.location.time_zones"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is anaheim california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.0359519365010783e-05, -0.17139151692390442, -0.1830359250307083], "norm_scores": [0.3737972378730774, 0.31492432951927185, 0.31127846240997314]}}
{"id": "WebQTest-298", "question": "what year did the orioles go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the orioles go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.006951007526367903, -0.017763087525963783, -0.07616452872753143], "norm_scores": [0.3421876132488251, 0.3385077714920044, 0.3193046450614929]}}
{"id": "WebQTest-300", "question": "who is sir francis bacon", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["common.topic.image"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir francis bacon [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.image </PATH>"], "scores": [-0.00010861603368539363, -0.11799339205026627, -0.1475452482700348], "norm_scores": [0.3634096384048462, 0.32299792766571045, 0.31359240412712097]}}
{"id": "WebQTest-301", "question": "what were amelia earhart 's achievements", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["base.activism.activist.area_of_activism"]], "ground_paths": [["fictional_universe.fictional_character.occupation"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were amelia earhart 's achievements [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.0062194690108299255, -0.03408699482679367, -0.05163751542568207], "norm_scores": [0.3415166437625885, 0.3321307897567749, 0.326352596282959]}}
{"id": "WebQTest-302", "question": "what are the official colors of the dallas cowboys", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.sports_team_colors.team"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official colors of the dallas cowboys [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team_colors.team </PATH>"], "scores": [-0.00010802443284774199, -0.09592849761247635, -0.11404955387115479], "norm_scores": [0.35702335834503174, 0.3244011104106903, 0.31857556104660034]}}
{"id": "WebQTest-303", "question": "where did margaret hoover go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did margaret hoover go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.009679840877652168, -0.024443330243229866, -0.027813028544187546], "norm_scores": [0.33699819445610046, 0.33205944299697876, 0.33094239234924316]}}
{"id": "WebQTest-304", "question": "what is jamie oliver children names", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie oliver children names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.011714517138898373, -0.016079897060990334, -0.1543724685907364], "norm_scores": [0.3493212163448334, 0.3477996289730072, 0.3028791546821594]}}
{"id": "WebQTest-305", "question": "where does bradley walsh live", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does bradley walsh live [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.017610307782888412, -0.01765216700732708, -0.036237530410289764], "norm_scores": [0.3354012370109558, 0.3353872299194336, 0.3292114734649658]}}
{"id": "WebQTest-306", "question": "who is the head coach of inter milan", "prediction": [["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["base.schemastaging.sports_team_extra.managers", "base.schemastaging.sports_team_manager_tenure.manager"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["base.schemastaging.sports_team_manager_tenure.team", "base.schemastaging.sports_team_manager_tenure.manager"], ["soccer.football_team_management_tenure.team", "soccer.football_team_manager.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the head coach of inter milan [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> base.schemastaging.sports_team_extra.managers <SEP> base.schemastaging.sports_team_manager_tenure.manager </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.019971473142504692, -0.02197812870144844, -0.024785825982689857], "norm_scores": [0.3340914249420166, 0.3334217071533203, 0.3324868381023407]}}
{"id": "WebQTest-308", "question": "who did reese witherspoon get married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did reese witherspoon get married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.017197104170918465, -0.028926583006978035, -0.03183851018548012], "norm_scores": [0.3362696170806885, 0.33234837651252747, 0.33138200640678406]}}
{"id": "WebQTest-310", "question": "what kind of money should i take to costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money should i take to costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.007656038273125887, -0.019024822860956192, -0.11800103634595871], "norm_scores": [0.3467140793800354, 0.3427946865558624, 0.3104911744594574]}}
{"id": "WebQTest-311", "question": "where is the ufc headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["sports.sports_league.sport", "sports.sports_team.location"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["common.topic.subject_of", "people.person.place_of_birth"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the ufc headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> sports.sports_league.sport <SEP> sports.sports_team.location </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.009707354940474033, -0.037980251014232635, -0.045657359063625336], "norm_scores": [0.34050536155700684, 0.3310130536556244, 0.3284815847873688]}}
{"id": "WebQTest-312", "question": "who plays captain kirk in star trek", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays captain kirk in star trek [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.02567358687520027, -0.028123395517468452, -0.029455499723553658], "norm_scores": [0.334026038646698, 0.33320876955986023, 0.3327651917934418]}}
{"id": "WebQTest-313", "question": "who does albert pujols play for 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does albert pujols play for 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.011042849160730839, -0.023744622245430946, -0.026187492534518242], "norm_scores": [0.33643436431884766, 0.33218806982040405, 0.3313775658607483]}}
{"id": "WebQTest-314", "question": "what kind of government is sweden", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011533759534358978, -0.013090678490698338, -0.14513027667999268], "norm_scores": [0.3480212986469269, 0.3474798798561096, 0.3044988214969635]}}
{"id": "WebQTest-315", "question": "where was country singer george jones born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was country singer george jones born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.010372373275458813, -0.01912989467382431, -0.050328824669122696], "norm_scores": [0.3387405276298523, 0.33578693866729736, 0.32547250390052795]}}
{"id": "WebQTest-316", "question": "what did shakespeare become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.person.profession"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["fictional_universe.fictional_character.occupation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shakespeare become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.005114179104566574, -0.034196674823760986, -0.05583597719669342], "norm_scores": [0.342245489358902, 0.3324354588985443, 0.3253190815448761]}}
{"id": "WebQTest-317", "question": "where does russia import from", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does russia import from [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.027781277894973755, -0.028658784925937653, -0.031038545072078705], "norm_scores": [0.3337927460670471, 0.3334999680519104, 0.3327072858810425]}}
{"id": "WebQTest-318", "question": "what is the money of switzerland called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the money of switzerland called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency </PATH>"], "scores": [-0.004192073829472065, -0.030360516160726547, -0.16575801372528076], "norm_scores": [0.35398462414741516, 0.34484153985977173, 0.3011738061904907]}}
{"id": "WebQTest-321", "question": "what do people in australia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in australia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006796162575483322, -0.022720888257026672, -0.07101522386074066], "norm_scores": [0.34223124384880066, 0.3368244469165802, 0.32094430923461914]}}
{"id": "WebQTest-322", "question": "what has ian somerhalder acted in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.film.produced_by"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.producer.film"], ["film.performance.actor", "film.film.starring"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ian somerhalder acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.01883377879858017, -0.030924372375011444, -0.034368593245744705], "norm_scores": [0.3364095091819763, 0.33236661553382874, 0.33122384548187256]}}
{"id": "WebQTest-323", "question": "what movies did ron howard director", "prediction": [["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"]], "ground_paths": [["film.producer.film"], ["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did ron howard director [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.director.film </PATH>"], "scores": [-0.022265637293457985, -0.03129255026578903, -0.03345475718379021], "norm_scores": [0.3355831801891327, 0.3325675427913666, 0.33184924721717834]}}
{"id": "WebQTest-326", "question": "what did gerald r ford die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gerald r ford die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.001233605551533401, -0.048778001219034195, -0.10327433794736862], "norm_scores": [0.3500713109970093, 0.33381685614585876, 0.31611186265945435]}}
{"id": "WebQTest-327", "question": "what two continents is turkey on", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["base.locations.countries.continent"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two continents is turkey on [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.012320845387876034, -0.02920166775584221, -0.04601679742336273], "norm_scores": [0.33896851539611816, 0.33329448103904724, 0.3277369737625122]}}
{"id": "WebQTest-328", "question": "where did kevin love go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin love go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.015608279034495354, -0.021262846887111664, -0.027319101616740227], "norm_scores": [0.33526456356048584, 0.33337414264678955, 0.3313612639904022]}}
{"id": "WebQTest-329", "question": "what type of government was formed when italy unified", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government was formed when italy unified [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.010263478383421898, -0.012886319309473038, -0.1109493300318718], "norm_scores": [0.34463775157928467, 0.343735009431839, 0.3116273283958435]}}
{"id": "WebQTest-331", "question": "where was the temple of karnak built", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["architecture.structure.architect"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the temple of karnak built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> architecture.structure.architect </PATH>"], "scores": [-0.0017740868497639894, -0.06534896790981293, -0.07462158054113388], "norm_scores": [0.3486572206020355, 0.32718130946159363, 0.32416149973869324]}}
{"id": "WebQTest-333", "question": "what movies did morgan freeman star in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.film.executive_produced_by"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["theater.theater_actor.theater_roles", "theater.theater_role.play"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["film.producer.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"], ["film.producer.films_executive_produced"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["common.topic.notable_types", "common.topic.notable_types"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did morgan freeman star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.018671393394470215, -0.037446532398462296, -0.038202349096536636], "norm_scores": [0.33760306239128113, 0.33132362365722656, 0.3310733139514923]}}
{"id": "WebQTest-334", "question": "who did john kennedy have affairs with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["music.recording.featured_artists", "music.recording.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did john kennedy have affairs with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.020508624613285065, -0.03306761011481285, -0.048154063522815704], "norm_scores": [0.3378089964389801, 0.3335929811000824, 0.3285980224609375]}}
{"id": "WebQTest-335", "question": "what type of money to take to cuba", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money to take to cuba [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.01085111778229475, -0.015274135395884514, -0.1093255952000618], "norm_scores": [0.34461304545402527, 0.3430921733379364, 0.3122948408126831]}}
{"id": "WebQTest-336", "question": "what currency should you take to morocco", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to morocco [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.011675777845084667, -0.014264133758842945, -0.10836652666330338], "norm_scores": [0.34420424699783325, 0.3433144986629486, 0.31248125433921814]}}
{"id": "WebQTest-337", "question": "who is mary mcleod bethune for kids", "prediction": [["people.person.profession"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is mary mcleod bethune for kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.0005280047771520913, -0.08397552371025085, -0.08549833297729492], "norm_scores": [0.35230138897895813, 0.3240959048271179, 0.3236027657985687]}}
{"id": "WebQTest-338", "question": "what do they call money in japan", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they call money in japan [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.013275821693241596, -0.017615387216210365, -0.03912702947854996], "norm_scores": [0.33668336272239685, 0.3352254331111908, 0.32809120416641235]}}
{"id": "WebQTest-339", "question": "what nationality is mother teresa", "prediction": [["people.person.nationality"], ["people.person.ethnicity"], ["people.ethnicity.people"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is mother teresa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.005968431476503611, -0.036303382366895676, -0.06338522583246231], "norm_scores": [0.3431330919265747, 0.3328804671764374, 0.3239864408969879]}}
{"id": "WebQTest-340", "question": "who is richard pryor son", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is richard pryor son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.003384578740224242, -0.03793758898973465, -0.09684143960475922], "norm_scores": [0.34760671854019165, 0.33580100536346436, 0.3165923058986664]}}
{"id": "WebQTest-341", "question": "what countries share borders with spain", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"]], "ground_paths": [["royalty.royal_line.kingdom_s_ruled", "people.family.country"], ["geography.river.basin_countries", "location.location.partially_contains"], ["location.country.form_of_government", "location.country.form_of_government"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["location.location.primarily_containedby", "location.location.contains_major_portion_of"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.country.currency_formerly_used", "finance.currency.countries_formerly_used"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.primarily_containedby"], ["location.location.partially_containedby", "location.location.containedby"], ["location.location.primarily_containedby", "base.locations.continents.countries_within"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.primarily_containedby", "location.location.primarily_containedby"], ["location.location.containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.contains"], ["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.primarily_containedby", "location.location.containedby"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["finance.currency.countries_used", "location.country.currency_used"], ["film.film.country", "film.film.country"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["location.country.internet_tld", "location.country.internet_tld"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries share borders with spain [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.0357404425740242, -0.04076432064175606, -0.045152951031923294], "norm_scores": [0.33493876457214355, 0.33326029777526855, 0.3318009376525879]}}
{"id": "WebQTest-342", "question": "what countries have english as their official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have english as their official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.017205432057380676, -0.020115429535508156, -0.022574428468942642], "norm_scores": [0.3342536985874176, 0.3332824409008026, 0.3324638903141022]}}
{"id": "WebQTest-343", "question": "what international organizations is china part of", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization_member.member_of", "organization.organization_membership.organization"], ["organization.organization_membership.member", "organization.organization_membership.organization"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization_member.member_of", "organization.organization_membership.organization"], ["organization.organization_membership.member", "organization.membership_organization.members"], ["organization.organization_membership.member", "organization.organization_membership.organization"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what international organizations is china part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_member.member_of <SEP> organization.organization_membership.organization </PATH>", "<PATH> organization.organization_membership.member <SEP> organization.organization_membership.organization </PATH>"], "scores": [-0.024640025570988655, -0.02553367428481579, -0.02911149337887764], "norm_scores": [0.3339293599128723, 0.33363109827041626, 0.3324395418167114]}}
{"id": "WebQTest-344", "question": "what county is west st paul in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is west st paul in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.013121897354722023, -0.025652291253209114, -0.04157443344593048], "norm_scores": [0.33789536356925964, 0.33368781208992004, 0.3284168541431427]}}
{"id": "WebQTest-346", "question": "who does donnie wahlberg play in the sixth sense", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.performance.actor", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does donnie wahlberg play in the sixth sense [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.020226193591952324, -0.030464665964245796, -0.03769433870911598], "norm_scores": [0.33641743659973145, 0.3329906165599823, 0.3305918872356415]}}
{"id": "WebQTest-347", "question": "what did baron de montesquie influence", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did baron de montesquie influence [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.004884708672761917, -0.023059450089931488, -0.10803870856761932], "norm_scores": [0.3467433452606201, 0.34049829840660095, 0.31275835633277893]}}
{"id": "WebQTest-349", "question": "who did carlos boozer play for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did carlos boozer play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.009555676020681858, -0.015355498529970646, -0.05296584963798523], "norm_scores": [0.33878374099731445, 0.33682453632354736, 0.3243917226791382]}}
{"id": "WebQTest-351", "question": "who owns the portland press herald", "prediction": [["book.newspaper.owner"], ["periodicals.newspaper_circulation_area.newspapers", "business.business_operation.industry"], ["book.newspaper_owner.newspapers_owned"]], "ground_paths": [["book.newspaper_owner.newspapers_owned"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns the portland press herald [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.owner </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> business.business_operation.industry </PATH>", "<PATH> book.newspaper_owner.newspapers_owned </PATH>"], "scores": [-0.022556768730282784, -0.03745853528380394, -0.03871218115091324], "norm_scores": [0.3367929458618164, 0.3318113684654236, 0.3313956558704376]}}
{"id": "WebQTest-352", "question": "what did albert speer design", "prediction": [["architecture.architect.structures_designed"], ["architecture.structure.architect"], ["architecture.architecture.architect"]], "ground_paths": [["architecture.architect.structures_designed"], ["architecture.structure.architect"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did albert speer design [/INST]", "raw_output": {"paths": ["<PATH> architecture.architect.structures_designed </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.architecture.architect </PATH>"], "scores": [-0.0006094881100580096, -0.08462149649858475, -0.1074313148856163], "norm_scores": [0.3548482656478882, 0.326254665851593, 0.3188970685005188]}}
{"id": "WebQTest-354", "question": "what art movement did leonardo da vinci belong to", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art movement did leonardo da vinci belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.004192091058939695, -0.02512449212372303, -0.060804206877946854], "norm_scores": [0.34196850657463074, 0.3348847031593323, 0.3231467604637146]}}
{"id": "WebQTest-356", "question": "what airport is near arlington tx", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport is near arlington tx [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01093058567494154, -0.02147664688527584, -0.043265510350465775], "norm_scores": [0.33810150623321533, 0.3345545828342438, 0.3273438811302185]}}
{"id": "WebQTest-359", "question": "what team does heskey play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does heskey play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.027843892574310303, -0.029007572680711746, -0.03254656866192818], "norm_scores": [0.3339851200580597, 0.33359670639038086, 0.33241817355155945]}}
{"id": "WebQTest-360", "question": "where did emperor hirohito live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor hirohito live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>"], "scores": [-0.023290250450372696, -0.025490205734968185, -0.02857080101966858], "norm_scores": [0.3341647684574127, 0.3334304094314575, 0.33240482211112976]}}
{"id": "WebQTest-361", "question": "what to do and see in vienna austria", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["base.arthist.int_zm_nyek.helynev"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do and see in vienna austria [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01329105906188488, -0.018958503380417824, -0.024748753756284714], "norm_scores": [0.33523792028427124, 0.33334335684776306, 0.3314187526702881]}}
{"id": "WebQTest-362", "question": "when did conflict start in ireland", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.events"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did conflict start in ireland [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.005099430680274963, -0.043099354952573776, -0.04890831187367439], "norm_scores": [0.34248337149620056, 0.32971319556236267, 0.32780346274375916]}}
{"id": "WebQTest-365", "question": "what is there to do in peoria illinois", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do in peoria illinois [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.010502680204808712, -0.017529787495732307, -0.04281075671315193], "norm_scores": [0.3377002775669098, 0.33533552289009094, 0.32696422934532166]}}
{"id": "WebQTest-366", "question": "what language do british speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do british speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016170265153050423, -0.020523497834801674, -0.03716395050287247], "norm_scores": [0.33614784479141235, 0.3346877098083496, 0.32916444540023804]}}
{"id": "WebQTest-367", "question": "what makes elvis presley famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"]], "ground_paths": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what makes elvis presley famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.004382247105240822, -0.04267799109220505, -0.06734050810337067], "norm_scores": [0.3446598947048187, 0.33171042799949646, 0.3236296474933624]}}
{"id": "WebQTest-370", "question": "what language do people speak in the netherlands", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the netherlands [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013607146218419075, -0.016609597951173782, -0.0410594679415226], "norm_scores": [0.33670905232429504, 0.3356996178627014, 0.32759132981300354]}}
{"id": "WebQTest-371", "question": "what are the major languages spoken in greece", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in greece [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01674826629459858, -0.018837787210941315, -0.02642780728638172], "norm_scores": [0.3346406817436218, 0.333942174911499, 0.33141714334487915]}}
{"id": "WebQTest-374", "question": "what type of artist is henri matisse", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"], ["visual_art.art_form.artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of artist is henri matisse [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.art_form.artists </PATH>"], "scores": [-0.0006309445016086102, -0.07349849492311478, -0.08786805719137192], "norm_scores": [0.3513476550579071, 0.3266563415527344, 0.3219960033893585]}}
{"id": "WebQTest-375", "question": "what was the first book charles dickens wrote", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first book charles dickens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>"], "scores": [-0.0029568143654614687, -0.0354122519493103, -0.057005636394023895], "norm_scores": [0.3430000841617584, 0.3320465683937073, 0.3249534070491791]}}
{"id": "WebQTest-376", "question": "what books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["film.film_story_contributor.film_story_credits"], ["film.film.story_by"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.014953119680285454, -0.018457595258951187, -0.040181346237659454], "norm_scores": [0.3365202844142914, 0.33534303307533264, 0.328136682510376]}}
{"id": "WebQTest-377", "question": "what school did michael jordan attend", "prediction": [["people.person.education", "education.education.institution"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did michael jordan attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.018573608249425888, -0.022448062896728516, -0.025692414492368698], "norm_scores": [0.33455562591552734, 0.3332619071006775, 0.3321824371814728]}}
{"id": "WebQTest-379", "question": "what year does hitler die", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["base.activism.activist.area_of_activism"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year does hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.012934679165482521, -0.021390505135059357, -0.03878709673881531], "norm_scores": [0.3371477425098419, 0.3343088924884796, 0.32854336500167847]}}
{"id": "WebQTest-380", "question": "where is mount st helens volcano", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount st helens volcano [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>"], "scores": [-0.007671202998608351, -0.02891649678349495, -0.04627866670489311], "norm_scores": [0.340008020401001, 0.33286064863204956, 0.32713133096694946]}}
{"id": "WebQTest-381", "question": "what shows are shot in new york", "prediction": [["film.film.featured_film_locations"], ["tv.tv_program.filming_locations"], ["tv.tv_location.tv_shows_filmed_here"]], "ground_paths": [["tv.tv_program.filming_locations"], ["film.film.featured_film_locations"], ["tv.tv_location.tv_shows_filmed_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what shows are shot in new york [/INST]", "raw_output": {"paths": ["<PATH> film.film.featured_film_locations </PATH>", "<PATH> tv.tv_program.filming_locations </PATH>", "<PATH> tv.tv_location.tv_shows_filmed_here </PATH>"], "scores": [-0.015756988897919655, -0.02193673513829708, -0.022220751270651817], "norm_scores": [0.33473965525627136, 0.33267742395401, 0.332582950592041]}}
{"id": "WebQTest-382", "question": "what book did tupac write", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did tupac write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.014474712312221527, -0.018701817840337753, -0.0395660437643528], "norm_scores": [0.33658671379089355, 0.33516693115234375, 0.3282463550567627]}}
{"id": "WebQTest-383", "question": "what language do chinese people write in", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.writing_system"], ["language.language_writing_system.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do chinese people write in [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.008357428945600986, -0.03391088917851448, -0.03924189507961273], "norm_scores": [0.33963263034820557, 0.33106380701065063, 0.3293035924434662]}}
{"id": "WebQTest-384", "question": "what was robert burns famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>"], "scores": [-0.0059728375636041164, -0.027441928163170815, -0.07152221351861954], "norm_scores": [0.3430163860321045, 0.335730642080307, 0.3212529718875885]}}
{"id": "WebQTest-385", "question": "when did william mckinley died", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did william mckinley died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0037212714087218046, -0.03694930300116539, -0.052102550864219666], "norm_scores": [0.3424553871154785, 0.3312632441520691, 0.3262813985347748]}}
{"id": "WebQTest-386", "question": "how deep is lake merritt oakland", "prediction": [["geography.lake.basin_countries"], ["location.location.containedby"], ["geography.lake.depth"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how deep is lake merritt oakland [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.lake.depth </PATH>"], "scores": [-0.011728744953870773, -0.03915635496377945, -0.046034663915634155], "norm_scores": [0.3402262032032013, 0.33102142810821533, 0.32875239849090576]}}
{"id": "WebQTest-388", "question": "what are the important holidays of islam", "prediction": [["time.holiday.featured_in_religions"], ["religion.religion.holidays"], ["location.location.events"]], "ground_paths": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the important holidays of islam [/INST]", "raw_output": {"paths": ["<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> religion.religion.holidays </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.008091858588159084, -0.016701577231287956, -0.14475730061531067], "norm_scores": [0.34919992089271545, 0.34620630741119385, 0.3045938313007355]}}
{"id": "WebQTest-389", "question": "what type of music did claude debussy play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did claude debussy play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.004151869099587202, -0.030395060777664185, -0.09643560647964478], "norm_scores": [0.3465069830417633, 0.33753183484077454, 0.31596115231513977]}}
{"id": "WebQTest-390", "question": "when was the last time the toronto maple leafs were in the stanley cup finals", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs were in the stanley cup finals [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.01071496307849884, -0.014800425618886948, -0.0254976749420166], "norm_scores": [0.3354299068450928, 0.3340623378753662, 0.3305077850818634]}}
{"id": "WebQTest-392", "question": "what team is hank baskett on 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is hank baskett on 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.01549430750310421, -0.019533269107341766, -0.020612958818674088], "norm_scores": [0.3343515694141388, 0.33300384879112244, 0.332644522190094]}}
{"id": "WebQTest-393", "question": "what are the two official languages of paraguay", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the two official languages of paraguay [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0150276068598032, -0.02564445696771145, -0.02797728218138218], "norm_scores": [0.3359568119049072, 0.3324088752269745, 0.3316343426704407]}}
{"id": "WebQTest-395", "question": "what state is the steelers from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.location", "location.location.containedby"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is the steelers from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.location <SEP> location.location.containedby </PATH>"], "scores": [-0.00406509917229414, -0.031258244067430496, -0.09502146393060684], "norm_scores": [0.3464726209640503, 0.33717790246009827, 0.31634944677352905]}}
{"id": "WebQTest-396", "question": "what college did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.010215896181762218, -0.01877470500767231, -0.02582589164376259], "norm_scores": [0.3360227644443512, 0.33315905928611755, 0.33081817626953125]}}
{"id": "WebQTest-397", "question": "what are the school colors for harvard university", "prediction": [["education.educational_institution.colors"], ["education.school_mascot.school"], ["education.educational_institution.colors", "education.educational_institution.colors"]], "ground_paths": [["education.educational_institution.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the school colors for harvard university [/INST]", "raw_output": {"paths": ["<PATH> education.educational_institution.colors </PATH>", "<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.colors <SEP> education.educational_institution.colors </PATH>"], "scores": [-0.003279350697994232, -0.047681260854005814, -0.06334497034549713], "norm_scores": [0.3450331687927246, 0.33004817366600037, 0.3249186873435974]}}
{"id": "WebQTest-398", "question": "where is the university of the rockies located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the university of the rockies located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.002778814174234867, -0.049751415848731995, -0.06309483200311661], "norm_scores": [0.345353901386261, 0.3295068144798279, 0.32513928413391113]}}
{"id": "WebQTest-399", "question": "what disease does robin roberts have", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease does robin roberts have [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.010587133467197418, -0.012199083343148232, -0.09679526090621948], "norm_scores": [0.3429599106311798, 0.34240755438804626, 0.3146325349807739]}}
{"id": "WebQTest-400", "question": "what was the capital city of the east roman empire", "prediction": [["location.location.containedby"], ["location.country.capital"], ["location.location.contains"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the capital city of the east roman empire [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.006755742244422436, -0.029161207377910614, -0.051311228424310684], "norm_scores": [0.3408007323741913, 0.3332498073577881, 0.325949490070343]}}
{"id": "WebQTest-401", "question": "who plays edward scissorhands", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.film.country", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays edward scissorhands [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.01688770204782486, -0.04235018789768219, -0.04926430433988571], "norm_scores": [0.3397892117500305, 0.33124658465385437, 0.3289642035961151]}}
{"id": "WebQTest-402", "question": "what does joey jordison play in slipknot", "prediction": [["music.group_membership.group", "music.group_membership.role"], ["music.group_member.instruments_played"], ["music.group_member.instruments_played", "music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"], ["music.group_membership.group", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does joey jordison play in slipknot [/INST]", "raw_output": {"paths": ["<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>"], "scores": [-0.01776735484600067, -0.018951211124658585, -0.04240213707089424], "norm_scores": [0.33619287610054016, 0.3357951045036316, 0.32801204919815063]}}
{"id": "WebQTest-403", "question": "what kind of language does turkey speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does turkey speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006915096193552017, -0.029947780072689056, -0.04793664813041687], "norm_scores": [0.3404789865016937, 0.33272644877433777, 0.3267945945262909]}}
{"id": "WebQTest-404", "question": "what was lebron james first team", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lebron james first team [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>"], "scores": [-0.013679767027497292, -0.016395125538110733, -0.029691608622670174], "norm_scores": [0.33541247248649597, 0.3345029354095459, 0.3300846517086029]}}
{"id": "WebQTest-405", "question": "who fought the battle of gettysburg", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"], ["military.military_person.participated_in_conflicts"]], "ground_paths": [["military.military_command.military_conflict", "military.military_combatant.military_commanders"], ["location.location.events", "location.administrative_division.first_level_division_of"], ["book.written_work.subjects", "book.written_work.subjects"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["location.location.events", "location.administrative_division.second_level_division_of"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["time.event.included_in_event", "time.event.locations"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved", "people.person.nationality"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["military.military_person.participated_in_conflicts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought the battle of gettysburg [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>"], "scores": [-0.006796664092689753, -0.026878444477915764, -0.046037912368774414], "norm_scores": [0.3399467468261719, 0.33318808674812317, 0.32686513662338257]}}
{"id": "WebQTest-406", "question": "who is the coach of the sf giants", "prediction": [["baseball.baseball_team.current_coaches"], ["baseball.baseball_team.current_manager"], ["baseball.baseball_team.current_coach"]], "ground_paths": [["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"], ["baseball.current_coaching_tenure.baseball_team", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach of the sf giants [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> baseball.baseball_team.current_manager </PATH>", "<PATH> baseball.baseball_team.current_coach </PATH>"], "scores": [-0.010942293331027031, -0.029287155717611313, -0.048209283500909805], "norm_scores": [0.33953073620796204, 0.33335888385772705, 0.32711032032966614]}}
{"id": "WebQTest-408", "question": "who was the first president of the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "people.person.nationality"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first president of the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>"], "scores": [-0.0006013630772940814, -0.08116098493337631, -0.08635292202234268], "norm_scores": [0.35206034779548645, 0.32481083273887634, 0.3231288194656372]}}
{"id": "WebQTest-409", "question": "where did giuliana rancic grow up", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did giuliana rancic grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0034170045983046293, -0.04137883707880974, -0.048150934278964996], "norm_scores": [0.3425828814506531, 0.3298215866088867, 0.3275955617427826]}}
{"id": "WebQTest-410", "question": "what are the major cities in ukraine", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major cities in ukraine [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.02072678878903389, -0.023042311891913414, -0.03196750208735466], "norm_scores": [0.3348390758037567, 0.33406463265419006, 0.3310963213443756]}}
{"id": "WebQTest-414", "question": "who does brian dawkins play for 2011", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does brian dawkins play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.012635807506740093, -0.012770931236445904, -0.05140011012554169], "norm_scores": [0.33762770891189575, 0.3375820815563202, 0.32479020953178406]}}
{"id": "WebQTest-415", "question": "what movies did tupac star in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["music.producer.releases_produced"], ["music.recording.artist", "music.recording.releases"], ["music.featured_artist.recordings", "music.recording.releases"], ["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did tupac star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.02114548161625862, -0.02340397983789444, -0.03961775079369545], "norm_scores": [0.33563339710235596, 0.3348762094974518, 0.32949039340019226]}}
{"id": "WebQTest-416", "question": "what form of currency does china have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does china have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.011154284700751305, -0.015324056148529053, -0.12000862509012222], "norm_scores": [0.3456977903842926, 0.3442592918872833, 0.3100428879261017]}}
{"id": "WebQTest-417", "question": "who plays kenneth", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays kenneth [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.023174697533249855, -0.024867387488484383, -0.03180454671382904], "norm_scores": [0.3344799280166626, 0.33391422033309937, 0.33160585165023804]}}
{"id": "WebQTest-418", "question": "what league are the indianapolis colts in", "prediction": [["american_football.football_team.division"], ["sports.sports_team.league"], ["american_football.football_team.current_division"]], "ground_paths": [["american_football.football_team.division"], ["american_football.football_team.conference"], ["american_football.football_team.conference", "american_football.football_conference.league"], ["sports.sports_team.sport", "sports.sports_league.sport"], ["sports.sports_league_participation.team", "sports.sports_league_participation.league"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what league are the indianapolis colts in [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_team.division </PATH>", "<PATH> sports.sports_team.league </PATH>", "<PATH> american_football.football_team.current_division </PATH>"], "scores": [-0.017194358631968498, -0.03784750774502754, -0.048787280917167664], "norm_scores": [0.33916014432907104, 0.33222725987434387, 0.3286125659942627]}}
{"id": "WebQTest-419", "question": "what country did magellan represent", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did magellan represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.person.nationality </PATH>"], "scores": [-0.003359683556482196, -0.04570683464407921, -0.0775376483798027], "norm_scores": [0.34637510776519775, 0.332013338804245, 0.32161152362823486]}}
{"id": "WebQTest-420", "question": "where is perpignan located", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is perpignan located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.007518947124481201, -0.026210758835077286, -0.058211199939250946], "norm_scores": [0.3410579562187195, 0.33474215865135193, 0.3241998553276062]}}
{"id": "WebQTest-421", "question": "who did kim richards marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kim richards marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.017237873747944832, -0.02599301002919674, -0.04670547693967819], "norm_scores": [0.3375817537307739, 0.3346390724182129, 0.3277791738510132]}}
{"id": "WebQTest-422", "question": "what happened in bosnia in the 90s", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened in bosnia in the 90s [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0062953997403383255, -0.03533473238348961, -0.052653323858976364], "norm_scores": [0.34175431728363037, 0.33197271823883057, 0.3262729346752167]}}
{"id": "WebQTest-423", "question": "when 's the last time the steelers won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the steelers won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.010197611525654793, -0.013700061477720737, -0.052708640694618225], "norm_scores": [0.3384228050708771, 0.3372395634651184, 0.3243376314640045]}}
{"id": "WebQTest-425", "question": "where english is spoken", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where english is spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013805131427943707, -0.02489829622209072, -0.029458994045853615], "norm_scores": [0.3363112509250641, 0.3326011002063751, 0.3310876786708832]}}
{"id": "WebQTest-426", "question": "where is giza pyramids located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is giza pyramids located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00246775196865201, -0.04664604365825653, -0.07975591719150543], "norm_scores": [0.3469323217868805, 0.33193907141685486, 0.32112857699394226]}}
{"id": "WebQTest-427", "question": "what is the zip code for midland tx", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for midland tx [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-7.622745761182159e-05, -0.1096206083893776, -0.14975911378860474], "norm_scores": [0.3626837134361267, 0.32505249977111816, 0.31226375699043274]}}
{"id": "WebQTest-428", "question": "what states make up the midwest us", "prediction": [["location.location.contains"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states make up the midwest us [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.011610371060669422, -0.015971023589372635, -0.12932363152503967], "norm_scores": [0.346668541431427, 0.3451601266860962, 0.3081713318824768]}}
{"id": "WebQTest-430", "question": "what instruments does justin bieber use", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_member.instruments_played"]], "ground_paths": [["music.group_member.membership", "music.group_membership.role"], ["music.artist.track_contributions", "music.track_contribution.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments does justin bieber use [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>"], "scores": [-0.004623296670615673, -0.03860171139240265, -0.05073993280529976], "norm_scores": [0.34228724241256714, 0.3308522403240204, 0.32686057686805725]}}
{"id": "WebQTest-431", "question": "what language does australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007716777268797159, -0.022499676793813705, -0.061789993196725845], "norm_scores": [0.3409840166568756, 0.3359803557395935, 0.3230355679988861]}}
{"id": "WebQTest-432", "question": "who did annie oakley married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did annie oakley married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.02561451680958271, -0.037423163652420044, -0.040196847170591354], "norm_scores": [0.33627188205718994, 0.3323243260383606, 0.33140382170677185]}}
{"id": "WebQTest-433", "question": "who has played lex luthor", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has played lex luthor [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.01962556689977646, -0.025600247085094452, -0.02816654182970524], "norm_scores": [0.33494794368743896, 0.33295270800590515, 0.3320993483066559]}}
{"id": "WebQTest-434", "question": "where does the zambezi river start", "prediction": [["geography.river.origin"], ["location.location.containedby"], []], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> </PATH>"], "scores": [-0.00036146107595413923, -0.1029268130660057, -0.11872502416372299], "norm_scores": [0.3583083748817444, 0.32338017225265503, 0.318311482667923]}}
{"id": "WebQTest-436", "question": "what is the nigeria time", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the nigeria time [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00018014461966231465, -0.08329910784959793, -0.1189408004283905], "norm_scores": [0.3560921549797058, 0.32769083976745605, 0.3162170648574829]}}
{"id": "WebQTest-437", "question": "where did they find jenni rivera 's body", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they find jenni rivera 's body [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0005061290576122701, -0.08274034410715103, -0.0840742215514183], "norm_scores": [0.3520030379295349, 0.3242145776748657, 0.32378241419792175]}}
{"id": "WebQTest-438", "question": "what university did romney graduated from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.religion", "religion.religion.organizations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did romney graduated from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.014485746622085571, -0.021671734750270844, -0.023882731795310974], "norm_scores": [0.33517831563949585, 0.33277833461761475, 0.3320434093475342]}}
{"id": "WebQTest-439", "question": "where is the time zone line in south dakota", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the time zone line in south dakota [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0018834250513464212, -0.047108616679906845, -0.08010803908109665], "norm_scores": [0.3471572697162628, 0.3318067491054535, 0.3210360109806061]}}
{"id": "WebQTest-440", "question": "where is jason mraz from", "prediction": [["music.artist.origin"], ["location.location.people_born_here"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"], ["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jason mraz from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0047730510123074055, -0.03933768719434738, -0.05163499340415001], "norm_scores": [0.3424369990825653, 0.3308030366897583, 0.326759934425354]}}
{"id": "WebQTest-441", "question": "what killed john bonham", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["music.group_member.instruments_played", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what killed john bonham [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> music.group_member.instruments_played <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.001796608092263341, -0.04418967291712761, -0.0939684733748436], "norm_scores": [0.3483784794807434, 0.3339183032512665, 0.3177032172679901]}}
{"id": "WebQTest-442", "question": "what industry does walmart operate in", "prediction": [["business.business_operation.industry"], ["business.industry.companies"], ["business.business_operation.industry", "business.business_operation.industry"]], "ground_paths": [["organization.organization.sectors"], ["business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what industry does walmart operate in [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>"], "scores": [-0.005589428823441267, -0.04838366433978081, -0.05149424448609352], "norm_scores": [0.343260258436203, 0.32888057827949524, 0.32785916328430176]}}
{"id": "WebQTest-443", "question": "what county is texarkana arkansas in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is texarkana arkansas in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.00834114383906126, -0.02102074958384037, -0.05759698897600174], "norm_scores": [0.3402126729488373, 0.3359261453151703, 0.3238612115383148]}}
{"id": "WebQTest-446", "question": "what music period did beethoven live in", "prediction": [["music.artist.genre"], ["music.artist.period"], ["music.artist.periods"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music period did beethoven live in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.artist.period </PATH>", "<PATH> music.artist.periods </PATH>"], "scores": [-0.0016670303884893656, -0.0687071830034256, -0.09026838093996048], "norm_scores": [0.35083189606666565, 0.3280831277370453, 0.32108497619628906]}}
{"id": "WebQTest-447", "question": "what did laura ingalls wilder wrote", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["book.book_edition.author_editor"], ["book.literary_series.author_s"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did laura ingalls wilder wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.010753227397799492, -0.025290383026003838, -0.03667285665869713], "norm_scores": [0.3378399610519409, 0.3329642415046692, 0.3291957676410675]}}
{"id": "WebQTest-448", "question": "what type of government system does italy have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government system does italy have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009394955821335316, -0.014824916608631611, -0.14407944679260254], "norm_scores": [0.3486049473285675, 0.3467171788215637, 0.304677814245224]}}
{"id": "WebQTest-449", "question": "what did john irving wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["book.author.book_editions_published"], ["film.film.story_by"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["film.film_story_contributor.film_story_credits"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john irving wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.015003609471023083, -0.025745218619704247, -0.029789999127388], "norm_scores": [0.3361752927303314, 0.33258357644081116, 0.33124107122421265]}}
{"id": "WebQTest-450", "question": "what is the national flower of hawaii", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national flower of hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.010301575995981693, -0.019693519920110703, -0.022629180923104286], "norm_scores": [0.3357506990432739, 0.33261215686798096, 0.33163711428642273]}}
{"id": "WebQTest-451", "question": "what did alice paul accomplish", "prediction": [["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did alice paul accomplish [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005655845161527395, -0.02732372283935547, -0.07263504713773727], "norm_scores": [0.343196839094162, 0.33584049344062805, 0.32096269726753235]}}
{"id": "WebQTest-453", "question": "what languages do they speak in costa rica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do they speak in costa rica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.004385811742395163, -0.028140896931290627, -0.08879604190587997], "norm_scores": [0.3453540802001953, 0.337246835231781, 0.31739911437034607]}}
{"id": "WebQTest-454", "question": "where did hank marvin come from", "prediction": [["people.person.place_of_birth"], ["music.artist.origin"], ["music.recording.artist"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hank marvin come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.011310013011097908, -0.02513088658452034, -0.05756387487053871], "norm_scores": [0.340011864900589, 0.33534494042396545, 0.32464316487312317]}}
{"id": "WebQTest-455", "question": "when did chipper jones get drafted", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did chipper jones get drafted [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.021794769912958145, -0.022861598059535027, -0.024877937510609627], "norm_scores": [0.3337944746017456, 0.33343860507011414, 0.33276695013046265]}}
{"id": "WebQTest-456", "question": "what part did winona ryder play in star trek", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did winona ryder play in star trek [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.02034747414290905, -0.029431963339447975, -0.029864918440580368], "norm_scores": [0.3354033827781677, 0.33237022161483765, 0.33222636580467224]}}
{"id": "WebQTest-459", "question": "where did jovan belcher kill himself", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jovan belcher kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.0019210227765142918, -0.06915844231843948, -0.08045117557048798], "norm_scores": [0.349717915058136, 0.32697686553001404, 0.3233051896095276]}}
{"id": "WebQTest-461", "question": "where did clay matthews go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did clay matthews go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012566663324832916, -0.01471712440252304, -0.03433310613036156], "norm_scores": [0.33598530292510986, 0.33526355028152466, 0.3287511169910431]}}
{"id": "WebQTest-462", "question": "what time is it in texas houston right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time is it in texas houston right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-1.623156640562229e-05, -0.1303955614566803, -0.15826687216758728], "norm_scores": [0.3661128580570221, 0.32136011123657227, 0.312527060508728]}}
{"id": "WebQTest-463", "question": "who is princess leia in star wars", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is princess leia in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.011706851422786713, -0.028952790424227715, -0.035813283175230026], "norm_scores": [0.3379424810409546, 0.33216431736946106, 0.3298932909965515]}}
{"id": "WebQTest-464", "question": "what is the political structure of china", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["organization.organization.founders"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political structure of china [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.006934919394552708, -0.019585635513067245, -0.16433948278427124], "norm_scores": [0.3518912196159363, 0.3474675714969635, 0.3006412088871002]}}
{"id": "WebQTest-465", "question": "what is new york city airport", "prediction": [["aviation.airport.serves"], ["location.location.containedby"], ["location.location.nearby_airports"]], "ground_paths": [["aviation.airport.serves"], ["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is new york city airport [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.nearby_airports </PATH>"], "scores": [-0.00717732310295105, -0.03135482966899872, -0.04631651192903519], "norm_scores": [0.34039896726608276, 0.3322676420211792, 0.32733339071273804]}}
{"id": "WebQTest-466", "question": "what language brazil speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language brazil speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.00019392117974348366, -0.10084505379199982, -0.11019811034202576], "norm_scores": [0.35713279247283936, 0.3229367733001709, 0.31993040442466736]}}
{"id": "WebQTest-467", "question": "who played obi wan in episode 2", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played obi wan in episode 2 [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.028956955298781395, -0.033186107873916626, -0.03373316675424576], "norm_scores": [0.33433467149734497, 0.3329237103462219, 0.3327416181564331]}}
{"id": "WebQTest-468", "question": "where did venus williams come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did venus williams come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004188917111605406, -0.038166292011737823, -0.05041523277759552], "norm_scores": [0.3422994017601013, 0.3308643400669098, 0.3268362879753113]}}
{"id": "WebQTest-469", "question": "what currency is used in panama", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is used in panama [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.009434400126338005, -0.01821248233318329, -0.15445493161678314], "norm_scores": [0.3501075804233551, 0.3470477759838104, 0.30284467339515686]}}
{"id": "WebQTest-470", "question": "where does the zambezi river originate", "prediction": [["geography.river.origin"], ["location.location.containedby"], ["location.location.partial_containedby"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river originate [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partial_containedby </PATH>"], "scores": [-0.0005473048659041524, -0.09386501461267471, -0.09427367895841599], "norm_scores": [0.3544294536113739, 0.3228512406349182, 0.3227193057537079]}}
{"id": "WebQTest-472", "question": "what team did ronaldo play for in 2003", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did ronaldo play for in 2003 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.019584063440561295, -0.024012120440602303, -0.03744276985526085], "norm_scores": [0.33580920100212097, 0.33432552218437195, 0.32986530661582947]}}
{"id": "WebQTest-473", "question": "where does robin williams live 2011", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.children", "people.person.place_of_birth"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.parents", "people.person.place_of_birth"], ["people.person.nationality", "location.administrative_division.second_level_division_of"], ["people.place_lived.person", "people.place_lived.location"], ["film.film.produced_by", "film.film_location.featured_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robin williams live 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.014747618697583675, -0.022957099601626396, -0.03021935559809208], "norm_scores": [0.3359682857990265, 0.333221435546875, 0.3308102786540985]}}
{"id": "WebQTest-474", "question": "who played jacob black in twilight", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jacob black in twilight [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>"], "scores": [-0.02961251698434353, -0.031107528135180473, -0.032463595271110535], "norm_scores": [0.3338163495063782, 0.33331766724586487, 0.33286598324775696]}}
{"id": "WebQTest-477", "question": "which airport to fly into in buenos aires", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["aviation.airport.serves"], ["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into in buenos aires [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.007390671409666538, -0.026047030463814735, -0.045759882777929306], "norm_scores": [0.3396884500980377, 0.33340984582901, 0.32690173387527466]}}
{"id": "WebQTest-479", "question": "what team did adrian peterson play for in college", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did adrian peterson play for in college [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.010024713352322578, -0.01707763969898224, -0.03867905214428902], "norm_scores": [0.33729952573776245, 0.3349289298057556, 0.3277715742588043]}}
{"id": "WebQTest-480", "question": "where does name pennsylvania come from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes", "symbols.namesake.named_after"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does name pennsylvania come from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes <SEP> symbols.namesake.named_after </PATH>"], "scores": [-0.004503665957599878, -0.031124619767069817, -0.09446704387664795], "norm_scores": [0.34629693627357483, 0.3371998369693756, 0.3165031969547272]}}
{"id": "WebQTest-481", "question": "what kind of money do they use in germany", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do they use in germany [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.0052054342813789845, -0.025630053132772446, -0.11246258020401001], "norm_scores": [0.3474541902542114, 0.3404295742511749, 0.31211626529693604]}}
{"id": "WebQTest-482", "question": "who plays blaine in batman", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays blaine in batman [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.01470230147242546, -0.025483686476945877, -0.03342408686876297], "norm_scores": [0.33661773800849915, 0.3330080211162567, 0.33037427067756653]}}
{"id": "WebQTest-484", "question": "what language does australia use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.008339149877429008, -0.021140215918421745, -0.06453292071819305], "norm_scores": [0.3409900963306427, 0.33665287494659424, 0.3223569989204407]}}
{"id": "WebQTest-487", "question": "who did veronica lake mary", "prediction": [["people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did veronica lake mary [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.028272120282053947, -0.030296755954623222, -0.03800050914287567], "norm_scores": [0.3346388339996338, 0.33396202325820923, 0.3313991129398346]}}
{"id": "WebQTest-488", "question": "who did sir francis drake marry", "prediction": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sir francis drake marry [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02626466564834118, -0.029092656448483467, -0.04042121395468712], "norm_scores": [0.3352196216583252, 0.33427298069000244, 0.3305074870586395]}}
{"id": "WebQTest-489", "question": "who plays stephanie plum in one for the money", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.film.executive_produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stephanie plum in one for the money [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.022655947133898735, -0.03390605375170708, -0.03603728488087654], "norm_scores": [0.3360756039619446, 0.33231592178344727, 0.33160844445228577]}}
{"id": "WebQTest-490", "question": "what did the ancient romans speak", "prediction": [["fictional_universe.fictional_setting.languages"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the ancient romans speak [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012857958674430847, -0.03099392168223858, -0.040862806141376495], "norm_scores": [0.33847692608833313, 0.33239367604255676, 0.3291294276714325]}}
{"id": "WebQTest-491", "question": "what does ringo sing", "prediction": [["music.recording.artist"], ["music.artist.track"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["theater.theatrical_composer.plays_composed", "film.film.sequel"], ["music.artist.label", "music.record_label.releases"], ["music.artist.album"], ["music.recording.artist"], ["music.album.artist"], ["music.album.artist", "music.album.compositions"], ["music.lyricist.lyrics_written"], ["film.performance.actor", "film.performance.film"], ["music.composer.compositions"], ["music.composition.composer"], ["organization.organization.founders", "film.film.production_companies"], ["music.recording.featured_artists"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does ringo sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.020244453102350235, -0.023322172462940216, -0.06035134196281433], "norm_scores": [0.33811041712760925, 0.33707141876220703, 0.3248181939125061]}}
{"id": "WebQTest-492", "question": "where was st. lucy born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st. lucy born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.006102202460169792, -0.02543804608285427, -0.08462368696928024], "norm_scores": [0.3441947400569916, 0.337603360414505, 0.3182018995285034]}}
{"id": "WebQTest-493", "question": "what type of government does australia have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does australia have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007134729065001011, -0.018952295184135437, -0.13438934087753296], "norm_scores": [0.3485824763774872, 0.3444873094558716, 0.30693018436431885]}}
{"id": "WebQTest-497", "question": "who was the father of king george vi", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the father of king george vi [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.012015716172754765, -0.01868647336959839, -0.05210297554731369], "norm_scores": [0.3385174870491028, 0.3362668454647064, 0.3252156376838684]}}
{"id": "WebQTest-500", "question": "what is the zip code for trenton", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for trenton [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-9.651358413975686e-05, -0.10414434224367142, -0.17004795372486115], "norm_scores": [0.3643135726451874, 0.3283129334449768, 0.3073734641075134]}}
{"id": "WebQTest-501", "question": "what has ashley greene been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ashley greene been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.017297333106398582, -0.03231197968125343, -0.03553469106554985], "norm_scores": [0.3370378911495209, 0.33201515674591064, 0.3309469223022461]}}
{"id": "WebQTest-502", "question": "where did al qaeda attack", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"]], "ground_paths": [["base.terrorism.terrorist_organization.involved_in_attacks"], ["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.disaster2.attack_process.attacker", "base.disaster2.attack.attack"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.conflict"], ["film.film.subjects", "film.film.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did al qaeda attack [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.018853146582841873, -0.03210209682583809, -0.034865666180849075], "norm_scores": [0.33659228682518005, 0.33216220140457153, 0.3312455117702484]}}
{"id": "WebQTest-503", "question": "what language is spoken in switzerland", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.01753774657845497, -0.02103920839726925, -0.023243984207510948], "norm_scores": [0.33435705304145813, 0.33318835496902466, 0.3324545621871948]}}
{"id": "WebQTest-504", "question": "who was philip in acts chapter 8", "prediction": [["people.person.gender", "people.person.gender"], ["people.person.profession"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["fictional_universe.person_in_fiction.representations_in_fiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was philip in acts chapter 8 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.014192678034305573, -0.01701294258236885, -0.05691898614168167], "norm_scores": [0.33836859464645386, 0.3374156355857849, 0.324215829372406]}}
{"id": "WebQTest-506", "question": "what the time zone in england", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.country.form_of_government"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in england [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.0012711260933429003, -0.04616174474358559, -0.14800909161567688], "norm_scores": [0.35465747117996216, 0.33908870816230774, 0.3062538802623749]}}
{"id": "WebQTest-508", "question": "what is the best currency to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the best currency to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.01676393672823906, -0.01750132068991661, -0.037372324615716934], "norm_scores": [0.3356982469558716, 0.33545082807540894, 0.3288508653640747]}}
{"id": "WebQTest-509", "question": "what system of government is practiced in nigeria", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government is practiced in nigeria [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.0054327743127942085, -0.02133551985025406, -0.14263314008712769], "norm_scores": [0.35013777017593384, 0.3446136713027954, 0.30524855852127075]}}
{"id": "WebQTest-512", "question": "who were mary shelley", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were mary shelley [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0018280729418620467, -0.07027177512645721, -0.07418759912252426], "norm_scores": [0.34915685653686523, 0.3260587453842163, 0.32478445768356323]}}
{"id": "WebQTest-513", "question": "what language do most australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006627109833061695, -0.024129360914230347, -0.06339048594236374], "norm_scores": [0.34159213304519653, 0.3356655240058899, 0.3227422833442688]}}
{"id": "WebQTest-514", "question": "where is denmark situated", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark situated [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.019820615649223328, -0.021436545997858047, -0.03488653153181076], "norm_scores": [0.3351844251155853, 0.3346432149410248, 0.3301723897457123]}}
{"id": "WebQTest-515", "question": "where was teddy roosevelt 's house", "prediction": [["symbols.name_source.namesakes", "location.location.containedby"], ["symbols.namesake.named_after", "location.location.containedby"], ["people.person.parents", "people.person.place_of_birth"]], "ground_paths": [["symbols.namesake.named_after", "transportation.bridge.locale"], ["base.usnris.nris_listing.significant_person", "travel.travel_destination.tourist_attractions"], ["people.person.parents", "people.person.place_of_birth"], ["book.written_work.author", "people.deceased_person.place_of_death"], ["people.person.parents", "people.deceased_person.place_of_death"], ["people.person.place_of_birth", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.children", "people.deceased_person.place_of_death"], ["symbols.namesake.named_after", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["symbols.name_source.namesakes", "location.location.containedby"], ["book.book_edition.author_editor", "book.book_edition.place_of_publication"], ["people.person.children", "people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["book.written_work.author", "base.biblioness.bibs_location.state"], ["book.written_work.subjects", "location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was teddy roosevelt 's house [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.023483529686927795, -0.02833649143576622, -0.03770739585161209], "norm_scores": [0.3354538679122925, 0.3338298797607422, 0.33071622252464294]}}
{"id": "WebQTest-516", "question": "what is the government system of malaysia", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of malaysia [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.010205941274762154, -0.013760432600975037, -0.11232259124517441], "norm_scores": [0.34490180015563965, 0.3436780571937561, 0.31142017245292664]}}
{"id": "WebQTest-517", "question": "what do people speak in canada", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in canada [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.01611948013305664, -0.017634712159633636, -0.03826725855469704], "norm_scores": [0.3359558582305908, 0.33544719219207764, 0.32859697937965393]}}
{"id": "WebQTest-518", "question": "what type of money do they use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money do they use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.006147947162389755, -0.022286932915449142, -0.11830165237188339], "norm_scores": [0.34747591614723206, 0.34191301465034485, 0.31061112880706787]}}
{"id": "WebQTest-519", "question": "what timezone is nashville tn", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is nashville tn [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-8.66321352077648e-06, -0.18942493200302124, -0.1900891363620758], "norm_scores": [0.37674227356910706, 0.31173238158226013, 0.3115253746509552]}}
{"id": "WebQTest-521", "question": "who was anakin skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was anakin skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.004094251431524754, -0.039707034826278687, -0.04800891503691673], "norm_scores": [0.3422255516052246, 0.33025240898132324, 0.32752203941345215]}}
{"id": "WebQTest-522", "question": "where did adolf hitler die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did adolf hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>"], "scores": [-0.0005313795409165323, -0.08328622579574585, -0.10269158333539963], "norm_scores": [0.3541751503944397, 0.326045423746109, 0.3197793960571289]}}
{"id": "WebQTest-523", "question": "what is the largest nation in europe", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the largest nation in europe [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.012462886050343513, -0.024037474766373634, -0.04103830084204674], "norm_scores": [0.3378011882305145, 0.3339138329029083, 0.32828497886657715]}}
{"id": "WebQTest-524", "question": "where was the first gulf war fought", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first gulf war fought [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.004495569970458746, -0.032090574502944946, -0.11478966474533081], "norm_scores": [0.3486321270465851, 0.33914312720298767, 0.31222471594810486]}}
{"id": "WebQTest-525", "question": "who are the senators of virginia 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "ground_paths": [["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of virginia 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.01639947108924389, -0.02474437654018402, -0.026830598711967468], "norm_scores": [0.33542269468307495, 0.3326352834701538, 0.33194205164909363]}}
{"id": "WebQTest-526", "question": "what band was george clinton in", "prediction": [["music.artist.genre", "music.artist.genre"], ["music.artist.label", "music.artist.label"], ["music.recording.artist", "music.recording.artist"]], "ground_paths": [["music.featured_artist.recordings", "music.recording.featured_artists"], ["music.recording.artist", "music.recording.artist"], ["music.recording.producer", "music.recording.artist"], ["music.producer.tracks_produced", "music.recording.artist"], ["music.group_member.membership", "music.musical_group.member"], ["music.composition.composer", "music.artist.track"], ["music.album.artist", "music.album.artist"], ["music.composer.compositions", "music.artist.track"], ["music.artist.genre", "music.artist.genre"], ["music.recording.featured_artists", "music.artist.album"], ["music.artist.album", "music.album.artist"], ["music.artist.origin", "music.artist.origin"], ["music.album.artist", "music.artist.album"], ["music.record_label.artist", "music.record_label.artist"], ["music.group_membership.member", "music.group_membership.group"], ["music.artist.album", "music.artist.album"], ["music.recording.featured_artists", "music.featured_artist.recordings"], ["music.featured_artist.recordings", "music.recording.artist"], ["music.producer.releases_produced", "music.album.artist"], ["base.saturdaynightlive.snl_episode.musical_guest", "base.saturdaynightlive.snl_musical_guest.episodes_as_musical_guest"], ["music.group_membership.member", "music.musical_group.member"], ["music.composition.composer", "music.recording.artist"], ["music.artist.label", "music.artist.label"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.recording.featured_artists", "music.recording.featured_artists"], ["music.recording.artist", "music.album.artist"], ["music.artist.genre", "music.genre.artists"], ["music.composer.compositions", "music.recording.artist"], ["broadcast.artist.content", "broadcast.artist.content"], ["music.recording.producer", "music.album.artist"], ["people.person.nationality", "music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what band was george clinton in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>"], "scores": [-0.014871780760586262, -0.036309320479631424, -0.05216159299015999], "norm_scores": [0.33988314867019653, 0.33267444372177124, 0.32744237780570984]}}
{"id": "WebQTest-527", "question": "what country borders slovakia", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["finance.currency.countries_formerly_used", "location.country.currency_formerly_used"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "location.location.contains"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.country.currency_used", "location.country.currency_used"], ["location.location.partially_containedby", "location.location.containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_containedby", "geography.river.origin"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.datedlocationtest.dated_location_break_up.new_locations", "base.datedlocationtest.dated_location_break_up.new_locations"], ["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_contains", "location.location.partially_contains"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["common.topic.notable_types", "common.topic.notable_types"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["base.locations.countries.continent", "base.locations.continents.countries_within"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["government.form_of_government.countries", "government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country borders slovakia [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03645907714962959, -0.0411885567009449, -0.041791852563619614], "norm_scores": [0.3344523012638092, 0.33287423849105835, 0.33267346024513245]}}
{"id": "WebQTest-528", "question": "what was james garfield most known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was james garfield most known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0011557007674127817, -0.05881665647029877, -0.08500884473323822], "norm_scores": [0.34921854734420776, 0.329651802778244, 0.32112959027290344]}}
{"id": "WebQTest-529", "question": "where did paula deen go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did paula deen go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.017770253121852875, -0.018566567450761795, -0.02750249393284321], "norm_scores": [0.3345019519329071, 0.33423569798469543, 0.3312622904777527]}}
{"id": "WebQTest-530", "question": "where does robert irvine come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robert irvine come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0041088745929300785, -0.03775319084525108, -0.048887889832258224], "norm_scores": [0.34209975600242615, 0.330781489610672, 0.32711878418922424]}}
{"id": "WebQTest-531", "question": "what style of music does michael jackson perform", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does michael jackson perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0009024448809213936, -0.059974901378154755, -0.1334061473608017], "norm_scores": [0.35479384660720825, 0.3344423472881317, 0.3107638359069824]}}
{"id": "WebQTest-532", "question": "what does david beckham play", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["book.written_work.subjects", "book.book_subject.works"], ["symbols.namesake.named_after", "cvg.computer_videogame.cvg_genre"], ["soccer.football_player.position_s", "sports.sport.positions"], ["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does david beckham play [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.02779567986726761, -0.03028484620153904, -0.043384674936532974], "norm_scores": [0.3353402614593506, 0.3345065712928772, 0.3301531970500946]}}
{"id": "WebQTest-533", "question": "what does julian lennon do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does julian lennon do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.0010367436334490776, -0.06243135780096054, -0.11612721532583237], "norm_scores": [0.35314008593559265, 0.33211132884025574, 0.314748615026474]}}
{"id": "WebQTest-534", "question": "what is the milwaukee brewers mascot", "prediction": [["sports.mascot.team"], ["sports.sports_team.team_mascot"], ["sports.mascot.team", "sports.mascot.team"]], "ground_paths": [["sports.sports_team.team_mascot"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers mascot [/INST]", "raw_output": {"paths": ["<PATH> sports.mascot.team </PATH>", "<PATH> sports.sports_team.team_mascot </PATH>", "<PATH> sports.mascot.team <SEP> sports.mascot.team </PATH>"], "scores": [-0.0011969878105446696, -0.045179903507232666, -0.14129091799259186], "norm_scores": [0.3538261353969574, 0.33860114216804504, 0.30757278203964233]}}
{"id": "WebQTest-535", "question": "what is the second biggest state in the united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby"]], "ground_paths": [["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the second biggest state in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.0016456154407933354, -0.05722009390592575, -0.07658695429563522], "norm_scores": [0.3479786813259125, 0.32916751503944397, 0.32285386323928833]}}
{"id": "WebQTest-537", "question": "what has charlie hunnam been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has charlie hunnam been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.0225338377058506, -0.026462888345122337, -0.03185049444437027], "norm_scores": [0.3348058760166168, 0.3334929943084717, 0.3317010998725891]}}
{"id": "WebQTest-538", "question": "what are all the movies taylor swift has been in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["music.album.genre", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the movies taylor swift has been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.02687985636293888, -0.037865810096263885, -0.038793232291936874], "norm_scores": [0.3358825147151947, 0.33221274614334106, 0.3319047689437866]}}
{"id": "WebQTest-540", "question": "what college did brett favre", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did brett favre [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.016669804230332375, -0.019099367782473564, -0.027915483340620995], "norm_scores": [0.3348523676395416, 0.3340398371219635, 0.33110782504081726]}}
{"id": "WebQTest-542", "question": "where did alex chilton die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alex chilton die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0002953584771603346, -0.09522540867328644, -0.10047013312578201], "norm_scores": [0.35535138845443726, 0.3231695294380188, 0.32147902250289917]}}
{"id": "WebQTest-543", "question": "what college did martin luther king jr go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did martin luther king jr go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.014504766091704369, -0.018060972914099693, -0.027282988652586937], "norm_scores": [0.3351483643054962, 0.33395862579345703, 0.33089300990104675]}}
{"id": "WebQTest-545", "question": "what kind of cancer did farrah fawcett die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did farrah fawcett die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.00807945616543293, -0.018456945195794106, -0.04296296089887619], "norm_scores": [0.3383643627166748, 0.3348711431026459, 0.3267645239830017]}}
{"id": "WebQTest-546", "question": "who is michael j fox 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.02113782800734043, -0.03335143253207207, -0.04247662425041199], "norm_scores": [0.33706942200660706, 0.33297762274742126, 0.3299529552459717]}}
{"id": "WebQTest-547", "question": "what money do i use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do i use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.005965399090200663, -0.024181954562664032, -0.1280047595500946], "norm_scores": [0.34878912568092346, 0.3424929082393646, 0.3087179660797119]}}
{"id": "WebQTest-548", "question": "what guitar did joe walsh play", "prediction": [["music.guitarist.guitars_played"], ["music.group_member.instruments_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did joe walsh play [/INST]", "raw_output": {"paths": ["<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.018742725253105164, -0.02054670639336109, -0.03202342614531517], "norm_scores": [0.33500781655311584, 0.33440402150154114, 0.33058810234069824]}}
{"id": "WebQTest-549", "question": "what are the official languages of the eu", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "language.human_language.countries_spoken_in"], ["organization.organization.geographic_scope", "location.country.languages_spoken"]], "ground_paths": [["organization.organization.founders", "location.country.official_language"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_language"], ["organization.organization_founder.organizations_founded", "location.country.official_language"], ["organization.organization.geographic_scope", "language.human_language.region"], ["organization.organization.founders", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.024167340248823166, -0.029885223135352135, -0.037664804607629776], "norm_scores": [0.3354700803756714, 0.3335573971271515, 0.3309725224971771]}}
{"id": "WebQTest-550", "question": "what is ryan seacrest radio show", "prediction": [["broadcast.content.producer"], ["internet.website.owner"], ["broadcast.producer.content", "broadcast.content.producer"]], "ground_paths": [["broadcast.artist.content"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ryan seacrest radio show [/INST]", "raw_output": {"paths": ["<PATH> broadcast.content.producer </PATH>", "<PATH> internet.website.owner </PATH>", "<PATH> broadcast.producer.content <SEP> broadcast.content.producer </PATH>"], "scores": [-0.03596610948443413, -0.054345447570085526, -0.05784503370523453], "norm_scores": [0.33782100677490234, 0.33166879415512085, 0.33051013946533203]}}
{"id": "WebQTest-552", "question": "what language do most italians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most italians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016650183126330376, -0.020455501973628998, -0.027872765436768532], "norm_scores": [0.33500364422798157, 0.3337312936782837, 0.33126506209373474]}}
{"id": "WebQTest-554", "question": "what money do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.004497204907238483, -0.02960958145558834, -0.13858145475387573], "norm_scores": [0.35091206431388855, 0.3422095477581024, 0.30687835812568665]}}
{"id": "WebQTest-555", "question": "what did drew barrymore go to rehab for", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.person.profession", "people.profession.specializations"], ["people.person.profession", "people.profession.specializations_of"]], "ground_paths": [["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did drew barrymore go to rehab for [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations_of </PATH>"], "scores": [-0.021670956164598465, -0.025453444570302963, -0.041025593876838684], "norm_scores": [0.33590230345726013, 0.3346341550350189, 0.32946354150772095]}}
{"id": "WebQTest-556", "question": "when is saint george day celebrated", "prediction": [["time.recurring_event.current_frequency"], ["time.day_of_year.holidays"], ["time.holiday.featured_in_religions"]], "ground_paths": [["time.holiday.day_of_year"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is saint george day celebrated [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.day_of_year.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>"], "scores": [-0.015447483398020267, -0.026135975494980812, -0.034030526876449585], "norm_scores": [0.3365918695926666, 0.3330133557319641, 0.3303947150707245]}}
{"id": "WebQTest-557", "question": "what government did the roman empire have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["fictional_universe.fictional_setting.languages", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government did the roman empire have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> location.country.form_of_government </PATH>"], "scores": [-0.005005601793527603, -0.02506224997341633, -0.08288324624300003], "norm_scores": [0.34420791268348694, 0.3373730182647705, 0.31841903924942017]}}
{"id": "WebQTest-559", "question": "what instrument did robin gibb play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.group_membership.group", "music.group_membership.role"]], "ground_paths": [["music.group_member.instruments_played"], ["music.group_membership.member", "music.group_membership.role"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_membership.member", "music.performance_role.regular_performances"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument did robin gibb play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>"], "scores": [-0.000854710815474391, -0.07532168179750443, -0.08053221553564072], "norm_scores": [0.3506739139556885, 0.32550889253616333, 0.3238172233104706]}}
{"id": "WebQTest-560", "question": "who voiced darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02565532736480236, -0.0284957867115736, -0.03070872835814953], "norm_scores": [0.3342108726501465, 0.33326292037963867, 0.33252620697021484]}}
{"id": "WebQTest-561", "question": "when was abe lincoln president", "prediction": [["government.us_president.vice_president"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"]], "ground_paths": [["base.inaugurations.inauguration.president"], ["base.inaugurations.inauguration_speaker.inauguration"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was abe lincoln president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.0258337315171957, -0.027587460353970528, -0.0398382693529129], "norm_scores": [0.33508235216140747, 0.3344952464103699, 0.33042237162590027]}}
{"id": "WebQTest-562", "question": "where are the texas rangers playing", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the texas rangers playing [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.0012153415009379387, -0.053212862461805344, -0.09040982276201248], "norm_scores": [0.34916216135025024, 0.3314705193042755, 0.31936731934547424]}}
{"id": "WebQTest-564", "question": "what is arkansas state capitol", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.state"], ["location.us_state.capital"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is arkansas state capitol [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.us_state.capital </PATH>"], "scores": [-0.020195456221699715, -0.02459787018597126, -0.02773727849125862], "norm_scores": [0.3346615135669708, 0.3331914246082306, 0.3321470320224762]}}
{"id": "WebQTest-567", "question": "what is the zip code for nogales az", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for nogales az [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00034628980210982263, -0.08165506273508072, -0.11150100082159042], "norm_scores": [0.35502421855926514, 0.32729998230934143, 0.31767576932907104]}}
{"id": "WebQTest-568", "question": "when did michael vick start playing for the eagles", "prediction": [["american_football.football_player.rushing", "american_football.player_rushing_statistics.season"], ["american_football.football_player.games", "american_football.player_game_statistics.season"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael vick start playing for the eagles [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.season </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.season </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.season </PATH>"], "scores": [-0.020647382363677025, -0.022087462246418, -0.027285756543278694], "norm_scores": [0.33423078060150146, 0.33374983072280884, 0.3320193886756897]}}
{"id": "WebQTest-570", "question": "what is william taft famous for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is william taft famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.009753542020916939, -0.0405099019408226, -0.04075692594051361], "norm_scores": [0.34023046493530273, 0.3299254775047302, 0.32984402775764465]}}
{"id": "WebQTest-572", "question": "where did robin cook die", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin cook die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.00032381238997913897, -0.11153833568096161, -0.11225869506597519], "norm_scores": [0.35857081413269043, 0.3208300769329071, 0.3205990791320801]}}
{"id": "WebQTest-573", "question": "where did melba beals live", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth", "location.us_state.capital"], ["people.person.nationality", "location.country.first_level_divisions"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did melba beals live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01706935651600361, -0.027205409482121468, -0.035888902842998505], "norm_scores": [0.33655625581741333, 0.3331621289253235, 0.33028164505958557]}}
{"id": "WebQTest-575", "question": "who is louisiana state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.locations.countries.states_provinces_within", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "symbols.name_source.namesakes"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.election.district", "government.election.winner"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["location.location.partially_containedby", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is louisiana state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.02257729135453701, -0.025825712829828262, -0.029521914198994637], "norm_scores": [0.33446648716926575, 0.3333817720413208, 0.3321518003940582]}}
{"id": "WebQTest-576", "question": "what are the 7 countries that are part of central america", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the 7 countries that are part of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.007674512919038534, -0.027965089306235313, -0.03492974117398262], "norm_scores": [0.3386355936527252, 0.33183372020721436, 0.32953065633773804]}}
{"id": "WebQTest-577", "question": "where did derek fisher go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did derek fisher go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01781916804611683, -0.023503566160798073, -0.024695836007595062], "norm_scores": [0.3347304165363312, 0.33283308148384094, 0.3324364721775055]}}
{"id": "WebQTest-578", "question": "who was kobe bryant drafted by", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kobe bryant drafted by [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.025156080722808838, -0.027252227067947388, -0.03058023564517498], "norm_scores": [0.3341691493988037, 0.333469420671463, 0.33236148953437805]}}
{"id": "WebQTest-579", "question": "where are the new orleans hornets moving to", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.previously_known_as"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the new orleans hornets moving to [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.011202570982277393, -0.029696986079216003, -0.04415839537978172], "norm_scores": [0.3390684723854065, 0.332855224609375, 0.3280763030052185]}}
{"id": "WebQTest-580", "question": "what language does algerian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does algerian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.011997810564935207, -0.01770053245127201, -0.051729220896959305], "norm_scores": [0.33836814761161804, 0.33644402027130127, 0.3251878619194031]}}
{"id": "WebQTest-581", "question": "where was anne boleyn executed", "prediction": [["people.deceased_person.place_of_death"], ["location.location.people_executed"], ["location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was anne boleyn executed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_executed </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0012201321078464389, -0.08259382843971252, -0.08344783633947372], "norm_scores": [0.3517521023750305, 0.3242623805999756, 0.3239855468273163]}}
{"id": "WebQTest-582", "question": "what did galileo do to become famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did galileo do to become famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.001753682503476739, -0.05866767838597298, -0.07363920658826828], "norm_scores": [0.3477882444858551, 0.3285469710826874, 0.3236647844314575]}}
{"id": "WebQTest-583", "question": "what school did martin luther king jr received his doctorate degree from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did martin luther king jr received his doctorate degree from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.006770828738808632, -0.023562319576740265, -0.029833568260073662], "norm_scores": [0.3377750813961029, 0.3321506977081299, 0.3300742208957672]}}
{"id": "WebQTest-584", "question": "where buddha come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.004570932127535343, -0.044428057968616486, -0.04644662141799927], "norm_scores": [0.34247565269470215, 0.3290940225124359, 0.3284303843975067]}}
{"id": "WebQTest-585", "question": "what did king henry viii rule", "prediction": [["royalty.kingdom.rulers"], ["royalty.monarch.kingdom"], ["people.person.nationality"]], "ground_paths": [["royalty.monarch.kingdom"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did king henry viii rule [/INST]", "raw_output": {"paths": ["<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.015237081795930862, -0.029863549396395683, -0.030528660863637924], "norm_scores": [0.3366658091545105, 0.331777423620224, 0.3315568268299103]}}
{"id": "WebQTest-586", "question": "what type of cancer did larry hagman have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did larry hagman have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.01488248910754919, -0.015048353932797909, -0.03208472579717636], "norm_scores": [0.3352578282356262, 0.3352022171020508, 0.329539954662323]}}
{"id": "WebQTest-588", "question": "what wars did robert e lee served in", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_person.participated_in_conflicts"]], "ground_paths": [["military.military_command.military_commander", "military.military_conflict.commanders"], ["people.person.nationality", "base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_command.military_conflict"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["military.military_conflict.military_personnel_involved"], ["military.military_person.participated_in_conflicts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wars did robert e lee served in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>"], "scores": [-0.006373761221766472, -0.02578865922987461, -0.04791320487856865], "norm_scores": [0.3401263356208801, 0.33358651399612427, 0.32628709077835083]}}
{"id": "WebQTest-589", "question": "what russian language called", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what russian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.014625770971179008, -0.025494683533906937, -0.026780109852552414], "norm_scores": [0.3358963131904602, 0.3322652578353882, 0.3318384289741516]}}
{"id": "WebQTest-590", "question": "who is in charge of libya now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in charge of libya now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.010183778591454029, -0.02412453666329384, -0.036530230194330215], "norm_scores": [0.3378203213214874, 0.33314353227615356, 0.3290361762046814]}}
{"id": "WebQTest-592", "question": "when did shays rebellion start", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shays rebellion start [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.014319440349936485, -0.020855959504842758, -0.033674973994493484], "norm_scores": [0.3362118601799011, 0.33402132987976074, 0.3297668397426605]}}
{"id": "WebQTest-593", "question": "what kind of disease does montel williams have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.person.medical_condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of disease does montel williams have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.00944533571600914, -0.01300816610455513, -0.08264695852994919], "norm_scores": [0.3417801856994629, 0.34056466817855835, 0.31765511631965637]}}
{"id": "WebQTest-595", "question": "when is the last time the ravens won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the ravens won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.010314264334738255, -0.02091146446764469, -0.03338839113712311], "norm_scores": [0.3370806872844696, 0.33352741599082947, 0.32939186692237854]}}
{"id": "WebQTest-596", "question": "where did kansas city royals come from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.previously_known_as"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kansas city royals come from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.014983125030994415, -0.022115154191851616, -0.03634980693459511], "norm_scores": [0.33650171756744385, 0.334110289812088, 0.3293880522251129]}}
{"id": "WebQTest-597", "question": "what style of music does john mayer play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.album.genre"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does john mayer play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.album.genre </PATH>"], "scores": [-0.0006490402156487107, -0.06409439444541931, -0.14947034418582916], "norm_scores": [0.3571111857891083, 0.3351579010486603, 0.30773094296455383]}}
{"id": "WebQTest-599", "question": "what inventions did thomas edison invent list", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions did thomas edison invent list [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>"], "scores": [-0.009055307134985924, -0.02257562056183815, -0.05303120240569115], "norm_scores": [0.3397260904312134, 0.33516380190849304, 0.3251100778579712]}}
{"id": "WebQTest-600", "question": "where did djokovic come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did djokovic come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.008754848502576351, -0.021341800689697266, -0.055777762085199356], "norm_scores": [0.3399559557437897, 0.3357037901878357, 0.324340283870697]}}
{"id": "WebQTest-602", "question": "what part of the country is new england", "prediction": [["location.location.contains"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.006835148669779301, -0.023381248116493225, -0.13802358508110046], "norm_scores": [0.3495718240737915, 0.34383535385131836, 0.3065928518772125]}}
{"id": "WebQTest-603", "question": "what money to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.013775804080069065, -0.02336430363357067, -0.03584219142794609], "norm_scores": [0.336855411529541, 0.3336409330368042, 0.3295036554336548]}}
{"id": "WebQTest-604", "question": "what kind of music did jimi hendrix play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did jimi hendrix play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0009117854060605168, -0.055763982236385345, -0.14214321970939636], "norm_scores": [0.355250746011734, 0.3362892270088196, 0.3084600567817688]}}
{"id": "WebQTest-605", "question": "what degrees did obama get in college", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did obama get in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.011154008097946644, -0.019856953993439674, -0.026890136301517487], "norm_scores": [0.33605292439460754, 0.33314093947410583, 0.3308061361312866]}}
{"id": "WebQTest-606", "question": "who were alexander graham bell 's siblings", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were alexander graham bell 's siblings [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.020807983353734016, -0.02273239754140377, -0.041659675538539886], "norm_scores": [0.3358588218688965, 0.3352131247520447, 0.3289280831813812]}}
{"id": "WebQTest-608", "question": "when did liverpool fc last win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did liverpool fc last win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.0045524719171226025, -0.029397092759609222, -0.057236991822719574], "norm_scores": [0.34198081493377686, 0.3335890769958496, 0.32443010807037354]}}
{"id": "WebQTest-609", "question": "who played jeannie", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jeannie [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.015705792233347893, -0.03125512972474098, -0.032841816544532776], "norm_scores": [0.3369748294353485, 0.33177563548088074, 0.3312495946884155]}}
{"id": "WebQTest-611", "question": "where was st peter killed", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st peter killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0009051896631717682, -0.08082447946071625, -0.09212800115346909], "norm_scores": [0.35260865092277527, 0.32552507519721985, 0.3218662440776825]}}
{"id": "WebQTest-612", "question": "who does jeremy shockey play for in 2012", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy shockey play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.013617733493447304, -0.01744317077100277, -0.02704702317714691], "norm_scores": [0.33525070548057556, 0.33397066593170166, 0.3307785987854004]}}
{"id": "WebQTest-613", "question": "what college did deion sanders jr go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did deion sanders jr go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>"], "scores": [-0.019796954467892647, -0.02383631095290184, -0.023860163986682892], "norm_scores": [0.33423423767089844, 0.33288687467575073, 0.3328789174556732]}}
{"id": "WebQTest-614", "question": "when did the battle of alamo start", "prediction": [["time.event.locations"], ["time.event.included_in_event"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the battle of alamo start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.011101162061095238, -0.028399333357810974, -0.04446018487215042], "norm_scores": [0.33897826075553894, 0.3331649899482727, 0.32785677909851074]}}
{"id": "WebQTest-615", "question": "what were the names of henry viii 's three children", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were the names of henry viii 's three children [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.012268357910215855, -0.012885541655123234, -0.13048891723155975], "norm_scores": [0.34627440571784973, 0.34606072306632996, 0.3076648414134979]}}
{"id": "WebQTest-616", "question": "what was queen elizabeth ii childhood nickname", "prediction": [["people.person.children"], ["people.person.parents"], ["royalty.monarch.kingdom"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was queen elizabeth ii childhood nickname [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> royalty.monarch.kingdom </PATH>"], "scores": [-0.023843182250857353, -0.029491683468222618, -0.03412995487451553], "norm_scores": [0.33510568737983704, 0.33321815729141235, 0.331676185131073]}}
{"id": "WebQTest-617", "question": "where did tim tebow grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tim tebow grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.010078922845423222, -0.01937372051179409, -0.06109366565942764], "norm_scores": [0.3400189280509949, 0.3368731439113617, 0.3231079578399658]}}
{"id": "WebQTest-618", "question": "where is chesapeake bay bridge", "prediction": [["transportation.bridge.locale"], ["transportation.bridge.body_of_water_spanned"], ["location.location.containedby"]], "ground_paths": [["transportation.bridge.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chesapeake bay bridge [/INST]", "raw_output": {"paths": ["<PATH> transportation.bridge.locale </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0009836413664743304, -0.06525616347789764, -0.08464700728654861], "norm_scores": [0.3499574363231659, 0.32817238569259644, 0.3218701481819153]}}
{"id": "WebQTest-619", "question": "who played victoria barkley", "prediction": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["film.film.country", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played victoria barkley [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.020194321870803833, -0.02109476923942566, -0.023692931979894638], "norm_scores": [0.3338221311569214, 0.33352166414260864, 0.33265626430511475]}}
{"id": "WebQTest-620", "question": "where did kennedy 's inaugural address take place", "prediction": [["time.event.locations"], ["location.location.events"], ["architecture.architect.structures_designed"]], "ground_paths": [["symbols.name_source.namesakes", "people.person.place_of_birth"], ["common.image.appears_in_topic_gallery", "common.image.image_location"], ["people.person.nationality", "location.administrative_division.country"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["symbols.name_source.namesakes", "location.location.containedby"], ["base.inaugurations.inauguration.president", "location.location.events"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kennedy 's inaugural address take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> architecture.architect.structures_designed </PATH>"], "scores": [-0.000709230313077569, -0.06875364482402802, -0.09887249022722244], "norm_scores": [0.35202348232269287, 0.32886698842048645, 0.31910958886146545]}}
{"id": "WebQTest-621", "question": "what countries have german as the official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have german as the official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.015278555452823639, -0.020686332136392593, -0.02437494695186615], "norm_scores": [0.3349464535713196, 0.3331400454044342, 0.33191344141960144]}}
{"id": "WebQTest-622", "question": "where did the taliban began", "prediction": [["organization.organization.place_founded"], ["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the taliban began [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.01953495293855667, -0.02533307857811451, -0.030101800337433815], "norm_scores": [0.3351534903049469, 0.3332158625125885, 0.3316306471824646]}}
{"id": "WebQTest-624", "question": "who plays elena gilbert on vampire diaries", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays elena gilbert on vampire diaries [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.025663301348686218, -0.03106645494699478, -0.031987909227609634], "norm_scores": [0.33463767170906067, 0.3328344523906708, 0.33252787590026855]}}
{"id": "WebQTest-626", "question": "what does the missouri river bisect", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does the missouri river bisect [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011158401146531105, -0.019581099972128868, -0.05755917355418205], "norm_scores": [0.33941197395324707, 0.336565226316452, 0.3240227997303009]}}
{"id": "WebQTest-627", "question": "what nationality is taylor lautner jacob black", "prediction": [["people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is taylor lautner jacob black [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.nationality </PATH>"], "scores": [-0.021027570590376854, -0.022501496598124504, -0.049224551767110825], "norm_scores": [0.3366182744503021, 0.3361224830150604, 0.32725921273231506]}}
{"id": "WebQTest-628", "question": "what animal represents california", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what animal represents california [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.0189677644520998, -0.027077946811914444, -0.03806217014789581], "norm_scores": [0.33635953068733215, 0.3336426019668579, 0.32999783754348755]}}
{"id": "WebQTest-629", "question": "what company does nike own", "prediction": [["business.business_operation.industry"], ["business.business_operation.industry", "business.business_operation.industry"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"]], "ground_paths": [["business.business_operation.industry", "business.business_operation.industry"], ["business.acquisition.acquiring_company", "business.acquisition.company_acquired"], ["business.industry.companies", "business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what company does nike own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>"], "scores": [-0.029134932905435562, -0.02940434403717518, -0.03176990896463394], "norm_scores": [0.3336559534072876, 0.3335660994052887, 0.3327779471874237]}}
{"id": "WebQTest-630", "question": "what language do colombian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do colombian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.008723760955035686, -0.018336230888962746, -0.07148831337690353], "norm_scores": [0.3413437306880951, 0.3380782902240753, 0.3205779194831848]}}
{"id": "WebQTest-631", "question": "what year did ray allen join the nba", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did ray allen join the nba [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.01803743839263916, -0.022687919437885284, -0.023668941110372543], "norm_scores": [0.3344767093658447, 0.33292484283447266, 0.33259838819503784]}}
{"id": "WebQTest-633", "question": "what is claire danes famous for", "prediction": [["people.person.profession"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is claire danes famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.0009876660769805312, -0.0748966857790947, -0.07531486451625824], "norm_scores": [0.3500022888183594, 0.32506680488586426, 0.32493090629577637]}}
{"id": "WebQTest-634", "question": "what county is san antonio in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san antonio in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.008463988080620766, -0.02678709849715233, -0.03900539502501488], "norm_scores": [0.3387804925441742, 0.3326295018196106, 0.3285900354385376]}}
{"id": "WebQTest-635", "question": "what characters does matt stone do", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_no_value"], ["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["people.person.profession", "people.person.profession"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.nationality", "people.person.nationality"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"], ["people.person.ethnicity", "people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what characters does matt stone do [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.016965534538030624, -0.036214277148246765, -0.0400630347430706], "norm_scores": [0.3380545377731323, 0.33160966634750366, 0.3303358256816864]}}
{"id": "WebQTest-636", "question": "what timezone is the state of michigan and", "prediction": [["location.location.time_zones"], ["base.locations.states_and_provences.country", "location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is the state of michigan and [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> base.locations.states_and_provences.country <SEP> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-1.1926414117624518e-05, -0.14555411040782928, -0.15417230129241943], "norm_scores": [0.36741903424263, 0.3176533877849579, 0.3149275779724121]}}
{"id": "WebQTest-637", "question": "what type of government did soviet union have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government did soviet union have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007543249987065792, -0.017810208722949028, -0.14520622789859772], "norm_scores": [0.3495064079761505, 0.3459363877773285, 0.304557204246521]}}
{"id": "WebQTest-638", "question": "what languages are there in japan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages are there in japan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.014546532183885574, -0.021956831216812134, -0.03185888007283211], "norm_scores": [0.33608320355415344, 0.3336019217967987, 0.33031487464904785]}}
{"id": "WebQTest-639", "question": "what year did the bulls get rodman", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_team.championships"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the bulls get rodman [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.017359739169478416, -0.033850617706775665, -0.04870322719216347], "norm_scores": [0.33866310119628906, 0.3331240713596344, 0.3282128572463989]}}
{"id": "WebQTest-640", "question": "who was saint paul the apostle", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was saint paul the apostle [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.00310328952036798, -0.05074119567871094, -0.06957422196865082], "norm_scores": [0.34612026810646057, 0.33001840114593506, 0.323861300945282]}}
{"id": "WebQTest-642", "question": "where did vince scully go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did vince scully go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.014705627225339413, -0.020568422973155975, -0.02365027368068695], "norm_scores": [0.3349803686141968, 0.33302217721939087, 0.33199745416641235]}}
{"id": "WebQTest-643", "question": "where does princess leia live", "prediction": [["fictional_universe.fictional_character.places_lived"], ["fictional_universe.fictional_character.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["fictional_universe.fictional_character.places_lived"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does princess leia live [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.places_lived </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.003942230716347694, -0.03663942590355873, -0.06772609800100327], "norm_scores": [0.34411096572875977, 0.3330414891242981, 0.3228475749492645]}}
{"id": "WebQTest-644", "question": "what famous people are from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.006422092206776142, -0.03561891242861748, -0.04842505231499672], "norm_scores": [0.34128621220588684, 0.33146578073501587, 0.3272480368614197]}}
{"id": "WebQTest-645", "question": "what type of art did leonardo da vinci make", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did leonardo da vinci make [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.00020023742399644107, -0.09806227684020996, -0.1028558760881424], "norm_scores": [0.3559717535972595, 0.322785884141922, 0.3212423026561737]}}
{"id": "WebQTest-646", "question": "where did mary bell live", "prediction": [["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mary bell live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.016020502895116806, -0.03339727967977524, -0.03502182289958], "norm_scores": [0.33738744258880615, 0.3315753936767578, 0.33103716373443604]}}
{"id": "WebQTest-648", "question": "who did kimberly williams marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["film.film.produced_by", "film.music_contributor.film"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kimberly williams marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.019958673045039177, -0.021708093583583832, -0.04610850661993027], "norm_scores": [0.3364238142967224, 0.3358357846736908, 0.3277404010295868]}}
{"id": "WebQTest-649", "question": "what other movies has ryan gosling been in", "prediction": [["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.film.executive_produced_by"], ["film.actor.film", "film.performance.film"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.featured_film_locations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has ryan gosling been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.025493258610367775, -0.028228314593434334, -0.033415187150239944], "norm_scores": [0.3345177471637726, 0.333604097366333, 0.3318781852722168]}}
{"id": "WebQTest-650", "question": "what political party does julia gillard belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does julia gillard belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.008217019028961658, -0.022006163373589516, -0.022509967908263206], "norm_scores": [0.33646079897880554, 0.33185315132141113, 0.33168601989746094]}}
{"id": "WebQTest-651", "question": "what is the currency of germany now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of germany now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.009075142443180084, -0.020767776295542717, -0.05969199165701866], "norm_scores": [0.3402496874332428, 0.33629441261291504, 0.32345589995384216]}}
{"id": "WebQTest-652", "question": "what kind of hispanic is selena gomez", "prediction": [["people.ethnicity.included_in_group"], ["people.person.ethnicity"], ["people.ethnicity.people"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of hispanic is selena gomez [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.01173488236963749, -0.027074500918388367, -0.03295315429568291], "norm_scores": [0.33740663528442383, 0.33227044343948364, 0.33032289147377014]}}
{"id": "WebQTest-653", "question": "who did denise van outen married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did denise van outen married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.016740992665290833, -0.03456665948033333, -0.03812655061483383], "norm_scores": [0.3377039134502411, 0.3317374587059021, 0.3305585980415344]}}
{"id": "WebQTest-654", "question": "what are egyptian money called", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are egyptian money called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.00853030662983656, -0.02844366431236267, -0.05766443908214569], "norm_scores": [0.34102490544319153, 0.3343011140823364, 0.32467395067214966]}}
{"id": "WebQTest-655", "question": "what is the zip code for wyoming michigan", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for wyoming michigan [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0014363961527124047, -0.05241348594427109, -0.08346373587846756], "norm_scores": [0.348244309425354, 0.3309367299079895, 0.3208189606666565]}}
{"id": "WebQTest-656", "question": "who is the president of costa rica in 2012", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of costa rica in 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.0034671539906412363, -0.03236721083521843, -0.04174647480249405], "norm_scores": [0.3408365547657013, 0.3311273157596588, 0.3280361294746399]}}
{"id": "WebQTest-657", "question": "where did martin luther king got shot at", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did martin luther king got shot at [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.00021697746706195176, -0.11322718858718872, -0.11986171454191208], "norm_scores": [0.3596634268760681, 0.32123035192489624, 0.31910619139671326]}}
{"id": "WebQTest-658", "question": "who wrote the gospel according to john", "prediction": [["book.book_subject.works"], ["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the gospel according to john [/INST]", "raw_output": {"paths": ["<PATH> book.book_subject.works </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.013546758331358433, -0.023837974295020103, -0.03368860110640526], "norm_scores": [0.33672061562538147, 0.33327311277389526, 0.33000627160072327]}}
{"id": "WebQTest-659", "question": "what form of government is the uk", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["form.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government is the uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> form.form_of_government.countries </PATH>"], "scores": [-0.007536870427429676, -0.018730230629444122, -0.15120740234851837], "norm_scores": [0.35025754570961, 0.3463588356971741, 0.30338364839553833]}}
{"id": "WebQTest-661", "question": "who did nasri play for before arsenal", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did nasri play for before arsenal [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.02620546706020832, -0.03548359125852585, -0.03817540407180786], "norm_scores": [0.3356981873512268, 0.332597941160202, 0.3317038416862488]}}
{"id": "WebQTest-662", "question": "what currency does the czech republic use now", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does the czech republic use now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.013364361599087715, -0.02213023416697979, -0.02799149975180626], "norm_scores": [0.33593663573265076, 0.3330047130584717, 0.3310585916042328]}}
{"id": "WebQTest-663", "question": "what age did william penn get married", "prediction": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what age did william penn get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.014004177413880825, -0.0351036861538887, -0.03693792596459389], "norm_scores": [0.33824363350868225, 0.3311816155910492, 0.33057472109794617]}}
{"id": "WebQTest-664", "question": "what was firefox programmed in", "prediction": [["computer.software_developer.software"], ["film.film.story_by"], ["computer.software.language"]], "ground_paths": [["computer.software.languages_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was firefox programmed in [/INST]", "raw_output": {"paths": ["<PATH> computer.software_developer.software </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> computer.software.language </PATH>"], "scores": [-0.05301337689161301, -0.05340719223022461, -0.057828374207019806], "norm_scores": [0.3339117765426636, 0.33378031849861145, 0.3323078751564026]}}
{"id": "WebQTest-665", "question": "what government does new zealand have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does new zealand have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>"], "scores": [-0.005960788577795029, -0.022624913603067398, -0.07797166705131531], "norm_scores": [0.34317150712013245, 0.3375002443790436, 0.319328248500824]}}
{"id": "WebQTest-666", "question": "where is isthmus of panama located on the map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is isthmus of panama located on the map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.0011292787967249751, -0.059358861297369, -0.09128262847661972], "norm_scores": [0.34999001026153564, 0.3301922678947449, 0.31981775164604187]}}
{"id": "WebQTest-667", "question": "what is the capital of republic of ireland", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.location.contains"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of republic of ireland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0074180178344249725, -0.028162941336631775, -0.05611862987279892], "norm_scores": [0.3410716950893402, 0.33406904339790344, 0.32485929131507874]}}
{"id": "WebQTest-668", "question": "what has kristen stewart acted in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.place_of_birth", "film.film.subjects"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has kristen stewart acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.026965618133544922, -0.027072271332144737, -0.03465795889496803], "norm_scores": [0.33419886231422424, 0.33416321873664856, 0.3316379189491272]}}
{"id": "WebQTest-669", "question": "what did dr josef mengele do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["base.crime.crime.victim_s"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr josef mengele do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> base.crime.crime.victim_s </PATH>"], "scores": [-0.0022270861081779003, -0.05708727613091469, -0.1077866405248642], "norm_scores": [0.3513162136077881, 0.33256208896636963, 0.3161216974258423]}}
{"id": "WebQTest-670", "question": "what kind of language does egypt speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does egypt speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.018982289358973503, -0.0243240874260664, -0.026111291721463203], "norm_scores": [0.33472031354904175, 0.3329370617866516, 0.33234256505966187]}}
{"id": "WebQTest-671", "question": "who are the senators of hawaii 2012", "prediction": [["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of hawaii 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.023297788575291634, -0.025429295375943184, -0.031496722251176834], "norm_scores": [0.3344810903072357, 0.33376893401145935, 0.33174991607666016]}}
{"id": "WebQTest-672", "question": "what form of government does north korea have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does north korea have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010173671878874302, -0.013394784182310104, -0.13160328567028046], "norm_scores": [0.34692859649658203, 0.3458128869533539, 0.3072584867477417]}}
{"id": "WebQTest-673", "question": "where did robert kardashian go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert kardashian go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.012927128002047539, -0.018261559307575226, -0.029321279376745224], "norm_scores": [0.3357485830783844, 0.3339623212814331, 0.3302891254425049]}}
{"id": "WebQTest-675", "question": "what county is st james city fl in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st james city fl in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.0202042106539011, -0.023452814668416977, -0.029217753559350967], "norm_scores": [0.33469629287719727, 0.33361074328422546, 0.33169302344322205]}}
{"id": "WebQTest-676", "question": "what did the akkadian empire include", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.country.languages_spoken"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the akkadian empire include [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.011987859383225441, -0.0297723226249218, -0.03476046770811081], "norm_scores": [0.3378541171550751, 0.33189865946769714, 0.3302472233772278]}}
{"id": "WebQTest-677", "question": "after how many years are the olympic games held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.current_frequency"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: after how many years are the olympic games held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.010615058243274689, -0.016711564734578133, -0.04763374477624893], "norm_scores": [0.33811426162719727, 0.33605921268463135, 0.3258265554904938]}}
{"id": "WebQTest-678", "question": "what system of government does guyana have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government does guyana have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007546896580606699, -0.017813853919506073, -0.14290881156921387], "norm_scores": [0.3492613434791565, 0.34569382667541504, 0.3050447702407837]}}
{"id": "WebQTest-680", "question": "what team did aguero play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did aguero play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.024124080315232277, -0.025910260155797005, -0.03016350232064724], "norm_scores": [0.33420291543006897, 0.3336065113544464, 0.3321905732154846]}}
{"id": "WebQTest-681", "question": "who is the president of the european union 2012", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["organization.organization_membership.organization", "organization.organization_membership.member"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of the european union 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_membership.member </PATH>"], "scores": [-0.010829164646565914, -0.017638996243476868, -0.029430730268359184], "norm_scores": [0.3361589014530182, 0.33387747406959534, 0.3299636244773865]}}
{"id": "WebQTest-683", "question": "what year was kenya moore crowned miss usa", "prediction": [["people.person.nationality", "location.location.events"], ["people.person.nationality", "time.event.locations"], ["award.competitor.competitions", "award.competitor.competitions"]], "ground_paths": [["time.recurring_event.instances"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was kenya moore crowned miss usa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.events </PATH>", "<PATH> people.person.nationality <SEP> time.event.locations </PATH>", "<PATH> award.competitor.competitions <SEP> award.competitor.competitions </PATH>"], "scores": [-0.016226453706622124, -0.031725797802209854, -0.046730805188417435], "norm_scores": [0.33845797181129456, 0.33325257897377014, 0.3282894194126129]}}
{"id": "WebQTest-684", "question": "what sort of government does nigeria have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sort of government does nigeria have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.005916577763855457, -0.021306393668055534, -0.1453476995229721], "norm_scores": [0.35031411051750183, 0.3449641168117523, 0.30472177267074585]}}
{"id": "WebQTest-685", "question": "where does chef ina garten live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does chef ina garten live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.019727839156985283, -0.022290991619229317, -0.029271753504872322], "norm_scores": [0.3346785604953766, 0.3338218629360199, 0.3314996063709259]}}
{"id": "WebQTest-686", "question": "when the atlanta braves won the world series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.mascot.team"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when the atlanta braves won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.mascot.team </PATH>"], "scores": [-0.010214201174676418, -0.015312322415411472, -0.05178140476346016], "norm_scores": [0.3385011553764343, 0.33677980303764343, 0.32471904158592224]}}
{"id": "WebQTest-688", "question": "what kind of language do they speak in iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do they speak in iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013683686964213848, -0.019145652651786804, -0.03455129638314247], "norm_scores": [0.3362586200237274, 0.33442696928977966, 0.3293144106864929]}}
{"id": "WebQTest-689", "question": "what was the first language spoken in spain", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first language spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.018194522708654404, -0.020972823724150658, -0.021272709593176842], "norm_scores": [0.333984375, 0.3330577313899994, 0.3329578936100006]}}
{"id": "WebQTest-690", "question": "what did scott joplin play", "prediction": [["music.group_member.instruments_played"], ["people.person.profession"], ["music.instrument.family"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did scott joplin play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.002002423396334052, -0.07569964230060577, -0.0849909782409668], "norm_scores": [0.3509615957736969, 0.3260267972946167, 0.323011577129364]}}
{"id": "WebQTest-691", "question": "what school did miley cyrus", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did miley cyrus [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.019756237044930458, -0.024724313989281654, -0.02591015212237835], "norm_scores": [0.3345701992511749, 0.3329121470451355, 0.3325176239013672]}}
{"id": "WebQTest-693", "question": "what language do most people speak in afghanistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most people speak in afghanistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01574484258890152, -0.018422307446599007, -0.032678019255399704], "norm_scores": [0.33551016449928284, 0.3346130847930908, 0.32987675070762634]}}
{"id": "WebQTest-694", "question": "who is the prime minister of jamaica west indies", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of jamaica west indies [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.010955112054944038, -0.027246421203017235, -0.03120545484125614], "norm_scores": [0.33740535378456116, 0.33195310831069946, 0.330641508102417]}}
{"id": "WebQTest-695", "question": "what role did paul mccartney play in the beatles", "prediction": [["music.musical_group.member", "music.group_membership.role"], ["music.musical_group.member", "music.performance_role.regular_performances"], ["music.producer.releases_produced", "music.group_membership.member", "music.group_membership.role"]], "ground_paths": [["music.group_membership.member", "music.group_membership.role"], ["music.group_member.membership", "music.performance_role.regular_performances"], ["music.group_member.instruments_played", "music.instrument.family"], ["music.group_member.instruments_played", "music.instrument.variation"], ["music.group_member.membership", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did paul mccartney play in the beatles [/INST]", "raw_output": {"paths": ["<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.musical_group.member <SEP> music.performance_role.regular_performances </PATH>", "<PATH> music.producer.releases_produced <SEP> music.group_membership.member <SEP> music.group_membership.role </PATH>"], "scores": [-0.01343217771500349, -0.02117483876645565, -0.02512010745704174], "norm_scores": [0.33549535274505615, 0.3329077363014221, 0.3315969407558441]}}
{"id": "WebQTest-696", "question": "when did juventus win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did juventus win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.005384093150496483, -0.023863336071372032, -0.08744612336158752], "norm_scores": [0.3444824814796448, 0.3381751477718353, 0.3173423409461975]}}
{"id": "WebQTest-698", "question": "who is robin roberts father", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is robin roberts father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.008355341851711273, -0.022557351738214493, -0.10119060426950455], "norm_scores": [0.34515583515167236, 0.34028857946395874, 0.3145555853843689]}}
{"id": "WebQTest-699", "question": "what is the capital of australia victoria state", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of australia victoria state [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.01554843783378601, -0.037992656230926514, -0.038513414561748505], "norm_scores": [0.338397741317749, 0.3308872580528259, 0.33071500062942505]}}
{"id": "WebQTest-700", "question": "where is french spoken most", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["base.rosetta.local_name.locale"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is french spoken most [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016709938645362854, -0.023283272981643677, -0.023900063708424568], "norm_scores": [0.3348643481731415, 0.3326703906059265, 0.332465261220932]}}
{"id": "WebQTest-701", "question": "what team is reggie bush on 2011", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is reggie bush on 2011 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.020263897255063057, -0.022831326350569725, -0.023870544508099556], "norm_scores": [0.33401966094970703, 0.33316320180892944, 0.3328171372413635]}}
{"id": "WebQTest-702", "question": "what highschool did emily osment go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did emily osment go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.014979701489210129, -0.021570907905697823, -0.022357601672410965], "norm_scores": [0.33488723635673523, 0.3326871991157532, 0.3324255645275116]}}
{"id": "WebQTest-703", "question": "what currency should i take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.00549985421821475, -0.03282159939408302, -0.05862908065319061], "norm_scores": [0.34231269359588623, 0.33308669924736023, 0.32460054755210876]}}
{"id": "WebQTest-704", "question": "who is thor heyerdahl", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is thor heyerdahl [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.001300347619690001, -0.05770394578576088, -0.09904906153678894], "norm_scores": [0.3506269156932831, 0.33139768242836, 0.31797540187835693]}}
{"id": "WebQTest-706", "question": "where did troy aikman play football", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did troy aikman play football [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.011650143191218376, -0.014039095491170883, -0.058523595333099365], "norm_scores": [0.33877381682395935, 0.3379654586315155, 0.32326075434684753]}}
{"id": "WebQTest-707", "question": "who played cletus hogg", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played cletus hogg [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.02050931565463543, -0.027133973315358162, -0.031556710600852966], "norm_scores": [0.33529922366142273, 0.3330853283405304, 0.3316154181957245]}}
{"id": "WebQTest-708", "question": "who won fa cup 1976", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fa cup 1976 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.0190994031727314, -0.023264167830348015, -0.02408614754676819], "norm_scores": [0.33435091376304626, 0.3329613208770752, 0.33268773555755615]}}
{"id": "WebQTest-709", "question": "what did general robert e lee die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["base.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did general robert e lee die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> base.cause_of_death.people </PATH>"], "scores": [-0.0016994413454085588, -0.04088938981294632, -0.11374694108963013], "norm_scores": [0.3501928150653839, 0.33673420548439026, 0.31307297945022583]}}
{"id": "WebQTest-710", "question": "what style of art did henri matisse do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of art did henri matisse do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0018665571697056293, -0.0369076244533062, -0.08214887976646423], "norm_scores": [0.346209853887558, 0.3342883884906769, 0.31950175762176514]}}
{"id": "WebQTest-711", "question": "who plays chuck bass in gossip girl", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["base.gossipgirl.character.played_by"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays chuck bass in gossip girl [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.021664349362254143, -0.030918650329113007, -0.034055691212415695], "norm_scores": [0.3357424736022949, 0.33264970779418945, 0.3316078186035156]}}
{"id": "WebQTest-712", "question": "what countries have territories in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have territories in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.008869579061865807, -0.03465425595641136, -0.04401319846510887], "norm_scores": [0.34013471007347107, 0.3314765691757202, 0.3283887803554535]}}
{"id": "WebQTest-713", "question": "what county is st. louis park in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st. louis park in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.006448350381106138, -0.03956979513168335, -0.04700082167983055], "norm_scores": [0.34156739711761475, 0.3304394781589508, 0.32799309492111206]}}
{"id": "WebQTest-714", "question": "what is the political system of south africa", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system of south africa [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011549481190741062, -0.012706873007118702, -0.07660234719514847], "norm_scores": [0.34061557054519653, 0.3402215838432312, 0.31916287541389465]}}
{"id": "WebQTest-716", "question": "what language group does germany belong to", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language group does germany belong to [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.012727261520922184, -0.02787584438920021, -0.03566000983119011], "norm_scores": [0.3375762403011322, 0.33250099420547485, 0.32992276549339294]}}
{"id": "WebQTest-717", "question": "what timezone is tucson arizona in right now", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is tucson arizona in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-8.916249498724937e-06, -0.17425556480884552, -0.18192872405052185], "norm_scores": [0.3740054666996002, 0.3141981065273285, 0.3117964565753937]}}
{"id": "WebQTest-718", "question": "when was lucy lawless born", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was lucy lawless born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0007281855796463788, -0.07610408216714859, -0.09062493592500687], "norm_scores": [0.3519366681575775, 0.32638421654701233, 0.32167908549308777]}}
{"id": "WebQTest-719", "question": "which political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "ground_paths": [["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.019069017842411995, -0.021369868889451027, -0.02212793566286564], "norm_scores": [0.3339291214942932, 0.33316168189048767, 0.3329091966152191]}}
{"id": "WebQTest-721", "question": "what kind of leukemia did jill clayburgh have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.person.medical_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of leukemia did jill clayburgh have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.011840437538921833, -0.01290364470332861, -0.058805666863918304], "norm_scores": [0.3386320173740387, 0.33827221393585205, 0.32309576869010925]}}
{"id": "WebQTest-723", "question": "what is there to see in sydney australia", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to see in sydney australia [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.014346452429890633, -0.016089050099253654, -0.028464235365390778], "norm_scores": [0.3350936472415924, 0.3345102369785309, 0.3303960859775543]}}
{"id": "WebQTest-724", "question": "where was marilyn monroe found dead", "prediction": [["people.deceased_person.place_of_death"], ["people.cause_of_death.people"], ["people.place_of_death.people_found_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was marilyn monroe found dead [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.place_of_death.people_found_here </PATH>"], "scores": [-0.0003012650995515287, -0.0856369361281395, -0.1170424073934555], "norm_scores": [0.35612285137176514, 0.3269934356212616, 0.3168836832046509]}}
{"id": "WebQTest-725", "question": "what states does the sierra nevada run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states does the sierra nevada run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0070194764994084835, -0.027444642037153244, -0.043502334505319595], "norm_scores": [0.33967888355255127, 0.33281126618385315, 0.3275097906589508]}}
{"id": "WebQTest-726", "question": "what language is spoken in greek", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in greek [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014315691776573658, -0.018232081085443497, -0.042921893298625946], "norm_scores": [0.33693966269493103, 0.3356226682662964, 0.3274376392364502]}}
{"id": "WebQTest-728", "question": "what is the name of the speaker of the house of representatives now 2011", "prediction": [["common.topic.image", "common.topic.image"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"]], "ground_paths": [["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the speaker of the house of representatives now 2011 [/INST]", "raw_output": {"paths": ["<PATH> common.topic.image <SEP> common.topic.image </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.010135889984667301, -0.016487281769514084, -0.02602500654757023], "norm_scores": [0.3358065187931061, 0.3336804509162903, 0.33051300048828125]}}
{"id": "WebQTest-729", "question": "who plays stanley hudson", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stanley hudson [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.021951178088784218, -0.030654167756438255, -0.03263546898961067], "norm_scores": [0.3354908525943756, 0.3325837254524231, 0.3319254517555237]}}
{"id": "WebQTest-730", "question": "what record label is kelly clarkson signed with", "prediction": [["music.artist.label"], ["music.record_label.artist"], ["music.recording.artist"]], "ground_paths": [["music.record_label.artist"], ["music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what record label is kelly clarkson signed with [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.0018551504472270608, -0.05834363028407097, -0.07274346053600311], "norm_scores": [0.3476273715496063, 0.3285347819328308, 0.3238378167152405]}}
{"id": "WebQTest-731", "question": "where does tim roth come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim roth come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.006856956984847784, -0.03255753219127655, -0.049578722566366196], "norm_scores": [0.340970516204834, 0.3323190212249756, 0.32671043276786804]}}
{"id": "WebQTest-733", "question": "what currency should you take to vietnam", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["base.monetarydebase.debased_currency.issuing_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to vietnam [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.011366473510861397, -0.016253355890512466, -0.07142037153244019], "norm_scores": [0.34050217270851135, 0.3388422429561615, 0.32065558433532715]}}
{"id": "WebQTest-734", "question": "who did roger federer married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"]], "ground_paths": [["people.person.children", "people.person.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did roger federer married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>"], "scores": [-0.024655887857079506, -0.03494665399193764, -0.04118018597364426], "norm_scores": [0.33631834387779236, 0.3328751027584076, 0.33080658316612244]}}
{"id": "WebQTest-735", "question": "what kind of money do i need in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i need in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.006501362193375826, -0.021519405767321587, -0.10448126494884491], "norm_scores": [0.3458099961280823, 0.3406554162502289, 0.31353455781936646]}}
{"id": "WebQTest-737", "question": "what is the name of the capital of belgium", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"], ["location.location.containedby"]], "ground_paths": [["location.country.capital"], ["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the capital of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011882735416293144, -0.02203523740172386, -0.030090462416410446], "norm_scores": [0.33649006485939026, 0.333091139793396, 0.33041879534721375]}}
{"id": "WebQTest-738", "question": "what kind of government does japan have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does japan have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009330379776656628, -0.014852628111839294, -0.1218872219324112], "norm_scores": [0.3462556302547455, 0.3443487882614136, 0.30939555168151855]}}
{"id": "WebQTest-739", "question": "what pieces of music did claude debussy compose", "prediction": [["music.composition.composer"], ["music.artist.genre"], ["music.recording.artist"]], "ground_paths": [["book.author.works_written", "book.book.editions"], ["base.musiteca.music_teacher.composition"], ["book.written_work.author"], ["opera.librettist.libretti"], ["music.recording.artist"], ["book.author.works_written"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["opera.opera.librettist"], ["music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what pieces of music did claude debussy compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.00745734479278326, -0.03405988961458206, -0.05381852015852928], "norm_scores": [0.3414781093597412, 0.332513689994812, 0.3260081708431244]}}
{"id": "WebQTest-740", "question": "who was selena gomez in barney and friends", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["film.actor.film", "film.performance.character"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.character"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.character"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was selena gomez in barney and friends [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.02415546216070652, -0.026607587933540344, -0.033684585243463516], "norm_scores": [0.3346644937992096, 0.3338448703289032, 0.33149057626724243]}}
{"id": "WebQTest-741", "question": "who plays caesar flickerman in the hunger games", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.film.starring", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays caesar flickerman in the hunger games [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.02644316293299198, -0.03005138598382473, -0.03864990547299385], "norm_scores": [0.3350908160209656, 0.3338839113712311, 0.33102530241012573]}}
{"id": "WebQTest-742", "question": "where did the gettysburg address", "prediction": [["book.written_work.subjects"], ["book.author.book_editions_published"], ["book.book_subject.works"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gettysburg address [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.012095711193978786, -0.04405466467142105, -0.048409365117549896], "norm_scores": [0.3409613072872162, 0.3302368223667145, 0.32880187034606934]}}
{"id": "WebQTest-743", "question": "what was the name of jfk 's brothers", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.children"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["film.film_subject.films", "film.film.subjects"], ["people.person.parents", "people.person.children"], ["people.family.members", "people.family_member.family"], ["people.family.members", "people.family.members"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["people.person.religion", "people.person.religion"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.gender", "people.person.gender"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of jfk 's brothers [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03132297843694687, -0.03434267267584801, -0.03680660203099251], "norm_scores": [0.3342786431312561, 0.33327075839042664, 0.33245059847831726]}}
{"id": "WebQTest-744", "question": "who was tupac signed to", "prediction": [["music.artist.label"], ["music.record_label.artist"], ["music.recording.artist"]], "ground_paths": [["music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was tupac signed to [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.009428457356989384, -0.024154076352715492, -0.057106439024209976], "norm_scores": [0.34027227759361267, 0.3352982699871063, 0.324429452419281]}}
{"id": "WebQTest-745", "question": "what was augustus caesar famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was augustus caesar famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0048390598967671394, -0.044786278158426285, -0.05196891725063324], "norm_scores": [0.34307628870010376, 0.3296414613723755, 0.32728227972984314]}}
{"id": "WebQTest-747", "question": "what language does colombia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does colombia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.010468016378581524, -0.01944650150835514, -0.06000421196222305], "norm_scores": [0.3398202359676361, 0.3367828130722046, 0.3233969509601593]}}
{"id": "WebQTest-748", "question": "what is the zip code for schenectady new york", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for schenectady new york [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00047858530888333917, -0.06546734273433685, -0.11016920953989029], "norm_scores": [0.3529590964317322, 0.3307501971721649, 0.31629064679145813]}}
{"id": "WebQTest-749", "question": "who was president in 1988 in the united states", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"]], "ground_paths": [["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president in 1988 in the united states [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>"], "scores": [-0.02338150329887867, -0.0237574465572834, -0.026100892573595047], "norm_scores": [0.33367720246315, 0.3335517644882202, 0.33277103304862976]}}
{"id": "WebQTest-751", "question": "who owns google 2012", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.leadership.organization", "organization.leadership.person"], ["organization.leadership.organization", "business.board_member.leader_of"], ["organization.organization.leadership", "business.board_member.leader_of"], ["conferences.conference.focus", "time.event.people_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns google 2012 [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.020623188465833664, -0.022461961954832077, -0.051506157964468], "norm_scores": [0.33695536851882935, 0.3363363742828369, 0.32670825719833374]}}
{"id": "WebQTest-752", "question": "what drink did john pemberton create", "prediction": [["food.beer_country_region.beers_from_here"], ["food.beer_country_region.beers_from_here", "food.beer_country_region.beers_from_here"], ["food.beer.from_region"]], "ground_paths": [["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drink did john pemberton create [/INST]", "raw_output": {"paths": ["<PATH> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer_country_region.beers_from_here <SEP> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer.from_region </PATH>"], "scores": [-0.008626764640212059, -0.02690540999174118, -0.04488201066851616], "norm_scores": [0.3394109010696411, 0.3332632780075073, 0.32732585072517395]}}
{"id": "WebQTest-753", "question": "what position did stanley matthews play", "prediction": [["sports.pro_athlete.teams", "sports.sports_position.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["soccer.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did stanley matthews play [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_position.teams </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.030614804476499557, -0.0358772836625576, -0.03718116879463196], "norm_scores": [0.33464890718460083, 0.33289244771003723, 0.3324586749076843]}}
{"id": "WebQTest-755", "question": "what was bonnie prince charlie full name", "prediction": [["people.person.nationality"], ["common.topic.notable_types"], ["people.person.children"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was bonnie prince charlie full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.0077265789732337, -0.05164780095219612, -0.05540580302476883], "norm_scores": [0.3435872197151184, 0.328823059797287, 0.32758966088294983]}}
{"id": "WebQTest-756", "question": "who makes the voice of stewie griffin", "prediction": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who makes the voice of stewie griffin [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.01924598403275013, -0.02188044786453247, -0.022684896364808083], "norm_scores": [0.33400845527648926, 0.3331297039985657, 0.3328618109226227]}}
{"id": "WebQTest-757", "question": "who is angelina jolie mom and dad", "prediction": [["people.person.parents"], ["people.person.children"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie mom and dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.010418621823191643, -0.015820639207959175, -0.09608029574155807], "norm_scores": [0.34334561228752136, 0.3414958417415619, 0.31515854597091675]}}
{"id": "WebQTest-758", "question": "who is the governor of indiana 2009", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["common.topic.image"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of indiana 2009 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.019093846902251244, -0.019242363050580025, -0.023654239252209663], "norm_scores": [0.3338561952114105, 0.3338066339492798, 0.3323371708393097]}}
{"id": "WebQTest-759", "question": "who brad pitt has dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.insult_victim", "base.popstra.public_insult.perpetrator"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.nationality", "people.person.place_of_birth"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.lived_with.participant", "base.popstra.celebrity.lived_with"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who brad pitt has dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.01640237309038639, -0.030535439029335976, -0.0502241887152195], "norm_scores": [0.3386719226837158, 0.33391913771629333, 0.32740896940231323]}}
{"id": "WebQTest-760", "question": "what party does barack obama belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"]], "ground_paths": [["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party does barack obama belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>"], "scores": [-0.01736941747367382, -0.01999291218817234, -0.02770944871008396], "norm_scores": [0.33477362990379333, 0.3338964879512787, 0.33132991194725037]}}
{"id": "WebQTest-761", "question": "what is san francisco zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is san francisco zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00023249766672961414, -0.08640416711568832, -0.16173462569713593], "norm_scores": [0.36123234033584595, 0.3314078152179718, 0.30735984444618225]}}
{"id": "WebQTest-762", "question": "who has stephen ireland played for", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has stephen ireland played for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.031947407871484756, -0.0321258008480072, -0.037290021777153015], "norm_scores": [0.3339463174343109, 0.3338867723941803, 0.3321669101715088]}}
{"id": "WebQTest-764", "question": "where did john mayer go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john mayer go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015168480575084686, -0.019200831651687622, -0.02106507308781147], "norm_scores": [0.33443737030029297, 0.33309149742126465, 0.3324711322784424]}}
{"id": "WebQTest-765", "question": "what celebrities has brody jenner dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["base.popstra.breakup.participant", "base.popstra.celebrity.breakup"], ["base.popstra.canoodled.participant", "base.popstra.celebrity.canoodled"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.breakup.participant", "base.popstra.breakup.participant"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what celebrities has brody jenner dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.canoodled.participant </PATH>"], "scores": [-0.01673884689807892, -0.027847109362483025, -0.038386594504117966], "norm_scores": [0.3369796574115753, 0.33325710892677307, 0.3297632038593292]}}
{"id": "WebQTest-766", "question": "what is malaysian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is malaysian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.006855990272015333, -0.02399473451077938, -0.12767955660820007], "norm_scores": [0.34852948784828186, 0.3426070213317871, 0.30886346101760864]}}
{"id": "WebQTest-768", "question": "who owns volkswagen car company", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization.board_members", "business.board_member.organization_board_memberships"]], "ground_paths": [["business.business_operation.industry", "business.business_operation.industry"], ["base.schemastaging.corresponding_entity.entities", "base.schemastaging.entity_hint.corresponds_to"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["organization.organization.parent", "organization.organization_relationship.parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns volkswagen car company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.board_members <SEP> business.board_member.organization_board_memberships </PATH>"], "scores": [-0.025474773719906807, -0.03043169528245926, -0.03934020921587944], "norm_scores": [0.3354257643222809, 0.33376720547676086, 0.33080703020095825]}}
{"id": "WebQTest-769", "question": "which is the main train station in rome", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["trains.travel_destination.tourist_attractions"]], "ground_paths": [["travel.travel_destination.how_to_get_here", "travel.transportation.transport_terminus"], ["location.location.containedby", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which is the main train station in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> trains.travel_destination.tourist_attractions </PATH>"], "scores": [-0.002919528866186738, -0.06397892534732819, -0.07236188650131226], "norm_scores": [0.3479856848716736, 0.32737359404563904, 0.324640691280365]}}
{"id": "WebQTest-770", "question": "what did steve mcnair died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did steve mcnair died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0009855712996795774, -0.05064363032579422, -0.11052919179201126], "norm_scores": [0.3511485159397125, 0.33413705229759216, 0.3147144317626953]}}
{"id": "WebQTest-771", "question": "who is the newly elected governor of california", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the newly elected governor of california [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>"], "scores": [-0.020867077633738518, -0.023149359971284866, -0.029585447162389755], "norm_scores": [0.33455562591552734, 0.33379292488098145, 0.331651508808136]}}
{"id": "WebQTest-772", "question": "who plays roxanne in ghost rider", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.film.language", "people.person.languages"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.film.starring", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays roxanne in ghost rider [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.film.language <SEP> people.person.languages </PATH>"], "scores": [-0.02100778929889202, -0.024327853694558144, -0.02908606268465519], "norm_scores": [0.3346003592014313, 0.33349135518074036, 0.331908255815506]}}
{"id": "WebQTest-773", "question": "where is the sony ericsson company", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the sony ericsson company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.00517308572307229, -0.033604174852371216, -0.045925915241241455], "norm_scores": [0.34105998277664185, 0.3314998149871826, 0.3274402320384979]}}
{"id": "WebQTest-774", "question": "what kind of currency does the dominican republic have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does the dominican republic have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-6.414750532712787e-05, -0.1085883229970932, -0.13422659039497375], "norm_scores": [0.36080172657966614, 0.3236958682537079, 0.3155023753643036]}}
{"id": "WebQTest-775", "question": "what to do this weekend in manchester with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do this weekend in manchester with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012516705319285393, -0.016412001103162766, -0.028916386887431145], "norm_scores": [0.3355877995491028, 0.33428311347961426, 0.33012911677360535]}}
{"id": "WebQTest-776", "question": "what sport is popular in italy", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team_location.teams"], ["cricket.cricket_team.country"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sport is popular in italy [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.010779590345919132, -0.01494564302265644, -0.12919867038726807], "norm_scores": [0.3467206656932831, 0.34527918696403503, 0.3080001771450043]}}
{"id": "WebQTest-777", "question": "where is the main headquarters of google", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["organization.organization.phone_number", "common.phone_number.service_location"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the main headquarters of google [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.015960607677698135, -0.025546055287122726, -0.03534314036369324], "norm_scores": [0.3365570604801178, 0.333346426486969, 0.3300965428352356]}}
{"id": "WebQTest-778", "question": "what was jesse owens education", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse owens education [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.012914113700389862, -0.01952807791531086, -0.03361927345395088], "norm_scores": [0.3363701105117798, 0.3341527283191681, 0.32947713136672974]}}
{"id": "WebQTest-779", "question": "where does carlos santana live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.children", "people.person.place_of_birth"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.country.first_level_divisions"], ["symbols.namesake.named_after", "music.artist.origin"], ["people.person.nationality", "location.location.containedby"], ["music.artist.origin", "location.location.containedby"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.children", "people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does carlos santana live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.019692933186888695, -0.022596973925828934, -0.028637520968914032], "norm_scores": [0.3346501290798187, 0.3336796760559082, 0.3316701650619507]}}
{"id": "WebQTest-780", "question": "what is the new movie john carter about", "prediction": [["film.film_story_contributor.film_story_credits"], ["film.film.subjects"], ["film.film.story_by"]], "ground_paths": [["film.film.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the new movie john carter about [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.02153576724231243, -0.02173161506652832, -0.032768648117780685], "norm_scores": [0.3346010148525238, 0.33453550934791565, 0.33086350560188293]}}
{"id": "WebQTest-782", "question": "what countries does nafta include", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["common.topic.notable_types"]], "ground_paths": [["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_member.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does nafta include [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.010898168198764324, -0.02586359530687332, -0.03793581947684288], "norm_scores": [0.3380124866962433, 0.33299165964126587, 0.32899585366249084]}}
{"id": "WebQTest-783", "question": "what county is houston tx in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is houston tx in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.005502650048583746, -0.03465421497821808, -0.044232286512851715], "norm_scores": [0.3409152925014496, 0.3311205506324768, 0.327964186668396]}}
{"id": "WebQTest-784", "question": "where do the seattle seahawks play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the seattle seahawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.0090051693841815, -0.01598113216459751, -0.08450694382190704], "norm_scores": [0.34242749214172363, 0.34004703164100647, 0.3175255060195923]}}
{"id": "WebQTest-785", "question": "who does michael keaton play in cars", "prediction": [["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.performance.film", "film.film_character.portrayed_in_films"], ["fictional_universe.fictional_universe.works_set_here", "fictional_universe.fictional_universe.characters"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael keaton play in cars [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.02835657075047493, -0.031556326895952225, -0.03312291204929352], "norm_scores": [0.33421897888183594, 0.333151251077652, 0.3326297700405121]}}
{"id": "WebQTest-787", "question": "where is the riviera casino", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the riviera casino [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.008966632187366486, -0.036618057638406754, -0.04627968743443489], "norm_scores": [0.34058764576911926, 0.33129891753196716, 0.32811346650123596]}}
{"id": "WebQTest-788", "question": "what is the holy book of the buddhist religion", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"], ["book.written_work.subjects"]], "ground_paths": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the holy book of the buddhist religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.004748037550598383, -0.026986543089151382, -0.06233491376042366], "norm_scores": [0.3422258496284485, 0.33469927310943604, 0.3230748772621155]}}
{"id": "WebQTest-789", "question": "who became president when henry harrison died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who became president when henry harrison died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.023115288466215134, -0.025281986221671104, -0.03203199431300163], "norm_scores": [0.3345646858215332, 0.33384057879447937, 0.3315947353839874]}}
{"id": "WebQTest-790", "question": "where did richard arkwright grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard arkwright grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0016589079750701785, -0.044841889292001724, -0.12697504460811615], "norm_scores": [0.3521183729171753, 0.3372364938259125, 0.31064513325691223]}}
{"id": "WebQTest-791", "question": "what other books did louis sachar write", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did louis sachar write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.01211883220821619, -0.02636706456542015, -0.02897544577717781], "norm_scores": [0.33679813146591187, 0.3320333957672119, 0.33116844296455383]}}
{"id": "WebQTest-792", "question": "when did roth ira originate", "prediction": [["finance.currency.countries_used"], ["common.topic.notable_types"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did roth ira originate [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.013817263767123222, -0.02985510043799877, -0.03957296907901764], "norm_scores": [0.33799049258232117, 0.332613080739975, 0.32939642667770386]}}
{"id": "WebQTest-794", "question": "who does the voice for darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>"], "scores": [-0.02336215041577816, -0.024138281121850014, -0.02814553864300251], "norm_scores": [0.3339509069919586, 0.33369180560112, 0.3323572874069214]}}
{"id": "WebQTest-796", "question": "what did thomas malthus do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas malthus do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0018244873499497771, -0.05587487295269966, -0.07033830136060715], "norm_scores": [0.3470818102359772, 0.3288199007511139, 0.32409825921058655]}}
{"id": "WebQTest-797", "question": "what is the zip code for newark california", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for newark california [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00013046254753135145, -0.09316146373748779, -0.1515093594789505], "norm_scores": [0.3609212636947632, 0.3288589417934418, 0.31021976470947266]}}
{"id": "WebQTest-798", "question": "what states share a border with tennessee", "prediction": [["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["time.event.locations", "location.location.events"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_children"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.location.containedby", "location.location.contains"], ["location.location.containedby", "location.location.containedby"], ["base.locations.countries.states_provinces_within", "location.country.first_level_divisions"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.locations.countries.states_provinces_within", "location.administrative_division.first_level_division_of"], ["location.location.containedby", "base.usnationalparks.us_national_park.state"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["book.written_work.subjects", "book.written_work.subjects"], ["location.location.partially_contains", "geography.river.origin"], ["location.location.partially_contains", "location.location.partially_containedby"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "time.event.locations"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_parent"], ["base.locations.countries.states_provinces_within", "base.locations.countries.states_provinces_within"], ["location.location.events", "time.event.locations"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["base.locations.countries.states_provinces_within", "location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states share a border with tennessee [/INST]", "raw_output": {"paths": ["<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03344208374619484, -0.033471349626779556, -0.040860794484615326], "norm_scores": [0.33415988087654114, 0.33415013551712036, 0.3316900134086609]}}
{"id": "WebQTest-800", "question": "what war did the us lose the most soldiers", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.military_conflict"], ["military.casualties.combatant", "military.casualties.military_conflict"]], "ground_paths": [["military.military_combatant.military_commanders", "military.military_command.military_conflict"], ["military.military_command.military_combatant", "military.military_command.military_conflict"], ["location.location.partially_containedby", "location.location.events"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_combatant.casualties", "military.casualties.military_conflict"], ["people.person.nationality", "military.military_person.participated_in_conflicts"], ["military.military_combatant.military_commanders", "military.military_conflict.commanders"], ["military.military_command.military_combatant", "military.military_conflict.commanders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what war did the us lose the most soldiers [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.military_conflict </PATH>", "<PATH> military.casualties.combatant <SEP> military.casualties.military_conflict </PATH>"], "scores": [-0.015095436945557594, -0.030293261632323265, -0.037674203515052795], "norm_scores": [0.337542325258255, 0.3324512243270874, 0.3300064504146576]}}
{"id": "WebQTest-801", "question": "what kind of government is egypt", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is egypt [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.006207228638231754, -0.02188733220100403, -0.12191890180110931], "norm_scores": [0.347804993391037, 0.3423939049243927, 0.3098010718822479]}}
{"id": "WebQTest-802", "question": "what musical instruments does justin bieber play", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played"], ["music.track_contribution.contributor", "music.track_contribution.role"]], "ground_paths": [["music.group_member.membership", "music.group_membership.role"], ["music.artist.track_contributions", "music.track_contribution.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what musical instruments does justin bieber play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>"], "scores": [-0.0036353357136249542, -0.04522654041647911, -0.055013660341501236], "norm_scores": [0.3437393307685852, 0.32973605394363403, 0.32652461528778076]}}
{"id": "WebQTest-805", "question": "where do john lennon die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do john lennon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0006559118046425283, -0.07858186215162277, -0.10944297909736633], "norm_scores": [0.3543643653392792, 0.3277986943721771, 0.3178369700908661]}}
{"id": "WebQTest-809", "question": "what is the current time in bangalore india", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in bangalore india [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-0.0001779156445991248, -0.07158602029085159, -0.1766444742679596], "norm_scores": [0.361100971698761, 0.336214542388916, 0.3026845157146454]}}
{"id": "WebQTest-810", "question": "what language do people from greece speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from greece speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01397499069571495, -0.014344223774969578, -0.04962163418531418], "norm_scores": [0.3373122811317444, 0.3371877372264862, 0.3255000114440918]}}
{"id": "WebQTest-811", "question": "where is the phillies spring training stadium", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["location.location.containedby"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the phillies spring training stadium [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.003468915354460478, -0.04637082293629646, -0.049908597022295], "norm_scores": [0.3433326184749603, 0.32891449332237244, 0.3277529180049896]}}
{"id": "WebQTest-812", "question": "what movies does matt dallas play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does matt dallas play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.012451011687517166, -0.03426322340965271, -0.037018436938524246], "norm_scores": [0.33850616216659546, 0.33120253682136536, 0.3302912712097168]}}
{"id": "WebQTest-813", "question": "who was john f kennedy 's family", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.parents", "people.person.children"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["film.film_subject.films", "film.film.subjects"], ["people.person.parents", "people.person.children"], ["people.family.members", "people.family_member.family"], ["people.family.members", "people.family.members"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.sibling_relationship.sibling", "people.person.sibling_s"], ["people.person.parents"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["people.person.profession", "people.person.profession"], ["people.person.profession", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["people.person.religion", "people.person.religion"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.gender", "people.person.gender"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["symbols.name_source.namesakes"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was john f kennedy 's family [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03270069509744644, -0.03275808319449425, -0.034417469054460526], "norm_scores": [0.33353039622306824, 0.33351126313209534, 0.33295831084251404]}}
{"id": "WebQTest-814", "question": "where is harley davidson corporate headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded"]], "ground_paths": [["organization.organization.place_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harley davidson corporate headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded </PATH>"], "scores": [-0.005136334802955389, -0.04085727781057358, -0.0433046892285347], "norm_scores": [0.3415929973125458, 0.3296063542366028, 0.32880067825317383]}}
{"id": "WebQTest-818", "question": "where was the ottoman empire based", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.country.capital"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the ottoman empire based [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.capital </PATH>"], "scores": [-0.00697559816762805, -0.04127009958028793, -0.06024106219410896], "norm_scores": [0.3431220054626465, 0.3315543234348297, 0.3253237009048462]}}
{"id": "WebQTest-819", "question": "where was bin laden found and killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was bin laden found and killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.020570622757077217, -0.03140702843666077, -0.03435094654560089], "norm_scores": [0.3360738754272461, 0.3324516713619232, 0.3314744234085083]}}
{"id": "WebQTest-820", "question": "what are the names of harry potter movies in order", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["fictional_universe.fictional_universe.works_set_here"], ["film.film_series.films_in_series"]], "ground_paths": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["film.film_series.films_in_series"], ["fictional_universe.fictional_universe.works_set_here"], ["film.film.film_series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of harry potter movies in order [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>", "<PATH> film.film_series.films_in_series </PATH>"], "scores": [-0.0066048698499798775, -0.03299441561102867, -0.0377652682363987], "norm_scores": [0.33975750207901, 0.33090871572494507, 0.32933375239372253]}}
{"id": "WebQTest-821", "question": "what did pamela courson die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did pamela courson die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.002355997683480382, -0.03714483976364136, -0.11452507972717285], "norm_scores": [0.34968671202659607, 0.3377307057380676, 0.3125825524330139]}}
{"id": "WebQTest-822", "question": "what did dr jack kevorkian do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr jack kevorkian do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0017840907676145434, -0.04907253012061119, -0.07177620381116867], "norm_scores": [0.3464747369289398, 0.33047181367874146, 0.3230534493923187]}}
{"id": "WebQTest-823", "question": "what does bolivia border", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["time.event.locations", "location.location.events"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["location.country.languages_spoken", "location.country.official_language"], ["location.location.partially_containedby", "location.location.partially_contains"], ["people.deceased_person.place_of_death", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["language.language_family.geographic_distribution", "language.human_language.main_country"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["symbols.name_source.namesakes", "people.person.nationality"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_containedby"], ["time.event.locations", "time.event.locations"], ["language.language_family.geographic_distribution", "location.country.official_language"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.contains", "location.location.contains"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["location.country.official_language", "location.country.languages_spoken"], ["location.location.contains", "location.location.containedby"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.country.official_language", "language.human_language.main_country"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does bolivia border [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03628017380833626, -0.043158214539289474, -0.04606248810887337], "norm_scores": [0.33518680930137634, 0.33288928866386414, 0.3319239020347595]}}
{"id": "WebQTest-825", "question": "who does nolan ryan play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does nolan ryan play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.008013402111828327, -0.017922913655638695, -0.04075785353779793], "norm_scores": [0.33807480335235596, 0.33474117517471313, 0.3271839916706085]}}
{"id": "WebQTest-827", "question": "what country does justin bieber come from", "prediction": [["people.person.nationality"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does justin bieber come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0040657492354512215, -0.04275372624397278, -0.05038490891456604], "norm_scores": [0.34284287691116333, 0.3298322856426239, 0.32732486724853516]}}
{"id": "WebQTest-828", "question": "what is the head judge of the supreme court called", "prediction": [["law.judicial_tenure.title", "law.judge.courts"], ["law.court.jurisdiction", "people.person.nationality"], ["law.court.jurisdiction", "government.government_office_or_title.office_holders"]], "ground_paths": [["government.governmental_body.offices_positions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the head judge of the supreme court called [/INST]", "raw_output": {"paths": ["<PATH> law.judicial_tenure.title <SEP> law.judge.courts </PATH>", "<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> law.court.jurisdiction <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.028558477759361267, -0.03401782363653183, -0.0430571511387825], "norm_scores": [0.33555230498313904, 0.3337253928184509, 0.3307223320007324]}}
{"id": "WebQTest-829", "question": "who was frida kahlo influenced by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was frida kahlo influenced by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.011475375853478909, -0.01228228397667408, -0.07342041283845901], "norm_scores": [0.34023699164390564, 0.33996254205703735, 0.3198004961013794]}}
{"id": "WebQTest-830", "question": "where is taylor swift from", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is taylor swift from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.015412218868732452, -0.021498821675777435, -0.03179628401994705], "norm_scores": [0.33583179116249084, 0.33379390835762024, 0.3303743302822113]}}
{"id": "WebQTest-831", "question": "what type of religion does argentina have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["religion.religion.countries"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion does argentina have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> religion.religion.countries </PATH>"], "scores": [-0.009161103516817093, -0.01404251717031002, -0.07490251213312149], "norm_scores": [0.3411218822002411, 0.3394607901573181, 0.3194173276424408]}}
{"id": "WebQTest-832", "question": "when is the last time the giants won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the giants won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.007362631615251303, -0.017218900844454765, -0.08122637867927551], "norm_scores": [0.34258419275283813, 0.3392241895198822, 0.31819161772727966]}}
{"id": "WebQTest-833", "question": "what is the main language used in china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language used in china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.006631582975387573, -0.030605334788560867, -0.04187889024615288], "norm_scores": [0.3399420380592346, 0.331889271736145, 0.32816869020462036]}}
{"id": "WebQTest-834", "question": "where did the battle of waterloo occur", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of waterloo occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0030862404964864254, -0.038077209144830704, -0.11473891884088516], "norm_scores": [0.3496541380882263, 0.3376309871673584, 0.3127148747444153]}}
{"id": "WebQTest-836", "question": "who did vladimir lenin marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did vladimir lenin marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.02522268518805504, -0.04423858970403671, -0.04448595270514488], "norm_scores": [0.33760005235671997, 0.3312409520149231, 0.3311590254306793]}}
{"id": "WebQTest-837", "question": "what year did the baltimore ravens win superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the baltimore ravens win superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.013352124951779842, -0.014686440117657185, -0.03229876235127449], "norm_scores": [0.33558189868927, 0.33513444662094116, 0.32928362488746643]}}
{"id": "WebQTest-838", "question": "what team does cristiano ronaldo play for 2011", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does cristiano ronaldo play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.019950849935412407, -0.022193480283021927, -0.0245098527520895], "norm_scores": [0.33408933877944946, 0.33334094285964966, 0.3325697183609009]}}
{"id": "WebQTest-841", "question": "what is money called in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is money called in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009981885552406311, -0.01760750450193882, -0.10241865366697311], "norm_scores": [0.344339519739151, 0.3417237102985382, 0.313936710357666]}}
{"id": "WebQTest-843", "question": "where did theodor schwann study", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann study [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.021133173257112503, -0.021667851135134697, -0.023907586932182312], "norm_scores": [0.3337009847164154, 0.33352261781692505, 0.33277642726898193]}}
{"id": "WebQTest-845", "question": "who are the virginia senators 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.government_position_held.governmental_body", "government.politician.government_positions_held"]], "ground_paths": [["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the virginia senators 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.016115786507725716, -0.02033941075205803, -0.03323723003268242], "norm_scores": [0.3357045650482178, 0.33428966999053955, 0.3300057649612427]}}
{"id": "WebQTest-846", "question": "what was dr seuss 's real name", "prediction": [["book.book.interior_illustrations_by", "people.person.nationality"], ["people.person.profession"], ["book.book_edition.author_editor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss 's real name [/INST]", "raw_output": {"paths": ["<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.0224122554063797, -0.03201749548316002, -0.036221977323293686], "norm_scores": [0.33593955636024475, 0.33272820711135864, 0.3313322067260742]}}
{"id": "WebQTest-847", "question": "what countries are members of the security council", "prediction": [["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_body_membership.member"]], "ground_paths": [["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_body_membership.member"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are members of the security council [/INST]", "raw_output": {"paths": ["<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body.members <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_body_membership.member </PATH>"], "scores": [-0.01037446316331625, -0.012870929203927517, -0.026091473177075386], "norm_scores": [0.3353552520275116, 0.3345191180706024, 0.3301256597042084]}}
{"id": "WebQTest-848", "question": "where are the nordic countries", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nordic countries [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.008704649284482002, -0.026168135926127434, -0.05323375016450882], "norm_scores": [0.34023603796958923, 0.33434590697288513, 0.32541802525520325]}}
{"id": "WebQTest-849", "question": "what country sponsored ferdinand magellan", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country sponsored ferdinand magellan [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0035901744849979877, -0.05738590285181999, -0.06765307486057281], "norm_scores": [0.346551775932312, 0.3284013569355011, 0.3250468671321869]}}
{"id": "WebQTest-850", "question": "what did vera ellen die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did vera ellen die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0023766474332660437, -0.03750386834144592, -0.1308058202266693], "norm_scores": [0.3514989912509918, 0.3393661677837372, 0.30913490056991577]}}
{"id": "WebQTest-851", "question": "what is the capital of modern day egypt", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern day egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.010417802259325981, -0.02472028136253357, -0.03873564675450325], "norm_scores": [0.33808013796806335, 0.33327916264533997, 0.3286406993865967]}}
{"id": "WebQTest-852", "question": "what province is toronto in ( 7 letters )", "prediction": [["location.province.capital"], ["base.biblioness.bibs_location.state"], ["base.biblioness.bibs_location.province"]], "ground_paths": [["location.province.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is toronto in ( 7 letters ) [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.biblioness.bibs_location.province </PATH>"], "scores": [-0.002837910782545805, -0.04265745356678963, -0.059351664036512375], "norm_scores": [0.34411367774009705, 0.3306804895401001, 0.32520586252212524]}}
{"id": "WebQTest-855", "question": "what is the currency of brazil now", "prediction": [["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.0012762353289872408, -0.05973434075713158, -0.06764017790555954], "norm_scores": [0.3473418653011322, 0.3276190161705017, 0.3250391185283661]}}
{"id": "WebQTest-856", "question": "what types of art did andy warhol do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what types of art did andy warhol do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.006766807287931442, -0.020267512649297714, -0.05928356200456619], "norm_scores": [0.3406657874584198, 0.33609744906425476, 0.32323676347732544]}}
{"id": "WebQTest-857", "question": "what other states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "ground_paths": [["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.containedby", "location.location.contains"], ["location.location.contains", "location.location.contains"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.025839053094387054, -0.036810144782066345, -0.038919419050216675], "norm_scores": [0.3360109329223633, 0.332344651222229, 0.3316444158554077]}}
{"id": "WebQTest-859", "question": "what are the predominant religions in russia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["language.human_language.main_country", "location.country.languages_spoken"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the predominant religions in russia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.00825947243720293, -0.012835321947932243, -0.10933443903923035], "norm_scores": [0.3449108600616455, 0.34333619475364685, 0.31175294518470764]}}
{"id": "WebQTest-861", "question": "who will coach carolina panthers in 2013", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will coach carolina panthers in 2013 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>"], "scores": [-0.004153246060013771, -0.03476015850901604, -0.03588264808058739], "norm_scores": [0.3402951657772064, 0.33003756403923035, 0.3296672999858856]}}
{"id": "WebQTest-862", "question": "where did louis sachar grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did louis sachar grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0043587712571024895, -0.028061088174581528, -0.10121030360460281], "norm_scores": [0.3467085361480713, 0.33858737349510193, 0.31470412015914917]}}
{"id": "WebQTest-865", "question": "what to do for fun in houston", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do for fun in houston [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013541202060878277, -0.01811136119067669, -0.0275003369897604], "norm_scores": [0.33539286255836487, 0.3338635563850403, 0.33074358105659485]}}
{"id": "WebQTest-866", "question": "where did otto frank go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did otto frank go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014330566860735416, -0.018239082768559456, -0.026921238750219345], "norm_scores": [0.3351669907569885, 0.33385956287384033, 0.33097347617149353]}}
{"id": "WebQTest-867", "question": "what else has jennifer lawrence played in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else has jennifer lawrence played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.02311793714761734, -0.03138776123523712, -0.032539065927267075], "norm_scores": [0.33530184626579285, 0.3325404226779938, 0.33215776085853577]}}
{"id": "WebQTest-868", "question": "who is the governor of pennsylvania state now", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of pennsylvania state now [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.02396727167069912, -0.024597417563199997, -0.025157392024993896], "norm_scores": [0.3335355818271637, 0.33332547545433044, 0.3331388831138611]}}
{"id": "WebQTest-869", "question": "where is ancient phoenician", "prediction": [["location.location.containedby"], ["language.human_language.countries_spoken_in"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ancient phoenician [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.023030105978250504, -0.02680879645049572, -0.030351802706718445], "norm_scores": [0.33456748723983765, 0.33330562710762024, 0.33212682604789734]}}
{"id": "WebQTest-870", "question": "where does the celtics practice", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the celtics practice [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.0021386451553553343, -0.03998677432537079, -0.14048664271831512], "norm_scores": [0.3529011309146881, 0.3397940695285797, 0.3073047399520874]}}
{"id": "WebQTest-872", "question": "where bin laden got killed", "prediction": [["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where bin laden got killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.013388672843575478, -0.02778894081711769, -0.04397173970937729], "norm_scores": [0.33834272623062134, 0.33350542187690735, 0.3281518220901489]}}
{"id": "WebQTest-873", "question": "where does toronto get its water from", "prediction": [["location.location.containedby"], ["base.infrastructure.sewage_treatment_plant.serves_location_s"], ["base.infrastructure.sewage_treatment_plant.serves_location_s", "base.infrastructure.sewage_treatment_plant.serves_location_s"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does toronto get its water from [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s <SEP> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>"], "scores": [-0.014725437387824059, -0.02486186847090721, -0.04056638851761818], "norm_scores": [0.33733585476875305, 0.3339337408542633, 0.32873043417930603]}}
{"id": "WebQTest-874", "question": "what language do people from bosnia speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from bosnia speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014737913385033607, -0.021019376814365387, -0.028491603210568428], "norm_scores": [0.335561603307724, 0.33346039056777954, 0.33097800612449646]}}
{"id": "WebQTest-875", "question": "where did hugo chavez go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hugo chavez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015532211400568485, -0.020697062835097313, -0.028328407555818558], "norm_scores": [0.33533036708831787, 0.3336029052734375, 0.331066757440567]}}
{"id": "WebQTest-876", "question": "who does japan import from", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.country.form_of_government", "location.country.form_of_government"], ["location.imports_and_exports.exported_to", "location.statistical_region.places_exported_to"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["base.aareas.schema.administrative_area.administrative_parent", "base.locations.countries.planet"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["film.film.featured_film_locations", "film.film.country"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["travel.travel_destination.tour_operators", "organization.organization.geographic_scope"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["fictional_universe.fictional_universe.locations", "fictional_universe.fictional_universe.locations"], ["cvg.computer_game_region.versions_released_in_this_region", "cvg.computer_game_region.versions_released_in_this_region"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.statistical_region.places_imported_from", "location.imports_and_exports.imported_from"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does japan import from [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.026760704815387726, -0.03590632975101471, -0.03798147290945053], "norm_scores": [0.33559995889663696, 0.3325446844100952, 0.33185529708862305]}}
{"id": "WebQTest-877", "question": "what is the name of sonia gandhi daughter", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of sonia gandhi daughter [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.008655390702188015, -0.018913045525550842, -0.11010868102312088], "norm_scores": [0.3456239402294159, 0.3420967757701874, 0.3122793138027191]}}
{"id": "WebQTest-878", "question": "who do islamic people pray to", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["base.argumentmaps.intellectual_dispute.positions"]], "ground_paths": [["religion.deity.deity_of"], ["religion.religion.deities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people pray to [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions </PATH>"], "scores": [-0.01298669446259737, -0.013867365196347237, -0.06421703845262527], "norm_scores": [0.339077353477478, 0.33877888321876526, 0.3221437633037567]}}
{"id": "WebQTest-879", "question": "where was mary todd lincoln from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mary todd lincoln from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.003638568799942732, -0.03255603462457657, -0.10549131035804749], "norm_scores": [0.34786733984947205, 0.3379519581794739, 0.31418076157569885]}}
{"id": "WebQTest-880", "question": "where did charles darwin die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0008638531435281038, -0.07093997299671173, -0.07477332651615143], "norm_scores": [0.3495185375213623, 0.32586410641670227, 0.3246173560619354]}}
{"id": "WebQTest-881", "question": "what movies does nick cannon play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["music.artist.track", "music.recording.releases"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does nick cannon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.016495773568749428, -0.03524789586663246, -0.035543475300073624], "norm_scores": [0.3375464677810669, 0.33127573132514954, 0.33117783069610596]}}
{"id": "WebQTest-882", "question": "when did the big earthquake hit san francisco", "prediction": [["time.event.locations"], ["location.location.events"], ["event.disaster.areas_affected"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the big earthquake hit san francisco [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>"], "scores": [-0.007124622818082571, -0.02906886115670204, -0.04884248599410057], "norm_scores": [0.3404330611228943, 0.3330438733100891, 0.3265230655670166]}}
{"id": "WebQTest-883", "question": "who was king george vi married to", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.spouse_s", "people.person.spouse_s"], ["film.film.subjects", "film.film_subject.films"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.family.members", "people.family.members"], ["people.person.religion", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king george vi married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.034568265080451965, -0.04010923206806183, -0.041029784828424454], "norm_scores": [0.33466824889183044, 0.33281898498535156, 0.3325127363204956]}}
{"id": "WebQTest-884", "question": "where was jesus after he died on the cross", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["symbols.name_source.namesakes"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was jesus after he died on the cross [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.008913828991353512, -0.028418615460395813, -0.07038670033216476], "norm_scores": [0.3423411548137665, 0.33572858572006226, 0.32193025946617126]}}
{"id": "WebQTest-885", "question": "who is sir james dyson", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir james dyson [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0006784655852243304, -0.06801575422286987, -0.10299625247716904], "norm_scores": [0.35240769386291504, 0.3294588327407837, 0.3181334435939789]}}
{"id": "WebQTest-886", "question": "who voiced darth vader in the original star wars movies", "prediction": [["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader in the original star wars movies [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.022281372919678688, -0.022507674992084503, -0.029479453340172768], "norm_scores": [0.33415743708610535, 0.33408182859420776, 0.3317607641220093]}}
{"id": "WebQTest-887", "question": "what high school did maya angelou go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what high school did maya angelou go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.016225364059209824, -0.02054787613451481, -0.022250069305300713], "norm_scores": [0.33448392152786255, 0.333041250705719, 0.33247482776641846]}}
{"id": "WebQTest-888", "question": "what 5 countries border switzerland", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["time.event.locations", "location.location.events"], ["location.country.form_of_government", "location.country.form_of_government"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.languages_spoken", "language.human_language.main_country"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["base.mystery.cryptid_classification.area_of_occurrence", "base.mystery.cryptid_observation_location.cryptid_s_occurring_here"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["location.location.events", "location.location.events"], ["location.country.official_language", "location.country.official_language"], ["geography.river.origin", "location.location.partially_contains"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_contains"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["geography.lake.basin_countries", "location.location.partially_contains"], ["film.film_location.featured_in_films", "film.film.featured_film_locations"], ["base.aareas.schema.administrative_area.subdivides_place"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["geography.lake.basin_countries", "location.location.partially_containedby"], ["location.location.partially_contains", "location.location.partially_contains"], ["book.written_work.subjects", "book.written_work.subjects"], ["location.location.containedby", "base.locations.countries.continent"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "time.event.locations"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["fictional_universe.work_of_fiction.setting", "fictional_universe.fictional_setting.works_set_here"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["film.film.country", "film.film.country"], ["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.is_reviewed"], ["location.location.contains", "location.location.contains"], ["location.location.contains", "location.location.containedby"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["people.person.nationality", "people.person.nationality"], ["location.location.containedby", "base.nations.supranational_body.member"], ["location.location.events", "time.event.locations"], ["location.location.contains", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 5 countries border switzerland [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03581692650914192, -0.039908986538648605, -0.046868566423654556], "norm_scores": [0.33501672744750977, 0.3336486220359802, 0.33133465051651]}}
{"id": "WebQTest-889", "question": "what country did benito mussolini govern", "prediction": [["people.person.nationality"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did benito mussolini govern [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.016125276684761047, -0.027079857885837555, -0.030499247834086418], "norm_scores": [0.33615320920944214, 0.3324908912181854, 0.3313559293746948]}}
{"id": "WebQTest-891", "question": "who was the leader of the first communist party of china", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization.founders", "influence.influence_node.influenced"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the first communist party of china [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.founders <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.0038173897191882133, -0.0293126180768013, -0.08390356600284576], "norm_scores": [0.34508177638053894, 0.3363950252532959, 0.3185231685638428]}}
{"id": "WebQTest-892", "question": "what did miles davis died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did miles davis died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.002172275213524699, -0.038983847945928574, -0.1262509524822235], "norm_scores": [0.35122621059417725, 0.33853211998939514, 0.3102417290210724]}}
{"id": "WebQTest-893", "question": "what is the zip code for azle texas", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for azle texas [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00013188015145715326, -0.09756826609373093, -0.13046656548976898], "norm_scores": [0.35907137393951416, 0.3257352113723755, 0.31519341468811035]}}
{"id": "WebQTest-894", "question": "what date did sally pearson won gold", "prediction": [["people.person.nationality", "olympics.olympic_participating_country.olympics_participated_in"], ["people.person.profession", "common.topic.notable_types"], ["people.person.nationality", "olympics.olympic_games.participating_countries"]], "ground_paths": [["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.olympics"], ["olympics.olympic_athlete_affiliation.athlete", "olympics.olympic_athlete_affiliation.olympics"], ["people.person.nationality", "olympics.olympic_games.participating_countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what date did sally pearson won gold [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.020184453576803207, -0.02769780531525612, -0.033087108284235], "norm_scores": [0.33560481667518616, 0.33309274911880493, 0.3313024342060089]}}
{"id": "WebQTest-895", "question": "when did the detroit pistons last win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the detroit pistons last win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.011059670709073544, -0.011570680886507034, -0.11434473842382431], "norm_scores": [0.34466609358787537, 0.3444899916648865, 0.31084394454956055]}}
{"id": "WebQTest-896", "question": "when did the philadelphia flyers win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the philadelphia flyers win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.009122264571487904, -0.014290022663772106, -0.10606348514556885], "norm_scores": [0.3445359468460083, 0.34276002645492554, 0.31270402669906616]}}
{"id": "WebQTest-898", "question": "what county is st paul nc in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul nc in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.01017965842038393, -0.02848474308848381, -0.0360281728208065], "norm_scores": [0.3382556140422821, 0.3321201503276825, 0.3296242356300354]}}
{"id": "WebQTest-899", "question": "where is olympic national park wa", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park wa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.00881660170853138, -0.030948448926210403, -0.03120690956711769], "norm_scores": [0.3382984399795532, 0.33089351654052734, 0.33080801367759705]}}
{"id": "WebQTest-900", "question": "where did nat turner 1831 rebellion occur", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did nat turner 1831 rebellion occur [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.012087763287127018, -0.014055353589355946, -0.058278683573007584], "norm_scores": [0.33865079283714294, 0.33798515796661377, 0.3233639895915985]}}
{"id": "WebQTest-901", "question": "who would play luke skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who would play luke skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.020749226212501526, -0.02167140133678913, -0.022998055443167686], "norm_scores": [0.3336857259273529, 0.3333781361579895, 0.33293616771698]}}
{"id": "WebQTest-903", "question": "where turkish people originate", "prediction": [["people.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "people.ethnicity.geographic_distribution"], ["people.ethnicity.included_in_group"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where turkish people originate [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>"], "scores": [-7.376981375273317e-05, -0.12650670111179352, -0.13429395854473114], "norm_scores": [0.36289331316947937, 0.3197936713695526, 0.3173130452632904]}}
{"id": "WebQTest-904", "question": "what state is barack obama from", "prediction": [["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["base.popstra.celebrity.vacations_in", "base.popstra.location.vacationers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is barack obama from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.state </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.01791687309741974, -0.023019997403025627, -0.04472833499312401], "norm_scores": [0.3368755877017975, 0.33516085147857666, 0.3279634714126587]}}
{"id": "WebQTest-905", "question": "what years did joe montana win super bowl", "prediction": [["sports.sports_championship_event.champion"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_championship_event.champion"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what years did joe montana win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_championship_event.champion </PATH>"], "scores": [-0.006061985623091459, -0.02921384759247303, -0.04868445172905922], "norm_scores": [0.34067055583000183, 0.33287400007247925, 0.3264554440975189]}}
{"id": "WebQTest-908", "question": "what awards gary paulsen won", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_winner.awards_won", "award.award_category.winners"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_honor.award_winner", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards gary paulsen won [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.020522113889455795, -0.02446872368454933, -0.02487858757376671], "norm_scores": [0.33425652980804443, 0.33293992280960083, 0.33280351758003235]}}
{"id": "WebQTest-910", "question": "where did alexander graham bell die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alexander graham bell die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.000391536857932806, -0.09057021141052246, -0.09427350759506226], "norm_scores": [0.35408779978752136, 0.3235540986061096, 0.32235807180404663]}}
{"id": "WebQTest-911", "question": "what year did the golden state warriors win their first nba championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the golden state warriors win their first nba championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.010641534812748432, -0.011326324194669724, -0.1016014963388443], "norm_scores": [0.3433629870414734, 0.34312793612480164, 0.3135090470314026]}}
{"id": "WebQTest-912", "question": "what currency should i take to turkey", "prediction": [["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.016768762841820717, -0.017121735960245132, -0.04141169413924217], "norm_scores": [0.3360999524593353, 0.3359813392162323, 0.3279187083244324]}}
{"id": "WebQTest-914", "question": "what countries are in the uk yahoo", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the uk yahoo [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.020538121461868286, -0.025963367894291878, -0.02800878696143627], "norm_scores": [0.3347676694393158, 0.33295637369155884, 0.33227601647377014]}}
{"id": "WebQTest-915", "question": "what superbowl did the baltimore ravens win", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what superbowl did the baltimore ravens win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.011439182795584202, -0.013678299263119698, -0.046171270310878754], "norm_scores": [0.33742430806159973, 0.3366696536540985, 0.32590606808662415]}}
{"id": "WebQTest-916", "question": "what is the language of russia s spoken", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the language of russia s spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.019276229664683342, -0.01982332207262516, -0.020325621590018272], "norm_scores": [0.33351072669029236, 0.33332833647727966, 0.333160936832428]}}
{"id": "WebQTest-918", "question": "who is the king of cambodia now", "prediction": [["people.person.nationality"], ["royalty.monarch.kingdom"], ["royalty.kingdom.rulers"]], "ground_paths": [["royalty.kingdom.rulers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the king of cambodia now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> royalty.kingdom.rulers </PATH>"], "scores": [-0.016208305954933167, -0.01686255633831024, -0.03710119053721428], "norm_scores": [0.33572033047676086, 0.3355007767677307, 0.3287789225578308]}}
{"id": "WebQTest-919", "question": "who are serena williams parents", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.profession"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are serena williams parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.008727061562240124, -0.021343177184462547, -0.1584215760231018], "norm_scores": [0.3510701060295105, 0.3466687798500061, 0.3022611439228058]}}
{"id": "WebQTest-920", "question": "what schools did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.010779867880046368, -0.02303708717226982, -0.028626466169953346], "norm_scores": [0.3366856873035431, 0.33258405327796936, 0.33073028922080994]}}
{"id": "WebQTest-921", "question": "what was the cause of death for huell howser", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for huell howser [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0010971054434776306, -0.049112845212221146, -0.09223589301109314], "norm_scores": [0.348917156457901, 0.3325595259666443, 0.3185233473777771]}}
{"id": "WebQTest-922", "question": "what books are written by suzanne collins", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.author.book_editions_published"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.book_editions_published"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books are written by suzanne collins [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.book_editions_published </PATH>"], "scores": [-0.014005915261805058, -0.02112865075469017, -0.03933838754892349], "norm_scores": [0.33693990111351013, 0.3345485031604767, 0.3285115957260132]}}
{"id": "WebQTest-923", "question": "what countries does england border", "prediction": [["location.location.events", "location.location.events"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"]], "ground_paths": [["base.biblioness.bibs_location.country", "base.aareas.schema.administrative_area.administrative_children"], ["time.event.locations", "location.location.events"], ["location.country.form_of_government", "location.country.form_of_government"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["law.court_jurisdiction_area.courts", "law.court_jurisdiction_area.courts"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["sports.sports_team_location.teams", "cricket.cricket_team.country"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["government.government_agency.jurisdiction", "government.governmental_jurisdiction.agencies"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["cricket.cricket_tournament_event.host", "time.event.locations"], ["periodicals.newspaper_circulation_area.newspapers", "periodicals.newspaper_circulation_area.newspapers"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["government.government_agency.jurisdiction", "government.government_agency.jurisdiction"], ["location.country.languages_spoken", "location.country.official_language"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["cricket.cricket_tournament_event.host", "location.location.events"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["law.court_jurisdiction_area.courts", "law.court.jurisdiction"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "time.event.locations"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["finance.currency.countries_used", "location.country.currency_used"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization.geographic_scope", "common.topic.subject_of"], ["location.country.internet_tld", "location.country.internet_tld"], ["people.person.nationality", "people.person.nationality"], ["location.location.events", "time.event.locations"], ["government.governmental_jurisdiction.government_bodies", "government.governmental_jurisdiction.government_bodies"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does england border [/INST]", "raw_output": {"paths": ["<PATH> location.location.events <SEP> location.location.events </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.04272563010454178, -0.04506503790616989, -0.04562796279788017], "norm_scores": [0.33391597867012024, 0.3331357538700104, 0.332948237657547]}}
{"id": "WebQTest-924", "question": "what team did messi play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did messi play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.02485872618854046, -0.024866020306944847, -0.038313884288072586], "norm_scores": [0.3348258137702942, 0.3348233699798584, 0.3303508460521698]}}
{"id": "WebQTest-925", "question": "what time zone is greensburg indiana", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is greensburg indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-2.255078470625449e-05, -0.15072685480117798, -0.16115085780620575], "norm_scores": [0.36882883310317993, 0.3172304034233093, 0.31394076347351074]}}
{"id": "WebQTest-927", "question": "who plays voldemort", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays voldemort [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02190912514925003, -0.022743066772818565, -0.023887407034635544], "norm_scores": [0.3336458206176758, 0.3333677053451538, 0.332986444234848]}}
{"id": "WebQTest-928", "question": "what colleges did harper lee attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did harper lee attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014029035344719887, -0.023616855964064598, -0.02897055074572563], "norm_scores": [0.33606356382369995, 0.3328568637371063, 0.3310796022415161]}}
{"id": "WebQTest-929", "question": "who founded the kkk", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["base.activism.activist_organization.founders"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the kkk [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activist_organization.founders </PATH>"], "scores": [-0.010038549080491066, -0.01807458885014057, -0.09761103987693787], "norm_scores": [0.3438613712787628, 0.34110915660858154, 0.31502944231033325]}}
{"id": "WebQTest-931", "question": "what jobs did john adams have before he was president", "prediction": [["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["tv.tv_program.country_of_origin", "government.government_office_or_title.jurisdiction"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["common.topic.image", "common.topic.image"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did john adams have before he was president [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.02229437045753002, -0.023551588878035545, -0.023960979655385017], "norm_scores": [0.33365827798843384, 0.33323904871940613, 0.33310264348983765]}}
{"id": "WebQTest-932", "question": "what year did super mario bros 2 come out", "prediction": [["film.film.sequel"], ["film.film.country"], ["common.topic.notable_types"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did super mario bros 2 come out [/INST]", "raw_output": {"paths": ["<PATH> film.film.sequel </PATH>", "<PATH> film.film.country </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.02253972366452217, -0.036225561052560806, -0.046406447887420654], "norm_scores": [0.3375158905982971, 0.3329281806945801, 0.329555869102478]}}
{"id": "WebQTest-933", "question": "what political party did andrew johnson belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.us_president.vice_president", "organization.organization.founders"]], "ground_paths": [["government.us_president.vice_president", "organization.organization.founders"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party did andrew johnson belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.us_president.vice_president <SEP> organization.organization.founders </PATH>"], "scores": [-0.016202805563807487, -0.02559499815106392, -0.026496315374970436], "norm_scores": [0.33552420139312744, 0.33238762617111206, 0.3320882022380829]}}
{"id": "WebQTest-935", "question": "where was osama bin laden killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was osama bin laden killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.016683217138051987, -0.024744989350438118, -0.035934608429670334], "norm_scores": [0.336371511220932, 0.3336706757545471, 0.32995784282684326]}}
{"id": "WebQTest-936", "question": "where were greek temples built", "prediction": [["location.location.containedby"], ["location.location.contains"], ["architecture.building.building_function"]], "ground_paths": [["architecture.building.building_function", "location.location.containedby"], ["architecture.building.building_function", "travel.tourist_attraction.near_travel_destination"], ["architecture.building_function.buildings", "location.location.contains"], ["architecture.building_function.buildings", "location.location.containedby"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were greek temples built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> architecture.building.building_function </PATH>"], "scores": [-0.006339575629681349, -0.04707368090748787, -0.07724425941705704], "norm_scores": [0.34582510590553284, 0.3320212662220001, 0.32215359807014465]}}
{"id": "WebQTest-937", "question": "where are people who speak arabic from", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are people who speak arabic from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01634090393781662, -0.023417731747031212, -0.03542691096663475], "norm_scores": [0.3362426161766052, 0.3338714838027954, 0.32988592982292175]}}
{"id": "WebQTest-938", "question": "who was queen elizabeth ii mom", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen elizabeth ii mom [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.005261866841465235, -0.02943076379597187, -0.12742196023464203], "norm_scores": [0.34951257705688477, 0.34116649627685547, 0.30932098627090454]}}
{"id": "WebQTest-939", "question": "what type of art did pablo picasso paint", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did pablo picasso paint [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.001721868640743196, -0.04025891795754433, -0.07158327847719193], "norm_scores": [0.34545665979385376, 0.3323970437049866, 0.32214629650115967]}}
{"id": "WebQTest-941", "question": "what is ella fitzgerald parents name", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ella fitzgerald parents name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.010722528211772442, -0.016826225444674492, -0.1329883635044098], "norm_scores": [0.34736356139183044, 0.34524983167648315, 0.307386577129364]}}
{"id": "WebQTest-942", "question": "who is anne hathaway parents", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is anne hathaway parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.014086094684898853, -0.014116400852799416, -0.15235038101673126], "norm_scores": [0.3483303189277649, 0.34831976890563965, 0.30334991216659546]}}
{"id": "WebQTest-943", "question": "who does don draper gets engaged to", "prediction": [["fictional_universe.fictional_character.character_created_by", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created", "fictional_universe.fictional_character.character_created_by"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does don draper gets engaged to [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>"], "scores": [-0.02240661531686783, -0.02579161338508129, -0.03371754661202431], "norm_scores": [0.3349664509296417, 0.33383452892303467, 0.331199049949646]}}
{"id": "WebQTest-944", "question": "what was van halen first album", "prediction": [["music.artist.album"], ["music.album.artist"], ["music.recording.artist"]], "ground_paths": [["music.album.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was van halen first album [/INST]", "raw_output": {"paths": ["<PATH> music.artist.album </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.021668272092938423, -0.025112541392445564, -0.030948296189308167], "norm_scores": [0.33474767208099365, 0.33359670639038086, 0.3316555619239807]}}
{"id": "WebQTest-945", "question": "what political system did spain have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political system did spain have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.004360673483461142, -0.026762954890727997, -0.14464983344078064], "norm_scores": [0.35125264525413513, 0.3434712886810303, 0.305276095867157]}}
{"id": "WebQTest-948", "question": "what are the supreme court cases", "prediction": [["law.court.jurisdiction", "government.governmental_jurisdiction.government_bodies"], ["law.court.jurisdiction", "book.written_work.subjects"], ["law.court.jurisdiction", "government.governmental_body.jurisdiction"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the supreme court cases [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> law.court.jurisdiction <SEP> book.written_work.subjects </PATH>", "<PATH> law.court.jurisdiction <SEP> government.governmental_body.jurisdiction </PATH>"], "scores": [-0.021389713510870934, -0.03324604034423828, -0.03924471512436867], "norm_scores": [0.33664172887802124, 0.33267393708229065, 0.3306843042373657]}}
{"id": "WebQTest-949", "question": "what is penn state 's main campus", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is penn state 's main campus [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>"], "scores": [-0.00317426398396492, -0.04839187115430832, -0.05156266316771507], "norm_scores": [0.34381356835365295, 0.32861340045928955, 0.3275730609893799]}}
{"id": "WebQTest-952", "question": "what country does alaska belong to", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does alaska belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.016043368726968765, -0.033642739057540894, -0.038329578936100006], "norm_scores": [0.33777907490730286, 0.3318863809108734, 0.33033454418182373]}}
{"id": "WebQTest-953", "question": "where is kansas city", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is kansas city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.00596894696354866, -0.05043266713619232, -0.056062668561935425], "norm_scores": [0.3439202904701233, 0.3289633095264435, 0.3271164298057556]}}
{"id": "WebQTest-954", "question": "what was pink floyd 's first album", "prediction": [["music.recording.artist"], ["music.artist.album"], ["music.artist.genre", "music.album.genre"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was pink floyd 's first album [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.artist.genre <SEP> music.album.genre </PATH>"], "scores": [-0.027507837861776352, -0.03512870520353317, -0.04249768704175949], "norm_scores": [0.3358488380908966, 0.33329910039901733, 0.33085206151008606]}}
{"id": "WebQTest-955", "question": "who did granville t. woods married", "prediction": [["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did granville t. woods married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03117719106376171, -0.037382081151008606, -0.03875816613435745], "norm_scores": [0.33486682176589966, 0.3327954411506653, 0.33233779668807983]}}
{"id": "WebQTest-957", "question": "what language did the sioux indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the sioux indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0007391876424662769, -0.07962402701377869, -0.07972440868616104], "norm_scores": [0.3510989546775818, 0.3244667947292328, 0.3244342505931854]}}
{"id": "WebQTest-958", "question": "what are some famous people from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some famous people from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.002712611574679613, -0.04386424273252487, -0.07512639462947845], "norm_scores": [0.34604117274284363, 0.33209002017974854, 0.32186877727508545]}}
{"id": "WebQTest-959", "question": "who did brett gardner play for in 2008", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did brett gardner play for in 2008 [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.009720763191580772, -0.016472896561026573, -0.03166884556412697], "norm_scores": [0.3365233838558197, 0.3342587649822235, 0.3292178213596344]}}
{"id": "WebQTest-960", "question": "who plays dwight in the office", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_series_season.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_series_season.episodes", "tv.tv_director.episodes_directed"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays dwight in the office [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.015707794576883316, -0.037263792008161545, -0.040225397795438766], "norm_scores": [0.3384718894958496, 0.33125388622283936, 0.3302742838859558]}}
{"id": "WebQTest-962", "question": "what county is san diego ca", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.location.containedby"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego ca [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.016256200149655342, -0.02417438104748726, -0.03036876954138279], "norm_scores": [0.3357846140861511, 0.33313629031181335, 0.3310791254043579]}}
{"id": "WebQTest-963", "question": "who is the miami dolphins quarterback", "prediction": [["american_football.football_player.position_s"], ["american_football.game_rushing_statistics.team", "american_football.game_rushing_statistics.player"], ["american_football.football_team.current_head_coach"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the miami dolphins quarterback [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.game_rushing_statistics.team <SEP> american_football.game_rushing_statistics.player </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>"], "scores": [-0.00782766193151474, -0.037034206092357635, -0.05056869983673096], "norm_scores": [0.34136971831321716, 0.3315436840057373, 0.3270866572856903]}}
{"id": "WebQTest-965", "question": "where did anne frank go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["book.book_subject.works", "book.written_work.subjects"], ["book.book_subject.works", "book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did anne frank go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>"], "scores": [-0.021142151206731796, -0.023943372070789337, -0.02880728803575039], "norm_scores": [0.3344966173171997, 0.3335609436035156, 0.33194246888160706]}}
{"id": "WebQTest-966", "question": "what basketball player is married to monica", "prediction": [["people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.gender", "people.person.gender"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball player is married to monica [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.033169377595186234, -0.04056844860315323, -0.04450364038348198], "norm_scores": [0.33541759848594666, 0.3329450190067291, 0.33163735270500183]}}
{"id": "WebQTest-967", "question": "what did hitler use to kill himself", "prediction": [["people.deceased_person.cause_of_death"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["base.activism.activist.area_of_activism", "base.activism.activism_issue.activists"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hitler use to kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activism_issue.activists </PATH>"], "scores": [-0.008470049127936363, -0.029814932495355606, -0.046701718121767044], "norm_scores": [0.3399774730205536, 0.3327975869178772, 0.3272249102592468]}}
{"id": "WebQTest-968", "question": "what do people speak in germany", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in germany [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013314313255250454, -0.01986239105463028, -0.03647047281265259], "norm_scores": [0.33663424849510193, 0.33443713188171387, 0.3289286196231842]}}
{"id": "WebQTest-969", "question": "when is the last time the denver broncos won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the denver broncos won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.015445361845195293, -0.015853283926844597, -0.023891758173704147], "norm_scores": [0.33431607484817505, 0.33417972922325134, 0.3315041959285736]}}
{"id": "WebQTest-970", "question": "where was caesar when he was stabbed", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was caesar when he was stabbed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0016195380594581366, -0.06898606568574905, -0.07001945376396179], "norm_scores": [0.3485851585865021, 0.3258756995201111, 0.32553911209106445]}}
{"id": "WebQTest-971", "question": "what style of music did bessie smith perform", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.written_work.author"]], "ground_paths": [["book.book_subject.works"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music did bessie smith perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.0005622906028293073, -0.07235248386859894, -0.10552964359521866], "norm_scores": [0.3532221019268036, 0.3287530243396759, 0.3180249035358429]}}
{"id": "WebQTest-972", "question": "what team did jackie robinson", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did jackie robinson [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014876171946525574, -0.0173341017216444, -0.02893327735364437], "norm_scores": [0.3351671099662781, 0.3343442976474762, 0.3304885923862457]}}
{"id": "WebQTest-973", "question": "what time zone is chicago in right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is chicago in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-8.794242603471503e-06, -0.16459287703037262, -0.1851940006017685], "norm_scores": [0.3732461929321289, 0.31660476326942444, 0.31014907360076904]}}
{"id": "WebQTest-975", "question": "who was the vice president under ronald reagan", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the vice president under ronald reagan [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.006383008789271116, -0.02006777562201023, -0.08052409440279007], "norm_scores": [0.34305915236473083, 0.33839645981788635, 0.3185443580150604]}}
{"id": "WebQTest-976", "question": "where do the chicago white sox play home games", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the chicago white sox play home games [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.0005531738279387355, -0.05989474430680275, -0.10925208032131195], "norm_scores": [0.3521888852119446, 0.33189746737480164, 0.315913587808609]}}
{"id": "WebQTest-977", "question": "what part did jeff conaway play in grease", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.performance.actor", "film.performance.character"], ["theater.theater_character.plays_appears_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did jeff conaway play in grease [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.026525849476456642, -0.03199666738510132, -0.03538143262267113], "norm_scores": [0.3349267244338989, 0.3330993950366974, 0.3319738507270813]}}
{"id": "WebQTest-978", "question": "what book did benjamin franklin published", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.book_editions_published"]], "ground_paths": [["book.author.book_editions_published"], ["people.person.profession", "common.topic.subject_of"], ["book.book_edition.author_editor", "book.book.genre"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did benjamin franklin published [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.book_editions_published </PATH>"], "scores": [-0.011776517145335674, -0.029133621603250504, -0.03333725035190582], "norm_scores": [0.3376709818840027, 0.33186057209968567, 0.33046844601631165]}}
{"id": "WebQTest-979", "question": "what currency to take to barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency to take to barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.010640074498951435, -0.015980631113052368, -0.1251882016658783], "norm_scores": [0.3464471995830536, 0.34460192918777466, 0.30895090103149414]}}
{"id": "WebQTest-980", "question": "where did they get the name stanley cup", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["sports.sports_championship_event.championship"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they get the name stanley cup [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.013218424282968044, -0.0227976031601429, -0.05776410922408104], "norm_scores": [0.33933982253074646, 0.33610475063323975, 0.3245554268360138]}}
{"id": "WebQTest-982", "question": "what is the zip code of syracuse ny", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code of syracuse ny [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0002740357886068523, -0.07987982034683228, -0.1358349621295929], "norm_scores": [0.35756346583366394, 0.3302028179168701, 0.31223371624946594]}}
{"id": "WebQTest-983", "question": "where did tennessee williams go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tennessee williams go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.013743576593697071, -0.020748645067214966, -0.021470563486218452], "norm_scores": [0.33497220277786255, 0.332633912563324, 0.3323938548564911]}}
{"id": "WebQTest-984", "question": "what is the name of airport in new york", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of airport in new york [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.009647197090089321, -0.026810333132743835, -0.031624190509319305], "norm_scores": [0.3376956880092621, 0.3319492042064667, 0.33035510778427124]}}
{"id": "WebQTest-985", "question": "who was queen victoria 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["base.crime.crime_victim.crime", "base.crime.crime_victim.crime"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.parents"], ["royalty.kingdom.rulers", "people.person.nationality"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen victoria 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03695835918188095, -0.03852829337120056, -0.0405745655298233], "norm_scores": [0.3339097201824188, 0.3333859145641327, 0.33270442485809326]}}
{"id": "WebQTest-986", "question": "where did benjamin franklin work", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employer.employees"], ["people.person.profession"]], "ground_paths": [["people.person.employment_history", "business.employer.employees"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.nationality", "government.government_agency.jurisdiction"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin work [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.017085915431380272, -0.02776559256017208, -0.04323471710085869], "norm_scores": [0.3374311923980713, 0.33384668827056885, 0.32872211933135986]}}
{"id": "WebQTest-987", "question": "what does barbara bush do for work", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.person.children"]], "ground_paths": [["government.politician.government_positions_held", "government.government_office_category.officeholders"], ["business.employment_tenure.person", "business.job_title.people_with_this_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does barbara bush do for work [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.002022282686084509, -0.06666673719882965, -0.07576815783977509], "norm_scores": [0.3488808274269104, 0.3270411193370819, 0.3240780830383301]}}
{"id": "WebQTest-988", "question": "who is jacob latimore", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jacob latimore [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.0003943931660614908, -0.07738801091909409, -0.14433863759040833], "norm_scores": [0.3581877648830414, 0.33164453506469727, 0.31016770005226135]}}
{"id": "WebQTest-989", "question": "what happened to dunkirk during ww2", "prediction": [["time.event.locations"], ["military.military_conflict.casualties", "military.casualties.event"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to dunkirk during ww2 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.030169714242219925, -0.03222453221678734, -0.03859977424144745], "norm_scores": [0.33449822664260864, 0.33381161093711853, 0.3316902220249176]}}
{"id": "WebQTest-990", "question": "what was scottie pippen good at", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"]], "ground_paths": [["basketball.basketball_player.position_s", "sports.sport.positions"], ["common.topic.notable_types", "type.type.domain"], ["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen good at [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>"], "scores": [-0.016992591321468353, -0.02901587076485157, -0.03221260383725166], "norm_scores": [0.3363669216632843, 0.3323468863964081, 0.33128616213798523]}}
{"id": "WebQTest-991", "question": "who played donna on west wing", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played donna on west wing [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.022219393402338028, -0.03569725528359413, -0.042996276170015335], "norm_scores": [0.3371487259864807, 0.33263516426086426, 0.33021607995033264]}}
{"id": "WebQTest-993", "question": "who played meg in season 1 of family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played meg in season 1 of family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.022427918389439583, -0.030455896630883217, -0.036504361778497696], "norm_scores": [0.3357928693294525, 0.33310791850090027, 0.3310992121696472]}}
{"id": "WebQTest-994", "question": "when was the last time the dallas cowboys won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dallas cowboys won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.006099556107074022, -0.025589637458324432, -0.03836938366293907], "norm_scores": [0.33910444378852844, 0.3325592875480652, 0.32833629846572876]}}
{"id": "WebQTest-996", "question": "where do ireland play rugby union", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_team_location.teams"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do ireland play rugby union [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.005133059807121754, -0.043077629059553146, -0.04803379997611046], "norm_scores": [0.34237512946128845, 0.32962727546691895, 0.3279975950717926]}}
{"id": "WebQTest-997", "question": "who killed lee harvey oswald shot", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.gender", "people.person.gender"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who killed lee harvey oswald shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.010592405684292316, -0.026518207043409348, -0.04375295713543892], "norm_scores": [0.33880120515823364, 0.33344823122024536, 0.3277505934238434]}}
{"id": "WebQTest-998", "question": "when was the last time knicks won the championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time knicks won the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.008724063634872437, -0.015261075459420681, -0.1143125593662262], "norm_scores": [0.3456285893917084, 0.34337660670280457, 0.31099483370780945]}}
{"id": "WebQTest-999", "question": "when did celtics win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did celtics win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.011914988048374653, -0.01226572971791029, -0.1489545851945877], "norm_scores": [0.3482397198677063, 0.3481176197528839, 0.3036426603794098]}}
{"id": "WebQTest-1000", "question": "what countries included in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.00635821046307683, -0.037869296967983246, -0.05659034848213196], "norm_scores": [0.3424670994281769, 0.3318438231945038, 0.3256891369819641]}}
{"id": "WebQTest-1001", "question": "what country is nicki minaj from", "prediction": [["people.person.nationality"], ["music.artist.origin"], ["music.recording.artist"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nicki minaj from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.0062815947458148, -0.0398409329354763, -0.05853209272027016], "norm_scores": [0.3429250717163086, 0.33160772919654846, 0.32546716928482056]}}
{"id": "WebQTest-1002", "question": "who played harry potter dumbledore actor", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played harry potter dumbledore actor [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.016381822526454926, -0.023866727948188782, -0.0240646842867136], "norm_scores": [0.3350207805633545, 0.33252254128456116, 0.33245670795440674]}}
{"id": "WebQTest-1003", "question": "who was the prime minister of pakistan in 1999", "prediction": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the prime minister of pakistan in 1999 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.00992086622864008, -0.01619180291891098, -0.020763510838150978], "norm_scores": [0.33523696660995483, 0.3331413269042969, 0.3316217362880707]}}
{"id": "WebQTest-1004", "question": "what movies did christopher plummer play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["music.artist.album", "film.film.soundtrack"], ["film.performance.actor"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["music.recording.artist", "music.recording.releases"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.producer.films_executive_produced"], ["film.actor.dubbing_performances", "film.film.dubbing_performances"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did christopher plummer play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.01571844518184662, -0.03394126147031784, -0.03574701026082039], "norm_scores": [0.3375968635082245, 0.3315006196498871, 0.3309025466442108]}}
{"id": "WebQTest-1005", "question": "what is capital city of morocco", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is capital city of morocco [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.006167918909341097, -0.03450958803296089, -0.04241468012332916], "norm_scores": [0.34054625034332275, 0.33103010058403015, 0.3284236192703247]}}
{"id": "WebQTest-1006", "question": "what movies is omar epps in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies is omar epps in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.01467208657413721, -0.03034198097884655, -0.03654502332210541], "norm_scores": [0.3375166654586792, 0.3322690427303314, 0.33021432161331177]}}
{"id": "WebQTest-1007", "question": "what district does nancy pelosi represent", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["government.politician.government_positions_held", "government.political_district.representatives"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["base.government2.elected_government_positions_held.officeholder", "base.government2.elected_government_positions_held.district_represented"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what district does nancy pelosi represent [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.022156305611133575, -0.025008345022797585, -0.03488985449075699], "norm_scores": [0.3350645899772644, 0.3341103196144104, 0.3308250904083252]}}
{"id": "WebQTest-1008", "question": "where is walton indiana", "prediction": [["location.location.containedby"], ["location.hud_county_place.county"], ["location.location.contains"]], "ground_paths": [["location.us_county.hud_county_place"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.005543653853237629, -0.046285320073366165, -0.05069626495242119], "norm_scores": [0.3429439067840576, 0.32925260066986084, 0.32780346274375916]}}
{"id": "WebQTest-1009", "question": "where did george herbert walker bush go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["base.yalebase.person.residential_college", "education.educational_institution.parent_institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george herbert walker bush go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015593555755913258, -0.01769811473786831, -0.01905396394431591], "norm_scores": [0.3339518904685974, 0.3332498371601105, 0.3327982723712921]}}
{"id": "WebQTest-1010", "question": "what did bruce jenner win gold medal for", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["common.topic.image", "common.image.appears_in_topic_gallery"]], "ground_paths": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bruce jenner win gold medal for [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>"], "scores": [-0.0072920252569019794, -0.01985151134431362, -0.04446027800440788], "norm_scores": [0.33886438608169556, 0.33463501930236816, 0.3265005946159363]}}
{"id": "WebQTest-1011", "question": "where do they speak tibetan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak tibetan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.013336359523236752, -0.033902909606695175, -0.035567570477724075], "norm_scores": [0.3381054401397705, 0.3312227725982666, 0.33067184686660767]}}
{"id": "WebQTest-1012", "question": "what to do in hollywood ca this weekend", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in hollywood ca this weekend [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.015069890767335892, -0.015326919965445995, -0.028192758560180664], "norm_scores": [0.3348170518875122, 0.33473098278045654, 0.33045199513435364]}}
{"id": "WebQTest-1014", "question": "where did kim kardashian come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim kardashian come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.003998815082013607, -0.039019014686346054, -0.04531184956431389], "norm_scores": [0.34186694025993347, 0.3301019072532654, 0.32803115248680115]}}
{"id": "WebQTest-1015", "question": "what town did justin bieber grew up in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["people.person.nationality"], ["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town did justin bieber grew up in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.013283533044159412, -0.02598571591079235, -0.033221568912267685], "norm_scores": [0.3369683623313904, 0.3327152132987976, 0.330316424369812]}}
{"id": "WebQTest-1017", "question": "who plays charlie in the santa clause movies", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.country", "people.person.nationality"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays charlie in the santa clause movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.02732664719223976, -0.02773655205965042, -0.02860153466463089], "norm_scores": [0.3335205316543579, 0.3333838880062103, 0.33309561014175415]}}
{"id": "WebQTest-1020", "question": "what highschool did r. kelly attend", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did r. kelly attend [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01693260483443737, -0.018538611009716988, -0.020075948908925056], "norm_scores": [0.33386117219924927, 0.33332544565200806, 0.3328133821487427]}}
{"id": "WebQTest-1021", "question": "what happened at benghazi", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened at benghazi [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.005054993089288473, -0.04429755359888077, -0.05094998702406883], "norm_scores": [0.3428579568862915, 0.3296639025211334, 0.3274781107902527]}}
{"id": "WebQTest-1022", "question": "what country is nike based in", "prediction": [["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"]], "ground_paths": [["film.film.production_companies", "film.film.country"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.founders", "people.person.nationality"], ["organization.organization.phone_number", "common.phone_number.service_location"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nike based in [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>"], "scores": [-0.004885352682322264, -0.03983411192893982, -0.04423059523105621], "norm_scores": [0.34163814783096313, 0.3299045264720917, 0.3284572958946228]}}
{"id": "WebQTest-1024", "question": "what kind of money do i bring to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i bring to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.007276848424226046, -0.02618308551609516, -0.06073751300573349], "norm_scores": [0.341388463973999, 0.33499473333358765, 0.3236168622970581]}}
{"id": "WebQTest-1025", "question": "who did steve spurrier play pro football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["common.topic.notable_types"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did steve spurrier play pro football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.00832116324454546, -0.021976003423333168, -0.03188025951385498], "norm_scores": [0.3374781906604767, 0.33290132880210876, 0.32962045073509216]}}
{"id": "WebQTest-1027", "question": "where do the blackhawks play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the blackhawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.00251593510620296, -0.03588753566145897, -0.10126441717147827], "norm_scores": [0.3480500876903534, 0.336626797914505, 0.315323144197464]}}
{"id": "WebQTest-1028", "question": "who plays john connor", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays john connor [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.02170778624713421, -0.021869519725441933, -0.02677617035806179], "norm_scores": [0.33391404151916504, 0.33386003971099854, 0.3322259187698364]}}
{"id": "WebQTest-1029", "question": "what is the main language spoken in mexico", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language spoken in mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.017487408593297005, -0.025913752615451813, -0.028276799246668816], "norm_scores": [0.33547165989875793, 0.33265674114227295, 0.3318715989589691]}}
{"id": "WebQTest-1030", "question": "what are italians classified as", "prediction": [["people.ethnicity.included_in_group"], ["people.ethnicity.includes_groups"], ["people.person.ethnicity"]], "ground_paths": [["people.ethnicity.includes_groups"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are italians classified as [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.ethnicity.includes_groups </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.009840341284871101, -0.020835019648075104, -0.06373442709445953], "norm_scores": [0.340530127286911, 0.33680659532546997, 0.3226633369922638]}}
{"id": "WebQTest-1031", "question": "what has angelina jolie accomplished", "prediction": [["people.person.profession"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has angelina jolie accomplished [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.014807633124291897, -0.0284507367759943, -0.030596716329455376], "norm_scores": [0.33661141991615295, 0.3320502042770386, 0.33133837580680847]}}
{"id": "WebQTest-1032", "question": "who played captain kirk in star trek movie", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played captain kirk in star trek movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02418592944741249, -0.026621753349900246, -0.027560526505112648], "norm_scores": [0.33397921919822693, 0.3331666886806488, 0.33285409212112427]}}
{"id": "WebQTest-1033", "question": "who played dana scully in x files", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.quotations", "people.person.quotations"], ["film.performance.character", "film.actor.film"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played dana scully in x files [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.02423587627708912, -0.03070879727602005, -0.03194005787372589], "norm_scores": [0.33491039276123047, 0.33274954557418823, 0.3323400914669037]}}
{"id": "WebQTest-1035", "question": "what was f. scott fitzgerald", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was f. scott fitzgerald [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0005439054220914841, -0.06900043040513992, -0.09737002104520798], "norm_scores": [0.35192087292671204, 0.32863569259643555, 0.3194434344768524]}}
{"id": "WebQTest-1036", "question": "what is the dominant language spoken in jamaica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the dominant language spoken in jamaica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01551958080381155, -0.02189222164452076, -0.02317376434803009], "norm_scores": [0.3348936438560486, 0.33276626467704773, 0.3323400616645813]}}
{"id": "WebQTest-1037", "question": "who is the coach for the new york knicks", "prediction": [["basketball.basketball_team.head_coach"], ["basketball.basketball_coach.team"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"]], "ground_paths": [["basketball.basketball_team.head_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach for the new york knicks [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_team.head_coach </PATH>", "<PATH> basketball.basketball_coach.team </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>"], "scores": [-0.004335579928010702, -0.02928311750292778, -0.06536238640546799], "norm_scores": [0.3429168164730072, 0.33446770906448364, 0.32261547446250916]}}
{"id": "WebQTest-1038", "question": "who was wesley snipes", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was wesley snipes [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0003795177908614278, -0.08439032733440399, -0.0993092805147171], "norm_scores": [0.3539537787437439, 0.3254326283931732, 0.3206135630607605]}}
{"id": "WebQTest-1039", "question": "where does sherrod brown live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.people_born_here"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sherrod brown live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.015114487148821354, -0.01805148273706436, -0.03616594895720482], "norm_scores": [0.3359948694705963, 0.33500951528549194, 0.32899561524391174]}}
{"id": "WebQTest-1040", "question": "what kind of language does colombia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does colombia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006391840521246195, -0.022445254027843475, -0.07265740633010864], "norm_scores": [0.34247076511383057, 0.33701685070991516, 0.3205123245716095]}}
{"id": "WebQTest-1041", "question": "what currency does england use now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does england use now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.003167823888361454, -0.04365774989128113, -0.06061964109539986], "norm_scores": [0.3442949652671814, 0.33063292503356934, 0.3250720798969269]}}
{"id": "WebQTest-1042", "question": "who was the original voice of meg griffin on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the original voice of meg griffin on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.014380130916833878, -0.028510723263025284, -0.03262500464916229], "norm_scores": [0.3369397819042206, 0.33221209049224854, 0.3308481276035309]}}
{"id": "WebQTest-1043", "question": "what kind of government is mali", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is mali [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.00810989085584879, -0.01792863756418228, -0.14008519053459167], "norm_scores": [0.34884631633758545, 0.34543782472610474, 0.3057158887386322]}}
{"id": "WebQTest-1044", "question": "who did cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.019701898097991943, -0.022337792441248894, -0.025531239807605743], "norm_scores": [0.3342742919921875, 0.3333943486213684, 0.3323313593864441]}}
{"id": "WebQTest-1045", "question": "what is the zip code for morgantown wv", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for morgantown wv [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-6.243631651159376e-05, -0.10608160495758057, -0.15312404930591583], "norm_scores": [0.36264941096305847, 0.32616955041885376, 0.31118106842041016]}}
{"id": "WebQTest-1046", "question": "what is jacksonville fl timezone", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jacksonville fl timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.5013983809622e-06, -0.1902889460325241, -0.19472138583660126], "norm_scores": [0.37738698720932007, 0.3119964003562927, 0.3106165826320648]}}
{"id": "WebQTest-1047", "question": "who does amy stiller play in dodgeball", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["film.film.starring", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does amy stiller play in dodgeball [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.02299734577536583, -0.02455291338264942, -0.026699453592300415], "norm_scores": [0.33391764760017395, 0.33339861035346985, 0.3326837122440338]}}
{"id": "WebQTest-1048", "question": "who were king henry vii children", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were king henry vii children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.01219315268099308, -0.015562156215310097, -0.14121407270431519], "norm_scores": [0.34775444865226746, 0.34658482670783997, 0.3056606948375702]}}
{"id": "WebQTest-1050", "question": "who does mila kunis play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does mila kunis play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.character </PATH>"], "scores": [-0.020130814984440804, -0.02311467006802559, -0.02354651503264904], "norm_scores": [0.33404475450515747, 0.3330495059490204, 0.33290570974349976]}}
{"id": "WebQTest-1051", "question": "when did the san francisco earthquake occur", "prediction": [["time.event.locations"], ["event.disaster.areas_affected"], ["location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the san francisco earthquake occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.002898327773436904, -0.050050221383571625, -0.06180457025766373], "norm_scores": [0.34521594643592834, 0.3293161392211914, 0.32546791434288025]}}
{"id": "WebQTest-1052", "question": "who played damien in the omen 2006", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played damien in the omen 2006 [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.017024053260684013, -0.02383429929614067, -0.037195783108472824], "norm_scores": [0.3363330662250519, 0.33405032753944397, 0.32961657643318176]}}
{"id": "WebQTest-1054", "question": "who played denver in four christmases", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.film", "film.actor.film"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played denver in four christmases [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>"], "scores": [-0.019595427438616753, -0.031202087178826332, -0.04735572636127472], "norm_scores": [0.33771440386772156, 0.33381733298301697, 0.3284682631492615]}}
{"id": "WebQTest-1055", "question": "what to do with kids in essex", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in essex [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012176692485809326, -0.017527269199490547, -0.03554769605398178], "norm_scores": [0.3365231454372406, 0.33472734689712524, 0.3287494480609894]}}
{"id": "WebQTest-1056", "question": "who did the cleveland cavaliers draft", "prediction": [["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"]], "ground_paths": [["sports.sports_team_roster.team", "sports.pro_athlete.teams"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"], ["sports.sports_team.roster", "sports.pro_athlete.teams"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the cleveland cavaliers draft [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.team <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.drafted_athlete.drafted </PATH>"], "scores": [-0.021304914727807045, -0.0235037412494421, -0.02432982623577118], "norm_scores": [0.3339139521121979, 0.3331805467605591, 0.33290544152259827]}}
{"id": "WebQTest-1057", "question": "what concentration camp did anne frank died in", "prediction": [["base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["film.film.story_by", "base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["book.written_work.subjects"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what concentration camp did anne frank died in [/INST]", "raw_output": {"paths": ["<PATH> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> film.film.story_by <SEP> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.0067185754887759686, -0.04047801345586777, -0.04541591554880142], "norm_scores": [0.3414314091205597, 0.3300972580909729, 0.328471302986145]}}
{"id": "WebQTest-1059", "question": "what language does east timor speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does east timor speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.007763315923511982, -0.029275521636009216, -0.047275617718696594], "norm_scores": [0.34013885259628296, 0.33289986848831177, 0.3269612491130829]}}
{"id": "WebQTest-1060", "question": "what is the currency in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.005321898497641087, -0.02723686210811138, -0.14420431852340698], "norm_scores": [0.3510430157184601, 0.34343358874320984, 0.3055233955383301]}}
{"id": "WebQTest-1061", "question": "where does greek language come from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["base.rosetta.languoid.child"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does greek language come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.008613768965005875, -0.02514112927019596, -0.05532699078321457], "norm_scores": [0.34037113189697266, 0.3347919285297394, 0.32483696937561035]}}
{"id": "WebQTest-1062", "question": "what is the name of the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.005203671287745237, -0.02449369803071022, -0.1399366855621338], "norm_scores": [0.35028183460235596, 0.3435896337032318, 0.30612853169441223]}}
{"id": "WebQTest-1063", "question": "where is audrey hepburn from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is audrey hepburn from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.0011287759989500046, -0.053479306399822235, -0.09906186163425446], "norm_scores": [0.3501761257648468, 0.3323158025741577, 0.3175080716609955]}}
{"id": "WebQTest-1065", "question": "who does darth vaders voice", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.021726936101913452, -0.022343473508954048, -0.037019744515419006], "norm_scores": [0.33509737253189087, 0.33489084243774414, 0.330011785030365]}}
{"id": "WebQTest-1067", "question": "what school did douglas macarthur go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did douglas macarthur go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.015326397493481636, -0.01706998609006405, -0.030882513150572777], "norm_scores": [0.3352530002593994, 0.33466893434524536, 0.3300780951976776]}}
{"id": "WebQTest-1068", "question": "what famous people are from barbados", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"], ["location.location.contains", "location.location.people_born_here"], ["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from barbados [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.013149724341928959, -0.02578909881412983, -0.04100518301129341], "norm_scores": [0.33784136176109314, 0.3335981070995331, 0.3285605013370514]}}
{"id": "WebQTest-1069", "question": "what is the name of the currency used in china", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in china [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.006708115339279175, -0.02072494849562645, -0.14599475264549255], "norm_scores": [0.3501327633857727, 0.34525924921035767, 0.304608017206192]}}
{"id": "WebQTest-1070", "question": "what are the names of michael jackson 's 3 children", "prediction": [["people.person.parents"], ["people.person.children"], ["music.composition.composer", "people.person.parents"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's 3 children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.composition.composer <SEP> people.person.parents </PATH>"], "scores": [-0.009086616337299347, -0.01706099696457386, -0.09451165050268173], "norm_scores": [0.3436214327812195, 0.34089216589927673, 0.3154864013195038]}}
{"id": "WebQTest-1072", "question": "who is the current governor of arizona 2010", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.country.first_level_divisions", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current governor of arizona 2010 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.019622065126895905, -0.020741047337651253, -0.023649055510759354], "norm_scores": [0.33390510082244873, 0.3335317075252533, 0.332563191652298]}}
{"id": "WebQTest-1074", "question": "who is lamar odom playing for this year", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is lamar odom playing for this year [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.016943736001849174, -0.01964208483695984, -0.03224542737007141], "norm_scores": [0.33533188700675964, 0.3344282805919647, 0.33023980259895325]}}
{"id": "WebQTest-1075", "question": "where were the great pyramids of giza built", "prediction": [["architecture.building.building_complex"], ["location.location.containedby"], ["architecture.structure.architect"]], "ground_paths": [["architecture.building.building_complex"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the great pyramids of giza built [/INST]", "raw_output": {"paths": ["<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> architecture.structure.architect </PATH>"], "scores": [-0.007183493580669165, -0.023340269923210144, -0.0940016433596611], "norm_scores": [0.3447304666042328, 0.3392055034637451, 0.3160640001296997]}}
{"id": "WebQTest-1077", "question": "what language do the russian speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the russian speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01684691570699215, -0.020181698724627495, -0.02491697110235691], "norm_scores": [0.33460113406181335, 0.3334871828556061, 0.33191171288490295]}}
{"id": "WebQTest-1078", "question": "what language do australia people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do australia people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007743499707430601, -0.02007538452744484, -0.078193798661232], "norm_scores": [0.34249886870384216, 0.3383011519908905, 0.3192000389099121]}}
{"id": "WebQTest-1079", "question": "what country was king tut 's tomb found", "prediction": [["people.person.nationality"], ["people.deceased_person.place_of_death"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was king tut 's tomb found [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.016480328515172005, -0.03046923689544201, -0.030953820794820786], "norm_scores": [0.336503267288208, 0.3318287432193756, 0.331667959690094]}}
{"id": "WebQTest-1080", "question": "where did abraham and his descendants live", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"], ["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did abraham and his descendants live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.01945008896291256, -0.033950626850128174, -0.03543180599808693], "norm_scores": [0.3367287516593933, 0.33188122510910034, 0.33139002323150635]}}
{"id": "WebQTest-1081", "question": "where did george washington carver go to collage", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george washington carver go to collage [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.017159353941679, -0.017851997166872025, -0.01839474029839039], "norm_scores": [0.33354756236076355, 0.3333166241645813, 0.33313578367233276]}}
{"id": "WebQTest-1082", "question": "what are the sports in spain", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sports in spain [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.010522235184907913, -0.015259715728461742, -0.13968044519424438], "norm_scores": [0.34793394804000854, 0.3462895154953003, 0.3057765066623688]}}
{"id": "WebQTest-1083", "question": "where president obama was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where president obama was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.003426808398216963, -0.03973114863038063, -0.08132433146238327], "norm_scores": [0.3460918962955475, 0.3337526023387909, 0.3201555013656616]}}
{"id": "WebQTest-1084", "question": "what sports do people play in poland", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports do people play in poland [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.010319377295672894, -0.014984449371695518, -0.15883007645606995], "norm_scores": [0.3499762713909149, 0.34834739565849304, 0.30167633295059204]}}
{"id": "WebQTest-1085", "question": "who was demi lovato dating", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.artist.genre", "music.artist.genre"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["music.recording.featured_artists", "music.album.featured_artists"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["music.recording.artist", "music.composition.composer"], ["music.album.artist", "music.album.artist"], ["music.artist.genre", "music.artist.genre"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.languages", "people.person.languages"], ["music.artist.album", "music.composer.compositions"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["people.person.ethnicity", "people.person.ethnicity"], ["music.composer.compositions", "music.composer.compositions"], ["music.artist.label", "music.artist.label"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.recording.artist", "music.composer.compositions"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["music.composer.compositions", "music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was demi lovato dating [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.028844397515058517, -0.03052939660847187, -0.040112849324941635], "norm_scores": [0.3347715735435486, 0.3342079818248749, 0.33102041482925415]}}
{"id": "WebQTest-1087", "question": "what films has scarlett johansson been in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.character"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has scarlett johansson been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.023758163675665855, -0.02414899691939354, -0.033891450613737106], "norm_scores": [0.3345010578632355, 0.33437034487724304, 0.3311285972595215]}}
{"id": "WebQTest-1088", "question": "who originally voiced meg on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced meg on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.024344542995095253, -0.025818560272455215, -0.02618402987718582], "norm_scores": [0.33370158076286316, 0.3332100808620453, 0.33308830857276917]}}
{"id": "WebQTest-1089", "question": "who played amy squirrel in bad teacher", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.language", "people.person.languages"], ["film.film.starring", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played amy squirrel in bad teacher [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.028020480647683144, -0.03585205599665642, -0.036039698868989944], "norm_scores": [0.3350968360900879, 0.3324827253818512, 0.33242037892341614]}}
{"id": "WebQTest-1090", "question": "where are boeing headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are boeing headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.005014970898628235, -0.044024236500263214, -0.053398534655570984], "norm_scores": [0.3431108295917511, 0.32998403906822205, 0.32690510153770447]}}
{"id": "WebQTest-1091", "question": "what type of religions are in the united states", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical.subjects"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religions are in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.subjects </PATH>"], "scores": [-0.007859120145440102, -0.015906307846307755, -0.0652783140540123], "norm_scores": [0.34057819843292236, 0.3378484845161438, 0.32157331705093384]}}
{"id": "WebQTest-1092", "question": "who did viggo mortensen play in lord of the rings", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.film_series.films_in_series", "book.book.characters"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["media_common.adaptation.adapted_from", "book.book_character.appears_in_book"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["media_common.adapted_work.adaptations", "book.book.characters"], ["film.performance.film", "film.performance.character"], ["film.film.starring", "film.performance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["music.featured_artist.recordings", "book.book_character.appears_in_book"], ["film.film.story_by", "fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did viggo mortensen play in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.021878115832805634, -0.022755514830350876, -0.0278851930052042], "norm_scores": [0.3340979814529419, 0.3338049650192261, 0.33209702372550964]}}
{"id": "WebQTest-1094", "question": "who was isaac newton and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was isaac newton and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0006578677566722035, -0.0650348961353302, -0.08150260895490646], "norm_scores": [0.3496517837047577, 0.3278515040874481, 0.3224967420101166]}}
{"id": "WebQTest-1095", "question": "who did william shakespeare marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.children", "people.person.parents"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["book.book_subject.works", "book.book_subject.works"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did william shakespeare marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.018222127109766006, -0.04381737485527992, -0.04500073567032814], "norm_scores": [0.33917778730392456, 0.330606609582901, 0.33021560311317444]}}
{"id": "WebQTest-1096", "question": "what movies queen latifah is in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.film.executive_produced_by"], ["people.person.nationality", "film.film.country"], ["film.producer.film", "film.film.sequel"], ["music.recording.artist"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["film.producer.film"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.producer.films_executive_produced"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.film.produced_by"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies queen latifah is in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.02495310641825199, -0.027282509952783585, -0.03956623747944832], "norm_scores": [0.3352142870426178, 0.3344343602657318, 0.3303513824939728]}}
{"id": "WebQTest-1097", "question": "where did michelle obama graduated from", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did michelle obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015978367999196053, -0.019647547975182533, -0.021022330969572067], "norm_scores": [0.3343020975589752, 0.3330777585506439, 0.33262014389038086]}}
{"id": "WebQTest-1098", "question": "what kind of music did franz liszt compose", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.composition.composer"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.005040430463850498, -0.03243850916624069, -0.0631631538271904], "norm_scores": [0.34287580847740173, 0.3336091935634613, 0.32351502776145935]}}
{"id": "WebQTest-1099", "question": "what country does david luiz play for", "prediction": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["people.person.nationality", "sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does david luiz play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.03111707977950573, -0.03145685791969299, -0.0336826927959919], "norm_scores": [0.33365610241889954, 0.33354276418685913, 0.3328011631965637]}}
{"id": "WebQTest-1100", "question": "what document did thomas jefferson wrote", "prediction": [["book.written_work.author"], ["book.author.book_editions_published"], ["book.author.works_written"]], "ground_paths": [["book.book_subject.works"], ["book.author.book_editions_published"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["book.written_work.subjects"], []], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did thomas jefferson wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.010852850042283535, -0.027331329882144928, -0.036971718072891235], "norm_scores": [0.3380804657936096, 0.33255505561828613, 0.32936450839042664]}}
{"id": "WebQTest-1101", "question": "who is martin luther king jr facts", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["base.activism.activist.area_of_activism"], ["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is martin luther king jr facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0017398431664332747, -0.06598687916994095, -0.06695039570331573], "norm_scores": [0.34786924719810486, 0.3262224793434143, 0.3259083032608032]}}
{"id": "WebQTest-1102", "question": "where is victoria british columbia", "prediction": [["location.location.containedby"], ["location.province.capital"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.aareas.schema.administrative_area.capital"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is victoria british columbia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.province.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.009738766588270664, -0.019912784919142723, -0.06862454861402512], "norm_scores": [0.34098362922668457, 0.33753204345703125, 0.3214842975139618]}}
{"id": "WebQTest-1103", "question": "how many mary mary sisters", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["music.artist.genre", "music.artist.genre"], ["people.person.parents", "people.person.children"]], "ground_paths": [["music.musical_group.member", "music.group_member.membership"], ["music.album.artist", "music.composition.composer"], ["music.album.artist", "music.composer.compositions"], ["music.artist.origin", "people.person.place_of_birth"], ["music.group_membership.group", "music.group_membership.member"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many mary mary sisters [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03877031430602074, -0.040010321885347366, -0.04843369126319885], "norm_scores": [0.33454394340515137, 0.33412936329841614, 0.3313267230987549]}}
{"id": "WebQTest-1104", "question": "what do we call the currency of france", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do we call the currency of france [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00865785963833332, -0.018674394115805626, -0.09841693937778473], "norm_scores": [0.34433069825172424, 0.34089893102645874, 0.3147704005241394]}}
{"id": "WebQTest-1106", "question": "what team does david beckham play for in 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["book.written_work.subjects", "book.written_work.subjects"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does david beckham play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02056290954351425, -0.025301501154899597, -0.02915271185338497], "norm_scores": [0.33481550216674805, 0.33323267102241516, 0.3319517970085144]}}
{"id": "WebQTest-1107", "question": "what movies did nick cannon star in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["music.artist.track", "music.recording.releases"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did nick cannon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.016991930082440376, -0.03510385379195213, -0.03567270562052727], "norm_scores": [0.3374338746070862, 0.3313773274421692, 0.331188827753067]}}
{"id": "WebQTest-1108", "question": "what radio station is npr on in nyc", "prediction": [["broadcast.radio_station.owner"], ["broadcast.radio_station_owner.radio_stations"], ["broadcast.content.location"]], "ground_paths": [["broadcast.producer.location", "broadcast.broadcast.area_served"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["broadcast.radio_network.affiliates", "broadcast.radio_affiliation_duration.station"], ["broadcast.producer.produces", "broadcast.producer.produces"], ["award.award_winner.awards_won", "award.award_honor.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what radio station is npr on in nyc [/INST]", "raw_output": {"paths": ["<PATH> broadcast.radio_station.owner </PATH>", "<PATH> broadcast.radio_station_owner.radio_stations </PATH>", "<PATH> broadcast.content.location </PATH>"], "scores": [-0.019064800813794136, -0.035356130450963974, -0.04434073343873024], "norm_scores": [0.33796554803848267, 0.3325042426586151, 0.3295302093029022]}}
{"id": "WebQTest-1109", "question": "who did tom landry play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom landry play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.011389237828552723, -0.012313135899603367, -0.057403042912483215], "norm_scores": [0.33851197361946106, 0.3381993770599365, 0.3232886493206024]}}
{"id": "WebQTest-1110", "question": "where was john jay born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john jay born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.010450373403728008, -0.016157815232872963, -0.12303350865840912], "norm_scores": [0.3462805449962616, 0.3443097770214081, 0.30940964818000793]}}
{"id": "WebQTest-1111", "question": "where is mount fuji located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_range"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount fuji located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_range </PATH>"], "scores": [-0.01189054362475872, -0.023122498765587807, -0.05065931752324104], "norm_scores": [0.3388907015323639, 0.3351055681705475, 0.3260037302970886]}}
{"id": "WebQTest-1113", "question": "where was toussaint l'ouverture born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was toussaint l'ouverture born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.004118870012462139, -0.03015589527785778, -0.08548811823129654], "norm_scores": [0.34528571367263794, 0.3364115059375763, 0.3183027505874634]}}
{"id": "WebQTest-1114", "question": "what countries makes up the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries makes up the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.019202468916773796, -0.030793096870183945, -0.031026149168610573], "norm_scores": [0.3359399735927582, 0.33206871151924133, 0.3319913148880005]}}
{"id": "WebQTest-1115", "question": "what is lawton ok zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is lawton ok zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00018200530030298978, -0.09788459539413452, -0.14078137278556824], "norm_scores": [0.3602622151374817, 0.3267285227775574, 0.3130092918872833]}}
{"id": "WebQTest-1116", "question": "what is the capital city of spain wiki answers", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital city of spain wiki answers [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.006933214608579874, -0.03259611502289772, -0.04041347652673721], "norm_scores": [0.33993515372276306, 0.33132240176200867, 0.32874244451522827]}}
{"id": "WebQTest-1117", "question": "where do they speak german", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak german [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.017067698761820793, -0.022837556898593903, -0.023299159482121468], "norm_scores": [0.3346681296825409, 0.33274272084236145, 0.33258914947509766]}}
{"id": "WebQTest-1118", "question": "when did mark mcgwire retired", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.ending_season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did mark mcgwire retired [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.013132110238075256, -0.015150364488363266, -0.02630094811320305], "norm_scores": [0.33501946926116943, 0.33434396982192993, 0.33063656091690063]}}
{"id": "WebQTest-1119", "question": "what is colorado technical university", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is colorado technical university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.025450747460126877, -0.03323103114962578, -0.03946275636553764], "norm_scores": [0.3357580006122589, 0.3331558406352997, 0.3310861587524414]}}
{"id": "WebQTest-1120", "question": "what was the currency used in france before the euro", "prediction": [["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the currency used in france before the euro [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.005938837770372629, -0.025731103494763374, -0.04818826913833618], "norm_scores": [0.3402479588985443, 0.3335798978805542, 0.3261721432209015]}}
{"id": "WebQTest-1121", "question": "what is kentucky state slogan", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is kentucky state slogan [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.013576305471360683, -0.02015666477382183, -0.023477375507354736], "norm_scores": [0.3351667821407318, 0.33296850323677063, 0.3318646550178528]}}
{"id": "WebQTest-1124", "question": "who played samantha stephens in bewitched", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played samantha stephens in bewitched [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.016074862331151962, -0.032250192016363144, -0.034180302172899246], "norm_scores": [0.33715301752090454, 0.3317433297634125, 0.331103652715683]}}
{"id": "WebQTest-1125", "question": "what area of science did sir isaac newton study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"], ["law.invention.inventor"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what area of science did sir isaac newton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.005640416871756315, -0.032848868519067764, -0.04492708295583725], "norm_scores": [0.3407580554485321, 0.3316115438938141, 0.3276303708553314]}}
{"id": "WebQTest-1126", "question": "what county is san diego in", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.location.containedby"]], "ground_paths": [["location.us_county.county_seat"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.019156500697135925, -0.022676831111311913, -0.025504842400550842], "norm_scores": [0.3344305455684662, 0.3332553207874298, 0.3323141932487488]}}
{"id": "WebQTest-1127", "question": "which country in north america is divided into provinces", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"]], "ground_paths": [["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in north america is divided into provinces [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.01638488657772541, -0.021248159930109978, -0.022739961743354797], "norm_scores": [0.3345809280872345, 0.3329577147960663, 0.3324613869190216]}}
{"id": "WebQTest-1130", "question": "what is real name of santa claus", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.occupation", "people.person.profession"]], "ground_paths": [["fictional_universe.fictional_character.based_on"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is real name of santa claus [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>"], "scores": [-0.005377465393394232, -0.040373705327510834, -0.05318945646286011], "norm_scores": [0.3425922393798828, 0.33081015944480896, 0.3265976309776306]}}
{"id": "WebQTest-1131", "question": "when did kelly slater go pro", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_award.award_winner", "sports.sports_award.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kelly slater go pro [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>"], "scores": [-0.01803763210773468, -0.02960909530520439, -0.03888458013534546], "norm_scores": [0.3369426429271698, 0.3330661952495575, 0.3299911320209503]}}
{"id": "WebQTest-1132", "question": "what did coretta scott king died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did coretta scott king died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.001420692540705204, -0.044572655111551285, -0.11337404698133469], "norm_scores": [0.35064932703971863, 0.33583998680114746, 0.3135106563568115]}}
{"id": "WebQTest-1133", "question": "what does wh smith stand for", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["organization.organization.founders"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does wh smith stand for [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.024893786758184433, -0.041858870536088943, -0.0472833588719368], "norm_scores": [0.33771950006484985, 0.33203840255737305, 0.3302421569824219]}}
{"id": "WebQTest-1135", "question": "who was vp for nixon", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for nixon [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.00576225807890296, -0.022781111299991608, -0.11673042178153992], "norm_scores": [0.3474523425102234, 0.34158915281295776, 0.31095847487449646]}}
{"id": "WebQTest-1136", "question": "what religion did adolf hitler follow", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"], ["religion.religion.founding_figures"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion did adolf hitler follow [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>"], "scores": [-0.0012375422520563006, -0.06009209528565407, -0.08088769018650055], "norm_scores": [0.34888386726379395, 0.32894304394721985, 0.3221730887889862]}}
{"id": "WebQTest-1137", "question": "where is the illinois river on a map", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the illinois river on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011290540918707848, -0.02675812505185604, -0.034853674471378326], "norm_scores": [0.33768224716186523, 0.33249932527542114, 0.32981839776039124]}}
{"id": "WebQTest-1138", "question": "where honduras in the world", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.contains"], ["base.locations.continents.countries_within"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where honduras in the world [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.016693037003278732, -0.019290277734398842, -0.03880595415830612], "norm_scores": [0.3360739052295685, 0.3352021872997284, 0.3287239074707031]}}
{"id": "WebQTest-1142", "question": "who does ronaldinho play for now 2011", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ronaldinho play for now 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.02473454922437668, -0.025550171732902527, -0.028408966958522797], "norm_scores": [0.33383217453956604, 0.3335600197315216, 0.33260777592658997]}}
{"id": "WebQTest-1143", "question": "what galileo galilei was famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["astronomy.astronomer.astronomical_objects_discovered", "astronomy.star_system_body.star_system"], ["book.written_work.author", "book.book_subject.works"], ["astronomy.astronomical_discovery.discoverer", "astronomy.star_system_body.star_system"], ["influence.influence_node.influenced", "base.argumentmaps.innovator.original_ideas"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["symbols.name_source.namesakes", "astronomy.star_system_body.star_system"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what galileo galilei was famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0012852251529693604, -0.06419450789690018, -0.07643748819828033], "norm_scores": [0.3488415479660034, 0.327572226524353, 0.32358619570732117]}}
{"id": "WebQTest-1144", "question": "what are the major trading partners of germany", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major trading partners of germany [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.statistical_region.places_imported_from </PATH>"], "scores": [-0.028552014380693436, -0.03916158154606819, -0.04108327254652977], "norm_scores": [0.335909366607666, 0.33236438035964966, 0.3317262530326843]}}
{"id": "WebQTest-1145", "question": "where did romney graduated college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.religion", "religion.religion.organizations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did romney graduated college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.017436936497688293, -0.01913648284971714, -0.02409183420240879], "norm_scores": [0.33426156640052795, 0.3336939215660095, 0.33204445242881775]}}
{"id": "WebQTest-1146", "question": "who did johnny depp play in corpse bride", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did johnny depp play in corpse bride [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>"], "scores": [-0.01785207912325859, -0.024940451607108116, -0.03258940950036049], "norm_scores": [0.335761159658432, 0.3333895802497864, 0.33084923028945923]}}
{"id": "WebQTest-1147", "question": "what is the zip code for jonesboro ar", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for jonesboro ar [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-7.578555232612416e-05, -0.10539986193180084, -0.1509413719177246], "norm_scores": [0.36231938004493713, 0.32609930634498596, 0.3115813732147217]}}
{"id": "WebQTest-1148", "question": "what college did se hinton go", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did se hinton go [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.0177596602588892, -0.019643740728497505, -0.023085180670022964], "norm_scores": [0.3341345489025116, 0.3335056006908417, 0.33235982060432434]}}
{"id": "WebQTest-1149", "question": "when was taylor swift fearless tour", "prediction": [["music.concert_tour.artist"], ["time.event.locations"], ["music.artist.concert_tours", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was taylor swift fearless tour [/INST]", "raw_output": {"paths": ["<PATH> music.concert_tour.artist </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> music.artist.concert_tours <SEP> time.event.locations </PATH>"], "scores": [-0.004366525914520025, -0.04935108870267868, -0.061918217688798904], "norm_scores": [0.3448174297809601, 0.3296496868133545, 0.32553285360336304]}}
{"id": "WebQTest-1151", "question": "where did charles darwin and his wife live", "prediction": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects", "location.location.contains"]], "ground_paths": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_burial", "travel.travel_destination.tourist_attractions"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["book.written_work.subjects", "location.location.contains"], ["people.person.nationality", "location.country.capital"], ["people.person.children", "people.deceased_person.place_of_death"], ["book.written_work.subjects"], ["book.written_work.subjects", "location.location.containedby"], ["book.written_work.subjects", "location.country.capital"], ["book.book_subject.works", "book.written_work.subjects"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin and his wife live [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.contains </PATH>"], "scores": [-0.030323542654514313, -0.03439180552959442, -0.0403180830180645], "norm_scores": [0.3348967134952545, 0.3335370421409607, 0.3315662443637848]}}
{"id": "WebQTest-1152", "question": "who is the voice of kitt", "prediction": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of kitt [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.03249501809477806, -0.04000014066696167, -0.04266320541501045], "norm_scores": [0.3352997303009033, 0.3327926993370056, 0.33190760016441345]}}
{"id": "WebQTest-1153", "question": "what is eminem 's real full name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.nationality"], ["music.artist.album", "music.composer.compositions"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is eminem 's real full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.album <SEP> music.composer.compositions </PATH>"], "scores": [-0.03357791155576706, -0.039723560214042664, -0.04236018285155296], "norm_scores": [0.3349938690662384, 0.3329414129257202, 0.3320647180080414]}}
{"id": "WebQTest-1154", "question": "who is amy grant 's first husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.children"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.artist.genre", "music.artist.genre"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["music.artist.origin", "music.artist.origin"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is amy grant 's first husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02540791593492031, -0.033779699355363846, -0.042991165071725845], "norm_scores": [0.3362210690975189, 0.3334180414676666, 0.33036088943481445]}}
{"id": "WebQTest-1155", "question": "what are the seven nations of central america", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the seven nations of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.009075908921658993, -0.025833699852228165, -0.043035343289375305], "norm_scores": [0.3389838933944702, 0.33335062861442566, 0.3276654779911041]}}
{"id": "WebQTest-1156", "question": "what is jamie spears baby name", "prediction": [["people.person.children"], ["people.person.parents"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie spears baby name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.celebrity.canoodled </PATH>"], "scores": [-0.009095725603401661, -0.02025098167359829, -0.12945330142974854], "norm_scores": [0.34776437282562256, 0.3439065217971802, 0.3083290457725525]}}
{"id": "WebQTest-1160", "question": "who is the mother of prince michael jackson", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the mother of prince michael jackson [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.007474386133253574, -0.03226028382778168, -0.04039674997329712], "norm_scores": [0.3397740423679352, 0.33145594596862793, 0.3287700116634369]}}
{"id": "WebQTest-1161", "question": "what is the clemson mascot name", "prediction": [["education.school_mascot.school"], ["education.educational_institution.mascot"], []], "ground_paths": [["education.school_mascot.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the clemson mascot name [/INST]", "raw_output": {"paths": ["<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.mascot </PATH>", "<PATH> </PATH>"], "scores": [-0.012326965108513832, -0.014334333129227161, -0.06718652695417404], "norm_scores": [0.3396032452583313, 0.3389222025871277, 0.321474552154541]}}
{"id": "WebQTest-1163", "question": "what rainforest is in south america", "prediction": [["location.location.containedby"], ["common.topic.notable_types"], ["location.location.partially_containedby"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rainforest is in south america [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.003598798532038927, -0.040074244141578674, -0.06206729635596275], "norm_scores": [0.34395113587379456, 0.3316314220428467, 0.32441747188568115]}}
{"id": "WebQTest-1164", "question": "what type of art does claude monet do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does claude monet do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.005912063177675009, -0.026678863912820816, -0.03601064905524254], "norm_scores": [0.33900636434555054, 0.33203887939453125, 0.3289547860622406]}}
{"id": "WebQTest-1166", "question": "when did we start war with iraq", "prediction": [["base.culturalevent.event.entity_involved"], ["time.event.locations"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did we start war with iraq [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.025100449100136757, -0.028078842908143997, -0.03233617916703224], "norm_scores": [0.33446869254112244, 0.3334740102291107, 0.33205729722976685]}}
{"id": "WebQTest-1168", "question": "what guitar did kurt cobain", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"]], "ground_paths": [["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did kurt cobain [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>"], "scores": [-0.012269383296370506, -0.03180355951189995, -0.03704241290688515], "norm_scores": [0.3382736146450043, 0.33172982931137085, 0.3299965262413025]}}
{"id": "WebQTest-1169", "question": "who did jennifer lawrence play in x men", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jennifer lawrence play in x men [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.021320587024092674, -0.028118189424276352, -0.030446868389844894], "norm_scores": [0.3351048231124878, 0.3328346312046051, 0.33206048607826233]}}
{"id": "WebQTest-1170", "question": "who is the governor of virginia 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of virginia 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.01731250435113907, -0.019070928916335106, -0.024846084415912628], "norm_scores": [0.3343656361103058, 0.3337782025337219, 0.3318561315536499]}}
{"id": "WebQTest-1171", "question": "who did darth vaders voice", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.022656068205833435, -0.02324225939810276, -0.036291249096393585], "norm_scores": [0.3349106013774872, 0.3347143530845642, 0.3303750455379486]}}
{"id": "WebQTest-1173", "question": "where is walton county ga", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.us_county.county_seat"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton county ga [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>"], "scores": [-0.009011073969304562, -0.03168601170182228, -0.04563140869140625], "norm_scores": [0.33994847536087036, 0.33232688903808594, 0.3277246654033661]}}
{"id": "WebQTest-1174", "question": "what cancer did jackie kennedy die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did jackie kennedy die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.014168650843203068, -0.014738954603672028, -0.03552064672112465], "norm_scores": [0.33576151728630066, 0.3355700969696045, 0.32866835594177246]}}
{"id": "WebQTest-1175", "question": "what does lauren conrad major in", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["education.education.student", "education.education.degree"]], "ground_paths": [["people.person.education", "education.field_of_study.students_majoring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does lauren conrad major in [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>"], "scores": [-0.008983578532934189, -0.021243920549750328, -0.045126285403966904], "norm_scores": [0.338716983795166, 0.33458957076072693, 0.32669341564178467]}}
{"id": "WebQTest-1176", "question": "what time zone is pennsylvania in right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is pennsylvania in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.6144756955327466e-05, -0.12386777997016907, -0.18268658220767975], "norm_scores": [0.3681134879589081, 0.32523229718208313, 0.3066542446613312]}}
{"id": "WebQTest-1177", "question": "where did mario lopez go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mario lopez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012599408626556396, -0.024006178602576256, -0.025872711092233658], "norm_scores": [0.3360810875892639, 0.3322692811489105, 0.33164966106414795]}}
{"id": "WebQTest-1178", "question": "where is tupac from originally", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tupac from originally [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004672067239880562, -0.03284427523612976, -0.0699053630232811], "norm_scores": [0.34375250339508057, 0.3342033624649048, 0.32204413414001465]}}
{"id": "WebQTest-1179", "question": "what role did george lucas play in star wars", "prediction": [["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.film.produced_by"], ["film.producer.films_executive_produced"]], "ground_paths": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did george lucas play in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.producer.films_executive_produced </PATH>"], "scores": [-0.029597917571663857, -0.042887069284915924, -0.04310633987188339], "norm_scores": [0.3363174796104431, 0.3318776786327362, 0.33180490136146545]}}
{"id": "WebQTest-1180", "question": "what sarah dessen books are movies", "prediction": [["film.film_story_contributor.film_story_credits"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["film.film.story_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sarah dessen books are movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.02508094161748886, -0.031206410378217697, -0.0329444482922554], "norm_scores": [0.33488941192626953, 0.3328443169593811, 0.33226633071899414]}}
{"id": "WebQTest-1181", "question": "what is the current time in kauai hawaii", "prediction": [["location.location.time_zones"], ["geography.island.island_group", "location.location.time_zones"], ["location.location.containedby", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in kauai hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> geography.island.island_group <SEP> location.location.time_zones </PATH>", "<PATH> location.location.containedby <SEP> location.location.time_zones </PATH>"], "scores": [-0.00023943843552842736, -0.08177081495523453, -0.09395299851894379], "norm_scores": [0.3530765175819397, 0.3254319429397583, 0.321491539478302]}}
{"id": "WebQTest-1183", "question": "where was farrah fawcett buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was farrah fawcett buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.0025060197804123163, -0.038529764860868454, -0.06823921203613281], "norm_scores": [0.34470894932746887, 0.3325122594833374, 0.3227787911891937]}}
{"id": "WebQTest-1185", "question": "what inspired steinbeck", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired steinbeck [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.01025885995477438, -0.014838794246315956, -0.11978288739919662], "norm_scores": [0.3458184003829956, 0.34423816204071045, 0.30994343757629395]}}
{"id": "WebQTest-1186", "question": "what kind of government does cuba have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does cuba have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011834396049380302, -0.011911412701010704, -0.14772599935531616], "norm_scores": [0.3480851650238037, 0.3480583727359772, 0.3038564920425415]}}
{"id": "WebQTest-1187", "question": "where are english speakers distributed", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are english speakers distributed [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01405881717801094, -0.02455262281000614, -0.031031804159283638], "norm_scores": [0.33639097213745117, 0.3328794240951538, 0.33072957396507263]}}
{"id": "WebQTest-1189", "question": "what is the capital of italy today", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of italy today [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013234994374215603, -0.026645537465810776, -0.02733614295721054], "norm_scores": [0.3363971412181854, 0.3319159746170044, 0.3316868543624878]}}
{"id": "WebQTest-1191", "question": "what language do people speak in the bahamas", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.00483875023201108, -0.03287544846534729, -0.042592886835336685], "norm_scores": [0.34068039059638977, 0.3312614858150482, 0.3280580937862396]}}
{"id": "WebQTest-1192", "question": "where did george michael go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george michael go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.016069378703832626, -0.019022131338715553, -0.025311656296253204], "norm_scores": [0.33468857407569885, 0.3337017893791199, 0.3316095471382141]}}
{"id": "WebQTest-1194", "question": "what club does cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what club does cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.020215308293700218, -0.02182481437921524, -0.02684674598276615], "norm_scores": [0.3342489004135132, 0.3337113559246063, 0.3320396840572357]}}
{"id": "WebQTest-1195", "question": "what time zone is in the bahamas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-8.129437446768861e-06, -0.15873539447784424, -0.18559342622756958], "norm_scores": [0.3725994825363159, 0.3179126977920532, 0.30948784947395325]}}
{"id": "WebQTest-1197", "question": "what language do people speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0005964980227872729, -0.07401609420776367, -0.08419191092252731], "norm_scores": [0.35099858045578003, 0.32615169882774353, 0.32284966111183167]}}
{"id": "WebQTest-1198", "question": "who influenced samuel taylor coleridge", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced samuel taylor coleridge [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.006995232775807381, -0.018612509593367577, -0.090274378657341], "norm_scores": [0.3438146412372589, 0.3398435413837433, 0.3163418769836426]}}
{"id": "WebQTest-1199", "question": "what does kim kardashian work as", "prediction": [["people.person.profession"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does kim kardashian work as [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0009686790872365236, -0.07494394481182098, -0.08855649083852768], "norm_scores": [0.35151445865631104, 0.32644960284233093, 0.32203590869903564]}}
{"id": "WebQTest-1200", "question": "where did the mississippi river end", "prediction": [["geography.river.mouth"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the mississippi river end [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.0014143968001008034, -0.06061774492263794, -0.08983869850635529], "norm_scores": [0.3499087989330292, 0.32979434728622437, 0.32029688358306885]}}
{"id": "WebQTest-1201", "question": "who played carlton lassiter on psych", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carlton lassiter on psych [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.019866732880473137, -0.026768052950501442, -0.03144048899412155], "norm_scores": [0.3353886604309082, 0.333081990480423, 0.33152931928634644]}}
{"id": "WebQTest-1203", "question": "who is president of israel", "prediction": [["people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is president of israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.019113115966320038, -0.020034179091453552, -0.02492138370871544], "norm_scores": [0.3340807855129242, 0.3337732255458832, 0.3321459889411926]}}
{"id": "WebQTest-1204", "question": "where did the anasazi indians live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.contains"], ["location.location.containedby"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the anasazi indians live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0007515060715377331, -0.08229516446590424, -0.09492246806621552], "norm_scores": [0.3531298339366913, 0.32547712326049805, 0.32139307260513306]}}
{"id": "WebQTest-1205", "question": "who plays harley quinn", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays harley quinn [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.016286557540297508, -0.03186149150133133, -0.039229050278663635], "norm_scores": [0.3376251757144928, 0.3324074447154999, 0.3299674093723297]}}
{"id": "WebQTest-1206", "question": "what was francis bacon contributions", "prediction": [["people.person.profession"], ["law.invention.inventor"], ["law.inventor.inventions"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was francis bacon contributions [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>"], "scores": [-0.006070365197956562, -0.04309683293104172, -0.04436595365405083], "norm_scores": [0.3417542576789856, 0.32933172583580017, 0.32891401648521423]}}
{"id": "WebQTest-1209", "question": "when does ziva start on ncis", "prediction": [["tv.tv_series_season.series"], ["tv.tv_program.seasons"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.seasons"]], "ground_paths": [["tv.tv_program.seasons"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does ziva start on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_series_season.series </PATH>", "<PATH> tv.tv_program.seasons </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.seasons </PATH>"], "scores": [-0.012617244385182858, -0.03592092916369438, -0.04570017382502556], "norm_scores": [0.33962497115135193, 0.33180198073387146, 0.3285730183124542]}}
{"id": "WebQTest-1210", "question": "what is the name of the national anthem of canada ( 2 words )", "prediction": [["music.composition.language"], ["music.composition.language", "music.composition.language"], ["location.location.containedby", "music.composition.language"]], "ground_paths": [["location.country.official_language", "music.composition.language"], ["location.country.national_anthem", "government.national_anthem.national_anthem_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the national anthem of canada ( 2 words ) [/INST]", "raw_output": {"paths": ["<PATH> music.composition.language </PATH>", "<PATH> music.composition.language <SEP> music.composition.language </PATH>", "<PATH> location.location.containedby <SEP> music.composition.language </PATH>"], "scores": [-0.01109752431511879, -0.020101791247725487, -0.05917664244771004], "norm_scores": [0.33966299891471863, 0.3366183042526245, 0.3237186670303345]}}
{"id": "WebQTest-1212", "question": "what did mark zuckerberg study", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.field_of_study.students_majoring"], ["people.person.profession", "base.skills.skill.performed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mark zuckerberg study [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.010184656828641891, -0.019442377611994743, -0.032973479479551315], "norm_scores": [0.3368983864784241, 0.33379387855529785, 0.3293077051639557]}}
{"id": "WebQTest-1213", "question": "which country was michael jackson born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was michael jackson born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.01329470332711935, -0.017198463901877403, -0.048295002430677414], "norm_scores": [0.3376428782939911, 0.3363274037837982, 0.3260297179222107]}}
{"id": "WebQTest-1215", "question": "who was stephen r covey", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was stephen r covey [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0009989718673750758, -0.060020461678504944, -0.09950603544712067], "norm_scores": [0.3510156571865082, 0.3308977484703064, 0.3180866241455078]}}
{"id": "WebQTest-1216", "question": "what country was stalin in charge of", "prediction": [["people.person.nationality"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was stalin in charge of [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.015025030821561813, -0.022368989884853363, -0.050042785704135895], "norm_scores": [0.3380352556705475, 0.3355618417263031, 0.3264029026031494]}}
{"id": "WebQTest-1217", "question": "what is monta ellis career high points", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_award.award"], ["award.award_honor.award_winner", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is monta ellis career high points [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.011958225630223751, -0.016894526779651642, -0.039956480264663696], "norm_scores": [0.3369878828525543, 0.33532848954200745, 0.3276836574077606]}}
{"id": "WebQTest-1219", "question": "what nationality is rihanna", "prediction": [["people.person.nationality"], ["people.ethnicity.people"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is rihanna [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.011485645547509193, -0.029665952548384666, -0.04150879755616188], "norm_scores": [0.33870670199394226, 0.3326045572757721, 0.3286888003349304]}}
{"id": "WebQTest-1220", "question": "what has shannen doherty played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.languages", "tv.tv_program.languages"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has shannen doherty played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.01624855026602745, -0.03168704733252525, -0.03427732363343239], "norm_scores": [0.3370620310306549, 0.3318982720375061, 0.331039696931839]}}
{"id": "WebQTest-1221", "question": "what college did joakim noah", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did joakim noah [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.018329575657844543, -0.021795064210891724, -0.025236329063773155], "norm_scores": [0.3344864547252655, 0.33332931995391846, 0.33218419551849365]}}
{"id": "WebQTest-1223", "question": "who plays billy elliot", "prediction": [["film.film.country", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["film.performance.film", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.language", "people.person.languages"], ["media_common.netflix_title.netflix_genres", "people.person.nationality"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays billy elliot [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.030180495232343674, -0.034292567521333694, -0.03668824955821037], "norm_scores": [0.33451420068740845, 0.3331414759159088, 0.3323443531990051]}}
{"id": "WebQTest-1225", "question": "what kourtney kardashian boyfriends name", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kourtney kardashian boyfriends name [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.020107487216591835, -0.02330528013408184, -0.024747334420681], "norm_scores": [0.33420470356941223, 0.3331376612186432, 0.3326576054096222]}}
{"id": "WebQTest-1226", "question": "what form of government does australia use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["type.type.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> type.type.form_of_government </PATH>"], "scores": [-0.007318586576730013, -0.01859830506145954, -0.15872947871685028], "norm_scores": [0.35108932852745056, 0.34715136885643005, 0.3017593324184418]}}
{"id": "WebQTest-1227", "question": "what team did howie long", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did howie long [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>"], "scores": [-0.011658445000648499, -0.01313045620918274, -0.05064357444643974], "norm_scores": [0.33780425786972046, 0.3373073637485504, 0.32488831877708435]}}
{"id": "WebQTest-1228", "question": "where did the greek alphabet come from", "prediction": [["language.human_language.language_family"], ["language.language_family.languages"], ["language.language_writing_system.parent_writing_systems"]], "ground_paths": [["language.language_writing_system.child_writing_systems"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the greek alphabet come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> language.language_writing_system.parent_writing_systems </PATH>"], "scores": [-0.020716045051813126, -0.02169612981379032, -0.0723491907119751], "norm_scores": [0.33913278579711914, 0.3388005793094635, 0.32206666469573975]}}
{"id": "WebQTest-1229", "question": "what timezone is elizabethtown kentucky", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is elizabethtown kentucky [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.25482436286984e-06, -0.17697632312774658, -0.18060442805290222], "norm_scores": [0.3741702139377594, 0.3134825527667999, 0.31234726309776306]}}
{"id": "WebQTest-1230", "question": "what to see in christchurch new zealand", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in christchurch new zealand [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012579805217683315, -0.012629751116037369, -0.045841116458177567], "norm_scores": [0.33701395988464355, 0.3369971513748169, 0.3259888291358948]}}
{"id": "WebQTest-1231", "question": "where is chris paul from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris paul from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.010659740306437016, -0.017031390219926834, -0.06896951049566269], "norm_scores": [0.34048277139663696, 0.33832022547721863, 0.3211970329284668]}}
{"id": "WebQTest-1232", "question": "what year was albert pujols rookie year", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.starting_season"], ["sports.sports_award.award_winner", "sports.sports_league_season.awards"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was albert pujols rookie year [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.013122404925525188, -0.013165593147277832, -0.050627849996089935], "norm_scores": [0.3374791443347931, 0.3374646008014679, 0.3250562846660614]}}
{"id": "WebQTest-1233", "question": "what are the colors of the ny giants", "prediction": [["sports.sports_team.colors"], ["sports.professional_sports_team.owner_s", "sports.sports_team.colors"], ["sports.professional_sports_team.colors"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the colors of the ny giants [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.colors </PATH>"], "scores": [-4.3656673369696364e-05, -0.10718312114477158, -0.1407705843448639], "norm_scores": [0.36138567328453064, 0.3246690332889557, 0.31394532322883606]}}
{"id": "WebQTest-1234", "question": "what business did andrew carnegie dominate", "prediction": [["people.person.profession"], ["book.written_work.subjects"], ["book.written_work.author"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what business did andrew carnegie dominate [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.010504229925572872, -0.04338381439447403, -0.049689896404743195], "norm_scores": [0.34138694405555725, 0.3303448259830475, 0.3282682001590729]}}
{"id": "WebQTest-1235", "question": "what was sir isaac newton 's inventions", "prediction": [["law.invention.inventor"], ["law.inventor.inventions"], ["symbols.namesake.named_after"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was sir isaac newton 's inventions [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.004311148077249527, -0.03151382505893707, -0.06142689287662506], "norm_scores": [0.3427417278289795, 0.33354389667510986, 0.32371431589126587]}}
{"id": "WebQTest-1237", "question": "who did shaq first play for", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did shaq first play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>"], "scores": [-0.018897462636232376, -0.020644620060920715, -0.022134268656373024], "norm_scores": [0.3338872790336609, 0.33330443501472473, 0.3328082859516144]}}
{"id": "WebQTest-1238", "question": "who was the artist of mickey mouse", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the artist of mickey mouse [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.0009562046616338193, -0.07180611044168472, -0.07561477273702621], "norm_scores": [0.3496916592121124, 0.32577335834503174, 0.32453498244285583]}}
{"id": "WebQTest-1239", "question": "what is the zip code for concord", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for concord [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00014305661898106337, -0.10164639353752136, -0.13789105415344238], "norm_scores": [0.36038678884506226, 0.32560160756111145, 0.3140115737915039]}}
{"id": "WebQTest-1240", "question": "what football team is michael vick on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team is michael vick on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.018984196707606316, -0.0231461301445961, -0.0375894270837307], "norm_scores": [0.3358619809150696, 0.3344670534133911, 0.3296709656715393]}}
{"id": "WebQTest-1241", "question": "where does the columbia river start", "prediction": [["geography.lake.outflow"], ["geography.river.origin"], []], "ground_paths": [["geography.lake.outflow"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the columbia river start [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.outflow </PATH>", "<PATH> geography.river.origin </PATH>", "<PATH> </PATH>"], "scores": [-0.007133121136575937, -0.029324760660529137, -0.06927008926868439], "norm_scores": [0.34272313117980957, 0.33520132303237915, 0.3220755159854889]}}
{"id": "WebQTest-1242", "question": "what are all the names of the harry potter books", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["book.book_character.appears_in_book"], ["fictional_universe.fictional_universe.works_set_here"]], "ground_paths": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["book.book_character.appears_in_book"], ["fictional_universe.fictional_universe.works_set_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the names of the harry potter books [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> book.book_character.appears_in_book </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>"], "scores": [-0.00815799180418253, -0.026137644425034523, -0.031682249158620834], "norm_scores": [0.3379598557949066, 0.3319377303123474, 0.3301023840904236]}}
{"id": "WebQTest-1244", "question": "who was sir walter raleigh and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was sir walter raleigh and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.0008461470133624971, -0.06039274111390114, -0.07641343027353287], "norm_scores": [0.34850379824638367, 0.32835739850997925, 0.3231387734413147]}}
{"id": "WebQTest-1245", "question": "what countries are in the euro dollar", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the euro dollar [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>"], "scores": [-0.011567874811589718, -0.01863926649093628, -0.04870672523975372], "norm_scores": [0.3382382392883301, 0.3358548581600189, 0.3259068727493286]}}
{"id": "WebQTest-1246", "question": "what currency can you use in barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.011847197078168392, -0.013952672481536865, -0.13805249333381653], "norm_scores": [0.34730294346809387, 0.34657248854637146, 0.30612459778785706]}}
{"id": "WebQTest-1247", "question": "who is angelina jolie", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.person.nationality"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.001963249174878001, -0.05188381299376488, -0.09308825433254242], "norm_scores": [0.3491365909576416, 0.3321353793144226, 0.3187280297279358]}}
{"id": "WebQTest-1248", "question": "where is nineveh city", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is nineveh city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.0007763162138871849, -0.0740528255701065, -0.10177605599164963], "norm_scores": [0.3529481589794159, 0.3280102014541626, 0.31904157996177673]}}
{"id": "WebQTest-1249", "question": "what is the state flower of new mexico", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of new mexico [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.013480016961693764, -0.01678154617547989, -0.022314758971333504], "norm_scores": [0.33468231558799744, 0.3335791826248169, 0.33173850178718567]}}
{"id": "WebQTest-1250", "question": "where is the national ffa foundation located today", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the national ffa foundation located today [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.00771034462377429, -0.035307876765728, -0.04131156578660011], "norm_scores": [0.34016650915145874, 0.33090710639953613, 0.3289263844490051]}}
{"id": "WebQTest-1251", "question": "which countries speak germanic languages", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak germanic languages [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.015028752386569977, -0.030376572161912918, -0.035109374672174454], "norm_scores": [0.3372807502746582, 0.33214375376701355, 0.33057549595832825]}}
{"id": "WebQTest-1252", "question": "who plays eowyn in lord of the rings", "prediction": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["film.film.starring", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays eowyn in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.03337387368083, -0.03401479870080948, -0.03435990586876869], "norm_scores": [0.33351412415504456, 0.3333004415035248, 0.33318543434143066]}}
{"id": "WebQTest-1254", "question": "where is augusta masters 2012", "prediction": [["time.event.locations"], ["time.recurring_event.instances"], ["location.location.events"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is augusta masters 2012 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0021492380183190107, -0.05222136154770851, -0.06810898333787918], "norm_scores": [0.3463408052921295, 0.3294258117675781, 0.32423338294029236]}}
{"id": "WebQTest-1255", "question": "what type of cancer did eva peron die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.008138648234307766, -0.020236603915691376, -0.0373406782746315], "norm_scores": [0.33792972564697266, 0.33386608958244324, 0.3282041847705841]}}
{"id": "WebQTest-1256", "question": "who did terrell owens play for in 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did terrell owens play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.018659813329577446, -0.02198081463575363, -0.022118836641311646], "norm_scores": [0.334087073802948, 0.3329794108867645, 0.3329334855079651]}}
{"id": "WebQTest-1257", "question": "where is denmark south carolina", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.us_county.hud_county_place"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark south carolina [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.013520387932658195, -0.020935649052262306, -0.0515563003718853], "norm_scores": [0.3383761942386627, 0.33587634563446045, 0.32574746012687683]}}
{"id": "WebQTest-1259", "question": "what books did ernest hemingway write", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["film.film_story_contributor.film_story_credits"], ["book.book_edition.author_editor"], ["film.film.story_by"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did ernest hemingway write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.016655122861266136, -0.022446775808930397, -0.027381589636206627], "norm_scores": [0.3351705074310303, 0.3332349359989166, 0.3315945267677307]}}
{"id": "WebQTest-1260", "question": "what year did baltimore ravens win super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did baltimore ravens win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.012760493904352188, -0.013838134706020355, -0.03910408914089203], "norm_scores": [0.33636924624443054, 0.3360069692134857, 0.32762378454208374]}}
{"id": "WebQTest-1261", "question": "where did the battle of passchendaele happen", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of passchendaele happen [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.013152687810361385, -0.01319358590990305, -0.09113556891679764], "norm_scores": [0.3418874144554138, 0.341873437166214, 0.3162391185760498]}}
{"id": "WebQTest-1262", "question": "what part of the country is ohio considered", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.012362427078187466, -0.015930291265249252, -0.08107735216617584], "norm_scores": [0.3412932753562927, 0.3400777578353882, 0.3186289370059967]}}
{"id": "WebQTest-1263", "question": "what team is cristiano ronaldo play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is cristiano ronaldo play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.021271975710988045, -0.024961000308394432, -0.025199826806783676], "norm_scores": [0.33418017625808716, 0.3329496383666992, 0.33287012577056885]}}
{"id": "WebQTest-1264", "question": "what standard time is michigan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is michigan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-1.560763848829083e-05, -0.14207673072814941, -0.18874110281467438], "norm_scores": [0.37097740173339844, 0.32184821367263794, 0.307174414396286]}}
{"id": "WebQTest-1265", "question": "what was john tyler", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was john tyler [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.0033766436390578747, -0.04196015000343323, -0.049035169184207916], "norm_scores": [0.3427569270133972, 0.3297840356826782, 0.32745903730392456]}}
{"id": "WebQTest-1266", "question": "what did romo do", "prediction": [["american_football.football_player.position_s"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.position"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did romo do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.position </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.013958006165921688, -0.038852762430906296, -0.05138375982642174], "norm_scores": [0.34028899669647217, 0.33192217350006104, 0.3277888298034668]}}
{"id": "WebQTest-1267", "question": "what currency can you use in jamaica", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in jamaica [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0006729391170665622, -0.061442770063877106, -0.16218821704387665], "norm_scores": [0.35817989706993103, 0.3370615541934967, 0.30475860834121704]}}
{"id": "WebQTest-1268", "question": "where did jay moriarty die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jay moriarty die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.00026747368974611163, -0.09779327362775803, -0.10651134699583054], "norm_scores": [0.3563430905342102, 0.3232313096523285, 0.3204255998134613]}}
{"id": "WebQTest-1269", "question": "where was the boston tea party 1773", "prediction": [["time.event.locations"], ["location.location.events"], ["event.disaster.areas_affected"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the boston tea party 1773 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>"], "scores": [-0.006603750865906477, -0.02535824477672577, -0.06411184370517731], "norm_scores": [0.3418178856372833, 0.33546701073646545, 0.3227151334285736]}}
{"id": "WebQTest-1270", "question": "what drugs lindsay lohan abuse", "prediction": [["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["people.person.substance_abuse", "people.substance_abuse.substance"]], "ground_paths": [["base.popstra.rehab_stay.patient", "base.popstra.rehab_stay.substance_abused"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"], ["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drugs lindsay lohan abuse [/INST]", "raw_output": {"paths": ["<PATH> base.popstra.substance_abuse.abuser <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> base.popstra.celebrity.substance_abuse <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> people.person.substance_abuse <SEP> people.substance_abuse.substance </PATH>"], "scores": [-0.006857213098555803, -0.02538113296031952, -0.05399720370769501], "norm_scores": [0.34064579010009766, 0.33439379930496216, 0.3249603807926178]}}
{"id": "WebQTest-1271", "question": "when did carolina panthers go to superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did carolina panthers go to superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.009583837352693081, -0.01813257858157158, -0.03854226693511009], "norm_scores": [0.3375020921230316, 0.3346291780471802, 0.3278687298297882]}}
{"id": "WebQTest-1272", "question": "when did the burma cyclone happen", "prediction": [["meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the burma cyclone happen [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.008301174268126488, -0.020902248099446297, -0.0497569814324379], "norm_scores": [0.3393431305885315, 0.3350938558578491, 0.325562983751297]}}
{"id": "WebQTest-1274", "question": "who won the governor election in texas", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.political_appointer.appointees"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["film.film_location.featured_in_films", "film.film.subjects"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.us_state.capital", "people.deceased_person.place_of_death"], ["location.administrative_division.country", "people.person.nationality"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "symbols.name_source.namesakes"], ["people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the governor election in texas [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.024375542998313904, -0.02449973300099373, -0.024957364425063133], "norm_scores": [0.3334117829799652, 0.3333703875541687, 0.3332178592681885]}}
{"id": "WebQTest-1275", "question": "what does helen flanagan do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does helen flanagan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0010541001101955771, -0.05837162211537361, -0.09845837950706482], "norm_scores": [0.3506946563720703, 0.33115893602371216, 0.31814640760421753]}}
{"id": "WebQTest-1276", "question": "when are the summer and winter olympics held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.current_frequency"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when are the summer and winter olympics held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.008637010119855404, -0.01742401532828808, -0.08646655827760696], "norm_scores": [0.3428915739059448, 0.33989179134368896, 0.3172166049480438]}}
{"id": "WebQTest-1277", "question": "where is tennessee river", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tennessee river [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.004756850190460682, -0.034010063856840134, -0.075105682015419], "norm_scores": [0.34444287419319153, 0.33451277017593384, 0.3210444152355194]}}
{"id": "WebQTest-1278", "question": "what are some major events in venezuela", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.events"]], "ground_paths": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some major events in venezuela [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.016439657658338547, -0.02217365987598896, -0.02495984174311161], "norm_scores": [0.3349187970161438, 0.3330038785934448, 0.33207735419273376]}}
{"id": "WebQTest-1279", "question": "what guitar does corey taylor play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar does corey taylor play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>"], "scores": [-0.007848835550248623, -0.03209129348397255, -0.051558710634708405], "norm_scores": [0.3409152030944824, 0.33274996280670166, 0.3263348340988159]}}
{"id": "WebQTest-1280", "question": "who was president after franklin d. roosevelt", "prediction": [["government.us_president.vice_president"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after franklin d. roosevelt [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.01922818273305893, -0.0237219650298357, -0.03610658645629883], "norm_scores": [0.33570796251296997, 0.33420273661613464, 0.3300893008708954]}}
{"id": "WebQTest-1281", "question": "where did kate gosselin grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kate gosselin grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.002271049888804555, -0.039701756089925766, -0.10031764954328537], "norm_scores": [0.34844809770584106, 0.33564653992652893, 0.3159053921699524]}}
{"id": "WebQTest-1282", "question": "under what political system does the united states government operate", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["government.form_of_government.countries", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: under what political system does the united states government operate [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>"], "scores": [-0.004750714637339115, -0.02326149307191372, -0.12746617197990417], "norm_scores": [0.3488970994949341, 0.34249815344810486, 0.30860480666160583]}}
{"id": "WebQTest-1284", "question": "what to do in paris in 1 day", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in paris in 1 day [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.008488692343235016, -0.020258987322449684, -0.039373595267534256], "norm_scores": [0.3380793333053589, 0.3341233432292938, 0.3277973532676697]}}
{"id": "WebQTest-1285", "question": "what city is mt lassen in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.lake.outflow"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is mt lassen in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.lake.outflow </PATH>"], "scores": [-0.005498386919498444, -0.030269647017121315, -0.1174183189868927], "norm_scores": [0.3484746813774109, 0.3399485647678375, 0.3115767538547516]}}
{"id": "WebQTest-1287", "question": "what is the state motto of ohio", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state motto of ohio [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.017288530245423317, -0.01920670084655285, -0.023794395849108696], "norm_scores": [0.3342694044113159, 0.3336288332939148, 0.3321017026901245]}}
{"id": "WebQTest-1289", "question": "what type of music did mozart composed", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.composition.composer"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did mozart composed [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.00549361715093255, -0.030636876821517944, -0.06738229840993881], "norm_scores": [0.3430345952510834, 0.33451712131500244, 0.3224482536315918]}}
{"id": "WebQTest-1291", "question": "how many kids were there in the kennedy family", "prediction": [["people.person.parents"], ["people.person.children"], ["people.family.members"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many kids were there in the kennedy family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.family.members </PATH>"], "scores": [-0.009521537460386753, -0.01722077466547489, -0.0860142707824707], "norm_scores": [0.34261947870254517, 0.3399916887283325, 0.3173888623714447]}}
{"id": "WebQTest-1292", "question": "which barcelona airport is closest to the port", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which barcelona airport is closest to the port [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>"], "scores": [-0.006117740646004677, -0.029138384386897087, -0.04996579885482788], "norm_scores": [0.3407919406890869, 0.33303630352020264, 0.32617178559303284]}}
{"id": "WebQTest-1293", "question": "where has the english language evolved from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where has the english language evolved from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.0060457466170191765, -0.025853950530290604, -0.06348948925733566], "norm_scores": [0.3419315814971924, 0.3352251648902893, 0.3228432536125183]}}
{"id": "WebQTest-1294", "question": "what country did hitler invade that started ww2", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did hitler invade that started ww2 [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>"], "scores": [-0.02891472913324833, -0.03127787262201309, -0.03686074540019035], "norm_scores": [0.3344789147377014, 0.33368945121765137, 0.3318316340446472]}}
{"id": "WebQTest-1295", "question": "what does matt damon play in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["music.recording.featured_artists", "music.recording.releases"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["tv.tv_program.program_creator"], ["film.film.written_by"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.producer.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.languages", "tv.tv_program.languages"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does matt damon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.022249335423111916, -0.022337907925248146, -0.04499278962612152], "norm_scores": [0.3358606994152069, 0.3358309864997864, 0.3283082842826843]}}
{"id": "WebQTest-1296", "question": "what was scottie pippen known for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen known for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>"], "scores": [-0.015600965358316898, -0.02684168517589569, -0.0330364964902401], "norm_scores": [0.336526095867157, 0.33276447653770447, 0.33070942759513855]}}
{"id": "WebQTest-1297", "question": "what continent does canada belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.locations.continents.countries_within"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does canada belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.016724418848752975, -0.019520990550518036, -0.027407314628362656], "norm_scores": [0.33483099937438965, 0.3338959217071533, 0.33127307891845703]}}
{"id": "WebQTest-1298", "question": "what does george wilson do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does george wilson do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.004853407386690378, -0.031687162816524506, -0.06696387380361557], "norm_scores": [0.34325313568115234, 0.3341648280620575, 0.32258209586143494]}}
{"id": "WebQTest-1300", "question": "what city does fulham play in", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["location.location.containedby"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does fulham play in [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0236583910882473, -0.023760294541716576, -0.028243841603398323], "norm_scores": [0.3338537812232971, 0.33381977677345276, 0.33232641220092773]}}
{"id": "WebQTest-1301", "question": "what are fun things to do in sacramento ca", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are fun things to do in sacramento ca [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.014837822876870632, -0.01658698357641697, -0.026130637153983116], "norm_scores": [0.3347814679145813, 0.33419641852378845, 0.33102211356163025]}}
{"id": "WebQTest-1303", "question": "what other movies is josh hutcherson in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies is josh hutcherson in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.017573025077581406, -0.03479079529643059, -0.037954434752464294], "norm_scores": [0.33752375841140747, 0.33176207542419434, 0.3307141661643982]}}
{"id": "WebQTest-1304", "question": "what was hitler the leader of", "prediction": [["organization.organization_founder.organizations_founded"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was hitler the leader of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.017651421949267387, -0.03515896573662758, -0.04092048853635788], "norm_scores": [0.33787843585014343, 0.3320145010948181, 0.33010709285736084]}}
{"id": "WebQTest-1305", "question": "what awards did marilyn monroe won", "prediction": [["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"]], "ground_paths": [["people.person.profession", "award.award_discipline.awards_in_this_discipline"], ["award.award_nomination.award_nominee", "award.award_category.nominees"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nominee.award_nominations", "award.award_category.nominees"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_honor.award_winner", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards did marilyn monroe won [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>"], "scores": [-0.019743245095014572, -0.026221726089715958, -0.02879762277007103], "norm_scores": [0.335061252117157, 0.3328975737094879, 0.3320411443710327]}}
{"id": "WebQTest-1306", "question": "what kind of government does iran have after 1979", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does iran have after 1979 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00428840471431613, -0.022894959896802902, -0.13787296414375305], "norm_scores": [0.35007643699645996, 0.34362292289733887, 0.3063006103038788]}}
{"id": "WebQTest-1307", "question": "where did harper lee attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harper lee attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.015754345804452896, -0.021644849330186844, -0.021869800984859467], "norm_scores": [0.33466866612434387, 0.3327031135559082, 0.3326282501220703]}}
{"id": "WebQTest-1308", "question": "what college did jrr tolkien go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did jrr tolkien go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.015612196177244186, -0.02250177413225174, -0.03065028414130211], "norm_scores": [0.3357723355293274, 0.33346694707870483, 0.33076074719429016]}}
{"id": "WebQTest-1309", "question": "what art did wassily kandinsky do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["book.author.school_or_movement"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art did wassily kandinsky do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> book.author.school_or_movement </PATH>"], "scores": [-0.007985513657331467, -0.027714256197214127, -0.046780120581388474], "norm_scores": [0.3398571014404297, 0.33321788907051086, 0.32692497968673706]}}
{"id": "WebQTest-1310", "question": "who won the league cup in 2002", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship.events", "sports.sports_team.championships"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the league cup in 2002 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>"], "scores": [-0.021556323394179344, -0.023064900189638138, -0.025676842778921127], "norm_scores": [0.3339588940143585, 0.33345547318458557, 0.3325856328010559]}}
{"id": "WebQTest-1311", "question": "what was antoni gaudi inspired by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was antoni gaudi inspired by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.01038268767297268, -0.013639165088534355, -0.07536458224058151], "norm_scores": [0.3408510088920593, 0.33974283933639526, 0.3194062113761902]}}
{"id": "WebQTest-1312", "question": "in what country do people speak danish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in what country do people speak danish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.010368227027356625, -0.019261928275227547, -0.04704344645142555], "norm_scores": [0.33839401602745056, 0.33539777994155884, 0.3262082040309906]}}
{"id": "WebQTest-1313", "question": "who were demeter 's siblings", "prediction": [["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"]], "ground_paths": [["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "fictional_universe.fictional_character.species"], ["fictional_universe.romantic_involvement.partner", "fictional_universe.fictional_character.romantically_involved_with"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were demeter 's siblings [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.fictional_character.siblings </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>"], "scores": [-0.021323295310139656, -0.024597473442554474, -0.027584850788116455], "norm_scores": [0.3343934714794159, 0.33330038189888, 0.3323061764240265]}}
{"id": "WebQTest-1314", "question": "where should a family stay in paris", "prediction": [["travel.travel_destination.accommodation"], ["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["travel.travel_destination.accommodation"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where should a family stay in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.003630120074376464, -0.033075086772441864, -0.0973452553153038], "norm_scores": [0.34703826904296875, 0.3369687497615814, 0.31599298119544983]}}
{"id": "WebQTest-1316", "question": "what was the name of benjamin franklin wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.children"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of benjamin franklin wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.027073834091424942, -0.03388899937272072, -0.04295742139220238], "norm_scores": [0.3358578681945801, 0.3335767388343811, 0.33056536316871643]}}
{"id": "WebQTest-1317", "question": "what the largest city in spain", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the largest city in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>"], "scores": [-0.020569859072566032, -0.02697523683309555, -0.04763026535511017], "norm_scores": [0.3370501399040222, 0.3348981440067291, 0.32805171608924866]}}
{"id": "WebQTest-1319", "question": "who has pudge rodriguez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has pudge rodriguez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.011770730838179588, -0.017292868345975876, -0.029101429507136345], "norm_scores": [0.33587348461151123, 0.3340238332748413, 0.33010271191596985]}}
{"id": "WebQTest-1320", "question": "what is the milwaukee brewers stadium called", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], []], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers stadium called [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> </PATH>"], "scores": [-0.0028890406247228384, -0.030882185325026512, -0.11474913358688354], "norm_scores": [0.34884968400001526, 0.33921968936920166, 0.31193068623542786]}}
{"id": "WebQTest-1321", "question": "where did the gallipoli war take place", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gallipoli war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.007837342098355293, -0.022083209827542305, -0.09108901023864746], "norm_scores": [0.3441186249256134, 0.3392511010169983, 0.3166302740573883]}}
{"id": "WebQTest-1322", "question": "what other movies has tom hardy played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has tom hardy played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.02268255688250065, -0.035123519599437714, -0.0384102463722229], "norm_scores": [0.33647021651268005, 0.33231014013290405, 0.33121970295906067]}}
{"id": "WebQTest-1323", "question": "what to visit in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.015414442867040634, -0.01858825795352459, -0.026386406272649765], "norm_scores": [0.3349052369594574, 0.33384400606155396, 0.33125075697898865]}}
{"id": "WebQTest-1324", "question": "what college did john nash go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.author", "education.dissertation.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john nash go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.017380448058247566, -0.017459126189351082, -0.03549254313111305], "norm_scores": [0.335348516702652, 0.33532217144966125, 0.32932931184768677]}}
{"id": "WebQTest-1325", "question": "what language do maltese speak", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do maltese speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.015405233018100262, -0.02985485829412937, -0.03335399553179741], "norm_scores": [0.33694249391555786, 0.3321087956428528, 0.33094874024391174]}}
{"id": "WebQTest-1326", "question": "what did thomas edison do for the world", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas edison do for the world [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.01352259237319231, -0.020612016320228577, -0.036460209637880325], "norm_scores": [0.33667096495628357, 0.33429259061813354, 0.3290364146232605]}}
{"id": "WebQTest-1327", "question": "where is shakira from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is shakira from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.01154423225671053, -0.0185557808727026, -0.05984460934996605], "norm_scores": [0.3394593894481659, 0.33708760142326355, 0.32345306873321533]}}
{"id": "WebQTest-1328", "question": "what did hayes do", "prediction": [["people.person.profession"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hayes do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.006124373525381088, -0.04664855822920799, -0.04758631810545921], "norm_scores": [0.34250426292419434, 0.3289020359516144, 0.32859373092651367]}}
{"id": "WebQTest-1329", "question": "who played bilbo in the fellowship of the ring", "prediction": [["film.performance.character", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film.starring", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in the fellowship of the ring [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.03496476262807846, -0.04002904146909714, -0.04154285043478012], "norm_scores": [0.3346281051635742, 0.33293774724006653, 0.33243411779403687]}}
{"id": "WebQTest-1330", "question": "what is the most populated state in united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.administrative_division.first_level_division_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most populated state in united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.008637484163045883, -0.027130309492349625, -0.03496178239583969], "norm_scores": [0.3383297622203827, 0.3321305811405182, 0.3295396566390991]}}
{"id": "WebQTest-1331", "question": "who is my state senator arizona", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["base.aareas.schema.administrative_area.capital", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.capital", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my state senator arizona [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.023082677274942398, -0.026771320030093193, -0.029448673129081726], "norm_scores": [0.33445125818252563, 0.33321985602378845, 0.3323288857936859]}}
{"id": "WebQTest-1332", "question": "when does the vietnam war end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does the vietnam war end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.015380211174488068, -0.032524336129426956, -0.03321598842740059], "norm_scores": [0.33723121881484985, 0.33149898052215576, 0.331269770860672]}}
{"id": "WebQTest-1333", "question": "where does the tennessee river go", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"], ["location.location.containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the tennessee river go [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012385555543005466, -0.03345751389861107, -0.03738067299127579], "norm_scores": [0.338470995426178, 0.3314133584499359, 0.3301157057285309]}}
{"id": "WebQTest-1334", "question": "who did tom hanks play in apollo 13", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["spaceflight.astronaut.missions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom hanks play in apollo 13 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.023231571540236473, -0.02842521108686924, -0.031998518854379654], "norm_scores": [0.33488595485687256, 0.3331511914730072, 0.33196285367012024]}}
{"id": "WebQTest-1335", "question": "what language do they in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.005468588322401047, -0.03507707640528679, -0.04332221671938896], "norm_scores": [0.34086886048316956, 0.3309241831302643, 0.3282068967819214]}}
{"id": "WebQTest-1336", "question": "what is utah 's state capitol", "prediction": [["base.aareas.schema.administrative_area.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["base.biblioness.bibs_location.city"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is utah 's state capitol [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.0198932234197855, -0.02526337280869484, -0.029906637966632843], "norm_scores": [0.3350442051887512, 0.3332498073577881, 0.3317060172557831]}}
{"id": "WebQTest-1337", "question": "what all did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>"], "scores": [-0.006496187299489975, -0.02572067268192768, -0.07854131609201431], "norm_scores": [0.3434717059135437, 0.3369317054748535, 0.3195965886116028]}}
{"id": "WebQTest-1339", "question": "what team does jeremy lin play for 2013", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jeremy lin play for 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014614436775445938, -0.018117515370249748, -0.02218199521303177], "norm_scores": [0.33456405997276306, 0.3333941102027893, 0.33204180002212524]}}
{"id": "WebQTest-1340", "question": "what is steve nash nationality", "prediction": [["people.person.nationality"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.country"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.country"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is steve nash nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.country </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.country </PATH>"], "scores": [-0.0021622213535010815, -0.05677083507180214, -0.059612661600112915], "norm_scores": [0.3458981513977051, 0.3275156021118164, 0.32658621668815613]}}
{"id": "WebQTest-1341", "question": "who are the u s senators of pennsylvania", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_burial"], ["location.location.containedby", "people.deceased_person.place_of_burial"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the u s senators of pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.023594582453370094, -0.028028268367052078, -0.028057901188731194], "norm_scores": [0.33432263135910034, 0.3328436017036438, 0.33283376693725586]}}
{"id": "WebQTest-1342", "question": "what kind of language does france speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does france speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014033556915819645, -0.02197793312370777, -0.027270328253507614], "norm_scores": [0.33569014072418213, 0.33303385972976685, 0.331275999546051]}}
{"id": "WebQTest-1343", "question": "what degree did martin luther king earn", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"], ["education.education.student", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degree did martin luther king earn [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.00482788123190403, -0.023355547338724136, -0.05248820409178734], "norm_scores": [0.3407038152217865, 0.33444949984550476, 0.32484665513038635]}}
{"id": "WebQTest-1344", "question": "what two states is st louis in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.time_zones"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two states is st louis in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.time_zones </PATH>"], "scores": [-0.0036068151239305735, -0.05740603432059288, -0.06076819449663162], "norm_scores": [0.3457738161087036, 0.32766297459602356, 0.32656317949295044]}}
{"id": "WebQTest-1348", "question": "what team did peyton manning 's dad play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did peyton manning 's dad play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.008513001725077629, -0.01620541512966156, -0.03826258331537247], "norm_scores": [0.3374928832054138, 0.3349066972732544, 0.3276004493236542]}}
{"id": "WebQTest-1350", "question": "what did jesse owens won", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_medal_honor.event"], ["people.person.nationality", "olympics.olympic_games.participating_countries"]], "ground_paths": [["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesse owens won [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.018938690423965454, -0.02059619128704071, -0.0435841828584671], "norm_scores": [0.3362475037574768, 0.3356906473636627, 0.3280618190765381]}}
{"id": "WebQTest-1351", "question": "what is the name of the currency used in peru", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in peru [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.009201012551784515, -0.016304045915603638, -0.11731230467557907], "norm_scores": [0.3459669053554535, 0.3435181975364685, 0.310514897108078]}}
{"id": "WebQTest-1353", "question": "what are the four nations of the united kingdom", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four nations of the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.011058188043534756, -0.02300850674510002, -0.024596599861979485], "norm_scores": [0.33617135882377625, 0.33217790722846985, 0.3316507637500763]}}
{"id": "WebQTest-1354", "question": "who is judy garland father", "prediction": [["people.person.parents"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is judy garland father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.0074206506833434105, -0.024154994636774063, -0.09837242215871811], "norm_scores": [0.3452482223510742, 0.33951878547668457, 0.3152329623699188]}}
{"id": "WebQTest-1355", "question": "what school did robert f kennedy go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did robert f kennedy go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015318826772272587, -0.021387940272688866, -0.02560543641448021], "norm_scores": [0.33515262603759766, 0.33312469720840454, 0.3317227065563202]}}
{"id": "WebQTest-1356", "question": "what drug killed marilyn monroe", "prediction": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"], ["medicine.disease.parent_disease"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drug killed marilyn monroe [/INST]", "raw_output": {"paths": ["<PATH> people.cause_of_death.people </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.parent_disease </PATH>"], "scores": [-0.007082737982273102, -0.023506473749876022, -0.05736235901713371], "norm_scores": [0.34075337648391724, 0.3352026343345642, 0.32404401898384094]}}
{"id": "WebQTest-1357", "question": "where is the fukushima daiichi nuclear power station", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear power station [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00513326283544302, -0.04173446446657181, -0.044630710035562515], "norm_scores": [0.3418414294719696, 0.32955583930015564, 0.32860276103019714]}}
{"id": "WebQTest-1358", "question": "what is los angeles california time zone", "prediction": [["location.location.time_zones"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is los angeles california time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.1391073712729849e-05, -0.16296598315238953, -0.1836051642894745], "norm_scores": [0.37286949157714844, 0.3168010115623474, 0.31032952666282654]}}
{"id": "WebQTest-1359", "question": "what did nicolas cage name his son", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicolas cage name his son [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.01158932689577341, -0.015417094342410564, -0.11910143494606018], "norm_scores": [0.3455132246017456, 0.3441932201385498, 0.3102935552597046]}}
{"id": "WebQTest-1361", "question": "what currency should take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.007783686276525259, -0.03258580341935158, -0.04905952885746956], "norm_scores": [0.3407076895236969, 0.3323613405227661, 0.3269309401512146]}}
{"id": "WebQTest-1362", "question": "who is the state governor of florida", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of florida [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.014535386115312576, -0.022855961695313454, -0.024802515283226967], "norm_scores": [0.3354017436504364, 0.33262255787849426, 0.3319757580757141]}}
{"id": "WebQTest-1363", "question": "who was jessica simpson married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["base.popstra.celebrity.infidelity_participant", "base.popstra.celebrity.infidelity_victim"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"], ["music.artist.genre", "music.artist.genre"], ["base.saturdaynightlive.snl_episode.host", "base.saturdaynightlive.snl_host.episodes_hosted"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["broadcast.artist.content", "broadcast.artist.content"], ["people.person.languages", "people.person.languages"], ["music.artist.album", "music.album.featured_artists"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was jessica simpson married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.02324858121573925, -0.03238198161125183, -0.04342276602983475], "norm_scores": [0.33659425377845764, 0.33353397250175476, 0.3298717737197876]}}
{"id": "WebQTest-1364", "question": "what do they speak in scotland", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014866890385746956, -0.015569861978292465, -0.03814331814646721], "norm_scores": [0.3359887897968292, 0.3357526659965515, 0.3282585144042969]}}
{"id": "WebQTest-1365", "question": "what did john dalton study", "prediction": [["people.person.profession"], ["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john dalton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.012829409912228584, -0.023926304653286934, -0.03460929915308952], "norm_scores": [0.3369930684566498, 0.33327415585517883, 0.329732745885849]}}
{"id": "WebQTest-1366", "question": "what songs do zac brown band sing", "prediction": [["music.recording.artist"], ["music.artist.album"], ["music.album.artist"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["music.featured_artist.recordings"], ["music.music_video.artist"], ["music.artist.album"], ["music.album.artist"], ["music.recording.artist"], ["music.recording.featured_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs do zac brown band sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.014161258935928345, -0.029801495373249054, -0.05200831964612007], "norm_scores": [0.3392888009548187, 0.33402350544929504, 0.32668769359588623]}}
{"id": "WebQTest-1367", "question": "where is glastonbury england", "prediction": [["location.location.containedby"], ["location.location.contains"], ["time.event.locations"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is glastonbury england [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.011209149844944477, -0.017283190041780472, -0.09360804408788681], "norm_scores": [0.34307095408439636, 0.3409934341907501, 0.3159356117248535]}}
{"id": "WebQTest-1368", "question": "who formed the american federation of labor", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "people.person.nationality"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who formed the american federation of labor [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>"], "scores": [-0.0015767718432471156, -0.04943246394395828, -0.07778654247522354], "norm_scores": [0.3472350835800171, 0.33100923895835876, 0.32175561785697937]}}
{"id": "WebQTest-1369", "question": "what to visit in denver", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in denver [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013482000678777695, -0.023951232433319092, -0.024623438715934753], "norm_scores": [0.33573880791664124, 0.3322422206401825, 0.3320189416408539]}}
{"id": "WebQTest-1370", "question": "where did thomas hobbes live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death"]], "ground_paths": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas hobbes live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.026097435504198074, -0.03169117867946625, -0.03185597062110901], "norm_scores": [0.3345958888530731, 0.3327294588088989, 0.33267465233802795]}}
{"id": "WebQTest-1373", "question": "where was john f kennedy when he got shot", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john f kennedy when he got shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.004119537770748138, -0.05151086300611496, -0.05249718204140663], "norm_scores": [0.3440578579902649, 0.3281328082084656, 0.3278093636035919]}}
{"id": "WebQTest-1374", "question": "what type of government does italy have 2011", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does italy have 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00891830027103424, -0.013712790794670582, -0.142289936542511], "norm_scores": [0.3483887314796448, 0.34672242403030396, 0.30488887429237366]}}
{"id": "WebQTest-1376", "question": "what province is montreal canada located", "prediction": [["location.province.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is montreal canada located [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.024103453382849693, -0.028452584519982338, -0.030586035922169685], "norm_scores": [0.3345378041267395, 0.3330860435962677, 0.3323761522769928]}}
{"id": "WebQTest-1378", "question": "who does kellan lutz play in prom night", "prediction": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kellan lutz play in prom night [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.023694565519690514, -0.024457309395074844, -0.029363540932536125], "norm_scores": [0.33404767513275146, 0.3337930142879486, 0.3321593403816223]}}
{"id": "WebQTest-1379", "question": "what did franz liszt do to earn a living", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did franz liszt do to earn a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.0010272689396515489, -0.06027213856577873, -0.09508214145898819], "norm_scores": [0.3505440056324005, 0.33037930727005005, 0.31907668709754944]}}
{"id": "WebQTest-1380", "question": "what was nikola tesla known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["law.inventor.inventions"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> law.inventor.inventions </PATH>"], "scores": [-0.002278002444654703, -0.049908872693777084, -0.06794571876525879], "norm_scores": [0.3460294306278229, 0.32993409037590027, 0.32403647899627686]}}
{"id": "WebQTest-1381", "question": "what is the zip code for princeton nj", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for princeton nj [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00032141624251380563, -0.07817492634057999, -0.11753812432289124], "norm_scores": [0.35530388355255127, 0.32869160175323486, 0.31600457429885864]}}
{"id": "WebQTest-1382", "question": "what other countries border argentina", "prediction": [["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["time.event.locations", "location.location.events"], ["language.human_language.main_country", "language.language_family.geographic_distribution"], ["location.country.form_of_government", "location.country.form_of_government"], ["geography.river.basin_countries", "location.location.partially_contains"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["film.film.featured_film_locations", "media_common.netflix_title.netflix_genres"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.country.languages_spoken", "location.country.official_language"], ["location.location.partially_containedby", "location.location.containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["book.book_subject.works", "book.book_subject.works"], ["film.film.featured_film_locations", "film.film.country"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.locations.continents.counties_within", "base.locations.counties_parishes_and_boroughs.coninent"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.primarily_containedby", "location.location.containedby"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["time.event.locations", "time.event.locations"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.contains", "location.location.contains"], ["location.location.contains", "location.location.containedby"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries border argentina [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03967796638607979, -0.04221806302666664, -0.045407894998788834], "norm_scores": [0.33425256609916687, 0.3334046006202698, 0.33234280347824097]}}
{"id": "WebQTest-1383", "question": "what language does people in iceland speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does people in iceland speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.015922317281365395, -0.017364252358675003, -0.04169604182243347], "norm_scores": [0.3363476097583771, 0.3358629643917084, 0.32778942584991455]}}
{"id": "WebQTest-1384", "question": "what is the predominant religion in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"]], "ground_paths": [["people.person.quotations", "people.person.religion"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["people.person.nationality", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["symbols.namesake.named_after", "religion.religion.notable_figures"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the predominant religion in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>"], "scores": [-0.003988629672676325, -0.028238924220204353, -0.05100412666797638], "norm_scores": [0.3412836492061615, 0.33310696482658386, 0.32560938596725464]}}
{"id": "WebQTest-1385", "question": "who has coached the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.historical_coaching_staff", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"]], "ground_paths": [["american_football.football_team.historical_coaching_staff", "american_football.football_coach.coaching_history"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has coached the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>"], "scores": [-0.006649606861174107, -0.032751332968473434, -0.03444454446434975], "norm_scores": [0.3393484055995941, 0.3306054174900055, 0.330046147108078]}}
{"id": "WebQTest-1387", "question": "what time zone is toronto gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is toronto gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0002158821007469669, -0.07528512179851532, -0.16743725538253784], "norm_scores": [0.3605303466320038, 0.33445653319358826, 0.30501312017440796]}}
{"id": "WebQTest-1388", "question": "what country did toussaint l'ouverture help free", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved"], ["base.activism.activist.area_of_activism"]], "ground_paths": [["military.military_service.military_person", "military.armed_force.personnel"], ["military.military_service.military_person", "military.military_service.military_force"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did toussaint l'ouverture help free [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.018080541864037514, -0.02607642114162445, -0.04930597543716431], "norm_scores": [0.3376902937889099, 0.3350009322166443, 0.327308714389801]}}
{"id": "WebQTest-1389", "question": "what jobs did robert burns have", "prediction": [["people.person.profession"], ["influence.influence_node.influenced", "people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did robert burns have [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0024039491545408964, -0.05556241422891617, -0.06398138403892517], "norm_scores": [0.34619924426078796, 0.3282764256000519, 0.3255242705345154]}}
{"id": "WebQTest-1390", "question": "who won utah attorney general", "prediction": [["base.aareas.schema.administrative_area.administrative_children", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["base.biblioness.bibs_location.city", "people.person.place_of_birth"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won utah attorney general [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.015809107571840286, -0.02575649507343769, -0.028819825500249863], "norm_scores": [0.3358888328075409, 0.33256417512893677, 0.33154699206352234]}}
{"id": "WebQTest-1392", "question": "what the language spoken in indonesia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the language spoken in indonesia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.015061438083648682, -0.023533357307314873, -0.026433981955051422], "norm_scores": [0.335541695356369, 0.33271101117134094, 0.3317473530769348]}}
{"id": "WebQTest-1394", "question": "who coaches the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sport.team_coaches"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who coaches the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.team_coaches </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.0062821172177791595, -0.03681798279285431, -0.037315722554922104], "norm_scores": [0.34020909667015076, 0.32997754216194153, 0.3298133313655853]}}
{"id": "WebQTest-1395", "question": "what other countries does south africa share borders with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["time.event.locations", "location.location.events"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.languages_spoken", "language.human_language.main_country"], ["location.location.containedby", "base.locations.continents.countries_within"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["location.country.official_language", "location.country.official_language"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.location.containedby", "location.location.containedby"], ["location.country.currency_used", "location.country.currency_used"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_contains"], ["film.film.featured_film_locations", "film.film.subjects"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.containedby", "base.locations.countries.continent"], ["government.form_of_government.countries", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.location.partially_contains", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["time.event.locations", "time.event.locations"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["common.topic.notable_types", "common.topic.notable_types"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.country.official_language", "language.human_language.main_country"], ["location.country.languages_spoken", "location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries does south africa share borders with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.032888635993003845, -0.0340297557413578, -0.04390581697225571], "norm_scores": [0.3346829116344452, 0.334301233291626, 0.3310158848762512]}}
{"id": "WebQTest-1396", "question": "who played darth vader at the end of return of the jedi", "prediction": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader at the end of return of the jedi [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02246934361755848, -0.026604238897562027, -0.03226403892040253], "norm_scores": [0.334881991147995, 0.33350011706352234, 0.33161792159080505]}}
{"id": "WebQTest-1398", "question": "what movies did joan crawford play in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.producer.film"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.deceased_person.place_of_death", "film.film.featured_film_locations"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did joan crawford play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.02377418242394924, -0.02819976955652237, -0.03476833552122116], "norm_scores": [0.3350476324558258, 0.33356812596321106, 0.33138424158096313]}}
{"id": "WebQTest-1399", "question": "what products does the united states export", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products does the united states export [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.023278994485735893, -0.026262642815709114, -0.036461398005485535], "norm_scores": [0.3351290822029114, 0.3341306447982788, 0.3307402729988098]}}
{"id": "WebQTest-1402", "question": "what is the kentucky state bird", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the kentucky state bird [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.01408342830836773, -0.02240024507045746, -0.028459709137678146], "norm_scores": [0.3358585238456726, 0.3330768346786499, 0.3310646712779999]}}
{"id": "WebQTest-1403", "question": "who was canadian prime minister in 1993", "prediction": [["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["base.firsts.first_achievement.achievement", "base.firsts.first_achievement.first"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was canadian prime minister in 1993 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.007677947636693716, -0.016784807667136192, -0.028356321156024933], "norm_scores": [0.33664724230766296, 0.3335953652858734, 0.3297573924064636]}}
{"id": "WebQTest-1405", "question": "what there to do in salt lake city utah", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what there to do in salt lake city utah [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.014444106258451939, -0.014676197431981564, -0.030988521873950958], "norm_scores": [0.33519259095191956, 0.33511480689048767, 0.32969263195991516]}}
{"id": "WebQTest-1408", "question": "who is the current ohio state senator", "prediction": [["government.political_district.representatives", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current ohio state senator [/INST]", "raw_output": {"paths": ["<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.country.first_level_divisions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.023584432899951935, -0.026975659653544426, -0.027314482256770134], "norm_scores": [0.3341250717639923, 0.33299386501312256, 0.3328810930252075]}}
{"id": "WebQTest-1409", "question": "what are the major languages spoken in the united kingdom", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01577874831855297, -0.02240118198096752, -0.023177048191428185], "norm_scores": [0.33489298820495605, 0.3326825201511383, 0.33242449164390564]}}
{"id": "WebQTest-1410", "question": "what did st. nicholas do in his life", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["base.saints.saint.venerated_in"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st. nicholas do in his life [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.saints.saint.venerated_in </PATH>"], "scores": [-0.017335109412670135, -0.036702025681734085, -0.03846583887934685], "norm_scores": [0.33784806728363037, 0.33136793971061707, 0.3307839632034302]}}
{"id": "WebQTest-1414", "question": "what happened to pope john paul ii", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.deceased_people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to pope john paul ii [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.deceased_people </PATH>"], "scores": [-0.0006402453291229904, -0.06558604538440704, -0.11493577063083649], "norm_scores": [0.353467732667923, 0.33124107122421265, 0.31529122591018677]}}
{"id": "WebQTest-1416", "question": "what was franklin pierce famous for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin pierce famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.01142462994903326, -0.03161263093352318, -0.03851136937737465], "norm_scores": [0.33860528469085693, 0.33183804154396057, 0.3295566737651825]}}
{"id": "WebQTest-1417", "question": "when was the last time the boston bruins went to the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the boston bruins went to the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.011144468560814857, -0.015055150724947453, -0.029399001970887184], "norm_scores": [0.33579492568969727, 0.3344843089580536, 0.32972076535224915]}}
{"id": "WebQTest-1418", "question": "who is meredith gray married to in real life", "prediction": [["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is meredith gray married to in real life [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.020707085728645325, -0.026789475232362747, -0.034502044320106506], "norm_scores": [0.3355439305305481, 0.3335091769695282, 0.3309468924999237]}}
{"id": "WebQTest-1419", "question": "what inspired michelangelo buonarroti", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michelangelo buonarroti [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.005573390983045101, -0.02003498561680317, -0.09723519533872604], "norm_scores": [0.3450588583946228, 0.3401046693325043, 0.31483644247055054]}}
{"id": "WebQTest-1420", "question": "what does monsanto own", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization.child"], ["business.business_operation.industry"]], "ground_paths": [["business.business_operation.industry"], ["business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does monsanto own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>", "<PATH> business.business_operation.industry </PATH>"], "scores": [-0.03388657793402672, -0.039580538868904114, -0.04120674356818199], "norm_scores": [0.33478087186813354, 0.33288002014160156, 0.33233916759490967]}}
{"id": "WebQTest-1421", "question": "who did woody harrelson play on cheers", "prediction": [["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["people.person.profession", "fictional_universe.fictional_character.occupation"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did woody harrelson play on cheers [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.024943675845861435, -0.025702284649014473, -0.025966137647628784], "norm_scores": [0.3335312306880951, 0.3332783281803131, 0.333190381526947]}}
{"id": "WebQTest-1422", "question": "what language do they speak in sweden yahoo", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in sweden yahoo [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012886532582342625, -0.019320005550980568, -0.03357556834816933], "norm_scores": [0.3363480567932129, 0.33419111371040344, 0.32946082949638367]}}
{"id": "WebQTest-1423", "question": "what language did they speak in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did they speak in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.004439399112015963, -0.03713648393750191, -0.04759225621819496], "norm_scores": [0.3418106138706207, 0.3308151364326477, 0.3273742198944092]}}
{"id": "WebQTest-1424", "question": "what form of government do we have in the philippines", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["form.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government do we have in the philippines [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> form.form_of_government.countries </PATH>"], "scores": [-0.00870335940271616, -0.013998163864016533, -0.15592944622039795], "norm_scores": [0.34991729259490967, 0.3480694591999054, 0.30201321840286255]}}
{"id": "WebQTest-1425", "question": "what colleges did mitt romney attend", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.education", "education.education.institution"], ["people.person.religion", "religion.religion.organizations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did mitt romney attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.018090836703777313, -0.023213444277644157, -0.02497764676809311], "norm_scores": [0.33466896414756775, 0.33295896649360657, 0.3323720693588257]}}
{"id": "WebQTest-1426", "question": "where is ibrahim afellay from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ibrahim afellay from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004871721845120192, -0.03809639438986778, -0.04907795786857605], "norm_scores": [0.3419881761074066, 0.3308124244213104, 0.32719945907592773]}}
{"id": "WebQTest-1427", "question": "who was the secretary of state when andrew jackson was president", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.appointed_by", "government.government_position_held.office_holder"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["government.us_vice_president.to_president"], ["people.person.profession", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["government.political_appointer.appointees", "government.government_position_held.office_holder"], ["people.person.nationality", "people.person.nationality"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["government.government_position_held.appointed_by", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the secretary of state when andrew jackson was president [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.02213292382657528, -0.0246819406747818, -0.029092516750097275], "norm_scores": [0.33439013361930847, 0.33353886008262634, 0.3320709764957428]}}
{"id": "WebQTest-1428", "question": "who does ron stoppable 's voice", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ron stoppable 's voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.02158496528863907, -0.03010205179452896, -0.03813159093260765], "norm_scores": [0.33612221479415894, 0.3332715630531311, 0.33060625195503235]}}
{"id": "WebQTest-1429", "question": "what currency is accepted in new zealand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is accepted in new zealand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.008141758851706982, -0.0201635230332613, -0.11956408619880676], "norm_scores": [0.34690770506858826, 0.3427622318267822, 0.3103300631046295]}}
{"id": "WebQTest-1432", "question": "what places are in the west midlands", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.contains"], ["location.location.containedby"], ["location.location.containedby", "common.topic.image"], [], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places are in the west midlands [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.0055108508095145226, -0.03666939586400986, -0.048262763768434525], "norm_scores": [0.3415917754173279, 0.3311123847961426, 0.32729583978652954]}}
{"id": "WebQTest-1433", "question": "what to do with kids in asheville nc", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in asheville nc [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.007975411601364613, -0.020354121923446655, -0.0388585664331913], "norm_scores": [0.3381478488445282, 0.3339878022670746, 0.3278643786907196]}}
{"id": "WebQTest-1434", "question": "who is in the american league in baseball", "prediction": [["baseball.baseball_team.league"], ["baseball.baseball_league.teams"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["sports.sports_league.sport", "sports.sports_team.sport"], ["sports.sports_league_participation.league", "sports.sports_team.league"], ["baseball.baseball_league.teams", "sports.sports_team.previously_known_as"], ["baseball.baseball_league.teams", "sports.defunct_sports_team.later_known_as"], ["sports.sports_league.sport", "sports.defunct_sports_team.sport"], ["sports.sports_league.teams", "sports.sports_team.league"], ["sports.sports_league.teams", "sports.sports_league_participation.team"], ["sports.sports_league_participation.league", "sports.sports_league_participation.team"], ["baseball.baseball_team.league", "sports.sports_team.previously_known_as"], ["baseball.baseball_team.league"], ["baseball.baseball_team.league", "sports.defunct_sports_team.later_known_as"], ["baseball.baseball_league.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in the american league in baseball [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.league </PATH>", "<PATH> baseball.baseball_league.teams </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.011056415736675262, -0.026900433003902435, -0.043750494718551636], "norm_scores": [0.33874017000198364, 0.3334154486656189, 0.32784441113471985]}}
{"id": "WebQTest-1435", "question": "who did kurt warner play 4", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.passing", "american_football.player_passing_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kurt warner play 4 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.02465003728866577, -0.028901923447847366, -0.03358600661158562], "norm_scores": [0.3347996473312378, 0.3333791494369507, 0.3318212032318115]}}
{"id": "WebQTest-1436", "question": "who is dirk nowitzki wife", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dirk nowitzki wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.033635735511779785, -0.039772745221853256, -0.04011308401823044], "norm_scores": [0.3347364068031311, 0.3326883912086487, 0.3325752019882202]}}
{"id": "WebQTest-1441", "question": "where all did picasso live", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["influence.influence_node.influenced_by", "people.person.nationality"], ["people.person.parents", "people.person.nationality"], ["symbols.namesake.named_after", "film.film.country"], ["influence.influence_node.influenced", "people.person.nationality"], ["book.written_work.subjects", "book.written_work.subjects"], ["symbols.name_source.namesakes", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["film.film_subject.films", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where all did picasso live [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.024718236178159714, -0.027301112189888954, -0.031040852889418602], "norm_scores": [0.3343231976032257, 0.3334607779979706, 0.3322160542011261]}}
{"id": "WebQTest-1442", "question": "what currency shall i take to croatia", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency shall i take to croatia [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.008523421362042427, -0.019731072708964348, -0.07993429899215698], "norm_scores": [0.34247347712516785, 0.33865654468536377, 0.318869948387146]}}
{"id": "WebQTest-1443", "question": "what team is raul ibanez on", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is raul ibanez on [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.015617627650499344, -0.019956065341830254, -0.024586258456110954], "norm_scores": [0.3348129391670227, 0.33336350321769714, 0.33182355761528015]}}
{"id": "WebQTest-1445", "question": "what tv series did mark harmon star in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.languages", "tv.tv_program.languages"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.languages", "tv.tv_program.languages"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv series did mark harmon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.languages <SEP> tv.tv_program.languages </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>"], "scores": [-0.020587433129549026, -0.024913353845477104, -0.03194325789809227], "norm_scores": [0.3350766599178314, 0.3336302936077118, 0.33129310607910156]}}
{"id": "WebQTest-1446", "question": "what year did jimmy the rev sullivan die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"], ["people.person.deceased_place_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did jimmy the rev sullivan die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_place_of_death </PATH>"], "scores": [-0.0007661147392354906, -0.06449130177497864, -0.07640647143125534], "norm_scores": [0.34898990392684937, 0.32744425535202026, 0.32356584072113037]}}
{"id": "WebQTest-1447", "question": "who plays anakin skywalker in star wars 1", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.starring", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film.language", "people.person.languages"], ["film.performance.character", "film.performance.actor"], ["film.film.country", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays anakin skywalker in star wars 1 [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.015969660133123398, -0.02271178551018238, -0.024142492562532425], "norm_scores": [0.33499258756637573, 0.3327415883541107, 0.3322658836841583]}}
{"id": "WebQTest-1448", "question": "where does michelle pfeiffer live now", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth", "location.us_county.county_seat"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does michelle pfeiffer live now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.01588435284793377, -0.03172041103243828, -0.032496146857738495], "norm_scores": [0.33694833517074585, 0.33165442943573, 0.33139726519584656]}}
{"id": "WebQTest-1449", "question": "what was laura ingalls wilder famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["book.written_work.author"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was laura ingalls wilder famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.0034973130095750093, -0.04659562557935715, -0.05300023406744003], "norm_scores": [0.3436993360519409, 0.3292011320590973, 0.327099472284317]}}
{"id": "WebQTest-1452", "question": "where is dwight howard now", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwight howard now [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.019851600751280785, -0.02177288569509983, -0.023523276671767235], "norm_scores": [0.33395498991012573, 0.3333139717578888, 0.3327310383319855]}}
{"id": "WebQTest-1453", "question": "who is nick lachey married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"], ["people.person.languages", "people.person.languages"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nick lachey married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.02069256454706192, -0.03554781898856163, -0.04312874749302864], "norm_scores": [0.33748799562454224, 0.3325115740299225, 0.3300003707408905]}}
{"id": "WebQTest-1454", "question": "where did madonna grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did madonna grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.007812011521309614, -0.03453467786312103, -0.03769398480653763], "norm_scores": [0.33965182304382324, 0.33069562911987305, 0.32965248823165894]}}
{"id": "WebQTest-1455", "question": "what capital city of brazil", "prediction": [["location.country.capital"], ["location.location.containedby"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital city of brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.004029421601444483, -0.042737796902656555, -0.07975352555513382], "norm_scores": [0.3461281657218933, 0.33298611640930176, 0.32088571786880493]}}
{"id": "WebQTest-1456", "question": "what college did john stockton go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john stockton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012714956887066364, -0.021596340462565422, -0.025436466559767723], "norm_scores": [0.33573755621910095, 0.33276891708374023, 0.3314934968948364]}}
{"id": "WebQTest-1457", "question": "what kind of currency does mexico use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does mexico use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.002524238545447588, -0.04480622336268425, -0.07188951224088669], "norm_scores": [0.345831036567688, 0.3315134644508362, 0.32265546917915344]}}
{"id": "WebQTest-1458", "question": "who owns chrysler corporation 2011", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["organization.organization.parent", "organization.organization.child"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["organization.organization.parent", "organization.organization.child"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns chrysler corporation 2011 [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization_relationship.child <SEP> organization.organization_relationship.parent </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>"], "scores": [-0.016550136730074883, -0.022731296718120575, -0.028036529198288918], "norm_scores": [0.3352985084056854, 0.3332323729991913, 0.33146917819976807]}}
{"id": "WebQTest-1459", "question": "what are some books that mark twain wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["book.book_subject.works"], ["film.film.story_by"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["film.film_story_contributor.film_story_credits"], ["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some books that mark twain wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.011469547636806965, -0.0247240848839283, -0.03373308107256889], "norm_scores": [0.33728909492492676, 0.3328480124473572, 0.3298628330230713]}}
{"id": "WebQTest-1460", "question": "what was gregor mendel contribution", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was gregor mendel contribution [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.008960225619375706, -0.04488025978207588, -0.05603950470685959], "norm_scores": [0.3426148593425751, 0.33052653074264526, 0.32685860991477966]}}
{"id": "WebQTest-1461", "question": "who has ray allen dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has ray allen dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.01866709627211094, -0.031158117577433586, -0.04385483264923096], "norm_scores": [0.33752840757369995, 0.33333855867385864, 0.3291330337524414]}}
{"id": "WebQTest-1462", "question": "who is running for vice president with barack obama 2012", "prediction": [["government.election_campaign.election", "government.election_campaign.candidate"], ["government.election_campaign.election", "government.politician.election_campaigns"], ["government.election_campaign.candidate", "government.election_campaign.candidate"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is running for vice president with barack obama 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.election <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election_campaign.election <SEP> government.politician.election_campaigns </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.candidate </PATH>"], "scores": [-0.01609155535697937, -0.030245181173086166, -0.031080910935997963], "norm_scores": [0.33657923340797424, 0.33184897899627686, 0.3315717577934265]}}
{"id": "WebQTest-1463", "question": "what language do people from the uk speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from the uk speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.015520303510129452, -0.017291126772761345, -0.040318142622709274], "norm_scores": [0.3362771272659302, 0.3356821835041046, 0.3280407190322876]}}
{"id": "WebQTest-1464", "question": "what to do with my kids in toronto", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["film.film_festival.location"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with my kids in toronto [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01391393132507801, -0.014189894311130047, -0.0335465706884861], "norm_scores": [0.3355385959148407, 0.3354460299015045, 0.3290153443813324]}}
{"id": "WebQTest-1465", "question": "what kind of government is south korea", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is south korea [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009752735495567322, -0.014378507621586323, -0.13483764231204987], "norm_scores": [0.34748682379722595, 0.34588316082954407, 0.30663001537323]}}
{"id": "WebQTest-1467", "question": "who did ben franklin marry", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.children"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben franklin marry [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.03212042525410652, -0.03298823907971382, -0.0444892942905426], "norm_scores": [0.3348020017147064, 0.33451157808303833, 0.33068639039993286]}}
{"id": "WebQTest-1468", "question": "what sri lanka capital", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sri lanka capital [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.009165680035948753, -0.027251040562987328, -0.042431678622961044], "norm_scores": [0.3390567898750305, 0.3329799175262451, 0.327963262796402]}}
{"id": "WebQTest-1470", "question": "what kind of government does poland have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does poland have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010808425024151802, -0.013435611501336098, -0.1464291512966156], "norm_scores": [0.34836530685424805, 0.3474512994289398, 0.3041834533214569]}}
{"id": "WebQTest-1471", "question": "what famous people came from delaware", "prediction": [["people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"], ["location.location.containedby", "people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people came from delaware [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.01319075282663107, -0.04009747505187988, -0.04672659561038017], "norm_scores": [0.34008142352104187, 0.3310529291629791, 0.32886558771133423]}}
{"id": "WebQTest-1472", "question": "when did romney become governor", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did romney become governor [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.020564202219247818, -0.031172040849924088, -0.03237701207399368], "norm_scores": [0.33582910895347595, 0.3322855234146118, 0.33188536763191223]}}
{"id": "WebQTest-1473", "question": "what currency does cyprus use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does cyprus use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.0028254385106265545, -0.03905195742845535, -0.11093373596668243], "norm_scores": [0.34941187500953674, 0.3369804322719574, 0.31360775232315063]}}
{"id": "WebQTest-1475", "question": "what type of guitar does johnny depp play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitar.guitarists"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of guitar does johnny depp play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.013511083088815212, -0.03175070509314537, -0.03582132235169411], "norm_scores": [0.3378535211086273, 0.33174705505371094, 0.33039939403533936]}}
{"id": "WebQTest-1477", "question": "what episode of how i met your mother is carrie underwood on", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what episode of how i met your mother is carrie underwood on [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>"], "scores": [-0.018002182245254517, -0.027155768126249313, -0.0299832820892334], "norm_scores": [0.33568552136421204, 0.3326268196105957, 0.3316876292228699]}}
{"id": "WebQTest-1478", "question": "who are the 9 justices of the supreme court 2011", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.written_work.author"], ["people.appointment.appointed_role", "people.appointee.position"]], "ground_paths": [["book.written_work.subjects", "book.author.works_written"], ["law.court.jurisdiction", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the 9 justices of the supreme court 2011 [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>", "<PATH> people.appointment.appointed_role <SEP> people.appointee.position </PATH>"], "scores": [-0.007943687029182911, -0.03293515369296074, -0.03374343737959862], "norm_scores": [0.3390004634857178, 0.33063334226608276, 0.33036619424819946]}}
{"id": "WebQTest-1479", "question": "who is dustin johnson the golfer", "prediction": [["people.person.profession"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft_pick.player", "sports.drafted_athlete.drafted"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dustin johnson the golfer [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.drafted_athlete.drafted </PATH>"], "scores": [-0.002401514910161495, -0.060113098472356796, -0.0658026933670044], "norm_scores": [0.346922367811203, 0.3274677097797394, 0.32560986280441284]}}
{"id": "WebQTest-1480", "question": "who did cher have a son with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["music.composer.compositions", "music.composer.compositions"], ["music.artist.label", "music.artist.label"], ["base.americancomedy.double_act.comic", "base.americancomedy.double_act.straight_man"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["people.person.profession", "people.person.profession"], ["award.award_nomination.award_nominee", "award.award_nominee.award_nominations"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.album.artist", "music.album.artist"], ["people.person.spouse_s", "people.person.spouse_s"], ["music.artist.concert_tours", "music.concert_tour.artist"], ["music.artist.genre", "music.artist.genre"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"], ["people.marriage.spouse", "people.marriage.spouse"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cher have a son with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.02159569412469864, -0.032938312739133835, -0.03974767029285431], "norm_scores": [0.33661723136901855, 0.33282068371772766, 0.33056211471557617]}}
{"id": "WebQTest-1481", "question": "what city is the university of oregon state in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is the university of oregon state in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.002629420720040798, -0.054415877908468246, -0.05651555210351944], "norm_scores": [0.3451761305332184, 0.3277556598186493, 0.3270682096481323]}}
{"id": "WebQTest-1482", "question": "who played carmen cortez in spy kids", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carmen cortez in spy kids [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02545326203107834, -0.03299754485487938, -0.033955082297325134], "norm_scores": [0.3351185917854309, 0.3325998783111572, 0.33228155970573425]}}
{"id": "WebQTest-1483", "question": "what is the political system in germany", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system in germany [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010144013911485672, -0.014315249398350716, -0.1283472180366516], "norm_scores": [0.34669822454452515, 0.3452550768852234, 0.30804675817489624]}}
{"id": "WebQTest-1484", "question": "what prompted shakespeare to write poetry", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what prompted shakespeare to write poetry [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.01649336889386177, -0.026150338351726532, -0.02831810526549816], "norm_scores": [0.3357243239879608, 0.33249783515930176, 0.3317778408527374]}}
{"id": "WebQTest-1485", "question": "what is the currency in turkey called", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in turkey called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.016307372599840164, -0.019566351547837257, -0.032950032502412796], "norm_scores": [0.33554330468177795, 0.3344515562057495, 0.3300051689147949]}}
{"id": "WebQTest-1486", "question": "what school did mark zuckerberg attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did mark zuckerberg attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015509179793298244, -0.01819072850048542, -0.022316597402095795], "norm_scores": [0.33438804745674133, 0.33349254727363586, 0.3321194350719452]}}
{"id": "WebQTest-1487", "question": "where does kate middleton live 2012", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kate middleton live 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.011205616407096386, -0.02150668576359749, -0.03417686000466347], "norm_scores": [0.3370359539985657, 0.33358195424079895, 0.329382061958313]}}
{"id": "WebQTest-1488", "question": "what type of art does marc chagall do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.associated_artists"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["book.school_or_movement.associated_authors"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does marc chagall do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.associated_artists </PATH>"], "scores": [-0.0015901802107691765, -0.03819528967142105, -0.11145900934934616], "norm_scores": [0.3496493101119995, 0.33708178997039795, 0.31326889991760254]}}
{"id": "WebQTest-1489", "question": "what character did billy d williams play in star wars", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["film.performance.actor", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what character did billy d williams play in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.02167847566306591, -0.022194085642695427, -0.029290443286299706], "norm_scores": [0.33423560857772827, 0.334063321352005, 0.3317010700702667]}}
{"id": "WebQTest-1490", "question": "what language is spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007448486518114805, -0.02847612090408802, -0.04831313714385033], "norm_scores": [0.34023433923721313, 0.33315470814704895, 0.3266110122203827]}}
{"id": "WebQTest-1493", "question": "what school did wilfred owen go to", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did wilfred owen go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.015054197981953621, -0.02431556023657322, -0.024860963225364685], "norm_scores": [0.33545535802841187, 0.33236294984817505, 0.3321816921234131]}}
{"id": "WebQTest-1494", "question": "who is kevin jonas married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is kevin jonas married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.028488149866461754, -0.034601472318172455, -0.0396822914481163], "norm_scores": [0.335258424282074, 0.3332151472568512, 0.33152642846107483]}}
{"id": "WebQTest-1495", "question": "where does airtran airways fly", "prediction": [["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airline_airport_presence.airline", "aviation.airline_airport_presence.airport"], ["aviation.airport.hub_for", "aviation.airport.serves"]], "ground_paths": [["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airline.focus_cities"], ["aviation.airline_airport_presence.airline", "aviation.airport.airlines"], ["aviation.airport.focus_city_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does airtran airways fly [/INST]", "raw_output": {"paths": ["<PATH> aviation.airline.airports_served <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airline_airport_presence.airline <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airport.hub_for <SEP> aviation.airport.serves </PATH>"], "scores": [-0.019061394035816193, -0.024439658969640732, -0.03866511955857277], "norm_scores": [0.3361092209815979, 0.33430638909339905, 0.32958438992500305]}}
{"id": "WebQTest-1496", "question": "where does the city of paris get its name", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["location.location.containedby"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the city of paris get its name [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011181321926414967, -0.01481532584875822, -0.07765865325927734], "norm_scores": [0.34105750918388367, 0.33982035517692566, 0.3191221058368683]}}
{"id": "WebQTest-1499", "question": "who did sandra bullock marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["base.popstra.infidelity.participant", "base.popstra.infidelity.perpetrator"], ["base.popstra.infidelity.victim", "base.popstra.celebrity.infidelity_perpetrator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sandra bullock marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.018116094172000885, -0.0371679849922657, -0.039659060537815094], "norm_scores": [0.3378588557243347, 0.3314829170703888, 0.3306581974029541]}}
{"id": "WebQTest-1500", "question": "where is olympic national park located", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.011327945627272129, -0.024353576824069023, -0.029155230149626732], "norm_scores": [0.33676958084106445, 0.33241137862205505, 0.3308190703392029]}}
{"id": "WebQTest-1501", "question": "where is the un based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization_founder.organizations_founded", "location.location.containedby"], ["book.book_subject.works", "base.reviews.review.mentions"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the un based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.013223514892160892, -0.04147268831729889, -0.04221836477518082], "norm_scores": [0.3397237956523895, 0.3302611708641052, 0.3300149738788605]}}
{"id": "WebQTest-1502", "question": "what city and state is yale located in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city and state is yale located in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.001741988817229867, -0.05884251371026039, -0.06693987548351288], "norm_scores": [0.3470557630062103, 0.3277938961982727, 0.32515034079551697]}}
{"id": "WebQTest-1503", "question": "where did thomas paine die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas paine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.0003766949230339378, -0.09519216418266296, -0.10324644297361374], "norm_scores": [0.35564595460891724, 0.3234744966030121, 0.32087960839271545]}}
{"id": "WebQTest-1504", "question": "where was barack hussein obama sr born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.parents"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was barack hussein obama sr born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.0007452404242940247, -0.06390104442834854, -0.10379084199666977], "norm_scores": [0.35200321674346924, 0.3304596245288849, 0.3175371289253235]}}
{"id": "WebQTest-1506", "question": "where is the home stadium of the new england patriots", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["location.location.containedby"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the home stadium of the new england patriots [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0034824004396796227, -0.02746119722723961, -0.10539815574884415], "norm_scores": [0.34729310870170593, 0.33906447887420654, 0.31364238262176514]}}
{"id": "WebQTest-1507", "question": "who was kate chopin", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kate chopin [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0004945332766510546, -0.0792810469865799, -0.09776610136032104], "norm_scores": [0.35316386818885803, 0.32640716433525085, 0.3204289376735687]}}
{"id": "WebQTest-1508", "question": "when did bill clinton go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did bill clinton go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014917208813130856, -0.020379774272441864, -0.021972956135869026], "norm_scores": [0.33472564816474915, 0.3329021632671356, 0.3323722183704376]}}
{"id": "WebQTest-1510", "question": "what are staffordshire terriers", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_of_breed.breed_of"], ["biology.breed_breed.breed_of"]], "ground_paths": [["biology.animal_breed.breed_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are staffordshire terriers [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_of_breed.breed_of </PATH>", "<PATH> biology.breed_breed.breed_of </PATH>"], "scores": [-0.007228048052638769, -0.039033010601997375, -0.05028555169701576], "norm_scores": [0.3416990637779236, 0.3310023546218872, 0.3272985816001892]}}
{"id": "WebQTest-1513", "question": "what to do downtown sacramento", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do downtown sacramento [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.017064716666936874, -0.019389154389500618, -0.022146141156554222], "norm_scores": [0.33415651321411133, 0.33338066935539246, 0.3324628174304962]}}
{"id": "WebQTest-1515", "question": "who is michael j fox parents", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.01187977846711874, -0.015868203714489937, -0.14652127027511597], "norm_scores": [0.34842613339424133, 0.34703925251960754, 0.3045346140861511]}}
{"id": "WebQTest-1516", "question": "where did sanjay gupta go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.honorary_degree_recipient.honorary_degrees", "education.honorary_degree.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did sanjay gupta go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01685069128870964, -0.01943640224635601, -0.02943171188235283], "norm_scores": [0.3350178599357605, 0.3341527283191681, 0.3308294117450714]}}
{"id": "WebQTest-1518", "question": "what countries are involved in the nafta", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["common.topic.notable_types"]], "ground_paths": [["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_member.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are involved in the nafta [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0114938635379076, -0.024934740737080574, -0.04161514714360237], "norm_scores": [0.3381832242012024, 0.33366817235946655, 0.32814860343933105]}}
{"id": "WebQTest-1519", "question": "what continent is cayman islands located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent is cayman islands located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.009139220230281353, -0.02137577347457409, -0.07827114313840866], "norm_scores": [0.34234365820884705, 0.3381800651550293, 0.31947630643844604]}}
{"id": "WebQTest-1520", "question": "what language speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.001081703114323318, -0.06727980822324753, -0.07576499879360199], "norm_scores": [0.34916409850120544, 0.32679855823516846, 0.3240373134613037]}}
{"id": "WebQTest-1522", "question": "who does kroy biermann play football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kroy biermann play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.02005382999777794, -0.023317182436585426, -0.02885570377111435], "norm_scores": [0.3346744179725647, 0.3335840106010437, 0.3317415714263916]}}
{"id": "WebQTest-1523", "question": "when is the last time the pittsburgh steelers won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the pittsburgh steelers won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.010471624322235584, -0.015989720821380615, -0.03462696820497513], "norm_scores": [0.336628794670105, 0.334776371717453, 0.328594833612442]}}
{"id": "WebQTest-1524", "question": "who invented islamic religion", "prediction": [["religion.religion.notable_figures"], ["religion.religion.founding_figures"], ["religion.belief.belief_of"]], "ground_paths": [["religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who invented islamic religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>", "<PATH> religion.belief.belief_of </PATH>"], "scores": [-0.00250987452454865, -0.043623410165309906, -0.06761456280946732], "norm_scores": [0.34522169828414917, 0.331316202878952, 0.3234621584415436]}}
{"id": "WebQTest-1525", "question": "who were we fighting in the gulf war", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["time.event.locations"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"]], "ground_paths": [["film.film_genre.films_in_this_genre", "film.film.country"], ["time.event.locations"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "tv.tv_program.country_of_origin"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["film.film.genre", "film.film.country"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["film.film.subjects", "film.film.country"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "film.film.country"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were we fighting in the gulf war [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>"], "scores": [-0.024664686992764473, -0.02734260819852352, -0.02928435243666172], "norm_scores": [0.33414456248283386, 0.3332509398460388, 0.33260446786880493]}}
{"id": "WebQTest-1526", "question": "how many languages do they speak in switzerland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in switzerland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01511932909488678, -0.022454669699072838, -0.023449160158634186], "norm_scores": [0.33507615327835083, 0.3326272666454315, 0.33229660987854004]}}
{"id": "WebQTest-1527", "question": "what political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "ground_paths": [["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.019160082563757896, -0.02099897898733616, -0.023375192657113075], "norm_scores": [0.3340061902999878, 0.33339256048202515, 0.33260127902030945]}}
{"id": "WebQTest-1528", "question": "what movies does logan lerman play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does logan lerman play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.016603490337729454, -0.03632419556379318, -0.03658540919423103], "norm_scores": [0.33775919675827026, 0.331163614988327, 0.33107712864875793]}}
{"id": "WebQTest-1529", "question": "where did emperor constantine die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor constantine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0006790003390051425, -0.07447975873947144, -0.08228029310703278], "norm_scores": [0.35081613063812256, 0.325857937335968, 0.3233259320259094]}}
{"id": "WebQTest-1531", "question": "where is mali located", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mali located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.017038455232977867, -0.022099284455180168, -0.041047513484954834], "norm_scores": [0.33656108379364014, 0.3348621129989624, 0.3285767734050751]}}
{"id": "WebQTest-1533", "question": "what planet did luke skywalker came from", "prediction": [["fictional_universe.fictional_setting.characters_that_have_lived_here"], ["fictional_universe.fictional_character.place_of_birth"], ["fictional_universe.fictional_character.places_lived"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what planet did luke skywalker came from [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> fictional_universe.fictional_character.places_lived </PATH>"], "scores": [-0.012144646607339382, -0.01878954842686653, -0.02025734819471836], "norm_scores": [0.3349750339984894, 0.33275651931762695, 0.33226847648620605]}}
{"id": "WebQTest-1534", "question": "who are the coaches for the oakland raiders", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.current_head_coach"], ["american_football.football_coach.current_team_head_coached"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the coaches for the oakland raiders [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>", "<PATH> american_football.football_coach.current_team_head_coached </PATH>"], "scores": [-0.01595071516931057, -0.02159891463816166, -0.02545669861137867], "norm_scores": [0.3350188434123993, 0.33313193917274475, 0.33184924721717834]}}
{"id": "WebQTest-1535", "question": "where was the first microsoft headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["base.engineering1.rfid_company.location", "base.biblioness.bibs_location.country"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first microsoft headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01790136843919754, -0.034059926867485046, -0.04244698956608772], "norm_scores": [0.3378692865371704, 0.3324536979198456, 0.3296770453453064]}}
{"id": "WebQTest-1536", "question": "where did john steinbeck go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john steinbeck go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.016494350507855415, -0.018151704221963882, -0.018415071070194244], "norm_scores": [0.3337310254573822, 0.3331783413887024, 0.3330906331539154]}}
{"id": "WebQTest-1537", "question": "what did niels bohr discover about the atomic structure", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["base.nobelprizes.nobel_honor.nobel_prize_winner"]], "ground_paths": [["base.nobelprizes.nobel_honor.nobel_prize_winner", "base.nobelprizes.nobel_honor.subject_area"], ["education.academic.advisees", "book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did niels bohr discover about the atomic structure [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.nobelprizes.nobel_honor.nobel_prize_winner </PATH>"], "scores": [-0.012196325697004795, -0.02507954277098179, -0.03447943180799484], "norm_scores": [0.33724960684776306, 0.33293262124061584, 0.3298177421092987]}}
{"id": "WebQTest-1538", "question": "what books did louis sachar wrote", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did louis sachar wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.012111688032746315, -0.02559828571975231, -0.029563961550593376], "norm_scores": [0.3367793560028076, 0.3322678208351135, 0.33095279335975647]}}
{"id": "WebQTest-1539", "question": "who is ryan reynolds married to 2012", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is ryan reynolds married to 2012 [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.017514344304800034, -0.03018481656908989, -0.031844612210989], "norm_scores": [0.336340069770813, 0.3321053683757782, 0.3315546214580536]}}
{"id": "WebQTest-1540", "question": "what is newt gingrich 's wife 's name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.profession", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["film.film.executive_produced_by", "film.film.executive_produced_by"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.religion", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is newt gingrich 's wife 's name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.023936238139867783, -0.036293692886829376, -0.03940675035119057], "norm_scores": [0.33643221855163574, 0.3323003351688385, 0.33126747608184814]}}
{"id": "WebQTest-1541", "question": "what type of music did antonio vivaldi play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did antonio vivaldi play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.002487373072654009, -0.037672221660614014, -0.11031558364629745], "norm_scores": [0.34925851225852966, 0.33718356490135193, 0.3135579526424408]}}
{"id": "WebQTest-1542", "question": "what school does james franco go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school does james franco go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014820358715951443, -0.016286414116621017, -0.023996204137802124], "norm_scores": [0.33451512455940247, 0.3340250849723816, 0.33145973086357117]}}
{"id": "WebQTest-1543", "question": "what is samuel langhorne clemens well known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is samuel langhorne clemens well known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.0014557319227606058, -0.04666855186223984, -0.09008749574422836], "norm_scores": [0.34831351041793823, 0.3329159915447235, 0.3187704384326935]}}
{"id": "WebQTest-1544", "question": "who was president after william henry harrison", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after william henry harrison [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.01526376698166132, -0.022412462159991264, -0.04375680908560753], "norm_scores": [0.33729249238967896, 0.33488988876342773, 0.3278176188468933]}}
{"id": "WebQTest-1546", "question": "where did rick santorum attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rick santorum attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01612161472439766, -0.02207951247692108, -0.027891118079423904], "norm_scores": [0.33530500531196594, 0.3333132266998291, 0.33138173818588257]}}
{"id": "WebQTest-1547", "question": "what is the title of george bush 's new book", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the title of george bush 's new book [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.009585184045135975, -0.023976128548383713, -0.045734599232673645], "norm_scores": [0.33895912766456604, 0.334116131067276, 0.32692480087280273]}}
{"id": "WebQTest-1548", "question": "where did antoine lavoisier die", "prediction": [["people.deceased_person.place_of_death"], ["deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did antoine lavoisier die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.00023348513059318066, -0.10376761108636856, -0.10504321753978729], "norm_scores": [0.35686996579170227, 0.3217701315879822, 0.32135990262031555]}}
{"id": "WebQTest-1550", "question": "what is the name of king george vi wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.spouse_s", "people.person.spouse_s"], ["film.film.subjects", "film.film_subject.films"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.family.members", "people.family.members"], ["people.person.religion", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of king george vi wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03643408417701721, -0.03690562769770622, -0.03885567933320999], "norm_scores": [0.3336547613143921, 0.33349746465682983, 0.3328477740287781]}}
{"id": "WebQTest-1551", "question": "what year did seattle seahawks go to the super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did seattle seahawks go to the super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.01802678033709526, -0.021706929430365562, -0.02517710067331791], "norm_scores": [0.33453747630119324, 0.3333085775375366, 0.33215394616127014]}}
{"id": "WebQTest-1553", "question": "where does sami yusuf live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sami yusuf live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.024400629103183746, -0.02885940670967102, -0.03914446756243706], "norm_scores": [0.3354673981666565, 0.3339749574661255, 0.33055761456489563]}}
{"id": "WebQTest-1554", "question": "what is the official language of brazil now", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the official language of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0005169843207113445, -0.07373283803462982, -0.09072425961494446], "norm_scores": [0.3517235517501831, 0.3268919289112091, 0.3213844895362854]}}
{"id": "WebQTest-1555", "question": "what is the parliament of nepal called", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.governing_officials", "government.governmental_body.members"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.governmental_body"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the parliament of nepal called [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_body.members </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.governmental_body </PATH>"], "scores": [-0.01677905023097992, -0.018474886193871498, -0.026960281655192375], "norm_scores": [0.3346523344516754, 0.33408528566360474, 0.33126240968704224]}}
{"id": "WebQTest-1556", "question": "when 's the last time the mets won the world series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the mets won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.010838429443538189, -0.011782526969909668, -0.07795192301273346], "norm_scores": [0.3408147394657135, 0.3404931128025055, 0.318692147731781]}}
{"id": "WebQTest-1557", "question": "where do florida panthers play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do florida panthers play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.007017405703663826, -0.024143006652593613, -0.06397522985935211], "norm_scores": [0.3415703773498535, 0.3357705771923065, 0.3226589858531952]}}
{"id": "WebQTest-1558", "question": "who is john cena married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is john cena married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.019973067566752434, -0.03371495008468628, -0.03516139090061188], "norm_scores": [0.33655545115470886, 0.3319621682167053, 0.3314823508262634]}}
{"id": "WebQTest-1559", "question": "what to do san jose ca", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["location.neighborhood.neighborhood_of"], ["film.film_festival.location"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do san jose ca [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01636008359491825, -0.01637090928852558, -0.028273141011595726], "norm_scores": [0.3346555531024933, 0.3346519470214844, 0.33069247007369995]}}
{"id": "WebQTest-1560", "question": "who founded new york university", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["education.educational_institution.parent_institution", "people.person.parents"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded new york university [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> people.person.parents </PATH>"], "scores": [-0.026239007711410522, -0.03158557042479515, -0.03570648282766342], "norm_scores": [0.33498087525367737, 0.33319467306137085, 0.3318244218826294]}}
{"id": "WebQTest-1561", "question": "who created arthur", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.program_creator"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "ground_paths": [["tv.tv_program.program_creator"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who created arthur [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.program_creator </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>"], "scores": [-0.007060375064611435, -0.03739389777183533, -0.047120023518800735], "norm_scores": [0.3411974608898163, 0.3310031592845917, 0.32779937982559204]}}
{"id": "WebQTest-1562", "question": "what countries constitute the eu", "prediction": [["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization.geographic_scope", "location.location.containedby"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["government.governmental_jurisdiction.governing_officials", "government.political_district.representatives"], ["organization.organization_membership.organization", "organization.organization_membership.member"], ["base.industrystandards.standard_organization.standards", "base.aareas.coding_scheme.geo_scope"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.district_represented"], ["government.government_position_held.jurisdiction_of_office", "government.political_district.representatives"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization.geographic_scope", "location.location.containedby"], ["organization.organization.geographic_scope", "location.location.primarily_containedby"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization.geographic_scope", "base.locations.countries.continent"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization.founders"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries constitute the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.continents.countries_within </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.location.containedby </PATH>"], "scores": [-0.02965894341468811, -0.03208840265870094, -0.03347744792699814], "norm_scores": [0.33402785658836365, 0.3332173526287079, 0.33275479078292847]}}
{"id": "WebQTest-1563", "question": "what are all the countries in north africa", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the countries in north africa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.005116470158100128, -0.033231064677238464, -0.071150042116642], "norm_scores": [0.34383445978164673, 0.3343023359775543, 0.32186323404312134]}}
{"id": "WebQTest-1564", "question": "what did bobby darin die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bobby darin die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0014589898055419326, -0.0443326011300087, -0.11557185649871826], "norm_scores": [0.350853830575943, 0.336129367351532, 0.3130168318748474]}}
{"id": "WebQTest-1565", "question": "where does houston dynamo play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does houston dynamo play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.012091903947293758, -0.013345078565180302, -0.0830318033695221], "norm_scores": [0.341266006231308, 0.34083864092826843, 0.31789541244506836]}}
{"id": "WebQTest-1566", "question": "what kind of guitar john mayer", "prediction": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"], ["music.instrument.family"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar john mayer [/INST]", "raw_output": {"paths": ["<PATH> music.guitar.guitarists </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.014562134630978107, -0.015938302502036095, -0.05214245989918709], "norm_scores": [0.3376391530036926, 0.33717483282089233, 0.32518601417541504]}}
{"id": "WebQTest-1567", "question": "what cancer did patrick swayze", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did patrick swayze [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0016408611554652452, -0.046027760952711105, -0.05877664312720299], "norm_scores": [0.3447028398513794, 0.32973712682724, 0.3255600333213806]}}
{"id": "WebQTest-1568", "question": "what university did obama graduated from", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.014165369793772697, -0.016146354377269745, -0.02622266300022602], "norm_scores": [0.3348921239376068, 0.33422937989234924, 0.33087849617004395]}}
{"id": "WebQTest-1569", "question": "what jamaican money called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jamaican money called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0005219798185862601, -0.07259882986545563, -0.13523980975151062], "norm_scores": [0.3565797209739685, 0.3317829668521881, 0.3116373121738434]}}
{"id": "WebQTest-1570", "question": "where is the arizona cardinals stadium located", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the arizona cardinals stadium located [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.006081426981836557, -0.025793008506298065, -0.05311623215675354], "norm_scores": [0.3407694399356842, 0.3341180980205536, 0.3251124918460846]}}
{"id": "WebQTest-1571", "question": "what is sir isaac newton theory", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["law.invention.inventor"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is sir isaac newton theory [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.007509279064834118, -0.023144178092479706, -0.06993835419416428], "norm_scores": [0.34200119972229004, 0.3366956114768982, 0.3213031589984894]}}
{"id": "WebQTest-1572", "question": "what should i do today in san francisco", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.neighborhood.neighborhood_of"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what should i do today in san francisco [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.015543317422270775, -0.01919184997677803, -0.021863402798771858], "norm_scores": [0.3344416916370392, 0.3332236707210541, 0.33233463764190674]}}
{"id": "WebQTest-1573", "question": "who influenced picasso art", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced picasso art [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.011971055530011654, -0.01306946761906147, -0.11077017337083817], "norm_scores": [0.34425458312034607, 0.3438766896724701, 0.31186872720718384]}}
{"id": "WebQTest-1574", "question": "what is my timezone in pennsylvania", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.4564137018169276e-05, -0.13218912482261658, -0.18906231224536896], "norm_scores": [0.3698313534259796, 0.32404181361198425, 0.30612680315971375]}}
{"id": "WebQTest-1575", "question": "what products and\\/or services does google offer customers", "prediction": [["organization.organization.sectors"], ["organization.organization_founder.organizations_founded"], ["organization.organization.sectors", "organization.organization.sectors"]], "ground_paths": [["projects.project_participation.participant", "projects.project_participation.project"], ["internet.website.owner"], ["business.consumer_company.products"], ["computer.computer_manufacturer_brand.computer_models"], ["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"], ["computer.software_developer.software"], ["business.consumer_company.products", "business.consumer_product.company"], ["computer.software.developer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products and\\/or services does google offer customers [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.sectors <SEP> organization.organization.sectors </PATH>"], "scores": [-0.043612852692604065, -0.05013665929436684, -0.05536256358027458], "norm_scores": [0.33536607027053833, 0.3331853151321411, 0.33144867420196533]}}
{"id": "WebQTest-1577", "question": "who did cliff lee play for last year", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cliff lee play for last year [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014443155378103256, -0.014615066349506378, -0.02977875992655754], "norm_scores": [0.3350521922111511, 0.3349946141242981, 0.3299531638622284]}}
{"id": "WebQTest-1580", "question": "what type of democracy is the united states", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of democracy is the united states [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.008823828771710396, -0.015205871313810349, -0.16186672449111938], "norm_scores": [0.3506641685962677, 0.34843334555625916, 0.30090245604515076]}}
{"id": "WebQTest-1582", "question": "what time in china hong kong", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.continent"]], "ground_paths": [["time.time_zone.locations_in_this_time_zone"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in china hong kong [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.005398978013545275, -0.024587465450167656, -0.08526970446109772], "norm_scores": [0.34432533383369446, 0.33778125047683716, 0.31789344549179077]}}
{"id": "WebQTest-1583", "question": "what all movies has taylor lautner been in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all movies has taylor lautner been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.02418498508632183, -0.02924690581858158, -0.02993757463991642], "norm_scores": [0.33453601598739624, 0.33284690976142883, 0.3326171040534973]}}
{"id": "WebQTest-1584", "question": "what disease did bernie mac die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did bernie mac die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.0045987884514033794, -0.03465811535716057, -0.03974864259362221], "norm_scores": [0.34061700105667114, 0.33053064346313477, 0.3288523256778717]}}
{"id": "WebQTest-1585", "question": "who plays spike spiegel", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["base.schemastaging.tv_character_extra.regular_dubbing_performances", "base.schemastaging.tv_star_dubbing_performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays spike spiegel [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.01669294387102127, -0.03531797602772713, -0.0353446900844574], "norm_scores": [0.33748799562454224, 0.33126044273376465, 0.3312515914440155]}}
{"id": "WebQTest-1586", "question": "who was queen isabella 's mother", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen isabella 's mother [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.008783120661973953, -0.01998555101454258, -0.12595555186271667], "norm_scores": [0.34742826223373413, 0.3435579538345337, 0.3090137541294098]}}
{"id": "WebQTest-1587", "question": "who does chris hemsworth have a baby with", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.friendship.friend", "celebrities.friendship.friend"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does chris hemsworth have a baby with [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.02164618857204914, -0.024837616831064224, -0.026846978813409805], "norm_scores": [0.3342663645744324, 0.33320125937461853, 0.33253243565559387]}}
{"id": "WebQTest-1589", "question": "where is the mts iceplex in winnipeg", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the mts iceplex in winnipeg [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.014011026360094547, -0.02601020038127899, -0.030140260234475136], "norm_scores": [0.3364655375480652, 0.33245235681533813, 0.3310821056365967]}}
{"id": "WebQTest-1590", "question": "what are the most popular religions in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"]], "ground_paths": [["people.person.quotations", "people.person.religion"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["people.person.nationality", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["symbols.namesake.named_after", "religion.religion.notable_figures"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the most popular religions in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>"], "scores": [-0.0030930715147405863, -0.030324898660182953, -0.05901704728603363], "norm_scores": [0.3426128029823303, 0.33340874314308167, 0.3239784240722656]}}
{"id": "WebQTest-1591", "question": "what was obama 's former occupation", "prediction": [["people.person.profession"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.profession", "people.profession.specializations"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was obama 's former occupation [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>"], "scores": [-0.00775432214140892, -0.035200007259845734, -0.039918798953294754], "norm_scores": [0.339988648891449, 0.33078429102897644, 0.329227089881897]}}
{"id": "WebQTest-1592", "question": "what are all the songs justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["music.featured_artist.albums"], ["music.artist.album"], ["music.recording.artist"], ["music.album.artist"], ["music.recording.featured_artists"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["music.composer.compositions"], ["music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the songs justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.01754618249833584, -0.02203201688826084, -0.033398423343896866], "norm_scores": [0.3355933427810669, 0.33409127593040466, 0.33031538128852844]}}
{"id": "WebQTest-1593", "question": "where is the thames river source", "prediction": [["geography.river.origin"], ["geography.river.basin_countries"], ["geography.river.mouth"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the thames river source [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> geography.river.mouth </PATH>"], "scores": [-0.005007354076951742, -0.0522017739713192, -0.053448766469955444], "norm_scores": [0.3440428078174591, 0.3281830847263336, 0.3277741074562073]}}
{"id": "WebQTest-1595", "question": "what new movies is robert pattinson in", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["music.artist.track", "music.recording.releases"], ["music.recording.artist", "music.recording.releases"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.producer.films_executive_produced"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what new movies is robert pattinson in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.028471168130636215, -0.0306230615824461, -0.032393328845500946], "norm_scores": [0.33400845527648926, 0.3332904875278473, 0.3327009975910187]}}
{"id": "WebQTest-1596", "question": "what language do they use in pakistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they use in pakistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.015978513285517693, -0.017744140699505806, -0.034293849021196365], "norm_scores": [0.3355606496334076, 0.3349686861038208, 0.329470694065094]}}
{"id": "WebQTest-1597", "question": "what kind of products does nike sell", "prediction": [["business.business_operation.industry"], ["business.industry.companies"], ["business.industry.parent_industry"]], "ground_paths": [["business.business_operation.industry"], ["business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of products does nike sell [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> business.industry.parent_industry </PATH>"], "scores": [-0.011599867604672909, -0.029109133407473564, -0.05329357832670212], "norm_scores": [0.33992713689804077, 0.33402708172798157, 0.3260457217693329]}}
{"id": "WebQTest-1598", "question": "who did lbj marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["book.written_work.subjects", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.profession", "common.topic.notable_types"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did lbj marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.026553548872470856, -0.040773238986730576, -0.045982349663972855], "norm_scores": [0.3370817303657532, 0.33232244849205017, 0.33059582114219666]}}
{"id": "WebQTest-1599", "question": "where did kim jong il die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim jong il die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people </PATH>"], "scores": [-0.0003679133078549057, -0.09705795347690582, -0.10981015861034393], "norm_scores": [0.35661157965660095, 0.3237453103065491, 0.3196430802345276]}}
{"id": "WebQTest-1603", "question": "what is the sacred text of daoism", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"], ["religion.religion.sacred_sites"]], "ground_paths": [["book.written_work.subjects"], ["religion.religion.texts"], ["religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the sacred text of daoism [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> religion.religion.sacred_sites </PATH>"], "scores": [-0.00929781049489975, -0.0146523742005229, -0.11548422276973724], "norm_scores": [0.3455524444580078, 0.3437071442604065, 0.31074047088623047]}}
{"id": "WebQTest-1604", "question": "what countries speak english natively", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak english natively [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016523152589797974, -0.021316569298505783, -0.02533259615302086], "norm_scores": [0.33484601974487305, 0.33324480056762695, 0.3319091498851776]}}
{"id": "WebQTest-1607", "question": "who won fedex cup 2012", "prediction": [["award.competition.winner", "people.person.profession"], ["award.competition.winner", "people.person.gender", "people.person.gender"], ["award.competitor.competitions", "people.person.profession"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fedex cup 2012 [/INST]", "raw_output": {"paths": ["<PATH> award.competition.winner <SEP> people.person.profession </PATH>", "<PATH> award.competition.winner <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> award.competitor.competitions <SEP> people.person.profession </PATH>"], "scores": [-0.01795256696641445, -0.027840644121170044, -0.029806898906826973], "norm_scores": [0.3357534110546112, 0.3324497938156128, 0.3317967653274536]}}
{"id": "WebQTest-1608", "question": "what language is mainly spoken in egypt", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is mainly spoken in egypt [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.022689389064908028, -0.02355571836233139, -0.026595382019877434], "norm_scores": [0.3338635563850403, 0.33357444405555725, 0.33256202936172485]}}
{"id": "WebQTest-1609", "question": "who played alf on tv show", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played alf on tv show [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.01759168691933155, -0.020736292004585266, -0.04864130914211273], "norm_scores": [0.3371216952800751, 0.3360632359981537, 0.32681503891944885]}}
{"id": "WebQTest-1610", "question": "who was gerald ford vp", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was gerald ford vp [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.005342402961105108, -0.0250253863632679, -0.0906137079000473], "norm_scores": [0.3449735939502716, 0.3382498621940613, 0.3167765140533447]}}
{"id": "WebQTest-1611", "question": "what is armenian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is armenian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.006633107550442219, -0.0259527787566185, -0.07880431413650513], "norm_scores": [0.34349656105041504, 0.3369240164756775, 0.31957945227622986]}}
{"id": "WebQTest-1612", "question": "what type of art did raphael sanzio create", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did raphael sanzio create [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.004737038630992174, -0.022173741832375526, -0.058806780725717545], "norm_scores": [0.3412875533103943, 0.3353882133960724, 0.3233242630958557]}}
{"id": "WebQTest-1613", "question": "what language do they speak in malta", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in malta [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01824084296822548, -0.02163846790790558, -0.027162352576851845], "norm_scores": [0.3347027003765106, 0.33356741070747375, 0.331729918718338]}}
{"id": "WebQTest-1614", "question": "what religion was king louis xiv", "prediction": [["people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["religion.religion.notable_figures"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion was king louis xiv [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>"], "scores": [-0.0017274986021220684, -0.05291816219687462, -0.06590301543474197], "norm_scores": [0.34626781940460205, 0.3289881944656372, 0.32474395632743835]}}
{"id": "WebQTest-1615", "question": "what football team does mike wright play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team does mike wright play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.015730196610093117, -0.01641266979277134, -0.030452299863100052], "norm_scores": [0.3350416421890259, 0.33481305837631226, 0.3301452696323395]}}
{"id": "WebQTest-1616", "question": "what did shannon hoon die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shannon hoon die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0024611426051706076, -0.03612865135073662, -0.12511828541755676], "norm_scores": [0.3506975471973419, 0.33908697962760925, 0.31021544337272644]}}
{"id": "WebQTest-1617", "question": "what did nicholas copernicus discover", "prediction": [["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"], ["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicholas copernicus discover [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.015550792217254639, -0.023184344172477722, -0.035410359501838684], "norm_scores": [0.3363909125328064, 0.33383283019065857, 0.32977622747421265]}}
{"id": "WebQTest-1618", "question": "what teams has manny ramirez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_coach.teams_coached", "sports.sports_team.coaches"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what teams has manny ramirez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.018610520288348198, -0.021901173517107964, -0.02190202660858631], "norm_scores": [0.3340650796890259, 0.332967609167099, 0.3329673111438751]}}
{"id": "WebQTest-1619", "question": "where do the appalachian mountains run", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.mountain_range.mountains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.mountain_range.mountains </PATH>"], "scores": [-0.00821986235678196, -0.02067682333290577, -0.08186160773038864], "norm_scores": [0.342862069606781, 0.3386175334453583, 0.31852036714553833]}}
{"id": "WebQTest-1620", "question": "when was president wilson in office", "prediction": [["book.written_work.subjects", "base.culturalevent.event.entity_involved"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "location.location.events"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was president wilson in office [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location.events </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.031240954995155334, -0.033402178436517715, -0.03340519219636917], "norm_scores": [0.3338140845298767, 0.33309343457221985, 0.33309242129325867]}}
{"id": "WebQTest-1621", "question": "what political party is jerry brown", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"]], "ground_paths": [["government.election_campaign.candidate", "government.election_campaign.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party is jerry brown [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>"], "scores": [-0.013722029514610767, -0.0186227485537529, -0.027374325320124626], "norm_scores": [0.33539584279060364, 0.33375614881515503, 0.3308480381965637]}}
{"id": "WebQTest-1622", "question": "what ocean is around hawaii", "prediction": [["location.location.contains", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what ocean is around hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.contains </PATH>"], "scores": [-0.027342185378074646, -0.02905171737074852, -0.0391300804913044], "norm_scores": [0.3348318934440613, 0.3342599868774414, 0.3309081196784973]}}
{"id": "WebQTest-1623", "question": "what is the most common language spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007309336680918932, -0.0279591903090477, -0.045580070465803146], "norm_scores": [0.33990317583084106, 0.3329561948776245, 0.3271406292915344]}}
{"id": "WebQTest-1624", "question": "what language tunisia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language tunisia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.01683960296213627, -0.017106184735894203, -0.04571422562003136], "norm_scores": [0.33655622601509094, 0.336466521024704, 0.3269772529602051]}}
{"id": "WebQTest-1625", "question": "what are some inventions that leonardo da vinci invented", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some inventions that leonardo da vinci invented [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.007112967316061258, -0.018873371183872223, -0.0754765197634697], "norm_scores": [0.3422044813632965, 0.3382036089897156, 0.3195919096469879]}}
{"id": "WebQTest-1626", "question": "who wrote st trinians", "prediction": [["film.film.written_by"], ["fictional_universe.fictional_universe.locations"], ["film.film_story_contributor.film_story_credits"]], "ground_paths": [["fictional_universe.fictional_universe.created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote st trinians [/INST]", "raw_output": {"paths": ["<PATH> film.film.written_by </PATH>", "<PATH> fictional_universe.fictional_universe.locations </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>"], "scores": [-0.022125190123915672, -0.03502233326435089, -0.04290716350078583], "norm_scores": [0.33708417415618896, 0.3327646553516388, 0.33015117049217224]}}
{"id": "WebQTest-1627", "question": "what are malaysian coins called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are malaysian coins called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0066568064503371716, -0.026985380798578262, -0.06389854848384857], "norm_scores": [0.3419688940048218, 0.3350873291492462, 0.3229437470436096]}}
{"id": "WebQTest-1628", "question": "what type government does mexico have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type government does mexico have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005958863999694586, -0.02247900888323784, -0.14284171164035797], "norm_scores": [0.3501783013343811, 0.3444408178329468, 0.30538082122802734]}}
{"id": "WebQTest-1629", "question": "what currency does dominican republic take", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does dominican republic take [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.00044890036224387586, -0.07322884351015091, -0.13398098945617676], "norm_scores": [0.3565310537815094, 0.3315044939517975, 0.3119644522666931]}}
{"id": "WebQTest-1630", "question": "what inspired michael jackson to become a singer", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michael jackson to become a singer [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.0061655850149691105, -0.0199592188000679, -0.09141939878463745], "norm_scores": [0.3442838490009308, 0.33956751227378845, 0.31614869832992554]}}
{"id": "WebQTest-1631", "question": "who is the texas state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.location.events", "military.military_person.participated_in_conflicts"], ["location.administrative_division.country", "people.person.nationality"], ["location.us_state.capital", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.political_district.elections", "government.election.winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the texas state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.01688196137547493, -0.02576443925499916, -0.028657659888267517], "norm_scores": [0.3356323838233948, 0.3326643407344818, 0.331703245639801]}}
{"id": "WebQTest-1632", "question": "what to visit in london city", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["organization.organization.geographic_scope"], ["transportation.bridge.locale"], ["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["metropolitan_transit.transit_system.area_served"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in london city [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01236580591648817, -0.020383555442094803, -0.030993608757853508], "norm_scores": [0.3362973630428314, 0.33361178636550903, 0.33009085059165955]}}
{"id": "WebQTest-1633", "question": "what genre of art is the mona lisa", "prediction": [["visual_art.artwork.art_form"], ["visual_art.visual_artist.art_forms"], ["visual_art.art_subject.artwork_on_the_subject"]], "ground_paths": [["visual_art.artwork.art_genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what genre of art is the mona lisa [/INST]", "raw_output": {"paths": ["<PATH> visual_art.artwork.art_form </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_subject.artwork_on_the_subject </PATH>"], "scores": [-0.018647143617272377, -0.028420330956578255, -0.039454828947782516], "norm_scores": [0.33673641085624695, 0.3334614634513855, 0.32980212569236755]}}
{"id": "WebQTest-1634", "question": "who does peter griffin voice", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peter griffin voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.017813876271247864, -0.03342119976878166, -0.03936464712023735], "norm_scores": [0.33747372031211853, 0.3322475552558899, 0.3302786946296692]}}
{"id": "WebQTest-1635", "question": "when did tony romo got drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did tony romo got drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.012911965139210224, -0.014802658930420876, -0.025998396798968315], "norm_scores": [0.3349960446357727, 0.33436328172683716, 0.3306407034397125]}}
{"id": "WebQTest-1637", "question": "what are the catholic holy days", "prediction": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"], ["time.recurring_event.holidays"]], "ground_paths": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the catholic holy days [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> time.recurring_event.holidays </PATH>"], "scores": [-0.01151268184185028, -0.014503461308777332, -0.09231958538293839], "norm_scores": [0.34253785014152527, 0.34151491522789, 0.3159472346305847]}}
{"id": "WebQTest-1638", "question": "what language is most commonly spoken in belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is most commonly spoken in belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.0111368577927351, -0.01947677694261074, -0.0354873463511467], "norm_scores": [0.3369683027267456, 0.3341696858406067, 0.3288620412349701]}}
{"id": "WebQTest-1639", "question": "where did frank baum die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"], ["people.place_of_death.people", "people.place_of_death.location"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did frank baum die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>"], "scores": [-0.0003427406772971153, -0.1015789583325386, -0.1075005829334259], "norm_scores": [0.3568747639656067, 0.32251471281051636, 0.32061052322387695]}}
{"id": "WebQTest-1640", "question": "where is tunis on a map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.containedby"], ["base.locations.countries.continent"], ["location.administrative_division.country"], []], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tunis on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.01369370985776186, -0.035686343908309937, -0.038986559957265854], "norm_scores": [0.3386074900627136, 0.3312419056892395, 0.3301505446434021]}}
{"id": "WebQTest-1641", "question": "who is nicolas cage married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nicolas cage married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.01724415272474289, -0.03181222453713417, -0.043023303151130676], "norm_scores": [0.3378278315067291, 0.33294200897216797, 0.3292301893234253]}}
{"id": "WebQTest-1643", "question": "who does the voice for chucky in child 's play", "prediction": [["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.film.starring", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for chucky in child 's play [/INST]", "raw_output": {"paths": ["<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.029486898332834244, -0.029977766796946526, -0.03077414445579052], "norm_scores": [0.33353090286254883, 0.3333672285079956, 0.3331018388271332]}}
{"id": "WebQTest-1644", "question": "what did mary wollstonecraft fight for", "prediction": [["people.person.profession"], ["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"]], "ground_paths": [["base.activism.activist.area_of_activism"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mary wollstonecraft fight for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>"], "scores": [-0.01846420019865036, -0.02202259562909603, -0.04240389168262482], "norm_scores": [0.33638402819633484, 0.33518919348716736, 0.3284267485141754]}}
{"id": "WebQTest-1646", "question": "what year did the phillies begin", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_team.previously_known"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the phillies begin [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.previously_known </PATH>"], "scores": [-0.00399348558858037, -0.03527605161070824, -0.05898948386311531], "norm_scores": [0.34297195076942444, 0.33240899443626404, 0.3246191442012787]}}
{"id": "WebQTest-1648", "question": "what language does iceland speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does iceland speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014776075258851051, -0.018007243052124977, -0.047235239297151566], "norm_scores": [0.3372867703437805, 0.33619871735572815, 0.32651451230049133]}}
{"id": "WebQTest-1649", "question": "what was dr seuss real name", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by", "people.person.nationality"], ["book.book_edition.author_editor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss real name [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.025359859690070152, -0.027707882225513458, -0.03870400786399841], "norm_scores": [0.3350757956504822, 0.3342899680137634, 0.330634206533432]}}
{"id": "WebQTest-1650", "question": "what did gloria steinem founded", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.founder.organizations_founded"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gloria steinem founded [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.founder.organizations_founded </PATH>"], "scores": [-0.005600706208497286, -0.02409064769744873, -0.10644835233688354], "norm_scores": [0.3465302586555481, 0.340181827545166, 0.3132878839969635]}}
{"id": "WebQTest-1652", "question": "what language they speak in scotland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016536317765712738, -0.01672852225601673, -0.030901402235031128], "norm_scores": [0.3349471688270569, 0.33488282561302185, 0.33017003536224365]}}
{"id": "WebQTest-1653", "question": "what boarding school did mark zuckerberg go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what boarding school did mark zuckerberg go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014462626539170742, -0.019859936088323593, -0.02554519660770893], "norm_scores": [0.3351660370826721, 0.3333619236946106, 0.3314720392227173]}}
{"id": "WebQTest-1654", "question": "what are bigos", "prediction": [["food.dish.cuisine"], ["food.diet_based_dish.diet"], ["food.diet_and_soup.diet"]], "ground_paths": [["food.type_of_dish.dishes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are bigos [/INST]", "raw_output": {"paths": ["<PATH> food.dish.cuisine </PATH>", "<PATH> food.diet_based_dish.diet </PATH>", "<PATH> food.diet_and_soup.diet </PATH>"], "scores": [-0.033061228692531586, -0.08136045187711716, -0.08812231570482254], "norm_scores": [0.34491369128227234, 0.328650563955307, 0.32643574476242065]}}
{"id": "WebQTest-1655", "question": "where are ike and tina turner 's children", "prediction": [["people.person.children"], ["people.person.parents"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are ike and tina turner 's children [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.008247923105955124, -0.020033368840813637, -0.0815339982509613], "norm_scores": [0.34274527430534363, 0.3387295603752136, 0.31852513551712036]}}
{"id": "WebQTest-1656", "question": "what are some of leonardo da vinci most famous inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some of leonardo da vinci most famous inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.006889285985380411, -0.0194394588470459, -0.07566019892692566], "norm_scores": [0.3423404395580292, 0.3380708694458008, 0.31958866119384766]}}
{"id": "WebQTest-1657", "question": "when did shaq come into the nba", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_league_season.awards"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shaq come into the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_league_season.awards </PATH>"], "scores": [-0.015702852979302406, -0.02132151834666729, -0.021512676030397415], "norm_scores": [0.3346043825149536, 0.33272960782051086, 0.3326660096645355]}}
{"id": "WebQTest-1658", "question": "where do they speak turkish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak turkish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014056965708732605, -0.022034479305148125, -0.027868401259183884], "norm_scores": [0.3357577621936798, 0.3330898880958557, 0.3311523199081421]}}
{"id": "WebQTest-1659", "question": "what is the zip code for walnut creek", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for walnut creek [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-7.520253711845726e-05, -0.10704953223466873, -0.1610325276851654], "norm_scores": [0.3636525869369507, 0.32675960659980774, 0.3095878064632416]}}
{"id": "WebQTest-1661", "question": "who was the voice of darth vader in the original star wars movie", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the voice of darth vader in the original star wars movie [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.01741395890712738, -0.02633872628211975, -0.030994601547718048], "norm_scores": [0.3358379900455475, 0.3328540623188019, 0.33130794763565063]}}
{"id": "WebQTest-1662", "question": "where is english spoken around the world", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken around the world [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012749755755066872, -0.025213096290826797, -0.028342153877019882], "norm_scores": [0.33645761013031006, 0.33229026198387146, 0.3312520980834961]}}
{"id": "WebQTest-1663", "question": "what music did louis armstrong play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0017320262268185616, -0.04934685304760933, -0.09239574521780014], "norm_scores": [0.3488178551197052, 0.33259814977645874, 0.31858399510383606]}}
{"id": "WebQTest-1664", "question": "what form of currency does germany use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does germany use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.0057668364606797695, -0.025508064776659012, -0.11977341771125793], "norm_scores": [0.3481040298938751, 0.3412993848323822, 0.3105965554714203]}}
{"id": "WebQTest-1665", "question": "where is the latin language from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the latin language from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.0019744201563298702, -0.05218298360705376, -0.06868389993906021], "norm_scores": [0.3464405834674835, 0.3294757604598999, 0.32408368587493896]}}
{"id": "WebQTest-1666", "question": "what are the top tourist attractions in rome", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["base.arthist.int_zm_nyek.helynev"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the top tourist attractions in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.01741836406290531, -0.01865200139582157, -0.019093504175543785], "norm_scores": [0.3336566090583801, 0.33324524760246277, 0.3330981433391571]}}
{"id": "WebQTest-1667", "question": "who were the children of king solomon", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were the children of king solomon [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.013553688302636147, -0.014137698337435722, -0.135145366191864], "norm_scores": [0.3466293215751648, 0.34642696380615234, 0.30694371461868286]}}
{"id": "WebQTest-1668", "question": "what did andy irons die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did andy irons die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.001999241765588522, -0.03981918841600418, -0.12484707683324814], "norm_scores": [0.351211816072464, 0.33817705512046814, 0.3106110990047455]}}
{"id": "WebQTest-1669", "question": "where is auburn university located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is auburn university located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.002378159901127219, -0.04521014168858528, -0.12762434780597687], "norm_scores": [0.35206902027130127, 0.3373076021671295, 0.3106233477592468]}}
{"id": "WebQTest-1670", "question": "who is tatyana ali parents", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is tatyana ali parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.012090330012142658, -0.016222545877099037, -0.1556433141231537], "norm_scores": [0.3493875563144684, 0.34794679284095764, 0.30266568064689636]}}
{"id": "WebQTest-1671", "question": "where did leif ericson grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did leif ericson grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0016845008358359337, -0.048814065754413605, -0.08603367954492569], "norm_scores": [0.3480592370033264, 0.3320358991622925, 0.3199048340320587]}}
{"id": "WebQTest-1672", "question": "what language do the maasai speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0028593626338988543, -0.04948494955897331, -0.06381696462631226], "norm_scores": [0.34538641571998596, 0.32965222001075745, 0.3249613642692566]}}
{"id": "WebQTest-1673", "question": "what political party was hitler in", "prediction": [["base.activism.activist.area_of_activism", "government.political_party.ideology"], ["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["base.activism.activist.area_of_activism", "government.political_party.ideology"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party was hitler in [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism <SEP> government.political_party.ideology </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.017930056899785995, -0.02050194889307022, -0.030528247356414795], "norm_scores": [0.33501821756362915, 0.33415767550468445, 0.3308240473270416]}}
{"id": "WebQTest-1674", "question": "who is the leader of north korea today", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the leader of north korea today [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.004287160467356443, -0.029572293162345886, -0.06732026487588882], "norm_scores": [0.34317746758461, 0.3346089720726013, 0.3222135603427887]}}
{"id": "WebQTest-1675", "question": "where do the appalachian mountains run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.007675096858292818, -0.020169544965028763, -0.09828635305166245], "norm_scores": [0.3447139263153076, 0.3404337167739868, 0.3148523271083832]}}
{"id": "WebQTest-1676", "question": "what money does spain use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does spain use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.006133012939244509, -0.026066243648529053, -0.13830524682998657], "norm_scores": [0.3500843644142151, 0.34317514300346375, 0.3067404627799988]}}
{"id": "WebQTest-1677", "question": "where was terry waite born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was terry waite born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.009699301794171333, -0.017129680141806602, -0.1418696641921997], "norm_scores": [0.34857913851737976, 0.3459986746311188, 0.3054221570491791]}}
{"id": "WebQTest-1678", "question": "what country did the battle of san juan hill take place", "prediction": [["location.location.events"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.combatant"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did the battle of san juan hill take place [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.combatant </PATH>"], "scores": [-0.006394737865775824, -0.027134550735354424, -0.05510612949728966], "norm_scores": [0.34107229113578796, 0.3340713679790497, 0.32485634088516235]}}
{"id": "WebQTest-1679", "question": "what kind of government does the united states have currently", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does the united states have currently [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.007578711491078138, -0.016705065965652466, -0.15724892914295197], "norm_scores": [0.3506425619125366, 0.34745705127716064, 0.30190038681030273]}}
{"id": "WebQTest-1680", "question": "when was the last time the orioles won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the orioles won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.009175769053399563, -0.013419589959084988, -0.09882145375013351], "norm_scores": [0.3436402380466461, 0.34218496084213257, 0.3141748011112213]}}
{"id": "WebQTest-1682", "question": "who did ben stiller date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["people.person.place_of_birth", "people.person.place_of_birth"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["people.person.languages", "people.person.languages"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben stiller date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.0153824333101511, -0.02998928911983967, -0.048171356320381165], "norm_scores": [0.3386108875274658, 0.3337008059024811, 0.3276882767677307]}}
{"id": "WebQTest-1683", "question": "what is the most common language in norway", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.015025039203464985, -0.02238555997610092, -0.032648906111717224], "norm_scores": [0.33611220121383667, 0.33364734053611755, 0.33024048805236816]}}
{"id": "WebQTest-1684", "question": "what influenced william shakespeare to start writing", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced william shakespeare to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.009741460904479027, -0.013445315882563591, -0.10537116974592209], "norm_scores": [0.3442218005657196, 0.3429492115974426, 0.3128289580345154]}}
{"id": "WebQTest-1685", "question": "where did benjamin franklin went to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.013227017596364021, -0.02313748560845852, -0.024050572887063026], "norm_scores": [0.3356410264968872, 0.3323310911655426, 0.3320278227329254]}}
{"id": "WebQTest-1686", "question": "who is the governor of arizona 2009", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of arizona 2009 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.020374609157443047, -0.02112063579261303, -0.02595321089029312], "norm_scores": [0.3340357840061188, 0.33378666639328003, 0.3321775197982788]}}
{"id": "WebQTest-1687", "question": "what movies has spike lee made", "prediction": [["film.film.directed_by"], ["film.film.produced_by"], ["film.producer.film"]], "ground_paths": [["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"], ["film.producer.film"], ["film.writer.film"], ["film.film.executive_produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has spike lee made [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.producer.film </PATH>"], "scores": [-0.027429990470409393, -0.0320788212120533, -0.03797387331724167], "norm_scores": [0.33502256870269775, 0.3334687352180481, 0.33150869607925415]}}
{"id": "WebQTest-1688", "question": "what did bernie mac died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bernie mac died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0015369936591014266, -0.04434752091765404, -0.1264110505580902], "norm_scores": [0.3520257771015167, 0.33727338910102844, 0.31070077419281006]}}
{"id": "WebQTest-1690", "question": "where is electrolux based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.founders", "people.deceased_person.place_of_death"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is electrolux based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.0047121597453951836, -0.0347261056303978, -0.054784856736660004], "norm_scores": [0.3422791063785553, 0.3321585953235626, 0.3255622982978821]}}
{"id": "WebQTest-1691", "question": "what is houston timezone", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is houston timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.556792974763084e-06, -0.19493909180164337, -0.1970841884613037], "norm_scores": [0.3782116770744324, 0.31122761964797974, 0.3105607032775879]}}
{"id": "WebQTest-1694", "question": "what was shakespeare 's wife called", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.children", "people.person.parents"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["book.book_subject.works", "book.book_subject.works"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was shakespeare 's wife called [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02572295255959034, -0.03671921044588089, -0.04061044752597809], "norm_scores": [0.3362150490283966, 0.3325381875038147, 0.3312467336654663]}}
{"id": "WebQTest-1695", "question": "when did truman become president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did truman become president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.00726517615839839, -0.02778916247189045, -0.061605922877788544], "norm_scores": [0.3416707217693329, 0.33472973108291626, 0.32359951734542847]}}
{"id": "WebQTest-1696", "question": "what did ferdinand magellan do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did ferdinand magellan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.001653030514717102, -0.06128673627972603, -0.06570164114236832], "norm_scores": [0.34721389412879944, 0.3271135687828064, 0.32567253708839417]}}
{"id": "WebQTest-1698", "question": "where do burmese cats originate from", "prediction": [["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.breed_of"]], "ground_paths": [["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do burmese cats originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.place_of_origin </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.breed_of </PATH>"], "scores": [-0.0039013379719108343, -0.027062488719820976, -0.06282628327608109], "norm_scores": [0.34247952699661255, 0.33463847637176514, 0.3228819966316223]}}
{"id": "WebQTest-1699", "question": "who played kirk", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played kirk [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.023314785212278366, -0.03163932263851166, -0.034645382314920425], "norm_scores": [0.33552053570747375, 0.33273908495903015, 0.3317403793334961]}}
{"id": "WebQTest-1700", "question": "where is mt st helens located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mt st helens located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>"], "scores": [-0.008046876639127731, -0.022117748856544495, -0.0915633887052536], "norm_scores": [0.34412702918052673, 0.33931878209114075, 0.3165541887283325]}}
{"id": "WebQTest-1701", "question": "what county is st paul minnesota in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul minnesota in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.014592451974749565, -0.018481824547052383, -0.03579038009047508], "norm_scores": [0.33611825108528137, 0.33481350541114807, 0.32906824350357056]}}
{"id": "WebQTest-1702", "question": "who was king henry viii son", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king henry viii son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.013698989525437355, -0.01588287390768528, -0.06370630115270615], "norm_scores": [0.33909329771995544, 0.33835357427597046, 0.3225531578063965]}}
{"id": "WebQTest-1705", "question": "where was franz ferdinand from", "prediction": [["people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["people.person.nationality"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was franz ferdinand from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0009435010724700987, -0.07243692874908447, -0.07734895497560501], "norm_scores": [0.3499632477760315, 0.32581663131713867, 0.32422012090682983]}}
{"id": "WebQTest-1706", "question": "what was benedict arnold most famous for", "prediction": [["base.honouriam.dishonoured_person.dishonor_bestowed"], ["people.person.profession"], ["base.honouriam.dishonoured_person.dishonor_bestowed", "base.honouri.dishonor.dishonor_bestowed"]], "ground_paths": [["base.honouriam.dishonoured_person.dishonor_bestowed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was benedict arnold most famous for [/INST]", "raw_output": {"paths": ["<PATH> base.honouriam.dishonoured_person.dishonor_bestowed </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> base.honouriam.dishonoured_person.dishonor_bestowed <SEP> base.honouri.dishonor.dishonor_bestowed </PATH>"], "scores": [-0.021590279415249825, -0.025129051879048347, -0.06013568118214607], "norm_scores": [0.3379913568496704, 0.33679741621017456, 0.32521122694015503]}}
{"id": "WebQTest-1707", "question": "who plays lex luthor on smallville", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.tv_program.languages", "people.person.languages"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays lex luthor on smallville [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.022167660295963287, -0.023832721635699272, -0.027572058141231537], "norm_scores": [0.33411890268325806, 0.33356302976608276, 0.3323180377483368]}}
{"id": "WebQTest-1708", "question": "what are dallas cowboys colors", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.sports_team.color"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are dallas cowboys colors [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.color </PATH>"], "scores": [-8.364530367543921e-05, -0.11494921892881393, -0.12230543047189713], "norm_scores": [0.3601737320423126, 0.32108983397483826, 0.3187364637851715]}}
{"id": "WebQTest-1710", "question": "what is the currency of the czech republic", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of the czech republic [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0022753479424864054, -0.03995584324002266, -0.12017949670553207], "norm_scores": [0.3506554961204529, 0.33768847584724426, 0.31165602803230286]}}
{"id": "WebQTest-1711", "question": "when was the last time the la kings won a playoff series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the la kings won a playoff series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.006765254773199558, -0.01741238869726658, -0.06317219883203506], "norm_scores": [0.34076613187789917, 0.3371572196483612, 0.32207661867141724]}}
{"id": "WebQTest-1712", "question": "where did spencer pratt go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did spencer pratt go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.016092831268906593, -0.025739146396517754, -0.030475951731204987], "norm_scores": [0.3360079526901245, 0.3327822983264923, 0.3312096893787384]}}
{"id": "WebQTest-1713", "question": "what disease did abraham lincoln had", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did abraham lincoln had [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0030701435171067715, -0.03651914373040199, -0.04709351062774658], "norm_scores": [0.3419930636882782, 0.3307429552078247, 0.3272639811038971]}}
{"id": "WebQTest-1714", "question": "who is vancouver canucks coach", "prediction": [["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["ice_hockey.hockey_coach.current_team"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is vancouver canucks coach [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.01218666136264801, -0.021046658977866173, -0.030430130660533905], "norm_scores": [0.33634912967681885, 0.333382248878479, 0.33026862144470215]}}
{"id": "WebQTest-1715", "question": "where did cs lewis wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did cs lewis wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.021157696843147278, -0.02861643396317959, -0.0361679345369339], "norm_scores": [0.33583295345306396, 0.3333373963832855, 0.3308296799659729]}}
{"id": "WebQTest-1716", "question": "what is carlos pena jr nationality", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["people.person.ethnicity"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is carlos pena jr nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.0011182291200384498, -0.06706386804580688, -0.08361240476369858], "norm_scores": [0.3500176966190338, 0.32768017053604126, 0.3223021626472473]}}
{"id": "WebQTest-1719", "question": "who was married to jane wyman", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was married to jane wyman [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.02029268629848957, -0.03939077630639076, -0.04353855922818184], "norm_scores": [0.33805420994758606, 0.33165931701660156, 0.3302864730358124]}}
{"id": "WebQTest-1721", "question": "what language does peru speak", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does peru speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010654926300048828, -0.030165711417794228, -0.039358459413051605], "norm_scores": [0.3387094736099243, 0.332165002822876, 0.3291254937648773]}}
{"id": "WebQTest-1723", "question": "where is english spoken in the world map", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken in the world map [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013036113232374191, -0.023687703534960747, -0.02846677415072918], "norm_scores": [0.33623698353767395, 0.3326745629310608, 0.33108848333358765]}}
{"id": "WebQTest-1724", "question": "when did arsenal won the league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["soccer.football_team.matches_played", "soccer.football_player_match_participation.season"]], "ground_paths": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did arsenal won the league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> soccer.football_team.matches_played <SEP> soccer.football_player_match_participation.season </PATH>"], "scores": [-0.010941260494291782, -0.013911126181483269, -0.08819649368524551], "norm_scores": [0.3421507775783539, 0.3411361575126648, 0.3167130947113037]}}
{"id": "WebQTest-1725", "question": "what timezone is phoenix az in right now", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is phoenix az in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.347077138954774e-06, -0.17728208005428314, -0.1853790283203125], "norm_scores": [0.3747636675834656, 0.31388381123542786, 0.31135255098342896]}}
{"id": "WebQTest-1726", "question": "where is south portland", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.location.contains"]], "ground_paths": [["location.hud_county_place.county"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is south portland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.007475271355360746, -0.03987852856516838, -0.0503089465200901], "norm_scores": [0.341741681098938, 0.3308456242084503, 0.3274127244949341]}}
{"id": "WebQTest-1727", "question": "what countries speak french as a first language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["base.rosetta.local_name.locale"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak french as a first language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011841533705592155, -0.024125827476382256, -0.027300164103507996], "norm_scores": [0.33642271161079407, 0.3323152959346771, 0.3312620520591736]}}
{"id": "WebQTest-1728", "question": "who was ptolemy and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was ptolemy and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.00176648807246238, -0.051014047116041183, -0.07126236706972122], "norm_scores": [0.346643328666687, 0.32998552918434143, 0.3233710825443268]}}
{"id": "WebQTest-1729", "question": "what was the name of henry viii first wife", "prediction": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["book.written_work.subjects", "theater.play.characters"], ["people.person.parents", "people.person.parents"], ["people.family_member.family", "people.family.members"], ["people.person.religion", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of henry viii first wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.034350257366895676, -0.035079099237918854, -0.03867271542549133], "norm_scores": [0.3338944911956787, 0.33365121483802795, 0.3324543237686157]}}
{"id": "WebQTest-1730", "question": "where does the parana river flow", "prediction": [["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the parana river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.019620317965745926, -0.022728197276592255, -0.02715892344713211], "norm_scores": [0.3345167934894562, 0.33347877860069275, 0.33200445771217346]}}
{"id": "WebQTest-1731", "question": "what is president nixon 's first name", "prediction": [["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is president nixon 's first name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.02341476082801819, -0.0351715013384819, -0.05343717709183693], "norm_scores": [0.3379822075366974, 0.334031879901886, 0.327985942363739]}}
{"id": "WebQTest-1732", "question": "where did theodor schwann get his education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann get his education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.016416035592556, -0.020214667543768883, -0.021187670528888702], "norm_scores": [0.3342862129211426, 0.33301880955696106, 0.3326949179172516]}}
{"id": "WebQTest-1733", "question": "who or what influenced frank lloyd wright", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced frank lloyd wright [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.009947629645466805, -0.013515785336494446, -0.09393571317195892], "norm_scores": [0.34294959902763367, 0.3417280614376068, 0.3153223395347595]}}
{"id": "WebQTest-1734", "question": "what year did houston rockets win their first championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did houston rockets win their first championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.006984140258282423, -0.017101716250181198, -0.12375786900520325], "norm_scores": [0.34725597500801086, 0.34376031160354614, 0.3089836835861206]}}
{"id": "WebQTest-1735", "question": "where do most of the people live in russia", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["location.location.partially_containedby"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do most of the people live in russia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.022038258612155914, -0.026352062821388245, -0.04163314402103424], "norm_scores": [0.3359886407852173, 0.3345423638820648, 0.32946905493736267]}}
{"id": "WebQTest-1736", "question": "where is holy roman empire located", "prediction": [["location.location.contains"], ["location.location.containedby"], ["fictional_universe.fictional_setting.languages", "language.human_language.main_country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is holy roman empire located [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.main_country </PATH>"], "scores": [-0.01832742989063263, -0.02078927494585514, -0.042930375784635544], "norm_scores": [0.33633360266685486, 0.3355066180229187, 0.32815974950790405]}}
{"id": "WebQTest-1737", "question": "what movies has liam hemsworth played in", "prediction": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has liam hemsworth played in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.026448048651218414, -0.030279798433184624, -0.03675324469804764], "norm_scores": [0.3349047899246216, 0.3336239755153656, 0.3314712345600128]}}
{"id": "WebQTest-1738", "question": "who plays nicholas newman on the young and the restless", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays nicholas newman on the young and the restless [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.021650642156600952, -0.02655302919447422, -0.027321793138980865], "norm_scores": [0.3345091938972473, 0.33287331461906433, 0.33261749148368835]}}
{"id": "WebQTest-1739", "question": "who is the voice of eric cartman on south park", "prediction": [["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.special_performance_type", "film.actor.film"], ["tv.regular_tv_appearance.special_performance_type", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of eric cartman on south park [/INST]", "raw_output": {"paths": ["<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.special_performance_type <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.01802620105445385, -0.028011854737997055, -0.030511751770973206], "norm_scores": [0.33583465218544006, 0.33249780535697937, 0.33166763186454773]}}
{"id": "WebQTest-1740", "question": "who is angelina jolies brother", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.place_of_birth"], ["people.person.children", "people.person.children"]], "ground_paths": [["film.producer.films_executive_produced", "film.producer.films_executive_produced"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.place_of_birth", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolies brother [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.021391376852989197, -0.03382045030593872, -0.03993614390492439], "norm_scores": [0.33678266406059265, 0.3326226770877838, 0.33059462904930115]}}
{"id": "WebQTest-1741", "question": "where is the headquarters of bank of america", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["common.image.appears_in_topic_gallery", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the headquarters of bank of america [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.005137257743626833, -0.036322299391031265, -0.05088250711560249], "norm_scores": [0.34192925691604614, 0.33143073320388794, 0.32663998007774353]}}
{"id": "WebQTest-1742", "question": "what type of government japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010510999709367752, -0.01427694596350193, -0.12928685545921326], "norm_scores": [0.34671083092689514, 0.3454075753688812, 0.30788156390190125]}}
{"id": "WebQTest-1744", "question": "what team does josh hamilton play for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does josh hamilton play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.013959149830043316, -0.01677221991121769, -0.029830459505319595], "norm_scores": [0.3354078531265259, 0.3344656229019165, 0.3301265239715576]}}
{"id": "WebQTest-1746", "question": "where is burundi country", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is burundi country [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.0032504405826330185, -0.05189087986946106, -0.06716881692409515], "norm_scores": [0.34594830870628357, 0.3295239210128784, 0.324527770280838]}}
{"id": "WebQTest-1748", "question": "through which countries of the sahel does the niger river flow", "prediction": [["location.location.partially_containedby"], ["geography.river.basin_countries"], ["location.location.partially_contains"]], "ground_paths": [["geography.river.basin_countries"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: through which countries of the sahel does the niger river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.017108917236328125, -0.018431581556797028, -0.03143806383013725], "norm_scores": [0.3350699543952942, 0.33462709188461304, 0.3303029537200928]}}
{"id": "WebQTest-1750", "question": "what city does paul ryan live in", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does paul ryan live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.020373916253447533, -0.02155897580087185, -0.03807637467980385], "norm_scores": [0.3354276418685913, 0.33503037691116333, 0.32954198122024536]}}
{"id": "WebQTest-1751", "question": "what city is acadia university in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is acadia university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.00684557156637311, -0.043814416974782944, -0.050102055072784424], "norm_scores": [0.34230491518974304, 0.32988137006759644, 0.32781368494033813]}}
{"id": "WebQTest-1752", "question": "what did f. scott fitzgerald do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did f. scott fitzgerald do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0005891710170544684, -0.07943473011255264, -0.08045312017202377], "norm_scores": [0.3511945605278015, 0.32456791400909424, 0.32423752546310425]}}
{"id": "WebQTest-1753", "question": "what countries does tomtom western europe include", "prediction": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["common.topic.notable_types"], ["location.location.containedby"]], "ground_paths": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["organization.organization.phone_number", "common.phone_number.service_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does tomtom western europe include [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.009108069352805614, -0.021740762516856194, -0.04462544247508049], "norm_scores": [0.3386899530887604, 0.334438294172287, 0.32687169313430786]}}
{"id": "WebQTest-1754", "question": "what is the national language of iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national language of iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.017644600942730904, -0.02192840538918972, -0.027887197211384773], "norm_scores": [0.33494833111763, 0.33351653814315796, 0.33153510093688965]}}
{"id": "WebQTest-1755", "question": "where did the second battle of ypres occur", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the second battle of ypres occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.009448007680475712, -0.0200367234647274, -0.06774290651082993], "norm_scores": [0.3409665524959564, 0.3373752236366272, 0.3216582238674164]}}
{"id": "WebQTest-1756", "question": "what languages do people speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do people speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013987119309604168, -0.023810308426618576, -0.02663290873169899], "norm_scores": [0.3358343243598938, 0.3325515389442444, 0.3316141664981842]}}
{"id": "WebQTest-1757", "question": "what did michael crabtree do", "prediction": [["american_football.football_player.games", "american_football.football_player_game_statistics.position_s"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.position"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.role"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"], ["american_football.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did michael crabtree do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.football_player_game_statistics.position_s </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.position </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.role </PATH>"], "scores": [-0.029643503949046135, -0.03211406245827675, -0.03416598588228226], "norm_scores": [0.33411067724227905, 0.33328625559806824, 0.3326030671596527]}}
{"id": "WebQTest-1758", "question": "where do they speak afrikaans", "prediction": [["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak afrikaans [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.004552451893687248, -0.04455875605344772, -0.04915563389658928], "norm_scores": [0.3427991271018982, 0.3293556869029999, 0.32784518599510193]}}
{"id": "WebQTest-1759", "question": "what 3 states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "ground_paths": [["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.containedby", "location.location.contains"], ["location.location.contains", "location.location.contains"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 3 states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.026709631085395813, -0.036260850727558136, -0.037580493837594986], "norm_scores": [0.33560624718666077, 0.3324160575866699, 0.3319776654243469]}}
{"id": "WebQTest-1760", "question": "who plays ziva david on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ziva david on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.021042611449956894, -0.030814819037914276, -0.032665710896253586], "norm_scores": [0.3357146978378296, 0.3324500322341919, 0.33183524012565613]}}
{"id": "WebQTest-1761", "question": "who ruled after king henry viii died", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.religion", "people.person.religion"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who ruled after king henry viii died [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>"], "scores": [-0.020864032208919525, -0.037468355149030685, -0.04539521411061287], "norm_scores": [0.3379177451133728, 0.3323531746864319, 0.3297290802001953]}}
{"id": "WebQTest-1763", "question": "who are the st louis cardinals coaches", "prediction": [["baseball.baseball_team.current_coaches"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"]], "ground_paths": [["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the st louis cardinals coaches [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> baseball.baseball_team.current_coaches <SEP> baseball.current_coaching_tenure.baseball_coach </PATH>"], "scores": [-0.016173025593161583, -0.03340963274240494, -0.04061521962285042], "norm_scores": [0.3379788398742676, 0.3322031795978546, 0.3298180103302002]}}
{"id": "WebQTest-1764", "question": "where is daud ibrahim", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is daud ibrahim [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.01616266556084156, -0.022145573049783707, -0.042351678013801575], "norm_scores": [0.3369061350822449, 0.334896445274353, 0.3281974196434021]}}
{"id": "WebQTest-1765", "question": "what region is turkey considered", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region is turkey considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.012538088485598564, -0.0277553703635931, -0.047811783850193024], "norm_scores": [0.3389556109905243, 0.3338366448879242, 0.3272077739238739]}}
{"id": "WebQTest-1766", "question": "what language do russians speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["people.ethnicity.included_in_group", "people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["people.ethnicity.geographic_distribution", "language.human_language.countries_spoken_in"], ["people.ethnicity.geographic_distribution", "location.country.languages_spoken"], ["language.human_language.main_country"], ["people.ethnicity.geographic_distribution", "language.human_language.main_country"], ["people.ethnicity.geographic_distribution", "location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do russians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01476266235113144, -0.020003890618681908, -0.028709206730127335], "norm_scores": [0.335466593503952, 0.33371293544769287, 0.3308204710483551]}}
{"id": "WebQTest-1767", "question": "where was martin luther king jr buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.00321555626578629, -0.03163081780076027, -0.08647190034389496], "norm_scores": [0.3457695245742798, 0.33608266711235046, 0.31814783811569214]}}
{"id": "WebQTest-1768", "question": "what are the official languages of argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.00819233525544405, -0.025198733434081078, -0.04731779173016548], "norm_scores": [0.33958539366722107, 0.33385908603668213, 0.3265555202960968]}}
{"id": "WebQTest-1769", "question": "where is the city wales", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the city wales [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.018458431586623192, -0.02109556272625923, -0.036644451320171356], "norm_scores": [0.33564427495002747, 0.33476030826568604, 0.3295953869819641]}}
{"id": "WebQTest-1770", "question": "what country does rafael nadal play for", "prediction": [["people.person.nationality"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does rafael nadal play for [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.017508307471871376, -0.0214172825217247, -0.02970084547996521], "norm_scores": [0.33512288331985474, 0.33381545543670654, 0.3310616910457611]}}
{"id": "WebQTest-1771", "question": "what is the islamic belief", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"], ["belief.belief.belief_of"]], "ground_paths": [["religion.belief.belief_of"], ["religion.religion.beliefs"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the islamic belief [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>", "<PATH> belief.belief.belief_of </PATH>"], "scores": [-0.008321001194417477, -0.018837716430425644, -0.11354531347751617], "norm_scores": [0.3460613489151001, 0.34244099259376526, 0.31149765849113464]}}
{"id": "WebQTest-1772", "question": "who is the prime minister of spain 2011", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of spain 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.008884184062480927, -0.019329626113176346, -0.029688352718949318], "norm_scores": [0.33681151270866394, 0.33331167697906494, 0.3298768103122711]}}
{"id": "WebQTest-1773", "question": "who was the son of king leonidas", "prediction": [["people.person.gender"], ["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the son of king leonidas [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.008963360451161861, -0.03037712164223194, -0.042164161801338196], "norm_scores": [0.33942508697509766, 0.3322339951992035, 0.32834091782569885]}}
{"id": "WebQTest-1774", "question": "who was judi dench married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was judi dench married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.021094299852848053, -0.02717079035937786, -0.046493809670209885], "norm_scores": [0.33682936429977417, 0.33478882908821106, 0.32838180661201477]}}
{"id": "WebQTest-1775", "question": "how many languages do they speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01647895947098732, -0.017514945939183235, -0.032987549901008606], "norm_scores": [0.3352788984775543, 0.33493176102638245, 0.3297893702983856]}}
{"id": "WebQTest-1776", "question": "where are the sahara desert located", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"]], "ground_paths": [["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the sahara desert located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0024144696071743965, -0.053731586784124374, -0.07141238451004028], "norm_scores": [0.3468240797519684, 0.3294750452041626, 0.32370084524154663]}}
{"id": "WebQTest-1777", "question": "what is rihanna mum called", "prediction": [["people.person.parents"], ["people.person.children"], ["music.artist.origin"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is rihanna mum called [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.012645653448998928, -0.015087426640093327, -0.1267315298318863], "norm_scores": [0.34605157375335693, 0.3452076315879822, 0.3087407648563385]}}
{"id": "WebQTest-1779", "question": "when did president theodore roosevelt take office", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.us_president.vice_president"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did president theodore roosevelt take office [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.027982311323285103, -0.034382034093141556, -0.0353286936879158], "norm_scores": [0.3348624110221863, 0.33272621035575867, 0.33241137862205505]}}
{"id": "WebQTest-1780", "question": "who does michael oher play for in 2010", "prediction": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael oher play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.016938364133238792, -0.018805496394634247, -0.0189749114215374], "norm_scores": [0.33376720547676086, 0.3331446051597595, 0.3330881595611572]}}
{"id": "WebQTest-1781", "question": "who or what influenced albert einstein", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced albert einstein [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.011270875111222267, -0.012472864240407944, -0.10255923867225647], "norm_scores": [0.34345927834510803, 0.3430466949939728, 0.3134939968585968]}}
{"id": "WebQTest-1782", "question": "where does egypt export to", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["location.location.contains", "location.location.contains"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.events", "time.event.locations"], ["location.location.events", "location.location.events"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does egypt export to [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.02632567472755909, -0.02904706820845604, -0.03223108872771263], "norm_scores": [0.3342922627925873, 0.3333837687969208, 0.33232393860816956]}}
{"id": "WebQTest-1784", "question": "where is the seat of the african union", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seat of the african union [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.009805254638195038, -0.0354975163936615, -0.03892315551638603], "norm_scores": [0.33945056796073914, 0.33084040880203247, 0.329708993434906]}}
{"id": "WebQTest-1785", "question": "what was kim richards in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was kim richards in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.017993858084082603, -0.03659377992153168, -0.03748369216918945], "norm_scores": [0.3375788629055023, 0.3313579559326172, 0.3310632109642029]}}
{"id": "WebQTest-1788", "question": "what artistic style did henri rousseau work in", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic style did henri rousseau work in [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0020445052068680525, -0.039630960673093796, -0.051484860479831696], "norm_scores": [0.34306809306144714, 0.33041271567344666, 0.3265191614627838]}}
{"id": "WebQTest-1789", "question": "what is john edwards indicted for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.election_campaign.candidate", "government.election_campaign.offense"]], "ground_paths": [["base.crime.acquitted_person.acquittal", "base.crime.acquittal.crime_type"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is john edwards indicted for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.offense </PATH>"], "scores": [-0.0222371444106102, -0.025787105783820152, -0.04297886788845062], "norm_scores": [0.336029589176178, 0.3348388075828552, 0.329131543636322]}}
{"id": "WebQTest-1790", "question": "where was huey newton killed", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death", "base.biblioness.bibs_location.city"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was huey newton killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> base.biblioness.bibs_location.city </PATH>"], "scores": [-0.0003542267659213394, -0.07682814449071884, -0.10525614023208618], "norm_scores": [0.35375815629959106, 0.3277134597301483, 0.318528413772583]}}
{"id": "WebQTest-1791", "question": "when was the printing press invented by gutenberg", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["book.book_edition.author_editor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the printing press invented by gutenberg [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.01497867051512003, -0.02667093090713024, -0.030055290088057518], "norm_scores": [0.336313933134079, 0.33240458369255066, 0.33128148317337036]}}
{"id": "WebQTest-1792", "question": "where is midlothian scotland", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.second_level_division_of"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is midlothian scotland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.0027527976781129837, -0.04851144924759865, -0.06237649917602539], "norm_scores": [0.3451380133628845, 0.32970085740089417, 0.3251610994338989]}}
{"id": "WebQTest-1793", "question": "where does marta play soccer", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does marta play soccer [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.0263899564743042, -0.03326904773712158, -0.038621753454208374], "norm_scores": [0.3354593515396118, 0.3331596255302429, 0.33138108253479004]}}
{"id": "WebQTest-1794", "question": "where is mitt romney 's family from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["people.person.parents"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mitt romney 's family from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.013394190929830074, -0.023594975471496582, -0.04477205500006676], "norm_scores": [0.3379565179347992, 0.3345266282558441, 0.3275167942047119]}}
{"id": "WebQTest-1795", "question": "what are john terry 's children called", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are john terry 's children called [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.009207822382450104, -0.018476411700248718, -0.1412639319896698], "norm_scores": [0.34878861904144287, 0.34557074308395386, 0.3056406080722809]}}
{"id": "WebQTest-1796", "question": "what city was michael jackson born in", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city was michael jackson born in [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.011680335737764835, -0.020202964544296265, -0.03928794711828232], "norm_scores": [0.33734965324401855, 0.3344867527484894, 0.32816362380981445]}}
{"id": "WebQTest-1797", "question": "who fought in the battle of vicksburg", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the battle of vicksburg [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.016347508877515793, -0.030913816764950752, -0.045420899987220764], "norm_scores": [0.33819380402565, 0.3333032727241516, 0.32850292325019836]}}
{"id": "WebQTest-1799", "question": "what kind of writing is nikolai gogol famous for", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.author.school_or_movement"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of writing is nikolai gogol famous for [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.school_or_movement </PATH>"], "scores": [-0.015624445863068104, -0.024050870910286903, -0.030066875740885735], "norm_scores": [0.3358781337738037, 0.33305975794792175, 0.33106210827827454]}}
{"id": "WebQTest-1800", "question": "what year was first world series", "prediction": [["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was first world series [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.011847538873553276, -0.01814383640885353, -0.05358105152845383], "norm_scores": [0.3386560082435608, 0.3365304172039032, 0.3248135447502136]}}
{"id": "WebQTest-1801", "question": "who plays simon birch", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["award.award_honor.honored_for", "award.award_winner.awards_won"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays simon birch [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.021567754447460175, -0.02822203002870083, -0.04274698719382286], "norm_scores": [0.3364272117614746, 0.33419597148895264, 0.32937684655189514]}}
{"id": "WebQTest-1802", "question": "where are the timezones in the usa", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the timezones in the usa [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-2.5018020096467808e-05, -0.12180602550506592, -0.15192057192325592], "norm_scores": [0.36437568068504333, 0.3225971460342407, 0.31302714347839355]}}
{"id": "WebQTest-1803", "question": "what does gm make", "prediction": [["automotive.company.make_s"], ["automotive.make.parent_company"], ["common.topic.subjects"]], "ground_paths": [["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does gm make [/INST]", "raw_output": {"paths": ["<PATH> automotive.company.make_s </PATH>", "<PATH> automotive.make.parent_company </PATH>", "<PATH> common.topic.subjects </PATH>"], "scores": [-0.012225990183651447, -0.018321797251701355, -0.07262570410966873], "norm_scores": [0.340679407119751, 0.3386090099811554, 0.32071158289909363]}}
{"id": "WebQTest-1804", "question": "what do people speak in uk", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01323179341852665, -0.02254413068294525, -0.036163050681352615], "norm_scores": [0.3369203507900238, 0.3337973952293396, 0.3292822539806366]}}
{"id": "WebQTest-1805", "question": "what do people in jamaica speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in jamaica speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.011255166493356228, -0.019195951521396637, -0.045076463371515274], "norm_scores": [0.33797088265419006, 0.33529773354530334, 0.326731413602829]}}
{"id": "WebQTest-1806", "question": "what position did lebron james play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["basketball.basketball_player.position_s"], ["sports.pro_athlete.teams", "sports.sports_team_roster.position"], ["basketball.basketball_position.players"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did lebron james play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.008288377895951271, -0.03350570797920227, -0.04642050340771675], "norm_scores": [0.3404042720794678, 0.33192750811576843, 0.32766827940940857]}}
{"id": "WebQTest-1807", "question": "what was wayne gretzky 's first team", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was wayne gretzky 's first team [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.008220517076551914, -0.020346013829112053, -0.03507509082555771], "norm_scores": [0.3376723527908325, 0.333602637052536, 0.3287249803543091]}}
{"id": "WebQTest-1808", "question": "who is the president of peru now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of peru now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.003997947089374065, -0.036657001823186874, -0.043301600962877274], "norm_scores": [0.34137535095214844, 0.33040642738342285, 0.3282183110713959]}}
{"id": "WebQTest-1809", "question": "what kind of guitar did jerry garcia play", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did jerry garcia play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.012790650129318237, -0.026479370892047882, -0.03317646682262421], "norm_scores": [0.33712881803512573, 0.3325453996658325, 0.33032575249671936]}}
{"id": "WebQTest-1810", "question": "what type of art leonardo da vinci do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art leonardo da vinci do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.00022211372561287135, -0.08933684229850769, -0.09952519088983536], "norm_scores": [0.3545836806297302, 0.32435208559036255, 0.3210642635822296]}}
{"id": "WebQTest-1811", "question": "who are the senators of new jersey now", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of new jersey now [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.017928991466760635, -0.018091682344675064, -0.021589983254671097], "norm_scores": [0.3337579667568207, 0.3337036669254303, 0.33253833651542664]}}
{"id": "WebQTest-1812", "question": "where was rihanna raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.019680727273225784, -0.02113998495042324, -0.02585740014910698], "norm_scores": [0.33418169617652893, 0.3336944282054901, 0.33212390542030334]}}
{"id": "WebQTest-1813", "question": "what team does messi play for 2011", "prediction": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does messi play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.025186214596033096, -0.02581079490482807, -0.038681138306856155], "norm_scores": [0.3348994255065918, 0.33469030261039734, 0.33041030168533325]}}
{"id": "WebQTest-1815", "question": "where does lil wayne go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does lil wayne go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.016177795827388763, -0.01650870591402054, -0.028241345658898354], "norm_scores": [0.3347080945968628, 0.3345973491668701, 0.3306945860385895]}}
{"id": "WebQTest-1817", "question": "where did jrr tolkien go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jrr tolkien go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.011665528640151024, -0.019401323050260544, -0.033597495406866074], "norm_scores": [0.33663222193717957, 0.3340381681919098, 0.32932960987091064]}}
{"id": "WebQTest-1818", "question": "what state is saint louis university in", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is saint louis university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.006039004307240248, -0.03395248204469681, -0.04044368118047714], "norm_scores": [0.34029197692871094, 0.3309245705604553, 0.32878345251083374]}}
{"id": "WebQTest-1819", "question": "what religion does canada follow", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical_subject.periodicals"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.monarch.kingdom", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does canada follow [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>"], "scores": [-0.012867878191173077, -0.02176249399781227, -0.02684120088815689], "norm_scores": [0.33587831258773804, 0.33290404081344604, 0.33121761679649353]}}
{"id": "WebQTest-1821", "question": "which countries have spanish as their main language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries have spanish as their main language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00882755871862173, -0.027700679376721382, -0.03212893381714821], "norm_scores": [0.3380351662635803, 0.3317151963710785, 0.3302495777606964]}}
{"id": "WebQTest-1822", "question": "who is in paul ryan 's family", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in paul ryan 's family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.02026015892624855, -0.02543617971241474, -0.03422490134835243], "norm_scores": [0.3354613184928894, 0.33372941613197327, 0.33080923557281494]}}
{"id": "WebQTest-1823", "question": "what government does iraq use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does iraq use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005956718698143959, -0.022477203980088234, -0.13046489655971527], "norm_scores": [0.34885188937187195, 0.3431359827518463, 0.30801212787628174]}}
{"id": "WebQTest-1824", "question": "what type of currency do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.003467182395979762, -0.03224243223667145, -0.13120485842227936], "norm_scores": [0.3506656587123871, 0.34071895480155945, 0.3086153268814087]}}
{"id": "WebQTest-1825", "question": "what did lincoln steffens wrote", "prediction": [["book.author.works_written"], ["book.written_work.author"], ["book.book_edition.author_editor"]], "ground_paths": [["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did lincoln steffens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.012576606124639511, -0.01619076356291771, -0.07289029657840729], "norm_scores": [0.3403836488723755, 0.3391556441783905, 0.320460706949234]}}
{"id": "WebQTest-1826", "question": "what schools did anders celsius attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did anders celsius attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.018492035567760468, -0.019395263865590096, -0.022706910967826843], "norm_scores": [0.33390194177627563, 0.3336005210876465, 0.3324975371360779]}}
{"id": "WebQTest-1827", "question": "what can do in new york", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.neighborhood.neighborhood_of"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what can do in new york [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013925785198807716, -0.016545215621590614, -0.03427320718765259], "norm_scores": [0.3358812928199768, 0.3350026309490204, 0.3291160464286804]}}
{"id": "WebQTest-1828", "question": "what currency does france accept", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does france accept [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.008269255049526691, -0.02166374959051609, -0.09617162495851517], "norm_scores": [0.3445252478122711, 0.33994126319885254, 0.3155335485935211]}}
{"id": "WebQTest-1829", "question": "what type of religion did massachusetts have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["religion.religion.states"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion did massachusetts have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> religion.religion.states </PATH>"], "scores": [-0.003440320258960128, -0.03852134943008423, -0.07956866919994354], "norm_scores": [0.3457546532154083, 0.33383551239967346, 0.320409893989563]}}
{"id": "WebQTest-1830", "question": "what language do they speak in colombia south america", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in colombia south america [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.008271953091025352, -0.018630068749189377, -0.057211168110370636], "norm_scores": [0.33991241455078125, 0.3364097476005554, 0.32367783784866333]}}
{"id": "WebQTest-1833", "question": "what kind of government does brazil has", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does brazil has [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.003675714135169983, -0.02926464192569256, -0.17210792005062103], "norm_scores": [0.3546445965766907, 0.3456847369670868, 0.29967060685157776]}}
{"id": "WebQTest-1835", "question": "what country did gregor mendel live in", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did gregor mendel live in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.016007743775844574, -0.016155729070305824, -0.04718479514122009], "norm_scores": [0.3367960453033447, 0.3367462158203125, 0.3264577388763428]}}
{"id": "WebQTest-1836", "question": "where laura ingalls wilder lived", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["people.person.nationality", "location.administrative_division.first_level_division_of"], ["influence.influence_node.influenced", "location.location.people_born_here"], ["people.deceased_person.place_of_death"], ["book.author.works_written", "fictional_universe.work_of_fiction.setting"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.parents", "people.deceased_person.place_of_death"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where laura ingalls wilder lived [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.025209922343492508, -0.02867705374956131, -0.02993636019527912], "norm_scores": [0.3342443108558655, 0.3330874443054199, 0.3326682150363922]}}
{"id": "WebQTest-1838", "question": "what sports are popular in germany", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["time.event.locations"]], "ground_paths": [["sports.sports_team_location.teams"], ["cricket.cricket_team.country"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports are popular in germany [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.010552867315709591, -0.01528686098754406, -0.13666467368602753], "norm_scores": [0.3476092517375946, 0.345967561006546, 0.30642321705818176]}}
{"id": "WebQTest-1839", "question": "what 's my timezone in oklahoma", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's my timezone in oklahoma [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.414386113348883e-06, -0.18913087248802185, -0.19149024784564972], "norm_scores": [0.37687191367149353, 0.3119316101074219, 0.311196506023407]}}
{"id": "WebQTest-1840", "question": "what do they speak in norway", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01717153564095497, -0.01870574802160263, -0.03282662481069565], "norm_scores": [0.3352404236793518, 0.334726482629776, 0.3300330936908722]}}
{"id": "WebQTest-1842", "question": "what currency does singapore use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does singapore use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.004023488145321608, -0.03187592700123787, -0.1376146376132965], "norm_scores": [0.3511877655982971, 0.3415412902832031, 0.30727097392082214]}}
{"id": "WebQTest-1843", "question": "what did the blackfoot indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken", "language.human_language.language_family", "language.human_language.language_family"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the blackfoot indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.language_family <SEP> language.human_language.language_family </PATH>"], "scores": [-0.0011555248638615012, -0.06746508926153183, -0.07614404708147049], "norm_scores": [0.34921136498451233, 0.32680636644363403, 0.32398226857185364]}}
{"id": "WebQTest-1844", "question": "who plays robin hood in prince of thieves", "prediction": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays robin hood in prince of thieves [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.012311091646552086, -0.03722997009754181, -0.04324775189161301], "norm_scores": [0.339567095041275, 0.33121001720428467, 0.3292228579521179]}}
{"id": "WebQTest-1845", "question": "what standard time is texas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is texas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-1.6951298675849102e-05, -0.13673529028892517, -0.18567875027656555], "norm_scores": [0.369990736246109, 0.3227117955684662, 0.3072974383831024]}}
{"id": "WebQTest-1847", "question": "where does kurdish people live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kurdish people live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.001174097415059805, -0.061354685574769974, -0.11716480553150177], "norm_scores": [0.3530976176261902, 0.3324747681617737, 0.31442761421203613]}}
{"id": "WebQTest-1848", "question": "what instrument does louis armstrong", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played", "music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument does louis armstrong [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>"], "scores": [-0.00025927135720849037, -0.07947651296854019, -0.12387730926275253], "norm_scores": [0.3561815917491913, 0.3290545344352722, 0.3147638440132141]}}
{"id": "WebQTest-1849", "question": "who does alyson stoner play in camp rock", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.film.starring", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does alyson stoner play in camp rock [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.024897068738937378, -0.0252528078854084, -0.02551104500889778], "norm_scores": [0.3334410786628723, 0.3333224952220917, 0.333236426115036]}}
{"id": "WebQTest-1850", "question": "what religion does madonna practice", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"], ["religion.religion.practices"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does madonna practice [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.practices </PATH>"], "scores": [-0.002648863010108471, -0.04042251035571098, -0.10305129736661911], "norm_scores": [0.34874749183654785, 0.33581972122192383, 0.3154328167438507]}}
{"id": "WebQTest-1851", "question": "what countries are located near egypt", "prediction": [["location.location.events", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_contains"], ["time.event.locations", "time.event.locations"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "location.location.events"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["location.location.partially_containedby", "location.location.contains"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.contains_major_portion_of", "location.location.containedby"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.containedby"], ["location.location.partially_contains", "location.location.partially_contains"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["government.form_of_government.countries", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["time.event.locations", "time.event.locations"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.contains", "location.location.contains"], ["location.location.contains", "location.location.containedby"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are located near egypt [/INST]", "raw_output": {"paths": ["<PATH> location.location.events <SEP> time.event.locations </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>"], "scores": [-0.04277374595403671, -0.05146802216768265, -0.052545033395290375], "norm_scores": [0.3353881537914276, 0.3324848413467407, 0.3321269750595093]}}
{"id": "WebQTest-1853", "question": "what made richard hammond famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what made richard hammond famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0017814980819821358, -0.048155527561903, -0.11879734694957733], "norm_scores": [0.3515858054161072, 0.3356536328792572, 0.312760591506958]}}
{"id": "WebQTest-1854", "question": "who originally voiced lois griffin", "prediction": [["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced lois griffin [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.027502357959747314, -0.028127212077379227, -0.033656686544418335], "norm_scores": [0.334086149930954, 0.33387747406959534, 0.33203640580177307]}}
{"id": "WebQTest-1855", "question": "what airport do you fly into in paris", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"], ["location.location.containedby"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into in paris [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0068148295395076275, -0.02493482269346714, -0.05945625528693199], "norm_scores": [0.34120801091194153, 0.3350810110569, 0.32371091842651367]}}
{"id": "WebQTest-1857", "question": "what rihanna new album called 2012", "prediction": [["music.album.artist"], ["music.artist.album"], ["music.recording.artist"]], "ground_paths": [["music.album.artist"], ["music.artist.album"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rihanna new album called 2012 [/INST]", "raw_output": {"paths": ["<PATH> music.album.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.009758329018950462, -0.021781740710139275, -0.05612143129110336], "norm_scores": [0.33981892466545105, 0.3357575833797455, 0.32442349195480347]}}
{"id": "WebQTest-1858", "question": "where does dave ramsey live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["location.location.people_born_here"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does dave ramsey live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.025496818125247955, -0.026417987421154976, -0.030910294502973557], "norm_scores": [0.3340369760990143, 0.33372944593429565, 0.33223357796669006]}}
{"id": "WebQTest-1860", "question": "what discovery did galileo make", "prediction": [["astronomy.astronomer.astronomical_objects_discovered"], ["law.invention.inventor"], ["symbols.namesake.named_after"]], "ground_paths": [["astronomy.astronomical_discovery.discoverer"], ["astronomy.astronomer.astronomical_objects_discovered"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what discovery did galileo make [/INST]", "raw_output": {"paths": ["<PATH> astronomy.astronomer.astronomical_objects_discovered </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0125895906239748, -0.03573931008577347, -0.039816465228796005], "norm_scores": [0.33895355463027954, 0.33119702339172363, 0.3298494517803192]}}
{"id": "WebQTest-1861", "question": "what is the government system of saudi arabia", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["location.location.contains", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of saudi arabia [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> location.location.contains <SEP> location.country.form_of_government </PATH>"], "scores": [-0.010758884251117706, -0.01274223905056715, -0.09607785195112228], "norm_scores": [0.34290754795074463, 0.34222814440727234, 0.3148643374443054]}}
{"id": "WebQTest-1864", "question": "who plays the role of tony dinozzo on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the role of tony dinozzo on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.017981315031647682, -0.030627714470028877, -0.03114810772240162], "norm_scores": [0.3362075984477997, 0.3319825232028961, 0.3318098485469818]}}
{"id": "WebQTest-1865", "question": "what did jesus do for living", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesus do for living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0010067548137158155, -0.07644875347614288, -0.07820802927017212], "norm_scores": [0.35050371289253235, 0.3250338137149811, 0.32446250319480896]}}
{"id": "WebQTest-1866", "question": "what is the state name of new york city", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state name of new york city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.011261869221925735, -0.050608452409505844, -0.05073486268520355], "norm_scores": [0.3421478271484375, 0.32894688844680786, 0.328905314207077]}}
{"id": "WebQTest-1868", "question": "who plays knox overstreet", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays knox overstreet [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.02715360000729561, -0.033492330461740494, -0.036914367228746414], "norm_scores": [0.3351242244243622, 0.33300668001174927, 0.33186906576156616]}}
{"id": "WebQTest-1870", "question": "what movies did david carradine play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.languages", "tv.tv_program.languages"], ["film.director.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["people.person.languages", "film.film.language"], ["film.film.edited_by"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did david carradine play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.01620553992688656, -0.03464721515774727, -0.03629211336374283], "norm_scores": [0.3376278281211853, 0.3314584493637085, 0.3309136927127838]}}
{"id": "WebQTest-1871", "question": "what is the stanley cup named after", "prediction": [["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team_owner.teams_owned"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the stanley cup named after [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team_owner.teams_owned </PATH>"], "scores": [-0.014763607643544674, -0.017041625455021858, -0.04750775173306465], "norm_scores": [0.3372100591659546, 0.336442768573761, 0.3263472020626068]}}
{"id": "WebQTest-1873", "question": "where is traditional chinese used", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is traditional chinese used [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0182182677090168, -0.025316860526800156, -0.028251489624381065], "norm_scores": [0.33523932099342346, 0.3328680396080017, 0.33189263939857483]}}
{"id": "WebQTest-1874", "question": "who does portugal trade with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"]], "ground_paths": [["military.military_combatant.belongs_to_combatant_group", "military.military_combatant.includes_allies"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.time_zones", "location.location.time_zones"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.person.nationality", "location.location.people_born_here"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["organization.organization.founders", "organization.organization.founders"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["book.written_work.subjects", "book.written_work.subjects"], ["government.form_of_government.countries", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does portugal trade with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.imports_and_exports.imported_from <SEP> location.statistical_region.places_imported_from </PATH>"], "scores": [-0.029048997908830643, -0.030834190547466278, -0.03781285509467125], "norm_scores": [0.3345051407814026, 0.3339084982872009, 0.33158639073371887]}}
{"id": "WebQTest-1875", "question": "what type of political system is brazil", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of political system is brazil [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.004714866168797016, -0.025335809215903282, -0.17195531725883484], "norm_scores": [0.35390883684158325, 0.34668564796447754, 0.2994055449962616]}}
{"id": "WebQTest-1876", "question": "what language did the ancient babylonians speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the ancient babylonians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.007751857861876488, -0.03009672462940216, -0.035875797271728516], "norm_scores": [0.33896341919898987, 0.33147335052490234, 0.3295632302761078]}}
{"id": "WebQTest-1877", "question": "what kind of painter was wassily kandinsky", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["book.author.school_or_movement"]], "ground_paths": [["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of painter was wassily kandinsky [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> book.author.school_or_movement </PATH>"], "scores": [-0.003342651529237628, -0.03777654841542244, -0.05913272872567177], "norm_scores": [0.3434195816516876, 0.3317956030368805, 0.3247848451137543]}}
{"id": "WebQTest-1878", "question": "what form of government does the united states use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>"], "scores": [-0.00705907866358757, -0.018199985846877098, -0.14852088689804077], "norm_scores": [0.35001638531684875, 0.34613850712776184, 0.3038451373577118]}}
{"id": "WebQTest-1879", "question": "what money does cuba use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does cuba use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.01211344450712204, -0.015247184783220291, -0.14395631849765778], "norm_scores": [0.3480258285999298, 0.3469369113445282, 0.3050372302532196]}}
{"id": "WebQTest-1880", "question": "who is my representative in illinois house", "prediction": [["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["government.governmental_body.members", "government.government_position_held.office_holder"], ["government.governmental_body.members", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my representative in illinois house [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.0225343257188797, -0.030443552881479263, -0.03094509430229664], "norm_scores": [0.3351491093635559, 0.33250880241394043, 0.3323420584201813]}}
{"id": "WebQTest-1881", "question": "what timezone is sweden gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0029865799006074667, -0.0325460247695446, -0.1337709128856659], "norm_scores": [0.35108909010887146, 0.3408629596233368, 0.3080480098724365]}}
{"id": "WebQTest-1882", "question": "what was franklin d roosevelt 's job before president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "base.aareas.schema.administrative_area.administrative_parent", "government.governmental_jurisdiction.government_positions"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "government.governmental_jurisdiction.government_positions"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.appointed_by", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin d roosevelt 's job before president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> base.aareas.schema.administrative_area.administrative_parent <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> government.governmental_jurisdiction.government_positions </PATH>"], "scores": [-0.019009051844477654, -0.025220248848199844, -0.027755532413721085], "norm_scores": [0.33499717712402344, 0.33292290568351746, 0.3320798873901367]}}
{"id": "WebQTest-1883", "question": "where did robert downey jr go to jail", "prediction": [["people.person.nationality", "location.location.containedby"], ["base.prison.imprisonment.prisoner", "base.prison.prison.prisoners"], ["people.person.languages", "location.country.official_language"]], "ground_paths": [["celebrities.rehab.celebrity", "celebrities.rehab.rehab_facility"], ["base.prison.imprisonment.prisoner", "base.prison.imprisonment.prison"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert downey jr go to jail [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> base.prison.imprisonment.prisoner <SEP> base.prison.prison.prisoners </PATH>", "<PATH> people.person.languages <SEP> location.country.official_language </PATH>"], "scores": [-0.006889660842716694, -0.027364855632185936, -0.04838944971561432], "norm_scores": [0.3402422070503235, 0.33334651589393616, 0.3264112174510956]}}
{"id": "WebQTest-1884", "question": "what degrees did barack obama get", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did barack obama get [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.010982096195220947, -0.0197589173913002, -0.02743624337017536], "norm_scores": [0.3361409902572632, 0.33320364356040955, 0.3306553065776825]}}
{"id": "WebQTest-1885", "question": "what does god shiva represent", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["common.topic.notable_types"]], "ground_paths": [["religion.deity.deity_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does god shiva represent [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.010790218599140644, -0.025620510801672935, -0.05322912707924843], "norm_scores": [0.33970528841018677, 0.33470451831817627, 0.32559019327163696]}}
{"id": "WebQTest-1886", "question": "what inventions were made by ben franklin", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions were made by ben franklin [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.007686679717153311, -0.020967505872249603, -0.07269860804080963], "norm_scores": [0.34201323986053467, 0.33750107884407043, 0.3204856514930725]}}
{"id": "WebQTest-1887", "question": "what is the name of the currency used in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["common.topic.notable_types"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.004797091241925955, -0.025733349844813347, -0.1622917801141739], "norm_scores": [0.352912575006485, 0.3456007242202759, 0.30148664116859436]}}
{"id": "WebQTest-1888", "question": "what year did the milwaukee brewers go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the milwaukee brewers go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.005433832760900259, -0.02187506854534149, -0.04776864871382713], "norm_scores": [0.3398768901824951, 0.3343345820903778, 0.32578855752944946]}}
{"id": "WebQTest-1889", "question": "who was vice president with bill clinton", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president with bill clinton [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.018383938819169998, -0.02197626419365406, -0.03727339580655098], "norm_scores": [0.3358294665813446, 0.3346252143383026, 0.32954534888267517]}}
{"id": "WebQTest-1890", "question": "what songs did franz liszt compose", "prediction": [["music.composition.composer"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"]], "ground_paths": [["music.composition.composer"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>"], "scores": [-0.0030995106790214777, -0.061431169509887695, -0.06150062009692192], "norm_scores": [0.3464272916316986, 0.3267976939678192, 0.3267750144004822]}}
{"id": "WebQTest-1891", "question": "where did the russian japanese war happen", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the russian japanese war happen [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.01133589819073677, -0.018421905115246773, -0.060984715819358826], "norm_scores": [0.33961594104766846, 0.33721789717674255, 0.3231661319732666]}}
{"id": "WebQTest-1892", "question": "what year did the la kings win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the la kings win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.00820627436041832, -0.016045160591602325, -0.11783872544765472], "norm_scores": [0.3462178111076355, 0.34351444244384766, 0.31026777625083923]}}
{"id": "WebQTest-1893", "question": "what kind of money does egypt have", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money does egypt have [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.0025844979099929333, -0.04179137945175171, -0.08460494130849838], "norm_scores": [0.3468843996524811, 0.33354729413986206, 0.31956833600997925]}}
{"id": "WebQTest-1894", "question": "what baseball team was jackie robinson on", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what baseball team was jackie robinson on [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014659332111477852, -0.01467068400233984, -0.031898461282253265], "norm_scores": [0.3352445363998413, 0.3352407217025757, 0.329514741897583]}}
{"id": "WebQTest-1895", "question": "what body of water does st lawrence river flow into", "prediction": [["geography.river.mouth"], ["geography.body_of_water.mouth"], ["location.location.containedby", "geography.lake.basin_countries"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what body of water does st lawrence river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> geography.body_of_water.mouth </PATH>", "<PATH> location.location.containedby <SEP> geography.lake.basin_countries </PATH>"], "scores": [-0.0027921986766159534, -0.051316630095243454, -0.0709904208779335], "norm_scores": [0.3464151918888092, 0.3300068974494934, 0.323577880859375]}}
{"id": "WebQTest-1896", "question": "what form of government does the united states have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>"], "scores": [-0.007239737547934055, -0.017851615324616432, -0.14712898433208466], "norm_scores": [0.3497850298881531, 0.3460927903652191, 0.3041221797466278]}}
{"id": "WebQTest-1897", "question": "what are the names of ariel 's six sisters", "prediction": [["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of ariel 's six sisters [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.016486620530486107, -0.021551428362727165, -0.027938341721892357], "norm_scores": [0.33516988158226013, 0.33347660303115845, 0.3313535153865814]}}
{"id": "WebQTest-1898", "question": "what voices does seth macfarlane play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what voices does seth macfarlane play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_universe.characters </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.019021136686205864, -0.02157467231154442, -0.024556783959269524], "norm_scores": [0.33423250913619995, 0.3333801031112671, 0.33238738775253296]}}
{"id": "WebQTest-1899", "question": "who was william mckinley married to", "prediction": [["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["people.person.parents", "people.person.parents"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"]], "ground_paths": [["people.person.children", "people.person.parents"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["people.person.profession", "common.topic.notable_types"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was william mckinley married to [/INST]", "raw_output": {"paths": ["<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_owner.pets_owned </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_ownership.owner </PATH>"], "scores": [-0.03768344223499298, -0.038184456527233124, -0.03846815600991249], "norm_scores": [0.3334762156009674, 0.3333091735839844, 0.3332146406173706]}}
{"id": "WebQTest-1900", "question": "who played darth vader in the first movie", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader in the first movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.019504748284816742, -0.01987551711499691, -0.024079609662294388], "norm_scores": [0.33388257026672363, 0.33375880122184753, 0.33235859870910645]}}
{"id": "WebQTest-1901", "question": "where did kevin james go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin james go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.013154578395187855, -0.01768380030989647, -0.026959437876939774], "norm_scores": [0.3353711664676666, 0.33385562896728516, 0.3307732343673706]}}
{"id": "WebQTest-1902", "question": "what year was hitler was born", "prediction": [["people.person.place_of_birth"], ["military.military_person.participated_in_conflicts"], ["base.activism.activist.area_of_activism", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was hitler was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> time.event.locations </PATH>"], "scores": [-0.011445146054029465, -0.037274427711963654, -0.043854594230651855], "norm_scores": [0.33983418345451355, 0.33116888999938965, 0.3289968967437744]}}
{"id": "WebQTest-1904", "question": "what 's there to see in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's there to see in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.015791023150086403, -0.017304999753832817, -0.023655928671360016], "norm_scores": [0.33437514305114746, 0.3338692784309387, 0.3317556083202362]}}
{"id": "WebQTest-1905", "question": "where was auschwitz concentration camp located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["time.event.locations"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was auschwitz concentration camp located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.0020046099089086056, -0.06489057838916779, -0.08821636438369751], "norm_scores": [0.3500848412513733, 0.32874736189842224, 0.32116782665252686]}}
{"id": "WebQTest-1907", "question": "what kind of currency does dominican republic use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does dominican republic use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-8.13682927400805e-05, -0.10546281933784485, -0.13166187703609467], "norm_scores": [0.3601410388946533, 0.3241201341152191, 0.3157387971878052]}}
{"id": "WebQTest-1909", "question": "what is spoken in czech republic", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is spoken in czech republic [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012289359234273434, -0.018311159685254097, -0.04420414939522743], "norm_scores": [0.3375430107116699, 0.3355165123939514, 0.32694047689437866]}}
{"id": "WebQTest-1910", "question": "where was shakespeare born at", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was shakespeare born at [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.012587560340762138, -0.014162176288664341, -0.08585438877344131], "norm_scores": [0.34155577421188354, 0.3410183787345886, 0.31742581725120544]}}
{"id": "WebQTest-1911", "question": "where did scott fitzgerald go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did scott fitzgerald go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.013868346810340881, -0.014949388802051544, -0.02631419338285923], "norm_scores": [0.33483442664146423, 0.33447265625, 0.33069294691085815]}}
{"id": "WebQTest-1912", "question": "what did joan crawford died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joan crawford died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0025291128549724817, -0.035639457404613495, -0.12057439982891083], "norm_scores": [0.35012930631637573, 0.3387262523174286, 0.3111444413661957]}}
{"id": "WebQTest-1913", "question": "where did rob kardashian go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rob kardashian go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.014017000794410706, -0.018471533432602882, -0.02741500549018383], "norm_scores": [0.3353176712989807, 0.33382728695869446, 0.33085504174232483]}}
{"id": "WebQTest-1914", "question": "what year was allen iverson mvp", "prediction": [["award.award_honor.award_winner", "award.award_honor.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["award.award_winner.awards", "award.award_honor.season"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["sports.sports_award.award_winner", "sports.sports_award.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was allen iverson mvp [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> award.award_winner.awards <SEP> award.award_honor.season </PATH>"], "scores": [-0.028204340487718582, -0.030146628618240356, -0.04248276352882385], "norm_scores": [0.3351338505744934, 0.3344835638999939, 0.33038264513015747]}}
{"id": "WebQTest-1915", "question": "where did harry s truman go to school", "prediction": [["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harry s truman go to school [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.010896657593548298, -0.026561522856354713, -0.0332605317234993], "norm_scores": [0.33757081627845764, 0.33232399821281433, 0.3301052153110504]}}
{"id": "WebQTest-1916", "question": "what county is utica ny in", "prediction": [["location.location.containedby"], ["location.hud_county_place.county"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is utica ny in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.021920308470726013, -0.02401113323867321, -0.02420595847070217], "norm_scores": [0.33381977677345276, 0.3331225514411926, 0.3330576717853546]}}
{"id": "WebQTest-1917", "question": "what did charles babbage discover", "prediction": [["law.invention.inventor"], ["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did charles babbage discover [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.003060748800635338, -0.05349324643611908, -0.06063605472445488], "norm_scores": [0.34543877840042114, 0.32844945788383484, 0.3261117935180664]}}
{"id": "WebQTest-1919", "question": "who plays king julian madagascar", "prediction": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.film", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays king julian madagascar [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.029604993760585785, -0.0350261852145195, -0.040319282561540604], "norm_scores": [0.3351278007030487, 0.333315908908844, 0.3315563201904297]}}
{"id": "WebQTest-1920", "question": "who is rick scott", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_holder"], ["government.politician.government_positions_held"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"], ["people.profession.people_with_this_profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is rick scott [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.politician.government_positions_held </PATH>"], "scores": [-0.0017714396817609668, -0.0576101616024971, -0.07852514088153839], "norm_scores": [0.34821251034736633, 0.329301655292511, 0.32248586416244507]}}
{"id": "WebQTest-1921", "question": "where did the crimean war take place", "prediction": [["time.event.locations"], ["location.location.events"], ["book.written_work.subjects"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the crimean war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.009482295252382755, -0.0199850182980299, -0.09317022562026978], "norm_scores": [0.34372878074645996, 0.3401376008987427, 0.31613361835479736]}}
{"id": "WebQTest-1922", "question": "what offices did theodore roosevelt hold", "prediction": [["business.employment_tenure.person", "business.employment_tenure.title"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["common.topic.notable_types", "type.type.expected_by"]], "ground_paths": [["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["government.government_position_held.appointed_by", "government.government_position_held.basic_title"], ["government.government_position_held.office_holder", "government.government_office_category.officeholders"], ["common.topic.notable_types", "type.type.expected_by"], ["business.employment_tenure.person", "business.employment_tenure.title"], ["government.government_position_held.office_holder", "government.government_position_held.basic_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what offices did theodore roosevelt hold [/INST]", "raw_output": {"paths": ["<PATH> business.employment_tenure.person <SEP> business.employment_tenure.title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.basic_title </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>"], "scores": [-0.016403665766119957, -0.018042005598545074, -0.027481839060783386], "norm_scores": [0.33474528789520264, 0.3341973125934601, 0.3310573697090149]}}
{"id": "WebQTest-1923", "question": "what movies has miley cyrus been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["music.artist.genre", "music.album.genre"]], "ground_paths": [["music.artist.genre", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["people.person.parents", "film.film.produced_by"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["people.person.children", "film.producer.film"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["people.person.children", "film.film.produced_by"], ["film.film_featured_song.performed_by", "film.film.featured_song"], ["music.recording_contribution.contributor", "music.recording_contribution.album"], ["people.person.children", "film.film.executive_produced_by"], ["music.concert_tour.artist", "music.album.supporting_tours"], ["people.person.languages", "film.film.language"], ["music.artist.genre", "music.album.genre"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["film.film.music"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.languages", "tv.tv_program.languages"], ["film.actor.film", "film.film.starring"], ["music.artist.concert_tours", "music.album.supporting_tours"], ["music.lyricist.lyrics_written", "film.film.featured_song"], ["music.artist.origin", "film.film.featured_film_locations"], ["music.artist.album", "media_common.adaptation.adapted_from"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has miley cyrus been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> music.artist.genre <SEP> music.album.genre </PATH>"], "scores": [-0.027756236493587494, -0.029186824336647987, -0.03748326748609543], "norm_scores": [0.33457231521606445, 0.33409401774406433, 0.3313336968421936]}}
{"id": "WebQTest-1925", "question": "who did james franco play in milk", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did james franco play in milk [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.019116463139653206, -0.02707803249359131, -0.032996468245983124], "norm_scores": [0.3357636034488678, 0.3331010043621063, 0.33113542199134827]}}
{"id": "WebQTest-1926", "question": "when did the libyan conflict start", "prediction": [["time.event.locations"], ["time.event.includes_event"], ["time.event.included_in_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the libyan conflict start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.included_in_event </PATH>"], "scores": [-0.010631666518747807, -0.025993330404162407, -0.03709030523896217], "norm_scores": [0.33799266815185547, 0.3328402042388916, 0.32916709780693054]}}
{"id": "WebQTest-1928", "question": "what wnba team is diana taurasi on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wnba team is diana taurasi on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.01914335787296295, -0.02531738206744194, -0.02884003147482872], "norm_scores": [0.3350987136363983, 0.3330361545085907, 0.3318650722503662]}}
{"id": "WebQTest-1929", "question": "what do see in paris", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do see in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012622724287211895, -0.016646521165966988, -0.04063265398144722], "norm_scores": [0.3368860185146332, 0.3355332016944885, 0.3275808095932007]}}
{"id": "WebQTest-1932", "question": "who made the laws in canada", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.government"], ["government.governmental_body.jurisdiction"]], "ground_paths": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_body.jurisdiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who made the laws in canada [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>", "<PATH> government.governmental_body.jurisdiction </PATH>"], "scores": [-0.015374161303043365, -0.03066328912973404, -0.047960758209228516], "norm_scores": [0.33866551518440247, 0.33352699875831604, 0.3278074264526367]}}
{"id": "WebQTest-1933", "question": "what year did adam morrison get drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did adam morrison get drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>"], "scores": [-0.019435467198491096, -0.022179415449500084, -0.023067042231559753], "norm_scores": [0.3340420722961426, 0.3331267535686493, 0.33283117413520813]}}
{"id": "WebQTest-1934", "question": "where michael jackson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where michael jackson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.005897837225347757, -0.03518041968345642, -0.04790540039539337], "norm_scores": [0.34129640460014343, 0.3314472734928131, 0.32725632190704346]}}
{"id": "WebQTest-1935", "question": "what college did maya moore go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did maya moore go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01291400846093893, -0.020988041535019875, -0.02477314881980419], "norm_scores": [0.3355514109134674, 0.3328530490398407, 0.3315955698490143]}}
{"id": "WebQTest-1936", "question": "when was the last time the dodgers won a championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.mascot.team"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dodgers won a championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.mascot.team </PATH>"], "scores": [-0.009757384657859802, -0.013610724359750748, -0.09955232590436935], "norm_scores": [0.3436104357242584, 0.34228891134262085, 0.3141006827354431]}}
{"id": "WebQTest-1937", "question": "who is captain james cook facts", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is captain james cook facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0017337179742753506, -0.06447235494852066, -0.07683578878641129], "norm_scores": [0.3488163948059082, 0.3276044726371765, 0.3235791027545929]}}
{"id": "WebQTest-1938", "question": "where did robin williams get married", "prediction": [["people.person.children", "people.person.place_of_birth"], ["people.marriage.spouse", "people.marriage.location_of_ceremony"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.spouse_s", "people.marriage.location_of_ceremony"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin williams get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.location_of_ceremony </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.023435182869434357, -0.026340259239077568, -0.028382429853081703], "norm_scores": [0.3342062532901764, 0.33323678374290466, 0.33255696296691895]}}
{"id": "WebQTest-1939", "question": "what form of government does canada", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does canada [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> form_of_government.countries </PATH>"], "scores": [-0.008332483470439911, -0.01732947677373886, -0.16856493055820465], "norm_scores": [0.3517424762248993, 0.34859204292297363, 0.2996654808521271]}}
{"id": "WebQTest-1940", "question": "who do islamic people follow", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["people.person.religion"]], "ground_paths": [["religion.deity.deity_of"], ["religion.religion.deities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people follow [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> people.person.religion </PATH>"], "scores": [-0.005642625968903303, -0.03155678138136864, -0.07297127693891525], "norm_scores": [0.3437245190143585, 0.3349316120147705, 0.32134392857551575]}}
{"id": "WebQTest-1941", "question": "what kind of government does peru", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does peru [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00973602943122387, -0.015381421893835068, -0.1425687074661255], "norm_scores": [0.3484342098236084, 0.34647271037101746, 0.30509305000305176]}}
{"id": "WebQTest-1942", "question": "who was in the israeli palestinian conflict", "prediction": [["time.event.people_involved"], ["people.person.nationality"], ["people.person.ethnicity"]], "ground_paths": [["time.event.people_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was in the israeli palestinian conflict [/INST]", "raw_output": {"paths": ["<PATH> time.event.people_involved </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.0005751698627136648, -0.07249345630407333, -0.11198820173740387], "norm_scores": [0.353960245847702, 0.32939785718917847, 0.3166419565677643]}}
{"id": "WebQTest-1943", "question": "who founded the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.spun_off_from"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.spun_off_from </PATH>"], "scores": [-0.0014247589278966188, -0.05193879455327988, -0.1062990203499794], "norm_scores": [0.3507322072982788, 0.33345532417297363, 0.31581249833106995]}}
{"id": "WebQTest-1944", "question": "where did diego velazquez die", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did diego velazquez die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.00024476987891830504, -0.10975998640060425, -0.11230678856372833], "norm_scores": [0.35838985443115234, 0.3212135434150696, 0.3203965127468109]}}
{"id": "WebQTest-1946", "question": "what is the country code for mexico", "prediction": [["location.country.internet_tld"], ["location.location.containedby"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.internet_tld"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the country code for mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.internet_tld </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016754552721977234, -0.03002631664276123, -0.03386921063065529], "norm_scores": [0.3367176949977875, 0.33227837085723877, 0.33100390434265137]}}
{"id": "WebQTest-1947", "question": "what currency do they use in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency do they use in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0044810413382947445, -0.029196538031101227, -0.1655995398759842], "norm_scores": [0.3537595570087433, 0.34512338042259216, 0.30111703276634216]}}
{"id": "WebQTest-1950", "question": "where is president kennedy buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is president kennedy buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.004929352551698685, -0.028889425098896027, -0.07538897544145584], "norm_scores": [0.3438446819782257, 0.3357040286064148, 0.3204513192176819]}}
{"id": "WebQTest-1952", "question": "where was the prophet mohammed from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the prophet mohammed from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.004416740033775568, -0.028846630826592445, -0.09478162974119186], "norm_scores": [0.34608495235443115, 0.33773258328437805, 0.3161824643611908]}}
{"id": "WebQTest-1954", "question": "what language do ukrainian people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do ukrainian people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.00900324061512947, -0.018198201432824135, -0.07249245047569275], "norm_scores": [0.34137484431266785, 0.3382502794265747, 0.32037490606307983]}}
{"id": "WebQTest-1955", "question": "who are the judges appointed in the supreme court", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"], ["book.written_work.subjects", "book.written_work.author"]], "ground_paths": [["book.written_work.subjects", "book.author.works_written"], ["law.court.jurisdiction", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the judges appointed in the supreme court [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.author.works_written </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>"], "scores": [-0.013444392010569572, -0.02184024080634117, -0.029649613425135612], "norm_scores": [0.3360706865787506, 0.3332608938217163, 0.33066847920417786]}}
{"id": "WebQTest-1956", "question": "which country in europe has the largest land area", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in europe has the largest land area [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.01403180230408907, -0.027660472318530083, -0.02876501902937889], "norm_scores": [0.33649203181266785, 0.3319372236728668, 0.3315707743167877]}}
{"id": "WebQTest-1957", "question": "when did kaley cuoco join charmed", "prediction": [["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.seasons"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kaley cuoco join charmed [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.seasons </PATH>"], "scores": [-0.025903796777129173, -0.026498477905988693, -0.02882044017314911], "norm_scores": [0.3337234556674957, 0.33352506160736084, 0.3327515125274658]}}
{"id": "WebQTest-1958", "question": "where is chris rock from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris rock from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.008108978159725666, -0.021819446235895157, -0.08019004762172699], "norm_scores": [0.3428369164466858, 0.33816853165626526, 0.3189944922924042]}}
{"id": "WebQTest-1960", "question": "when was the last time the toronto maple leafs win the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs win the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.00930536538362503, -0.011999369598925114, -0.07283850759267807], "norm_scores": [0.34062814712524414, 0.3397117257118225, 0.31966012716293335]}}
{"id": "WebQTest-1961", "question": "what kind of language do egyptian speak", "prediction": [["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.main_country"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do egyptian speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>"], "scores": [-0.03451431542634964, -0.03667887672781944, -0.04058372974395752], "norm_scores": [0.334248423576355, 0.33352571725845337, 0.33222585916519165]}}
{"id": "WebQTest-1962", "question": "what language do brazilians use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do brazilians use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.001413823221810162, -0.0611884705722332, -0.07208456099033356], "norm_scores": [0.3479779362678528, 0.3277871608734131, 0.32423490285873413]}}
{"id": "WebQTest-1963", "question": "what position did vince lombardi play", "prediction": [["american_football.football_player.position_s"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_coach.sports_coached"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_member.position_s"]], "ground_paths": [["american_football.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did vince lombardi play [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_member.position_s </PATH>"], "scores": [-0.01012453343719244, -0.02947762981057167, -0.0454120934009552], "norm_scores": [0.3394247889518738, 0.33291903138160706, 0.32765620946884155]}}
{"id": "WebQTest-1964", "question": "who played stanley kowalski in a streetcar named desire movie", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["theater.play.country_of_origin", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.performance.character", "film.actor.film"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["film.film.starring", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played stanley kowalski in a streetcar named desire movie [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.028054848313331604, -0.033522699028253555, -0.03691321238875389], "norm_scores": [0.3349267244338989, 0.3331003785133362, 0.3319728970527649]}}
{"id": "WebQTest-1965", "question": "who did the voice for lola bunny", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice for lola bunny [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.015480207279324532, -0.027530059218406677, -0.030993491411209106], "norm_scores": [0.3364025354385376, 0.3323732614517212, 0.33122411370277405]}}
{"id": "WebQTest-1966", "question": "what book did charles darwin wrote in 1859", "prediction": [["book.written_work.author"], ["book.written_work.subjects"], ["common.topic.notable_types"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["book.written_work.author"], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did charles darwin wrote in 1859 [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0015248963609337807, -0.06559861451387405, -0.06746560335159302], "norm_scores": [0.3479323387145996, 0.3263382017612457, 0.32572948932647705]}}
{"id": "WebQTest-1967", "question": "what language do serbs speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do serbs speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.008465172722935677, -0.031285833567380905, -0.04841403663158417], "norm_scores": [0.34033557772636414, 0.3326568603515625, 0.32700756192207336]}}
{"id": "WebQTest-1968", "question": "what was the cause of death for laci peterson", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for laci peterson [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0024712656158953905, -0.03420759364962578, -0.101507268846035], "norm_scores": [0.3478899598121643, 0.3370225727558136, 0.3150874376296997]}}
{"id": "WebQTest-1969", "question": "what team is kris humphries play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is kris humphries play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.020840534940361977, -0.020912950858473778, -0.021771183237433434], "norm_scores": [0.3334447741508484, 0.33342060446739197, 0.33313459157943726]}}
{"id": "WebQTest-1971", "question": "what movies gerard butler has been in", "prediction": [["film.performance.actor", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["film.film.produced_by"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.producer.film"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.film.produced_by", "film.film.sequel"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["music.recording.artist"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.producer.films_executive_produced"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies gerard butler has been in [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.02988787554204464, -0.03142160922288895, -0.032416217029094696], "norm_scores": [0.3337847888469696, 0.3332732617855072, 0.3329419493675232]}}
{"id": "WebQTest-1972", "question": "where did john franklin live", "prediction": [["people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john franklin live [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.02358122728765011, -0.03429431468248367, -0.03743671625852585], "norm_scores": [0.33606845140457153, 0.3324873447418213, 0.3314441740512848]}}
{"id": "WebQTest-1975", "question": "what was the last movie sean connery was in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.film.produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the last movie sean connery was in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.015807729214429855, -0.03129439428448677, -0.034579142928123474], "norm_scores": [0.3371502757072449, 0.33196917176246643, 0.3308805227279663]}}
{"id": "WebQTest-1976", "question": "what team did drogba play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["people.person.place_of_birth", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did drogba play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02622726745903492, -0.030861759558320045, -0.03791281208395958], "norm_scores": [0.3351477384567261, 0.3335981070995331, 0.33125415444374084]}}
{"id": "WebQTest-1977", "question": "where did crabtree go to college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did crabtree go to college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.016995500773191452, -0.017405709251761436, -0.028415895998477936], "norm_scores": [0.33464574813842773, 0.3345085084438324, 0.3308457136154175]}}
{"id": "WebQTest-1978", "question": "where does freddie ljungberg come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does freddie ljungberg come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004750910680741072, -0.03864094242453575, -0.03882628679275513], "norm_scores": [0.3409273028373718, 0.3295668959617615, 0.3295058012008667]}}
{"id": "WebQTest-1979", "question": "who is number 22 on miami dolphins", "prediction": [["american_football.football_player.position_s", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["american_football.football_player.position_s"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is number 22 on miami dolphins [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.player </PATH>", "<PATH> american_football.football_player.position_s </PATH>"], "scores": [-0.0205969400703907, -0.026154205203056335, -0.027217324823141098], "norm_scores": [0.334687739610672, 0.3328329622745514, 0.3324792981147766]}}
{"id": "WebQTest-1981", "question": "what state did john adams live in", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"]], "ground_paths": [["people.deceased_person.place_of_burial", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["tv.tv_program.country_of_origin", "base.locations.countries.states_provinces_within"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["people.person.place_of_birth", "location.location.containedby"], ["symbols.name_source.namesakes", "base.usnationalparks.us_national_park.state"], ["symbols.name_source.namesakes", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state did john adams live in [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>"], "scores": [-0.02495826967060566, -0.03375878930091858, -0.04233666509389877], "norm_scores": [0.3362463712692261, 0.3333002030849457, 0.33045342564582825]}}
{"id": "WebQTest-1982", "question": "what type of music did vivaldi compose", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did vivaldi compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0026682456955313683, -0.040173135697841644, -0.10867523401975632], "norm_scores": [0.349331796169281, 0.3364728093147278, 0.314195454120636]}}
{"id": "WebQTest-1985", "question": "what team does mike fisher play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does mike fisher play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.014779126271605492, -0.017140725627541542, -0.023598069325089455], "norm_scores": [0.3345755934715271, 0.33378639817237854, 0.331637978553772]}}
{"id": "WebQTest-1986", "question": "what films has daniel craig been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.film.executive_produced_by"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.actor.film", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has daniel craig been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.020561615005135536, -0.03431955724954605, -0.036407314240932465], "norm_scores": [0.33663058280944824, 0.3320309519767761, 0.331338495016098]}}
{"id": "WebQTest-1987", "question": "what materials did eduardo paolozzi use in his work", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.art_forms", "visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists", "visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.artworks", "visual_art.artwork.media"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what materials did eduardo paolozzi use in his work [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.art_forms <SEP> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists <SEP> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.0024827150627970695, -0.033899009227752686, -0.04618217423558235], "norm_scores": [0.3417268693447113, 0.33115795254707336, 0.32711517810821533]}}
{"id": "WebQTest-1988", "question": "what did joe biden study in college", "prediction": [["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.major_field_of_study"], ["common.topic.subjects", "book.written_work.school_or_movement"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joe biden study in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.015525488182902336, -0.016818569973111153, -0.028315547853708267], "norm_scores": [0.3348962962627411, 0.334463506937027, 0.3306402266025543]}}
{"id": "WebQTest-1990", "question": "who plays timon 's voice in the lion king", "prediction": [["film.film.starring", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["music.recording.releases", "music.featured_artist.recordings"], ["music.recording.releases", "music.artist.track"], ["film.film.featured_song", "music.recording.artist"], ["film.film.country", "people.person.nationality"], ["film.film.language", "people.person.languages"], ["film.film.starring", "film.actor.film"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays timon 's voice in the lion king [/INST]", "raw_output": {"paths": ["<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.030824119225144386, -0.03640671819448471, -0.0386013500392437], "norm_scores": [0.3348192870616913, 0.33295533061027527, 0.33222541213035583]}}
{"id": "WebQTest-1991", "question": "what part of the country is ohio in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.states_and_provences.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.states_and_provences.country </PATH>"], "scores": [-0.012065251357853413, -0.018380802124738693, -0.07715218514204025], "norm_scores": [0.3412165343761444, 0.33906838297843933, 0.31971511244773865]}}
{"id": "WebQTest-1992", "question": "which island is kailua on", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.island.island_group"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which island is kailua on [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>"], "scores": [-0.008763307705521584, -0.03268720582127571, -0.04471258446574211], "norm_scores": [0.3400146961212158, 0.3319767713546753, 0.3280085027217865]}}
{"id": "WebQTest-1993", "question": "what language do speak in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do speak in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0070969401858747005, -0.02833852916955948, -0.05022905021905899], "norm_scores": [0.34051045775413513, 0.33335375785827637, 0.3261357545852661]}}
{"id": "WebQTest-1994", "question": "what did whoopi goldberg won a grammy for", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_category.nominees"], ["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did whoopi goldberg won a grammy for [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.01662135124206543, -0.021751901134848595, -0.025211399421095848], "norm_scores": [0.3348592519760132, 0.3331456482410431, 0.3319950997829437]}}
{"id": "WebQTest-1996", "question": "who played maggie in himym", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.place_of_birth", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played maggie in himym [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.017510412260890007, -0.03573006018996239, -0.03648655116558075], "norm_scores": [0.337478905916214, 0.33138585090637207, 0.33113524317741394]}}
{"id": "WebQTest-1997", "question": "who does owen schmitt play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does owen schmitt play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.019592806696891785, -0.021671583876013756, -0.028116844594478607], "norm_scores": [0.33451130986213684, 0.33381664752960205, 0.3316720128059387]}}
{"id": "WebQTest-1998", "question": "what party was thomas jefferson affiliated with", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was thomas jefferson affiliated with [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.018711715936660767, -0.01901470124721527, -0.026529569178819656], "norm_scores": [0.33423468470573425, 0.33413344621658325, 0.3316318690776825]}}
{"id": "WebQTest-1999", "question": "what religions are popular in france", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.monarch.kingdom", "people.person.religion"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["periodicals.newspaper_circulation_area.newspapers", "book.periodical.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religions are popular in france [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>"], "scores": [-0.00958347786217928, -0.013020915910601616, -0.07303756475448608], "norm_scores": [0.340705543756485, 0.3395363986492157, 0.31975802779197693]}}
{"id": "WebQTest-2000", "question": "who played berger in sex and the city", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played berger in sex and the city [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.023332346230745316, -0.023385146632790565, -0.05300499498844147], "norm_scores": [0.3366197943687439, 0.3366020619869232, 0.3267781138420105]}}
{"id": "WebQTest-2001", "question": "who won the 2000 fa cup final", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "ground_paths": [["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_award.award", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event", "sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the 2000 fa cup final [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.01980830356478691, -0.022541344165802002, -0.0243894774466753], "norm_scores": [0.3341464102268219, 0.33323442935943604, 0.3326191306114197]}}
{"id": "WebQTest-2002", "question": "what are the names of michael jackson 's kids", "prediction": [["people.person.parents"], ["people.person.children"], ["music.composition.composer", "people.person.parents"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.composition.composer <SEP> people.person.parents </PATH>"], "scores": [-0.008138584904372692, -0.0193666722625494, -0.09401199966669083], "norm_scores": [0.3440510928630829, 0.3402096629142761, 0.3157392740249634]}}
{"id": "WebQTest-2003", "question": "what movies have robert pattinson in them", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["music.artist.track", "music.recording.releases"], ["music.recording.artist", "music.recording.releases"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.producer.films_executive_produced"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies have robert pattinson in them [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.028752248734235764, -0.028989823535084724, -0.031077180057764053], "norm_scores": [0.3336179852485657, 0.3335387706756592, 0.33284324407577515]}}
{"id": "WebQTest-2004", "question": "what the time zone in japan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in japan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>"], "scores": [-0.00046899032895453274, -0.06369427591562271, -0.10559406131505966], "norm_scores": [0.35224366188049316, 0.33066239953041077, 0.31709393858909607]}}
{"id": "WebQTest-2005", "question": "who plays bob kelso in scrubs", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays bob kelso in scrubs [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.015772750601172447, -0.027446461841464043, -0.03340350463986397], "norm_scores": [0.3365963399410248, 0.3326898515224457, 0.3307138979434967]}}
{"id": "WebQTest-2006", "question": "what race is vanessa carlton", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what race is vanessa carlton [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.00657736836001277, -0.04157315939664841, -0.05240821838378906], "norm_scores": [0.3423704206943512, 0.3305961489677429, 0.32703346014022827]}}
{"id": "WebQTest-2007", "question": "what do people in czech republic speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in czech republic speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012806516140699387, -0.013974006287753582, -0.06671398878097534], "norm_scores": [0.3394026458263397, 0.3390066623687744, 0.32159075140953064]}}
{"id": "WebQTest-2008", "question": "which countries speak arabic language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak arabic language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016856631264090538, -0.017313020303845406, -0.033058490604162216], "norm_scores": [0.3351799249649048, 0.33502694964408875, 0.32979315519332886]}}
{"id": "WebQTest-2009", "question": "where does tim cook work", "prediction": [["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["organization.leadership.person", "organization.organization.leadership"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim cook work [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.019651304930448532, -0.03491972014307976, -0.041329484432935715], "norm_scores": [0.33744990825653076, 0.3323367238044739, 0.330213338136673]}}
{"id": "WebQTest-2010", "question": "what degrees does romney have", "prediction": [["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.degree"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees does romney have [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>"], "scores": [-0.018284184858202934, -0.018480980768799782, -0.019516458734869957], "norm_scores": [0.33349210023880005, 0.33342647552490234, 0.3330813944339752]}}
{"id": "WebQTest-2011", "question": "where is bob marley buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is bob marley buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.0036452780477702618, -0.03193133324384689, -0.08361899107694626], "norm_scores": [0.34539321064949036, 0.3357602655887604, 0.31884652376174927]}}
{"id": "WebQTest-2012", "question": "what is london uk time zone", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is london uk time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.371113027445972e-05, -0.10327800363302231, -0.18901775777339935], "norm_scores": [0.36633747816085815, 0.33040547370910645, 0.3032570779323578]}}
{"id": "WebQTest-2013", "question": "where to stay in chicago tourist", "prediction": [["location.location.containedby"], ["travel.travel_destination.accommodation"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where to stay in chicago tourist [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.021108385175466537, -0.022286519408226013, -0.025188226252794266], "norm_scores": [0.3339175879955292, 0.3335244059562683, 0.3325580358505249]}}
{"id": "WebQTest-2014", "question": "what type of breast cancer did sheryl crow have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.parent_disease"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of breast cancer did sheryl crow have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.parent_disease </PATH>"], "scores": [-0.002750407438725233, -0.03431909903883934, -0.052411746233701706], "norm_scores": [0.3424099385738373, 0.33176934719085693, 0.3258207440376282]}}
{"id": "WebQTest-2015", "question": "what type of government does argentina have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does argentina have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.00884641706943512, -0.015618114732205868, -0.13651783764362335], "norm_scores": [0.3480205535888672, 0.34567180275917053, 0.30630767345428467]}}
{"id": "WebQTest-2016", "question": "where does fabio capello come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does fabio capello come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0056616440415382385, -0.033925656229257584, -0.048373814672231674], "norm_scores": [0.34125980734825134, 0.33174943923950195, 0.3269907236099243]}}
{"id": "WebQTest-2017", "question": "what party was abe lincoln part of", "prediction": [["organization.organization_founder.organizations_founded"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["influence.influence_node.influenced_by", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was abe lincoln part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.013751372694969177, -0.02041589468717575, -0.025864403694868088], "norm_scores": [0.335422158241272, 0.33319416642189026, 0.33138367533683777]}}
{"id": "WebQTest-2018", "question": "what did jack london do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jack london do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0012849445920437574, -0.055622149258852005, -0.06704340875148773], "norm_scores": [0.3468044102191925, 0.3284628689289093, 0.3247327208518982]}}
{"id": "WebQTest-2019", "question": "who did morgan freeman marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did morgan freeman marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.018895471468567848, -0.035966917872428894, -0.04281608387827873], "norm_scores": [0.33790215849876404, 0.3321826457977295, 0.32991522550582886]}}
{"id": "WebQTest-2020", "question": "what are the mountains in northern italy called", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the mountains in northern italy called [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.013010778464376926, -0.02883470430970192, -0.03366389498114586], "norm_scores": [0.3373979330062866, 0.3321009576320648, 0.33050107955932617]}}
{"id": "WebQTest-2021", "question": "what influenced john steinbeck to start writing", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced john steinbeck to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.009609841741621494, -0.013463395647704601, -0.10399509966373444], "norm_scores": [0.34410545229911804, 0.3427819609642029, 0.3131125867366791]}}
{"id": "WebQTest-2022", "question": "where is dwayne johnson ethnicity", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality"]], "ground_paths": [["people.person.ethnicity"], ["people.ethnicity.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwayne johnson ethnicity [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.01628439500927925, -0.016351517289876938, -0.04604576155543327], "norm_scores": [0.336631178855896, 0.3366085886955261, 0.3267602026462555]}}
{"id": "WebQTest-2023", "question": "where are the nato headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.founders", "base.biblioness.bibs_location.country"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nato headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.007353409193456173, -0.04740400239825249, -0.05377671495079994], "norm_scores": [0.3430086374282837, 0.3295423984527588, 0.3274489939212799]}}
{"id": "WebQTest-2024", "question": "in which state was the battle of antietam fought", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which state was the battle of antietam fought [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0013137349160388112, -0.051083724945783615, -0.11563478410243988], "norm_scores": [0.3516891896724701, 0.3346140384674072, 0.3136967420578003]}}
{"id": "WebQTest-2025", "question": "which ocean does the zambezi river flow into", "prediction": [["geography.river.mouth"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which ocean does the zambezi river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.002415176946669817, -0.05087854713201523, -0.06775972992181778], "norm_scores": [0.3460882008075714, 0.3297155201435089, 0.3241962492465973]}}
{"id": "WebQTest-2027", "question": "what team did david beckham play for before la galaxy", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player.loans", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for before la galaxy [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.024323269724845886, -0.03093300573527813, -0.03194354474544525], "norm_scores": [0.33491629362106323, 0.3327098488807678, 0.33237382769584656]}}
{"id": "WebQTest-2028", "question": "who is the current leader of france 2010", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["common.topic.image", "common.topic.image"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current leader of france 2010 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> common.topic.image <SEP> common.topic.image </PATH>"], "scores": [-0.00044837346649728715, -0.07215052098035812, -0.10692635178565979], "norm_scores": [0.353381484746933, 0.3289303779602051, 0.31768813729286194]}}
{"id": "WebQTest-2029", "question": "where was the palace of knossos located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the palace of knossos located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0009235464385710657, -0.07599812000989914, -0.09960782527923584], "norm_scores": [0.3528951406478882, 0.327371746301651, 0.3197331428527832]}}
{"id": "WebQTest-2030", "question": "where is roswell area 51", "prediction": [["aviation.airport.serves"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is roswell area 51 [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00444302661344409, -0.031444985419511795, -0.08015976846218109], "norm_scores": [0.3447754979133606, 0.3355904519557953, 0.3196340501308441]}}
{"id": "WebQTest-2031", "question": "when was father chris riley born", "prediction": [["people.person.place_of_birth"], ["people.person.children"], ["people.person.parents"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was father chris riley born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.0014398910570889711, -0.06650383025407791, -0.08940081298351288], "norm_scores": [0.35053229331970215, 0.32845139503479004, 0.3210163116455078]}}
